seed: 71
cuda: -1 # use_gpu
env:
  env_type: rmdp
  env_name: MRPOWalker2dRandomNormal-v0

  num_eval_tasks: 100
  worst_percentile: 0.10

train:
  # 3000*1000=3M steps, cost 8h (train) + 12h (test)
  num_iters: 3000 # number meta-training iterates
  num_init_rollouts_pool: 10 # before training
  num_rollouts_per_iter: 1
  buffer_size: 1e6 # we should full buffer size as VRM

  num_updates_per_iter: 0.2 # equiv to 0.2*1000 = 200 steps, now we fixed it
  batch_size: 64 # to tune based on sampled_seq_len

eval:
  eval_stochastic: false # also eval stochastic policy
  log_interval: 50 # 100 # num of iters
  save_interval: 50 # -1
  log_tensorboard: false

policy:
  arch: mlp
  algo: sac # [td3, sac]

  dqn_layers: [256, 256]
  policy_layers: [256, 256]
  lr: 0.0003
  gamma: 0.99
  tau: 0.005

  # sac alpha
  entropy_alpha: 0.01 # tend to be det policy...
  automatic_entropy_tuning: true
  alpha_lr: 0.0003
