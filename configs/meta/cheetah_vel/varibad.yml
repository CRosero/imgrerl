seed: 7
cuda: 1 # use_gpu, mem ~ 10G...
env:
  env_type: meta
  env_name: HalfCheetahVel-v0
  max_rollouts_per_task: 2 # k=2, H=200, H^+ =400

  num_tasks: 120
  num_train_tasks: 100
  num_eval_tasks: 20

train:
  # sample complexity: BAMDP horizon * (num_init_rollouts_pool * num_train_tasks
  #  + num_iters * num_tasks_sample * num_rollouts_per_iter)
  # 0.5k iters -> 5M steps: SAC 1M steps, VAE 10000 steps
  num_iters: 500 # number meta-training iterates
  num_init_rollouts_pool: 500 # before training
  num_rollouts_per_iter: 25

  rl_updates_per_iter: 2000 
  vae_updates_per_iter: 20
  policy_batch_size: 4096 # 16*256
  vae_batch_num_rollouts: 32

  log_interval: 4 # 5 num of iters
  save_interval: 100 # -1
  log_tensorboard: false

policy:
  buffer_size: 1e6
  # bamdp related
  sample_embeddings: false # (otherwise: pass mean) obs_dim + 2*task_dim
  switch_to_belief_reward: null # when to switch from R to R+; None is to not switch

  policy: sac
  dqn_layers: [128, 128, 128]
  policy_layers: [128, 128, 128]
  lr: 0.0003
  # sac alpha
  entropy_alpha: 0.2 # not used
  automatic_entropy_tuning: true
  alpha_lr: 0.0003

  gamma: 0.99
  tau: 0.005

vae:
  buffer_size: 1e5 # much smaller, 250 epsiodes
  task_embedding_size: 5 # dim of latent space
  
  optim:
    vae_lr: 0.0003
    rew_loss_coeff: 1.0
    state_loss_coeff: 1.0 # (vs reward loss)
    kl_weight: 1.0
    kl_to_gauss_prior: false
    train_by_batch: false # by split, otherwise out of mem

  # encoder
  encoder:
    aggregator_hidden_size: 128
    layers_before_aggregator: []
    layers_after_aggregator: []
    action_embedding_size: 16
    state_embedding_size: 32
    reward_embedding_size: 16

  decoder:
    disable_stochasticity_in_latent: false
    # decoder: reward function r(s,a,s',m)
    decode_reward: true
    reward_decoder_layers: [64, 32]
    rew_pred_type: deterministic # gaussian, deterministic
    input_prev_state: True
    input_action: True
    # decoder: state transition p(s'|s,a,m)
    decode_state: false # not used
