Hostname: uc2n518.localdomain
Found slurm config: SLURM
Seeting default slurm config
/home/kit/stud/uprnr/imgrerl/test_results/fixed_test-wr-sot-k1m1-t-seed2/fixed_test-wr-sot-k1m1-t-seed2/fixed_test-wr-sot-k1m1-t-seed2__env.ewalker-run/log/rep_00
[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
True
params: 
 {'env': {'env': 'walker-run'}} 

additionalVars: 
 {'seed': 2, 'agent': {'image_augmentation_K': 1, 'image_augmentation_M': 1, 'image_augmentation_type': <AugmentationType.SAME_OVER_TIME: 2>, 'image_augmentation_actor_critic_same_aug': True}}
conf_dict: 
 --------Config-------- 
seed: 2
cuda_id: 0
Subconfig: env
	env: walker-run
	obs_type: image
Subconfig: agent
	algo_name: sac
	encoder: lstm
	action_embedding_size: 32
	observ_embedding_size: 0
	reward_embedding_size: 32
	rnn_hidden_size: 512
	dqn_layers: [512, 512, 512]
	policy_layers: [512, 512, 512]
	lr: 0.0003
	gamma: 0.99
	tau: 0.005
	entropy_alpha: 1.0
	image_augmentation_type: AugmentationType.SAME_OVER_TIME
	image_augmentation_K: 1
	image_augmentation_M: 1
	image_augmentation_actor_critic_same_aug: True
Subconfig: rl
	sampled_seq_len: 64
	buffer_size: 1000000.0
	batch_size: 32
	rollouts_per_iter: 1
	num_updates_per_iter: 100
	num_init_rollouts: 5
Subconfig: eval
	interval: 20
	num_rollouts: 20
---------------------- 
 
Init feature extractor:  6 ;  32 ;  <function relu at 0x1543acc1e7a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x1543acc1e7a0>
Init feature extractor:  6 ;  164 ;  <function relu at 0x1543acc1e7a0>
Critic: 
Critic_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (current_shortcut_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=164, bias=True)
  )
  (qf1): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
  (qf2): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
)
Init feature extractor:  6 ;  32 ;  <function relu at 0x1543acc1e7a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x1543acc1e7a0>
Actor: 
Actor_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (policy): TanhGaussianPolicy(
    (fc0): Linear(in_features=612, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=6, bias=True)
    (last_fc_log_std): Linear(in_features=512, out_features=6, bias=True)
  )
)
buffer RAM usage: 11.48 GB
Collecting train stats
[CW] ---- Iteration:     0 ----
[CW] collect: return: 28.73058, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.70863, qf2_loss: 1.69716, policy_loss: -7.81586, policy_entropy: 4.09692, alpha: 0.98504, time: 49.51728
[CW] eval: return: 25.32887, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     1 ----
[CW] collect: return: 23.39466, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.08447, qf2_loss: 0.08454, policy_loss: -8.51443, policy_entropy: 4.10062, alpha: 0.95626, time: 49.28052
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     2 ----
[CW] collect: return: 23.49103, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.07739, qf2_loss: 0.07725, policy_loss: -9.21424, policy_entropy: 4.10223, alpha: 0.92871, time: 49.61516
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     3 ----
[CW] collect: return: 24.35848, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.06786, qf2_loss: 0.06768, policy_loss: -10.14155, policy_entropy: 4.09974, alpha: 0.90231, time: 49.48533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     4 ----
[CW] collect: return: 21.24694, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06366, qf2_loss: 0.06361, policy_loss: -11.16623, policy_entropy: 4.10154, alpha: 0.87698, time: 49.26616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     5 ----
[CW] collect: return: 21.52894, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.05953, qf2_loss: 0.05956, policy_loss: -12.21930, policy_entropy: 4.10189, alpha: 0.85267, time: 49.26446
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     6 ----
[CW] collect: return: 24.87381, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.06491, qf2_loss: 0.06453, policy_loss: -13.30895, policy_entropy: 4.10197, alpha: 0.82930, time: 49.81068
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     7 ----
[CW] collect: return: 24.86133, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.07951, qf2_loss: 0.07903, policy_loss: -14.46732, policy_entropy: 4.10154, alpha: 0.80682, time: 49.77880
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     8 ----
[CW] collect: return: 23.48260, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.07814, qf2_loss: 0.07858, policy_loss: -15.67088, policy_entropy: 4.10195, alpha: 0.78519, time: 49.78960
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     9 ----
[CW] collect: return: 25.02567, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.06660, qf2_loss: 0.06699, policy_loss: -16.87164, policy_entropy: 4.10161, alpha: 0.76435, time: 49.87233
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    10 ----
[CW] collect: return: 25.47192, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.08888, qf2_loss: 0.08996, policy_loss: -18.04884, policy_entropy: 4.10200, alpha: 0.74426, time: 50.07424
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    11 ----
[CW] collect: return: 26.01434, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.06792, qf2_loss: 0.06852, policy_loss: -19.22622, policy_entropy: 4.10134, alpha: 0.72488, time: 50.12306
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    12 ----
[CW] collect: return: 25.67501, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.08097, qf2_loss: 0.08190, policy_loss: -20.36269, policy_entropy: 4.09998, alpha: 0.70616, time: 49.94895
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    13 ----
[CW] collect: return: 30.54361, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.07721, qf2_loss: 0.07814, policy_loss: -21.48862, policy_entropy: 4.10095, alpha: 0.68809, time: 49.84587
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    14 ----
[CW] collect: return: 29.59486, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.07533, qf2_loss: 0.07623, policy_loss: -22.57365, policy_entropy: 4.10210, alpha: 0.67062, time: 49.97716
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    15 ----
[CW] collect: return: 25.82799, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.07643, qf2_loss: 0.07732, policy_loss: -23.64054, policy_entropy: 4.10188, alpha: 0.65372, time: 50.28417
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    16 ----
[CW] collect: return: 25.38449, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.07874, qf2_loss: 0.07969, policy_loss: -24.67217, policy_entropy: 4.10191, alpha: 0.63736, time: 50.30292
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    17 ----
[CW] collect: return: 28.38306, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.07752, qf2_loss: 0.07843, policy_loss: -25.67869, policy_entropy: 4.10306, alpha: 0.62152, time: 49.87560
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    18 ----
[CW] collect: return: 24.49450, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.07328, qf2_loss: 0.07420, policy_loss: -26.65861, policy_entropy: 4.10132, alpha: 0.60618, time: 49.71884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    19 ----
[CW] collect: return: 23.60928, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.09157, qf2_loss: 0.09277, policy_loss: -27.61063, policy_entropy: 4.10119, alpha: 0.59131, time: 49.97335
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    20 ----
[CW] collect: return: 24.68913, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.06272, qf2_loss: 0.06336, policy_loss: -28.53233, policy_entropy: 4.10201, alpha: 0.57689, time: 50.22702
[CW] eval: return: 24.64601, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    21 ----
[CW] collect: return: 27.78276, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.08649, qf2_loss: 0.08761, policy_loss: -29.43930, policy_entropy: 4.10162, alpha: 0.56290, time: 50.28625
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    22 ----
[CW] collect: return: 25.90587, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.06344, qf2_loss: 0.06404, policy_loss: -30.30484, policy_entropy: 4.10192, alpha: 0.54933, time: 49.96769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    23 ----
[CW] collect: return: 24.60239, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.08049, qf2_loss: 0.08143, policy_loss: -31.15859, policy_entropy: 4.10056, alpha: 0.53614, time: 50.69352
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    24 ----
[CW] collect: return: 25.45005, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.07307, qf2_loss: 0.07395, policy_loss: -31.97736, policy_entropy: 4.10105, alpha: 0.52334, time: 50.82895
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    25 ----
[CW] collect: return: 32.88102, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.07923, qf2_loss: 0.08024, policy_loss: -32.77692, policy_entropy: 4.10064, alpha: 0.51090, time: 50.43247
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    26 ----
[CW] collect: return: 21.49829, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.08079, qf2_loss: 0.08166, policy_loss: -33.55166, policy_entropy: 4.10084, alpha: 0.49881, time: 50.68472
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    27 ----
[CW] collect: return: 22.54592, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.06300, qf2_loss: 0.06366, policy_loss: -34.30703, policy_entropy: 4.10119, alpha: 0.48705, time: 50.39552
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    28 ----
[CW] collect: return: 24.73301, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.08767, qf2_loss: 0.08854, policy_loss: -35.02925, policy_entropy: 4.10084, alpha: 0.47561, time: 50.27694
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    29 ----
[CW] collect: return: 23.07901, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.06810, qf2_loss: 0.06874, policy_loss: -35.73606, policy_entropy: 4.10135, alpha: 0.46448, time: 50.38606
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    30 ----
[CW] collect: return: 26.18739, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.06684, qf2_loss: 0.06750, policy_loss: -36.41109, policy_entropy: 4.10054, alpha: 0.45365, time: 50.18132
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    31 ----
[CW] collect: return: 20.96128, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.06917, qf2_loss: 0.06979, policy_loss: -37.07958, policy_entropy: 4.10132, alpha: 0.44310, time: 50.77903
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    32 ----
[CW] collect: return: 25.12264, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.09266, qf2_loss: 0.09351, policy_loss: -37.71149, policy_entropy: 4.10249, alpha: 0.43284, time: 50.69125
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    33 ----
[CW] collect: return: 28.24315, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.06531, qf2_loss: 0.06590, policy_loss: -38.33576, policy_entropy: 4.10136, alpha: 0.42283, time: 50.53564
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    34 ----
[CW] collect: return: 24.26986, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.07366, qf2_loss: 0.07432, policy_loss: -38.93110, policy_entropy: 4.10198, alpha: 0.41309, time: 50.73505
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    35 ----
[CW] collect: return: 22.31362, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.06584, qf2_loss: 0.06637, policy_loss: -39.50418, policy_entropy: 4.10107, alpha: 0.40360, time: 50.61889
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    36 ----
[CW] collect: return: 24.55594, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.06666, qf2_loss: 0.06713, policy_loss: -40.06495, policy_entropy: 4.10128, alpha: 0.39434, time: 50.57247
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    37 ----
[CW] collect: return: 26.28102, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.08152, qf2_loss: 0.08223, policy_loss: -40.59989, policy_entropy: 4.10097, alpha: 0.38532, time: 50.78918
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    38 ----
[CW] collect: return: 24.03862, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.11346, qf2_loss: 0.11446, policy_loss: -41.11850, policy_entropy: 4.10083, alpha: 0.37653, time: 50.63326
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    39 ----
[CW] collect: return: 25.00255, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.02850, qf2_loss: 0.02860, policy_loss: -41.62201, policy_entropy: 4.10163, alpha: 0.36795, time: 50.65565
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    40 ----
[CW] collect: return: 22.71190, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.06767, qf2_loss: 0.06818, policy_loss: -42.10599, policy_entropy: 4.10134, alpha: 0.35958, time: 50.74997
[CW] eval: return: 24.87226, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    41 ----
[CW] collect: return: 25.31669, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.06583, qf2_loss: 0.06628, policy_loss: -42.57502, policy_entropy: 4.10134, alpha: 0.35142, time: 51.17991
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    42 ----
[CW] collect: return: 24.24063, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.07414, qf2_loss: 0.07468, policy_loss: -43.02632, policy_entropy: 4.10118, alpha: 0.34346, time: 50.89027
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    43 ----
[CW] collect: return: 26.65638, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.10472, qf2_loss: 0.10558, policy_loss: -43.46263, policy_entropy: 4.10111, alpha: 0.33569, time: 51.00904
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    44 ----
[CW] collect: return: 25.17054, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.02286, qf2_loss: 0.02296, policy_loss: -43.88069, policy_entropy: 4.10115, alpha: 0.32811, time: 50.74494
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    45 ----
[CW] collect: return: 25.11500, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.06978, qf2_loss: 0.07032, policy_loss: -44.27983, policy_entropy: 4.10245, alpha: 0.32071, time: 50.79040
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    46 ----
[CW] collect: return: 23.45723, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.06030, qf2_loss: 0.06069, policy_loss: -44.66678, policy_entropy: 4.10261, alpha: 0.31348, time: 50.61560
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    47 ----
[CW] collect: return: 30.26127, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.07456, qf2_loss: 0.07512, policy_loss: -45.03959, policy_entropy: 4.10329, alpha: 0.30643, time: 50.80948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    48 ----
[CW] collect: return: 24.62636, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.06707, qf2_loss: 0.06757, policy_loss: -45.40221, policy_entropy: 4.10255, alpha: 0.29954, time: 50.54065
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    49 ----
[CW] collect: return: 26.19921, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.06778, qf2_loss: 0.06826, policy_loss: -45.73908, policy_entropy: 4.10173, alpha: 0.29281, time: 50.70793
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    50 ----
[CW] collect: return: 27.01077, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.05916, qf2_loss: 0.05954, policy_loss: -46.07118, policy_entropy: 4.10051, alpha: 0.28625, time: 50.86709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    51 ----
[CW] collect: return: 26.11724, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.08452, qf2_loss: 0.08507, policy_loss: -46.38481, policy_entropy: 4.10180, alpha: 0.27983, time: 50.81073
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    52 ----
[CW] collect: return: 27.34065, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.04625, qf2_loss: 0.04652, policy_loss: -46.68912, policy_entropy: 4.10281, alpha: 0.27357, time: 50.64805
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    53 ----
[CW] collect: return: 24.98397, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.06396, qf2_loss: 0.06437, policy_loss: -46.97790, policy_entropy: 4.10215, alpha: 0.26745, time: 50.82800
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    54 ----
[CW] collect: return: 23.94343, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.04506, qf2_loss: 0.04532, policy_loss: -47.26016, policy_entropy: 4.10119, alpha: 0.26147, time: 50.67244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    55 ----
[CW] collect: return: 25.35009, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.09400, qf2_loss: 0.09465, policy_loss: -47.51644, policy_entropy: 4.10229, alpha: 0.25563, time: 50.51983
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    56 ----
[CW] collect: return: 23.36718, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.04985, qf2_loss: 0.05011, policy_loss: -47.77171, policy_entropy: 4.10125, alpha: 0.24993, time: 50.56068
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    57 ----
[CW] collect: return: 25.66307, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.07139, qf2_loss: 0.07185, policy_loss: -48.01766, policy_entropy: 4.10215, alpha: 0.24435, time: 50.58341
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    58 ----
[CW] collect: return: 25.96720, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.04005, qf2_loss: 0.04027, policy_loss: -48.24626, policy_entropy: 4.10120, alpha: 0.23890, time: 50.57955
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    59 ----
[CW] collect: return: 23.54876, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.11650, qf2_loss: 0.11720, policy_loss: -48.46401, policy_entropy: 4.10102, alpha: 0.23358, time: 50.87687
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    60 ----
[CW] collect: return: 26.23904, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.01248, qf2_loss: 0.01251, policy_loss: -48.67303, policy_entropy: 4.10132, alpha: 0.22838, time: 50.57980
[CW] eval: return: 25.14366, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    61 ----
[CW] collect: return: 22.44091, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.06516, qf2_loss: 0.06563, policy_loss: -48.87245, policy_entropy: 4.10195, alpha: 0.22330, time: 50.66572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    62 ----
[CW] collect: return: 21.63439, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.08994, qf2_loss: 0.09037, policy_loss: -49.05794, policy_entropy: 4.10144, alpha: 0.21833, time: 50.85373
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    63 ----
[CW] collect: return: 21.32218, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.02411, qf2_loss: 0.02424, policy_loss: -49.23467, policy_entropy: 4.10164, alpha: 0.21347, time: 50.78760
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    64 ----
[CW] collect: return: 21.45781, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.07324, qf2_loss: 0.07370, policy_loss: -49.39996, policy_entropy: 4.10126, alpha: 0.20873, time: 50.77090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    65 ----
[CW] collect: return: 24.22056, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.09626, qf2_loss: 0.09694, policy_loss: -49.55713, policy_entropy: 4.10221, alpha: 0.20409, time: 50.62420
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    66 ----
[CW] collect: return: 22.02059, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.01578, qf2_loss: 0.01582, policy_loss: -49.70988, policy_entropy: 4.10132, alpha: 0.19956, time: 50.48969
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    67 ----
[CW] collect: return: 29.54924, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.05672, qf2_loss: 0.05692, policy_loss: -49.84668, policy_entropy: 4.10114, alpha: 0.19513, time: 50.57658
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    68 ----
[CW] collect: return: 20.70069, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.04480, qf2_loss: 0.04502, policy_loss: -49.98031, policy_entropy: 4.10059, alpha: 0.19080, time: 50.54986
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    69 ----
[CW] collect: return: 22.55428, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.09094, qf2_loss: 0.09146, policy_loss: -50.10243, policy_entropy: 4.10099, alpha: 0.18656, time: 50.60347
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    70 ----
[CW] collect: return: 26.84755, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.03908, qf2_loss: 0.03929, policy_loss: -50.21081, policy_entropy: 4.10174, alpha: 0.18242, time: 50.59857
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    71 ----
[CW] collect: return: 31.68281, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.03648, qf2_loss: 0.03673, policy_loss: -50.31999, policy_entropy: 4.10114, alpha: 0.17838, time: 50.74778
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    72 ----
[CW] collect: return: 21.81751, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.07480, qf2_loss: 0.07522, policy_loss: -50.42434, policy_entropy: 4.10232, alpha: 0.17442, time: 50.63226
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    73 ----
[CW] collect: return: 25.17956, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.05179, qf2_loss: 0.05209, policy_loss: -50.50979, policy_entropy: 4.10159, alpha: 0.17055, time: 50.44943
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    74 ----
[CW] collect: return: 28.60571, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.08545, qf2_loss: 0.08596, policy_loss: -50.58923, policy_entropy: 4.10259, alpha: 0.16677, time: 50.84452
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    75 ----
[CW] collect: return: 24.77583, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.01984, qf2_loss: 0.01991, policy_loss: -50.67098, policy_entropy: 4.10101, alpha: 0.16308, time: 50.89441
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    76 ----
[CW] collect: return: 23.78075, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.04599, qf2_loss: 0.04625, policy_loss: -50.73653, policy_entropy: 4.10193, alpha: 0.15946, time: 50.84353
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    77 ----
[CW] collect: return: 25.44215, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.06572, qf2_loss: 0.06612, policy_loss: -50.79420, policy_entropy: 4.10100, alpha: 0.15593, time: 50.90522
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    78 ----
[CW] collect: return: 25.86507, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.05903, qf2_loss: 0.05918, policy_loss: -50.85245, policy_entropy: 4.10071, alpha: 0.15247, time: 50.76374
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    79 ----
[CW] collect: return: 24.11408, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.04375, qf2_loss: 0.04405, policy_loss: -50.89733, policy_entropy: 4.10134, alpha: 0.14910, time: 50.72014
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    80 ----
[CW] collect: return: 23.69945, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.03889, qf2_loss: 0.03914, policy_loss: -50.93968, policy_entropy: 4.10130, alpha: 0.14580, time: 50.65216
[CW] eval: return: 25.36288, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    81 ----
[CW] collect: return: 25.23070, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.05377, qf2_loss: 0.05419, policy_loss: -50.96808, policy_entropy: 4.10152, alpha: 0.14257, time: 50.91001
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    82 ----
[CW] collect: return: 23.42599, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.05809, qf2_loss: 0.05847, policy_loss: -51.00423, policy_entropy: 4.10154, alpha: 0.13941, time: 50.96261
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    83 ----
[CW] collect: return: 24.28012, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.03480, qf2_loss: 0.03502, policy_loss: -51.02162, policy_entropy: 4.10069, alpha: 0.13632, time: 50.70352
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    84 ----
[CW] collect: return: 25.12901, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.05563, qf2_loss: 0.05602, policy_loss: -51.03859, policy_entropy: 4.10131, alpha: 0.13331, time: 50.88925
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    85 ----
[CW] collect: return: 24.85810, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.03877, qf2_loss: 0.03900, policy_loss: -51.05186, policy_entropy: 4.10065, alpha: 0.13036, time: 50.81108
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    86 ----
[CW] collect: return: 23.84262, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.05916, qf2_loss: 0.05957, policy_loss: -51.05371, policy_entropy: 4.10139, alpha: 0.12747, time: 51.06274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    87 ----
[CW] collect: return: 25.70355, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.03981, qf2_loss: 0.04010, policy_loss: -51.05545, policy_entropy: 4.10147, alpha: 0.12465, time: 50.78564
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    88 ----
[CW] collect: return: 23.63790, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.05046, qf2_loss: 0.05079, policy_loss: -51.05001, policy_entropy: 4.10074, alpha: 0.12189, time: 51.01929
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    89 ----
[CW] collect: return: 26.24070, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.03654, qf2_loss: 0.03680, policy_loss: -51.04467, policy_entropy: 4.10108, alpha: 0.11919, time: 50.83912
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    90 ----
[CW] collect: return: 21.51744, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.05921, qf2_loss: 0.05965, policy_loss: -51.02446, policy_entropy: 4.10162, alpha: 0.11655, time: 51.05973
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    91 ----
[CW] collect: return: 27.69205, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.03956, qf2_loss: 0.03979, policy_loss: -51.00791, policy_entropy: 4.10162, alpha: 0.11398, time: 50.91797
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    92 ----
[CW] collect: return: 22.52477, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.05002, qf2_loss: 0.05036, policy_loss: -50.97925, policy_entropy: 4.10079, alpha: 0.11145, time: 50.80777
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    93 ----
[CW] collect: return: 26.86593, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.04265, qf2_loss: 0.04297, policy_loss: -50.95472, policy_entropy: 4.10021, alpha: 0.10899, time: 50.64241
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    94 ----
[CW] collect: return: 27.30857, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.04270, qf2_loss: 0.04299, policy_loss: -50.91536, policy_entropy: 4.10093, alpha: 0.10658, time: 50.70733
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    95 ----
[CW] collect: return: 21.13958, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.05172, qf2_loss: 0.05205, policy_loss: -50.87758, policy_entropy: 4.10014, alpha: 0.10422, time: 50.83697
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    96 ----
[CW] collect: return: 23.66767, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.03580, qf2_loss: 0.03603, policy_loss: -50.83707, policy_entropy: 4.10082, alpha: 0.10192, time: 50.99780
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    97 ----
[CW] collect: return: 25.31864, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.06672, qf2_loss: 0.06710, policy_loss: -50.78806, policy_entropy: 4.10084, alpha: 0.09966, time: 51.05419
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    98 ----
[CW] collect: return: 27.44842, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.02384, qf2_loss: 0.02395, policy_loss: -50.73568, policy_entropy: 4.10006, alpha: 0.09746, time: 50.74519
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    99 ----
[CW] collect: return: 26.87209, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.04928, qf2_loss: 0.04845, policy_loss: -50.67858, policy_entropy: 4.10156, alpha: 0.09530, time: 50.73324
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   100 ----
[CW] collect: return: 24.28930, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.05899, qf2_loss: 0.05980, policy_loss: -50.62264, policy_entropy: 4.10150, alpha: 0.09320, time: 50.50647
[CW] eval: return: 24.72652, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   101 ----
[CW] collect: return: 22.18903, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.01510, qf2_loss: 0.01527, policy_loss: -50.56390, policy_entropy: 4.10174, alpha: 0.09114, time: 51.00549
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   102 ----
[CW] collect: return: 24.19910, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.03794, qf2_loss: 0.03831, policy_loss: -50.49881, policy_entropy: 4.10070, alpha: 0.08912, time: 50.55554
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   103 ----
[CW] collect: return: 26.74631, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.05279, qf2_loss: 0.05319, policy_loss: -50.43243, policy_entropy: 4.10071, alpha: 0.08715, time: 51.16301
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   104 ----
[CW] collect: return: 24.71691, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.02674, qf2_loss: 0.02689, policy_loss: -50.35690, policy_entropy: 4.10138, alpha: 0.08522, time: 51.28502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   105 ----
[CW] collect: return: 23.98216, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.04936, qf2_loss: 0.04962, policy_loss: -50.28203, policy_entropy: 4.10155, alpha: 0.08334, time: 50.74574
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   106 ----
[CW] collect: return: 21.82962, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.03833, qf2_loss: 0.03854, policy_loss: -50.20937, policy_entropy: 4.10071, alpha: 0.08149, time: 50.89795
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   107 ----
[CW] collect: return: 24.05297, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.03218, qf2_loss: 0.03236, policy_loss: -50.12399, policy_entropy: 4.10076, alpha: 0.07969, time: 54.70092
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   108 ----
[CW] collect: return: 23.33681, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.05087, qf2_loss: 0.05113, policy_loss: -50.03949, policy_entropy: 4.10085, alpha: 0.07793, time: 51.69091
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   109 ----
[CW] collect: return: 31.09852, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.02472, qf2_loss: 0.02484, policy_loss: -49.95478, policy_entropy: 4.10035, alpha: 0.07621, time: 51.71224
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   110 ----
[CW] collect: return: 25.31145, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.04652, qf2_loss: 0.04668, policy_loss: -49.86318, policy_entropy: 4.10088, alpha: 0.07452, time: 51.66154
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   111 ----
[CW] collect: return: 22.67788, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.05218, qf2_loss: 0.05245, policy_loss: -49.77264, policy_entropy: 4.10054, alpha: 0.07287, time: 51.54942
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   112 ----
[CW] collect: return: 21.57454, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.01606, qf2_loss: 0.01608, policy_loss: -49.67276, policy_entropy: 4.10069, alpha: 0.07126, time: 51.60595
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   113 ----
[CW] collect: return: 26.99463, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.03788, qf2_loss: 0.03805, policy_loss: -49.57739, policy_entropy: 4.10035, alpha: 0.06968, time: 51.69793
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   114 ----
[CW] collect: return: 26.06972, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.04175, qf2_loss: 0.04199, policy_loss: -49.48134, policy_entropy: 4.10162, alpha: 0.06814, time: 51.54997
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   115 ----
[CW] collect: return: 34.44829, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.03482, qf2_loss: 0.03497, policy_loss: -49.37440, policy_entropy: 4.10158, alpha: 0.06664, time: 51.44011
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   116 ----
[CW] collect: return: 24.89408, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.04196, qf2_loss: 0.04212, policy_loss: -49.27218, policy_entropy: 4.10013, alpha: 0.06516, time: 51.60418
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   117 ----
[CW] collect: return: 25.37485, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.03150, qf2_loss: 0.03167, policy_loss: -49.16470, policy_entropy: 4.10030, alpha: 0.06372, time: 51.29356
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   118 ----
[CW] collect: return: 29.10807, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.03158, qf2_loss: 0.03176, policy_loss: -49.05610, policy_entropy: 4.09853, alpha: 0.06231, time: 51.26315
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   119 ----
[CW] collect: return: 25.17546, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.04844, qf2_loss: 0.04866, policy_loss: -48.94761, policy_entropy: 4.10001, alpha: 0.06093, time: 51.48402
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   120 ----
[CW] collect: return: 28.95686, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.02444, qf2_loss: 0.02449, policy_loss: -48.83556, policy_entropy: 4.09938, alpha: 0.05959, time: 51.34666
[CW] eval: return: 24.91911, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   121 ----
[CW] collect: return: 24.85243, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.04268, qf2_loss: 0.04277, policy_loss: -48.71610, policy_entropy: 4.09848, alpha: 0.05827, time: 51.34382
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   122 ----
[CW] collect: return: 23.85302, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.02689, qf2_loss: 0.02708, policy_loss: -48.60396, policy_entropy: 4.09985, alpha: 0.05698, time: 51.39935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   123 ----
[CW] collect: return: 23.56346, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.03510, qf2_loss: 0.03530, policy_loss: -48.48221, policy_entropy: 4.10032, alpha: 0.05572, time: 51.50988
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   124 ----
[CW] collect: return: 23.85285, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.01855, qf2_loss: 0.01863, policy_loss: -48.36145, policy_entropy: 4.09847, alpha: 0.05449, time: 51.09908
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   125 ----
[CW] collect: return: 23.34535, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.04050, qf2_loss: 0.04065, policy_loss: -48.24102, policy_entropy: 4.09951, alpha: 0.05328, time: 50.87671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   126 ----
[CW] collect: return: 25.03114, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.02737, qf2_loss: 0.02753, policy_loss: -48.11471, policy_entropy: 4.09933, alpha: 0.05211, time: 50.66468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   127 ----
[CW] collect: return: 24.95618, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.04126, qf2_loss: 0.04150, policy_loss: -47.99453, policy_entropy: 4.09990, alpha: 0.05095, time: 50.78383
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   128 ----
[CW] collect: return: 24.13563, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.02574, qf2_loss: 0.02588, policy_loss: -47.86519, policy_entropy: 4.09899, alpha: 0.04983, time: 50.64758
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   129 ----
[CW] collect: return: 24.95274, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.03654, qf2_loss: 0.03638, policy_loss: -47.73373, policy_entropy: 4.09883, alpha: 0.04872, time: 51.02909
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   130 ----
[CW] collect: return: 23.36238, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.03605, qf2_loss: 0.03645, policy_loss: -47.60832, policy_entropy: 4.09868, alpha: 0.04765, time: 51.04694
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   131 ----
[CW] collect: return: 25.12416, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.02325, qf2_loss: 0.02342, policy_loss: -47.47874, policy_entropy: 4.10029, alpha: 0.04659, time: 50.80234
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   132 ----
[CW] collect: return: 26.03518, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.03256, qf2_loss: 0.03267, policy_loss: -47.34391, policy_entropy: 4.09857, alpha: 0.04556, time: 50.92925
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   133 ----
[CW] collect: return: 24.93168, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.02954, qf2_loss: 0.02975, policy_loss: -47.21463, policy_entropy: 4.09780, alpha: 0.04456, time: 50.96252
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   134 ----
[CW] collect: return: 27.51946, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.02365, qf2_loss: 0.02376, policy_loss: -47.07530, policy_entropy: 4.09859, alpha: 0.04357, time: 50.97689
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   135 ----
[CW] collect: return: 22.99695, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.03247, qf2_loss: 0.03265, policy_loss: -46.94252, policy_entropy: 4.09825, alpha: 0.04261, time: 50.76667
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   136 ----
[CW] collect: return: 26.25343, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.03105, qf2_loss: 0.03122, policy_loss: -46.80521, policy_entropy: 4.09958, alpha: 0.04167, time: 50.60165
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   137 ----
[CW] collect: return: 24.98475, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.03644, qf2_loss: 0.03656, policy_loss: -46.66726, policy_entropy: 4.09977, alpha: 0.04074, time: 50.62897
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   138 ----
[CW] collect: return: 28.69534, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.02235, qf2_loss: 0.02247, policy_loss: -46.52766, policy_entropy: 4.09888, alpha: 0.03984, time: 50.14404
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   139 ----
[CW] collect: return: 26.82132, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.03834, qf2_loss: 0.03851, policy_loss: -46.38934, policy_entropy: 4.09953, alpha: 0.03896, time: 51.22551
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   140 ----
[CW] collect: return: 25.36746, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.02103, qf2_loss: 0.02113, policy_loss: -46.24729, policy_entropy: 4.09862, alpha: 0.03810, time: 50.43965
[CW] eval: return: 25.42706, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   141 ----
[CW] collect: return: 23.55370, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.02874, qf2_loss: 0.02891, policy_loss: -46.10425, policy_entropy: 4.09998, alpha: 0.03726, time: 50.56990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   142 ----
[CW] collect: return: 25.89390, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.02962, qf2_loss: 0.03003, policy_loss: -45.96410, policy_entropy: 4.09927, alpha: 0.03643, time: 50.57781
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   143 ----
[CW] collect: return: 23.82901, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.01796, qf2_loss: 0.01804, policy_loss: -45.81862, policy_entropy: 4.09996, alpha: 0.03563, time: 50.53357
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   144 ----
[CW] collect: return: 26.22343, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.03702, qf2_loss: 0.03693, policy_loss: -45.67610, policy_entropy: 4.09720, alpha: 0.03484, time: 50.40157
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   145 ----
[CW] collect: return: 25.53738, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.01693, qf2_loss: 0.01693, policy_loss: -45.52925, policy_entropy: 4.09823, alpha: 0.03407, time: 50.48227
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   146 ----
[CW] collect: return: 27.47865, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.02724, qf2_loss: 0.02735, policy_loss: -45.38337, policy_entropy: 4.09740, alpha: 0.03332, time: 50.72650
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   147 ----
[CW] collect: return: 21.34477, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.03158, qf2_loss: 0.03172, policy_loss: -45.24101, policy_entropy: 4.09772, alpha: 0.03258, time: 50.52310
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   148 ----
[CW] collect: return: 25.04533, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.01844, qf2_loss: 0.01851, policy_loss: -45.09020, policy_entropy: 4.09813, alpha: 0.03186, time: 51.08250
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   149 ----
[CW] collect: return: 22.05347, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.02324, qf2_loss: 0.02331, policy_loss: -44.94285, policy_entropy: 4.09866, alpha: 0.03116, time: 51.27123
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   150 ----
[CW] collect: return: 23.75668, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.02551, qf2_loss: 0.02559, policy_loss: -44.79605, policy_entropy: 4.09729, alpha: 0.03047, time: 51.05440
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   151 ----
[CW] collect: return: 23.63252, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.02224, qf2_loss: 0.02224, policy_loss: -44.64574, policy_entropy: 4.09810, alpha: 0.02979, time: 51.10470
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   152 ----
[CW] collect: return: 23.20957, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.04507, qf2_loss: 0.04514, policy_loss: -44.50280, policy_entropy: 4.09727, alpha: 0.02913, time: 50.96682
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   153 ----
[CW] collect: return: 27.13286, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.00840, qf2_loss: 0.00831, policy_loss: -44.34719, policy_entropy: 4.09725, alpha: 0.02849, time: 50.84237
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   154 ----
[CW] collect: return: 23.45745, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.02087, qf2_loss: 0.02100, policy_loss: -44.19888, policy_entropy: 4.09810, alpha: 0.02786, time: 50.50071
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   155 ----
[CW] collect: return: 22.18019, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.04886, qf2_loss: 0.04902, policy_loss: -44.04988, policy_entropy: 4.09670, alpha: 0.02724, time: 50.42342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   156 ----
[CW] collect: return: 26.35566, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.00595, qf2_loss: 0.00594, policy_loss: -43.89598, policy_entropy: 4.09745, alpha: 0.02664, time: 50.53846
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   157 ----
[CW] collect: return: 25.76370, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.03363, qf2_loss: 0.03357, policy_loss: -43.74367, policy_entropy: 4.09423, alpha: 0.02605, time: 50.75922
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   158 ----
[CW] collect: return: 24.57245, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.01082, qf2_loss: 0.01106, policy_loss: -43.59545, policy_entropy: 4.09581, alpha: 0.02548, time: 50.20642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   159 ----
[CW] collect: return: 37.34362, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.02844, qf2_loss: 0.02859, policy_loss: -43.44286, policy_entropy: 4.09672, alpha: 0.02491, time: 51.02703
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   160 ----
[CW] collect: return: 23.26315, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.02697, qf2_loss: 0.02703, policy_loss: -43.29498, policy_entropy: 4.09533, alpha: 0.02436, time: 50.66861
[CW] eval: return: 24.78010, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   161 ----
[CW] collect: return: 24.37518, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.01653, qf2_loss: 0.01646, policy_loss: -43.13802, policy_entropy: 4.09599, alpha: 0.02382, time: 50.95192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   162 ----
[CW] collect: return: 25.27102, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.03166, qf2_loss: 0.03172, policy_loss: -42.99001, policy_entropy: 4.09539, alpha: 0.02330, time: 50.69104
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   163 ----
[CW] collect: return: 23.97263, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.01635, qf2_loss: 0.01637, policy_loss: -42.83449, policy_entropy: 4.09695, alpha: 0.02278, time: 50.56243
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   164 ----
[CW] collect: return: 23.83434, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.01571, qf2_loss: 0.01575, policy_loss: -42.68386, policy_entropy: 4.09159, alpha: 0.02228, time: 50.67830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   165 ----
[CW] collect: return: 26.32832, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.03293, qf2_loss: 0.03307, policy_loss: -42.53184, policy_entropy: 4.09368, alpha: 0.02179, time: 50.45870
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   166 ----
[CW] collect: return: 29.72747, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.01057, qf2_loss: 0.01053, policy_loss: -42.37852, policy_entropy: 4.09310, alpha: 0.02131, time: 50.67776
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   167 ----
[CW] collect: return: 27.92575, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.02568, qf2_loss: 0.02559, policy_loss: -42.22654, policy_entropy: 4.09107, alpha: 0.02083, time: 50.40755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   168 ----
[CW] collect: return: 21.17354, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.02555, qf2_loss: 0.02564, policy_loss: -42.07459, policy_entropy: 4.09281, alpha: 0.02037, time: 50.61159
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   169 ----
[CW] collect: return: 24.17285, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.01808, qf2_loss: 0.01802, policy_loss: -41.92108, policy_entropy: 4.09162, alpha: 0.01992, time: 50.57817
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   170 ----
[CW] collect: return: 24.81454, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.03480, qf2_loss: 0.03470, policy_loss: -41.76532, policy_entropy: 4.08987, alpha: 0.01948, time: 50.88412
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   171 ----
[CW] collect: return: 27.98331, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.01118, qf2_loss: 0.01108, policy_loss: -41.61504, policy_entropy: 4.08780, alpha: 0.01905, time: 50.84924
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   172 ----
[CW] collect: return: 24.67036, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.01791, qf2_loss: 0.01779, policy_loss: -41.45955, policy_entropy: 4.08175, alpha: 0.01863, time: 50.65847
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   173 ----
[CW] collect: return: 23.64313, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.01490, qf2_loss: 0.01485, policy_loss: -41.30748, policy_entropy: 4.08648, alpha: 0.01822, time: 50.90439
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   174 ----
[CW] collect: return: 27.40249, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.03361, qf2_loss: 0.03355, policy_loss: -41.15437, policy_entropy: 4.08770, alpha: 0.01782, time: 50.76517
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   175 ----
[CW] collect: return: 23.33041, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.01312, qf2_loss: 0.01308, policy_loss: -41.00255, policy_entropy: 4.08721, alpha: 0.01742, time: 50.90816
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   176 ----
[CW] collect: return: 30.56281, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.02348, qf2_loss: 0.02340, policy_loss: -40.84678, policy_entropy: 4.08456, alpha: 0.01704, time: 50.72635
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   177 ----
[CW] collect: return: 23.69609, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.01578, qf2_loss: 0.01583, policy_loss: -40.69367, policy_entropy: 4.08683, alpha: 0.01666, time: 50.97747
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   178 ----
[CW] collect: return: 23.92200, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.02194, qf2_loss: 0.02195, policy_loss: -40.54153, policy_entropy: 4.08609, alpha: 0.01629, time: 50.28587
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   179 ----
[CW] collect: return: 22.95140, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.01681, qf2_loss: 0.01678, policy_loss: -40.38816, policy_entropy: 4.08434, alpha: 0.01593, time: 50.72234
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   180 ----
[CW] collect: return: 22.57601, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.01816, qf2_loss: 0.01813, policy_loss: -40.23766, policy_entropy: 4.08624, alpha: 0.01558, time: 50.65143
[CW] eval: return: 25.69134, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   181 ----
[CW] collect: return: 28.86614, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.01761, qf2_loss: 0.01752, policy_loss: -40.08277, policy_entropy: 4.07547, alpha: 0.01524, time: 50.56463
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   182 ----
[CW] collect: return: 24.89731, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.02659, qf2_loss: 0.02658, policy_loss: -39.93269, policy_entropy: 4.07260, alpha: 0.01490, time: 50.75210
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   183 ----
[CW] collect: return: 30.20415, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.01531, qf2_loss: 0.01549, policy_loss: -39.78059, policy_entropy: 4.07379, alpha: 0.01457, time: 50.41523
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   184 ----
[CW] collect: return: 29.20225, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.01722, qf2_loss: 0.01713, policy_loss: -39.62528, policy_entropy: 4.07264, alpha: 0.01425, time: 50.48436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   185 ----
[CW] collect: return: 22.29725, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.01937, qf2_loss: 0.01939, policy_loss: -39.47759, policy_entropy: 4.07263, alpha: 0.01394, time: 50.79193
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   186 ----
[CW] collect: return: 23.64116, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.01600, qf2_loss: 0.01597, policy_loss: -39.32271, policy_entropy: 4.05339, alpha: 0.01363, time: 51.20539
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   187 ----
[CW] collect: return: 25.90803, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.02091, qf2_loss: 0.02080, policy_loss: -39.17227, policy_entropy: 4.05831, alpha: 0.01333, time: 50.59140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   188 ----
[CW] collect: return: 25.65130, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.01931, qf2_loss: 0.01932, policy_loss: -39.02144, policy_entropy: 4.06151, alpha: 0.01303, time: 50.77436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   189 ----
[CW] collect: return: 30.63812, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.02268, qf2_loss: 0.02269, policy_loss: -38.87163, policy_entropy: 4.05616, alpha: 0.01275, time: 50.83026
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   190 ----
[CW] collect: return: 24.36429, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.01285, qf2_loss: 0.01282, policy_loss: -38.71677, policy_entropy: 4.05750, alpha: 0.01246, time: 50.86621
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   191 ----
[CW] collect: return: 20.70524, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.01653, qf2_loss: 0.01652, policy_loss: -38.57048, policy_entropy: 4.04919, alpha: 0.01219, time: 50.89142
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   192 ----
[CW] collect: return: 28.37471, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.01850, qf2_loss: 0.01842, policy_loss: -38.41784, policy_entropy: 4.03935, alpha: 0.01192, time: 50.75417
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   193 ----
[CW] collect: return: 21.47554, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.01839, qf2_loss: 0.01832, policy_loss: -38.27131, policy_entropy: 4.04421, alpha: 0.01166, time: 50.96088
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   194 ----
[CW] collect: return: 32.74049, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.01578, qf2_loss: 0.01569, policy_loss: -38.12195, policy_entropy: 4.02598, alpha: 0.01140, time: 50.74700
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   195 ----
[CW] collect: return: 23.73907, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.01221, qf2_loss: 0.01222, policy_loss: -37.96816, policy_entropy: 4.01960, alpha: 0.01115, time: 50.64788
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   196 ----
[CW] collect: return: 23.63152, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.02149, qf2_loss: 0.02136, policy_loss: -37.81763, policy_entropy: 4.00855, alpha: 0.01091, time: 50.46821
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   197 ----
[CW] collect: return: 24.30686, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.01429, qf2_loss: 0.01424, policy_loss: -37.67308, policy_entropy: 3.99162, alpha: 0.01067, time: 50.83963
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   198 ----
[CW] collect: return: 22.92287, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.01732, qf2_loss: 0.01735, policy_loss: -37.51886, policy_entropy: 3.97264, alpha: 0.01043, time: 50.26839
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   199 ----
[CW] collect: return: 23.60936, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.02170, qf2_loss: 0.02181, policy_loss: -37.37063, policy_entropy: 3.98973, alpha: 0.01020, time: 50.64351
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   200 ----
[CW] collect: return: 25.46419, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.01493, qf2_loss: 0.01495, policy_loss: -37.22850, policy_entropy: 3.95191, alpha: 0.00998, time: 50.70558
[CW] eval: return: 25.37590, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   201 ----
[CW] collect: return: 25.02799, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.02328, qf2_loss: 0.02360, policy_loss: -37.07672, policy_entropy: 3.90610, alpha: 0.00976, time: 50.68637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   202 ----
[CW] collect: return: 38.55599, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.01465, qf2_loss: 0.01458, policy_loss: -36.93299, policy_entropy: 3.93158, alpha: 0.00955, time: 50.91436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   203 ----
[CW] collect: return: 35.21909, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.01594, qf2_loss: 0.01605, policy_loss: -36.78345, policy_entropy: 3.89325, alpha: 0.00934, time: 50.71670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   204 ----
[CW] collect: return: 23.90588, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.01450, qf2_loss: 0.01446, policy_loss: -36.63873, policy_entropy: 3.84631, alpha: 0.00913, time: 50.72051
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   205 ----
[CW] collect: return: 34.35293, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.01823, qf2_loss: 0.01828, policy_loss: -36.49260, policy_entropy: 3.78712, alpha: 0.00894, time: 50.67068
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   206 ----
[CW] collect: return: 27.39191, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.01204, qf2_loss: 0.01207, policy_loss: -36.34635, policy_entropy: 3.68193, alpha: 0.00874, time: 50.78431
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   207 ----
[CW] collect: return: 23.89280, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.01867, qf2_loss: 0.01887, policy_loss: -36.20612, policy_entropy: 3.75955, alpha: 0.00855, time: 50.97271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   208 ----
[CW] collect: return: 26.46657, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.01085, qf2_loss: 0.01078, policy_loss: -36.06654, policy_entropy: 3.57557, alpha: 0.00837, time: 50.87672
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   209 ----
[CW] collect: return: 25.35812, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.02714, qf2_loss: 0.02714, policy_loss: -35.92290, policy_entropy: 3.54645, alpha: 0.00819, time: 51.06403
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   210 ----
[CW] collect: return: 14.57588, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.00927, qf2_loss: 0.00925, policy_loss: -35.77770, policy_entropy: 3.47129, alpha: 0.00802, time: 51.01653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   211 ----
[CW] collect: return: 17.63281, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.01528, qf2_loss: 0.01533, policy_loss: -35.63632, policy_entropy: 3.46947, alpha: 0.00785, time: 50.30081
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   212 ----
[CW] collect: return: 25.91894, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.02052, qf2_loss: 0.02055, policy_loss: -35.49349, policy_entropy: 3.42710, alpha: 0.00768, time: 50.29562
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   213 ----
[CW] collect: return: 17.65024, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.00796, qf2_loss: 0.00791, policy_loss: -35.35658, policy_entropy: 3.43499, alpha: 0.00752, time: 50.29784
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   214 ----
[CW] collect: return: 13.16707, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 0.01563, qf2_loss: 0.01571, policy_loss: -35.21094, policy_entropy: 3.25066, alpha: 0.00736, time: 50.53513
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   215 ----
[CW] collect: return: 26.74863, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.01606, qf2_loss: 0.01604, policy_loss: -35.07220, policy_entropy: 3.29797, alpha: 0.00720, time: 50.75659
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   216 ----
[CW] collect: return: 14.66635, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.01935, qf2_loss: 0.01924, policy_loss: -34.93435, policy_entropy: 3.18203, alpha: 0.00705, time: 50.49992
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   217 ----
[CW] collect: return: 26.44238, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.01376, qf2_loss: 0.01383, policy_loss: -34.79547, policy_entropy: 3.13900, alpha: 0.00690, time: 50.43518
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   218 ----
[CW] collect: return: 18.53059, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.01792, qf2_loss: 0.01765, policy_loss: -34.65036, policy_entropy: 3.22122, alpha: 0.00676, time: 50.38939
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   219 ----
[CW] collect: return: 16.41753, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.01432, qf2_loss: 0.01447, policy_loss: -34.51051, policy_entropy: 3.28004, alpha: 0.00661, time: 50.84474
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   220 ----
[CW] collect: return: 13.83225, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.01945, qf2_loss: 0.01918, policy_loss: -34.37557, policy_entropy: 3.17943, alpha: 0.00647, time: 50.58663
[CW] eval: return: 24.05305, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   221 ----
[CW] collect: return: 13.39563, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.01917, qf2_loss: 0.01925, policy_loss: -34.23545, policy_entropy: 3.25918, alpha: 0.00633, time: 51.23640
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   222 ----
[CW] collect: return: 14.07843, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.01707, qf2_loss: 0.01712, policy_loss: -34.09592, policy_entropy: 3.24784, alpha: 0.00620, time: 51.29482
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   223 ----
[CW] collect: return: 19.81866, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.01769, qf2_loss: 0.01746, policy_loss: -33.95632, policy_entropy: 3.07653, alpha: 0.00607, time: 51.19542
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   224 ----
[CW] collect: return: 13.67542, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.01612, qf2_loss: 0.01629, policy_loss: -33.82631, policy_entropy: 2.93778, alpha: 0.00594, time: 51.05821
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   225 ----
[CW] collect: return: 26.75789, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.01720, qf2_loss: 0.01709, policy_loss: -33.68546, policy_entropy: 2.91534, alpha: 0.00581, time: 50.92990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   226 ----
[CW] collect: return: 25.75600, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.01763, qf2_loss: 0.01786, policy_loss: -33.54861, policy_entropy: 3.33819, alpha: 0.00569, time: 51.08556
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   227 ----
[CW] collect: return: 25.66775, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.01725, qf2_loss: 0.01710, policy_loss: -33.41069, policy_entropy: 3.03569, alpha: 0.00557, time: 50.96509
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   228 ----
[CW] collect: return: 14.52817, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 0.01954, qf2_loss: 0.01952, policy_loss: -33.27412, policy_entropy: 3.13477, alpha: 0.00545, time: 51.87301
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   229 ----
[CW] collect: return: 26.38268, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.02371, qf2_loss: 0.02393, policy_loss: -33.13943, policy_entropy: 3.13769, alpha: 0.00533, time: 51.00298
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   230 ----
[CW] collect: return: 25.56992, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.01511, qf2_loss: 0.01539, policy_loss: -33.00722, policy_entropy: 3.16340, alpha: 0.00521, time: 50.91026
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   231 ----
[CW] collect: return: 26.66209, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 0.01629, qf2_loss: 0.01644, policy_loss: -32.87019, policy_entropy: 2.39960, alpha: 0.00510, time: 50.59623
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   232 ----
[CW] collect: return: 15.76330, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.02340, qf2_loss: 0.02321, policy_loss: -32.74383, policy_entropy: 2.20608, alpha: 0.00500, time: 50.62991
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   233 ----
[CW] collect: return: 28.09190, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.01926, qf2_loss: 0.01969, policy_loss: -32.60787, policy_entropy: 2.05732, alpha: 0.00490, time: 51.01405
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   234 ----
[CW] collect: return: 13.70310, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.01661, qf2_loss: 0.01691, policy_loss: -32.47288, policy_entropy: 2.32345, alpha: 0.00481, time: 50.75760
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   235 ----
[CW] collect: return: 13.47771, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.02051, qf2_loss: 0.02104, policy_loss: -32.33748, policy_entropy: 2.27286, alpha: 0.00471, time: 50.89095
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   236 ----
[CW] collect: return: 34.11980, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.02569, qf2_loss: 0.02508, policy_loss: -32.21137, policy_entropy: 2.35624, alpha: 0.00461, time: 51.17005
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   237 ----
[CW] collect: return: 13.83872, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 0.02515, qf2_loss: 0.02588, policy_loss: -32.06467, policy_entropy: 2.98294, alpha: 0.00451, time: 50.80276
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   238 ----
[CW] collect: return: 24.95210, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 0.01824, qf2_loss: 0.01842, policy_loss: -31.94438, policy_entropy: 2.80812, alpha: 0.00441, time: 50.58193
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   239 ----
[CW] collect: return: 27.86232, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 0.02575, qf2_loss: 0.02564, policy_loss: -31.81487, policy_entropy: 1.97213, alpha: 0.00432, time: 50.88548
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   240 ----
[CW] collect: return: 30.03706, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.01835, qf2_loss: 0.01854, policy_loss: -31.68103, policy_entropy: 2.20152, alpha: 0.00424, time: 50.52082
[CW] eval: return: 23.11265, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   241 ----
[CW] collect: return: 26.77406, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 0.02442, qf2_loss: 0.02460, policy_loss: -31.54890, policy_entropy: 1.96413, alpha: 0.00415, time: 50.69935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   242 ----
[CW] collect: return: 26.67895, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 0.02554, qf2_loss: 0.02556, policy_loss: -31.42113, policy_entropy: 2.45538, alpha: 0.00406, time: 50.25130
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   243 ----
[CW] collect: return: 13.40228, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 0.03106, qf2_loss: 0.03171, policy_loss: -31.30416, policy_entropy: 2.19131, alpha: 0.00398, time: 50.53884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   244 ----
[CW] collect: return: 25.58318, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 0.02287, qf2_loss: 0.02253, policy_loss: -31.17258, policy_entropy: -1.15948, alpha: 0.00391, time: 50.70611
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   245 ----
[CW] collect: return: 13.18553, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 0.03184, qf2_loss: 0.03158, policy_loss: -31.04318, policy_entropy: 1.30357, alpha: 0.00385, time: 50.69986
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   246 ----
[CW] collect: return: 16.11478, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 0.02664, qf2_loss: 0.02676, policy_loss: -30.92030, policy_entropy: 1.78653, alpha: 0.00377, time: 50.33403
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   247 ----
[CW] collect: return: 25.34302, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 0.02845, qf2_loss: 0.02877, policy_loss: -30.79953, policy_entropy: 0.08477, alpha: 0.00370, time: 50.23163
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   248 ----
[CW] collect: return: 26.08716, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 0.04188, qf2_loss: 0.04158, policy_loss: -30.66999, policy_entropy: 0.92223, alpha: 0.00364, time: 50.47120
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   249 ----
[CW] collect: return: 25.52932, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 0.03773, qf2_loss: 0.03835, policy_loss: -30.54164, policy_entropy: -1.25845, alpha: 0.00358, time: 50.55634
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   250 ----
[CW] collect: return: 14.41590, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 0.03306, qf2_loss: 0.03304, policy_loss: -30.44133, policy_entropy: -3.15444, alpha: 0.00354, time: 50.55572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   251 ----
[CW] collect: return: 13.84539, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 0.03996, qf2_loss: 0.03990, policy_loss: -30.29600, policy_entropy: 0.06362, alpha: 0.00350, time: 50.81650
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   252 ----
[CW] collect: return: 28.67104, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 0.03201, qf2_loss: 0.03198, policy_loss: -30.19278, policy_entropy: -1.88897, alpha: 0.00344, time: 50.42289
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   253 ----
[CW] collect: return: 36.65964, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 0.03578, qf2_loss: 0.03569, policy_loss: -30.11865, policy_entropy: -5.26564, alpha: 0.00342, time: 50.33673
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   254 ----
[CW] collect: return: 50.23439, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 0.04327, qf2_loss: 0.04360, policy_loss: -30.02039, policy_entropy: -5.48296, alpha: 0.00341, time: 50.92913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   255 ----
[CW] collect: return: 55.75287, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 0.03680, qf2_loss: 0.03693, policy_loss: -29.92703, policy_entropy: -6.39491, alpha: 0.00341, time: 50.47134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   256 ----
[CW] collect: return: 72.85403, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 0.05154, qf2_loss: 0.05112, policy_loss: -29.86926, policy_entropy: -7.04037, alpha: 0.00341, time: 50.52090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   257 ----
[CW] collect: return: 59.73014, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 0.05496, qf2_loss: 0.05512, policy_loss: -29.79211, policy_entropy: -8.16327, alpha: 0.00343, time: 50.86870
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   258 ----
[CW] collect: return: 22.34304, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 0.04321, qf2_loss: 0.04345, policy_loss: -29.69687, policy_entropy: -7.68341, alpha: 0.00346, time: 50.62710
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   259 ----
[CW] collect: return: 72.43476, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 0.05957, qf2_loss: 0.05934, policy_loss: -29.64452, policy_entropy: -8.14262, alpha: 0.00349, time: 50.51622
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   260 ----
[CW] collect: return: 32.62488, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 0.05806, qf2_loss: 0.05789, policy_loss: -29.56171, policy_entropy: -7.20810, alpha: 0.00352, time: 50.41132
[CW] eval: return: 42.77509, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   261 ----
[CW] collect: return: 22.09740, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 0.06208, qf2_loss: 0.06129, policy_loss: -29.44494, policy_entropy: -6.77111, alpha: 0.00353, time: 51.00480
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   262 ----
[CW] collect: return: 57.89875, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 0.05888, qf2_loss: 0.05916, policy_loss: -29.26443, policy_entropy: -2.85499, alpha: 0.00352, time: 50.75112
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   263 ----
[CW] collect: return: 29.12602, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 0.06430, qf2_loss: 0.06348, policy_loss: -29.25569, policy_entropy: -7.18000, alpha: 0.00348, time: 50.67326
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   264 ----
[CW] collect: return: 28.99219, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 0.05086, qf2_loss: 0.04969, policy_loss: -29.16654, policy_entropy: -8.14508, alpha: 0.00352, time: 50.31339
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   265 ----
[CW] collect: return: 68.40000, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 0.05737, qf2_loss: 0.05507, policy_loss: -29.06719, policy_entropy: -6.44274, alpha: 0.00355, time: 50.49722
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   266 ----
[CW] collect: return: 54.31075, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 0.06440, qf2_loss: 0.06358, policy_loss: -29.01498, policy_entropy: -7.89846, alpha: 0.00357, time: 50.55508
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   267 ----
[CW] collect: return: 22.48240, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 0.05396, qf2_loss: 0.05312, policy_loss: -28.91458, policy_entropy: -8.00654, alpha: 0.00361, time: 50.64972
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   268 ----
[CW] collect: return: 44.63430, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 0.05550, qf2_loss: 0.05436, policy_loss: -28.83031, policy_entropy: -7.25392, alpha: 0.00364, time: 50.43615
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   269 ----
[CW] collect: return: 9.70143, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 0.05248, qf2_loss: 0.05253, policy_loss: -28.75345, policy_entropy: -6.91076, alpha: 0.00368, time: 50.80403
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   270 ----
[CW] collect: return: 46.63503, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 0.05143, qf2_loss: 0.05122, policy_loss: -28.69857, policy_entropy: -6.40473, alpha: 0.00369, time: 50.77959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   271 ----
[CW] collect: return: 22.65460, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 0.05578, qf2_loss: 0.05501, policy_loss: -28.59844, policy_entropy: -5.46533, alpha: 0.00369, time: 50.59106
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   272 ----
[CW] collect: return: 40.65048, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 0.04817, qf2_loss: 0.04823, policy_loss: -28.50135, policy_entropy: -4.04012, alpha: 0.00365, time: 50.66324
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   273 ----
[CW] collect: return: 41.89075, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 0.04870, qf2_loss: 0.04854, policy_loss: -28.48600, policy_entropy: -4.95014, alpha: 0.00362, time: 50.57440
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   274 ----
[CW] collect: return: 22.90330, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 0.09736, qf2_loss: 0.09688, policy_loss: -28.42236, policy_entropy: -5.87945, alpha: 0.00360, time: 50.57844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   275 ----
[CW] collect: return: 96.88836, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 0.05378, qf2_loss: 0.05337, policy_loss: -28.37276, policy_entropy: -6.11653, alpha: 0.00360, time: 50.68389
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   276 ----
[CW] collect: return: 63.69161, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 0.05970, qf2_loss: 0.05897, policy_loss: -28.32355, policy_entropy: -6.09962, alpha: 0.00360, time: 50.86272
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   277 ----
[CW] collect: return: 59.89948, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 0.06240, qf2_loss: 0.06195, policy_loss: -28.25955, policy_entropy: -6.93067, alpha: 0.00361, time: 50.62124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   278 ----
[CW] collect: return: 15.27275, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 0.06690, qf2_loss: 0.06614, policy_loss: -28.24640, policy_entropy: -8.69742, alpha: 0.00368, time: 50.61556
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   279 ----
[CW] collect: return: 22.99771, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 0.06759, qf2_loss: 0.06670, policy_loss: -28.16336, policy_entropy: -6.72518, alpha: 0.00373, time: 50.72121
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   280 ----
[CW] collect: return: 24.01725, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 0.06398, qf2_loss: 0.06351, policy_loss: -28.15495, policy_entropy: -8.45897, alpha: 0.00379, time: 50.60000
[CW] eval: return: 24.41337, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   281 ----
[CW] collect: return: 23.33395, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 0.06032, qf2_loss: 0.05980, policy_loss: -28.09622, policy_entropy: -8.38480, alpha: 0.00389, time: 50.98887
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   282 ----
[CW] collect: return: 26.42198, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 0.06754, qf2_loss: 0.06636, policy_loss: -28.11569, policy_entropy: -8.86715, alpha: 0.00401, time: 50.65897
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   283 ----
[CW] collect: return: 12.66452, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 0.06702, qf2_loss: 0.06624, policy_loss: -28.09092, policy_entropy: -8.64194, alpha: 0.00413, time: 50.50212
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   284 ----
[CW] collect: return: 26.72070, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 0.07583, qf2_loss: 0.07515, policy_loss: -28.02230, policy_entropy: -8.38914, alpha: 0.00425, time: 50.83550
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   285 ----
[CW] collect: return: 35.40961, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 0.07611, qf2_loss: 0.07509, policy_loss: -27.98823, policy_entropy: -7.60862, alpha: 0.00435, time: 50.70123
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   286 ----
[CW] collect: return: 13.40472, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 0.09063, qf2_loss: 0.08930, policy_loss: -27.97944, policy_entropy: -7.04214, alpha: 0.00443, time: 50.73831
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   287 ----
[CW] collect: return: 11.23252, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 0.07197, qf2_loss: 0.07155, policy_loss: -27.94686, policy_entropy: -7.25187, alpha: 0.00450, time: 50.47031
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   288 ----
[CW] collect: return: 46.05635, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 0.06199, qf2_loss: 0.06179, policy_loss: -27.88031, policy_entropy: -6.21468, alpha: 0.00455, time: 50.74208
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   289 ----
[CW] collect: return: 38.16876, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 0.06291, qf2_loss: 0.06333, policy_loss: -27.86938, policy_entropy: -6.40318, alpha: 0.00456, time: 51.40291
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   290 ----
[CW] collect: return: 30.69354, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 0.06677, qf2_loss: 0.06609, policy_loss: -27.81615, policy_entropy: -5.60823, alpha: 0.00456, time: 51.03682
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   291 ----
[CW] collect: return: 50.34923, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 0.07377, qf2_loss: 0.07283, policy_loss: -27.78483, policy_entropy: -6.69961, alpha: 0.00457, time: 51.24842
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   292 ----
[CW] collect: return: 59.45396, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 0.08170, qf2_loss: 0.08004, policy_loss: -27.77790, policy_entropy: -6.49156, alpha: 0.00462, time: 50.92765
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   293 ----
[CW] collect: return: 29.74674, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 0.05950, qf2_loss: 0.05957, policy_loss: -27.70455, policy_entropy: -6.12004, alpha: 0.00464, time: 51.04736
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   294 ----
[CW] collect: return: 62.61788, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 0.06461, qf2_loss: 0.06400, policy_loss: -27.65218, policy_entropy: -5.62982, alpha: 0.00464, time: 50.84138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   295 ----
[CW] collect: return: 58.34451, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 0.07184, qf2_loss: 0.07167, policy_loss: -27.66019, policy_entropy: -5.69508, alpha: 0.00460, time: 50.80493
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   296 ----
[CW] collect: return: 57.97181, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 0.08259, qf2_loss: 0.08103, policy_loss: -27.60303, policy_entropy: -5.93494, alpha: 0.00458, time: 51.13548
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   297 ----
[CW] collect: return: 31.83041, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 0.05170, qf2_loss: 0.05172, policy_loss: -27.51898, policy_entropy: -5.71285, alpha: 0.00457, time: 51.35085
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   298 ----
[CW] collect: return: 39.30399, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 0.05084, qf2_loss: 0.05043, policy_loss: -27.49655, policy_entropy: -5.50221, alpha: 0.00454, time: 50.64063
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   299 ----
[CW] collect: return: 77.31152, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 0.05306, qf2_loss: 0.05228, policy_loss: -27.40428, policy_entropy: -4.66515, alpha: 0.00446, time: 51.25873
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   300 ----
[CW] collect: return: 89.34307, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 0.05537, qf2_loss: 0.05472, policy_loss: -27.36036, policy_entropy: -4.96156, alpha: 0.00434, time: 50.45473
[CW] eval: return: 62.64121, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   301 ----
[CW] collect: return: 46.64497, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 0.05905, qf2_loss: 0.05828, policy_loss: -27.25737, policy_entropy: -4.56071, alpha: 0.00425, time: 51.15974
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   302 ----
[CW] collect: return: 89.51920, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 0.05876, qf2_loss: 0.05849, policy_loss: -27.18663, policy_entropy: -3.98505, alpha: 0.00410, time: 51.23398
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   303 ----
[CW] collect: return: 78.25777, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 0.06483, qf2_loss: 0.06416, policy_loss: -27.10675, policy_entropy: -4.07461, alpha: 0.00395, time: 50.48980
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   304 ----
[CW] collect: return: 47.48578, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 0.05941, qf2_loss: 0.05879, policy_loss: -27.02365, policy_entropy: -3.11865, alpha: 0.00380, time: 50.82060
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   305 ----
[CW] collect: return: 39.81750, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 0.05149, qf2_loss: 0.05050, policy_loss: -26.94421, policy_entropy: -3.46185, alpha: 0.00365, time: 50.57072
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   306 ----
[CW] collect: return: 48.13438, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 0.05190, qf2_loss: 0.05078, policy_loss: -26.91341, policy_entropy: -3.97420, alpha: 0.00352, time: 50.43413
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   307 ----
[CW] collect: return: 80.54115, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 0.05338, qf2_loss: 0.05224, policy_loss: -26.84923, policy_entropy: -6.16063, alpha: 0.00347, time: 50.55486
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   308 ----
[CW] collect: return: 80.50169, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 0.04683, qf2_loss: 0.04635, policy_loss: -26.81151, policy_entropy: -6.48734, alpha: 0.00349, time: 51.10111
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   309 ----
[CW] collect: return: 77.59669, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 0.05890, qf2_loss: 0.05755, policy_loss: -26.74854, policy_entropy: -6.23676, alpha: 0.00350, time: 50.44216
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   310 ----
[CW] collect: return: 58.59125, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 0.05506, qf2_loss: 0.05434, policy_loss: -26.72826, policy_entropy: -6.72817, alpha: 0.00353, time: 50.86143
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   311 ----
[CW] collect: return: 86.55004, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 0.04671, qf2_loss: 0.04576, policy_loss: -26.68706, policy_entropy: -6.60228, alpha: 0.00357, time: 50.80176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   312 ----
[CW] collect: return: 59.01697, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 0.05090, qf2_loss: 0.05003, policy_loss: -26.60280, policy_entropy: -6.38892, alpha: 0.00360, time: 50.57728
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   313 ----
[CW] collect: return: 57.16506, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 0.07263, qf2_loss: 0.07104, policy_loss: -26.56250, policy_entropy: -6.54792, alpha: 0.00365, time: 50.54812
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   314 ----
[CW] collect: return: 95.44660, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 0.04990, qf2_loss: 0.04937, policy_loss: -26.52886, policy_entropy: -6.23472, alpha: 0.00366, time: 50.94695
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   315 ----
[CW] collect: return: 85.32667, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 0.04504, qf2_loss: 0.04423, policy_loss: -26.46826, policy_entropy: -5.86101, alpha: 0.00367, time: 50.61532
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   316 ----
[CW] collect: return: 63.91061, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 0.05300, qf2_loss: 0.05202, policy_loss: -26.41368, policy_entropy: -5.75303, alpha: 0.00366, time: 50.54047
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   317 ----
[CW] collect: return: 89.86052, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 0.05165, qf2_loss: 0.05050, policy_loss: -26.40614, policy_entropy: -6.10307, alpha: 0.00364, time: 50.55241
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   318 ----
[CW] collect: return: 62.42146, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 0.05840, qf2_loss: 0.05768, policy_loss: -26.36645, policy_entropy: -6.01670, alpha: 0.00365, time: 50.25924
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   319 ----
[CW] collect: return: 57.11389, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 0.05282, qf2_loss: 0.05235, policy_loss: -26.32821, policy_entropy: -6.15619, alpha: 0.00365, time: 50.62657
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   320 ----
[CW] collect: return: 57.17968, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 0.05412, qf2_loss: 0.05336, policy_loss: -26.24045, policy_entropy: -6.05788, alpha: 0.00367, time: 50.92737
[CW] eval: return: 73.83470, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   321 ----
[CW] collect: return: 77.13676, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 0.05885, qf2_loss: 0.05791, policy_loss: -26.21836, policy_entropy: -5.81309, alpha: 0.00366, time: 50.85842
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   322 ----
[CW] collect: return: 60.28026, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 0.06334, qf2_loss: 0.06176, policy_loss: -26.15232, policy_entropy: -5.74992, alpha: 0.00364, time: 50.48210
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   323 ----
[CW] collect: return: 71.54171, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 0.05712, qf2_loss: 0.05629, policy_loss: -26.12964, policy_entropy: -5.67969, alpha: 0.00363, time: 50.49380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   324 ----
[CW] collect: return: 83.21840, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 0.06317, qf2_loss: 0.06186, policy_loss: -26.12259, policy_entropy: -5.85899, alpha: 0.00358, time: 50.56451
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   325 ----
[CW] collect: return: 69.01520, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 0.05316, qf2_loss: 0.05212, policy_loss: -26.07735, policy_entropy: -5.56415, alpha: 0.00357, time: 50.69172
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   326 ----
[CW] collect: return: 91.26476, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 0.05439, qf2_loss: 0.05371, policy_loss: -26.06649, policy_entropy: -5.84922, alpha: 0.00353, time: 51.05147
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   327 ----
[CW] collect: return: 83.79399, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 0.04848, qf2_loss: 0.04782, policy_loss: -26.01846, policy_entropy: -5.24697, alpha: 0.00348, time: 50.79457
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   328 ----
[CW] collect: return: 92.77308, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 0.06172, qf2_loss: 0.05989, policy_loss: -25.95261, policy_entropy: -5.80963, alpha: 0.00346, time: 50.82641
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   329 ----
[CW] collect: return: 66.39555, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 0.04917, qf2_loss: 0.04856, policy_loss: -25.94396, policy_entropy: -6.01305, alpha: 0.00343, time: 50.65190
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   330 ----
[CW] collect: return: 71.62356, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 0.05344, qf2_loss: 0.05279, policy_loss: -25.92361, policy_entropy: -5.98052, alpha: 0.00343, time: 50.88464
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   331 ----
[CW] collect: return: 73.62770, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 0.05432, qf2_loss: 0.05346, policy_loss: -25.88528, policy_entropy: -6.25736, alpha: 0.00344, time: 50.95156
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   332 ----
[CW] collect: return: 72.95654, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 0.04444, qf2_loss: 0.04326, policy_loss: -25.87957, policy_entropy: -6.12558, alpha: 0.00346, time: 50.64283
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   333 ----
[CW] collect: return: 126.04412, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 0.05395, qf2_loss: 0.05286, policy_loss: -25.89018, policy_entropy: -6.07161, alpha: 0.00348, time: 50.73163
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   334 ----
[CW] collect: return: 133.21602, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 0.04582, qf2_loss: 0.04551, policy_loss: -25.87052, policy_entropy: -6.48941, alpha: 0.00350, time: 50.90467
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   335 ----
[CW] collect: return: 93.37490, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 0.05497, qf2_loss: 0.05417, policy_loss: -25.83083, policy_entropy: -6.00990, alpha: 0.00354, time: 51.05960
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   336 ----
[CW] collect: return: 88.99693, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 0.05136, qf2_loss: 0.05064, policy_loss: -25.80963, policy_entropy: -6.25345, alpha: 0.00355, time: 50.80996
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   337 ----
[CW] collect: return: 124.06213, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 0.04757, qf2_loss: 0.04681, policy_loss: -25.79721, policy_entropy: -6.61431, alpha: 0.00360, time: 50.87603
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   338 ----
[CW] collect: return: 113.87472, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 0.04925, qf2_loss: 0.04827, policy_loss: -25.79853, policy_entropy: -6.67527, alpha: 0.00371, time: 50.48208
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   339 ----
[CW] collect: return: 54.69221, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 0.05191, qf2_loss: 0.05150, policy_loss: -25.80791, policy_entropy: -6.34560, alpha: 0.00378, time: 51.06249
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   340 ----
[CW] collect: return: 120.11994, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 0.04996, qf2_loss: 0.04896, policy_loss: -25.77923, policy_entropy: -6.28899, alpha: 0.00382, time: 51.28555
[CW] eval: return: 113.43791, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   341 ----
[CW] collect: return: 97.45964, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 0.04413, qf2_loss: 0.04308, policy_loss: -25.77984, policy_entropy: -5.97240, alpha: 0.00384, time: 51.13361
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   342 ----
[CW] collect: return: 89.01188, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 0.04346, qf2_loss: 0.04291, policy_loss: -25.76479, policy_entropy: -5.97681, alpha: 0.00384, time: 50.96666
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   343 ----
[CW] collect: return: 100.76309, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 0.04321, qf2_loss: 0.04288, policy_loss: -25.73146, policy_entropy: -6.12426, alpha: 0.00385, time: 50.99545
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   344 ----
[CW] collect: return: 100.48575, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 0.03999, qf2_loss: 0.03943, policy_loss: -25.72857, policy_entropy: -6.17602, alpha: 0.00387, time: 50.99623
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   345 ----
[CW] collect: return: 85.10560, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 0.05073, qf2_loss: 0.04962, policy_loss: -25.70236, policy_entropy: -5.90374, alpha: 0.00390, time: 51.00602
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   346 ----
[CW] collect: return: 113.64926, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 0.04429, qf2_loss: 0.04375, policy_loss: -25.71690, policy_entropy: -6.27788, alpha: 0.00389, time: 50.69344
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   347 ----
[CW] collect: return: 95.41304, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 0.04091, qf2_loss: 0.04064, policy_loss: -25.71145, policy_entropy: -6.33516, alpha: 0.00395, time: 51.05825
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   348 ----
[CW] collect: return: 78.26433, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 0.03983, qf2_loss: 0.03966, policy_loss: -25.65329, policy_entropy: -6.20512, alpha: 0.00400, time: 51.03249
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   349 ----
[CW] collect: return: 101.90404, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 0.04512, qf2_loss: 0.04365, policy_loss: -25.61590, policy_entropy: -6.28489, alpha: 0.00404, time: 50.98756
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   350 ----
[CW] collect: return: 74.14635, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 0.03920, qf2_loss: 0.03856, policy_loss: -25.60663, policy_entropy: -5.85620, alpha: 0.00409, time: 51.13219
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   351 ----
[CW] collect: return: 27.25736, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 0.04188, qf2_loss: 0.04154, policy_loss: -25.55486, policy_entropy: -4.65397, alpha: 0.00395, time: 51.09753
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   352 ----
[CW] collect: return: 92.04592, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 0.03692, qf2_loss: 0.03660, policy_loss: -25.52887, policy_entropy: -4.79663, alpha: 0.00374, time: 50.51381
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   353 ----
[CW] collect: return: 77.21750, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 0.04417, qf2_loss: 0.04319, policy_loss: -25.49813, policy_entropy: -4.96643, alpha: 0.00360, time: 50.92222
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   354 ----
[CW] collect: return: 97.98086, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 0.04058, qf2_loss: 0.04053, policy_loss: -25.46196, policy_entropy: -5.09383, alpha: 0.00348, time: 50.99090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   355 ----
[CW] collect: return: 96.53036, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 0.03722, qf2_loss: 0.03630, policy_loss: -25.40301, policy_entropy: -4.88715, alpha: 0.00338, time: 50.79657
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   356 ----
[CW] collect: return: 79.63713, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 0.03256, qf2_loss: 0.03208, policy_loss: -25.41086, policy_entropy: -5.10056, alpha: 0.00328, time: 51.12995
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   357 ----
[CW] collect: return: 95.56563, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 0.03953, qf2_loss: 0.03889, policy_loss: -25.33173, policy_entropy: -5.02457, alpha: 0.00320, time: 50.67350
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   358 ----
[CW] collect: return: 108.95137, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 0.03742, qf2_loss: 0.03729, policy_loss: -25.29662, policy_entropy: -5.10848, alpha: 0.00312, time: 50.67054
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   359 ----
[CW] collect: return: 93.72215, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 0.03272, qf2_loss: 0.03189, policy_loss: -25.26599, policy_entropy: -5.31395, alpha: 0.00305, time: 52.75788
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   360 ----
[CW] collect: return: 82.97433, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 0.05751, qf2_loss: 0.05651, policy_loss: -25.21817, policy_entropy: -5.63482, alpha: 0.00301, time: 51.00077
[CW] eval: return: 108.72000, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   361 ----
[CW] collect: return: 105.35819, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 0.03443, qf2_loss: 0.03404, policy_loss: -25.19690, policy_entropy: -5.44202, alpha: 0.00298, time: 50.78742
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   362 ----
[CW] collect: return: 88.34902, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 0.03506, qf2_loss: 0.03462, policy_loss: -25.13842, policy_entropy: -5.26535, alpha: 0.00292, time: 50.63652
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   363 ----
[CW] collect: return: 107.10083, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 0.04248, qf2_loss: 0.04197, policy_loss: -25.12534, policy_entropy: -5.62920, alpha: 0.00287, time: 50.74198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   364 ----
[CW] collect: return: 106.86117, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 0.02967, qf2_loss: 0.02973, policy_loss: -25.06375, policy_entropy: -6.28649, alpha: 0.00286, time: 50.72012
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   365 ----
[CW] collect: return: 109.60217, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 0.03776, qf2_loss: 0.03695, policy_loss: -25.06581, policy_entropy: -6.20596, alpha: 0.00288, time: 50.80829
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   366 ----
[CW] collect: return: 118.11514, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 0.02819, qf2_loss: 0.02848, policy_loss: -25.02996, policy_entropy: -6.56130, alpha: 0.00292, time: 50.94151
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   367 ----
[CW] collect: return: 107.95421, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 0.03497, qf2_loss: 0.03500, policy_loss: -24.99471, policy_entropy: -6.82194, alpha: 0.00298, time: 52.93501
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   368 ----
[CW] collect: return: 18.79833, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 0.04435, qf2_loss: 0.04384, policy_loss: -24.92493, policy_entropy: -6.14904, alpha: 0.00302, time: 50.69424
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   369 ----
[CW] collect: return: 117.24515, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 0.04394, qf2_loss: 0.04360, policy_loss: -24.93640, policy_entropy: -6.64651, alpha: 0.00307, time: 50.88821
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   370 ----
[CW] collect: return: 122.30646, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 0.03192, qf2_loss: 0.03199, policy_loss: -24.87595, policy_entropy: -6.78876, alpha: 0.00314, time: 50.83081
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   371 ----
[CW] collect: return: 123.63356, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 0.03858, qf2_loss: 0.03813, policy_loss: -24.85913, policy_entropy: -7.13122, alpha: 0.00324, time: 51.10887
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   372 ----
[CW] collect: return: 123.90941, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 0.05151, qf2_loss: 0.05026, policy_loss: -24.82302, policy_entropy: -6.59029, alpha: 0.00334, time: 50.76150
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   373 ----
[CW] collect: return: 121.29003, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 0.03645, qf2_loss: 0.03696, policy_loss: -24.80355, policy_entropy: -6.65186, alpha: 0.00340, time: 50.65068
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   374 ----
[CW] collect: return: 114.31799, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 0.03668, qf2_loss: 0.03638, policy_loss: -24.77085, policy_entropy: -6.96161, alpha: 0.00348, time: 50.90916
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   375 ----
[CW] collect: return: 128.71640, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 0.04109, qf2_loss: 0.04127, policy_loss: -24.74770, policy_entropy: -6.97430, alpha: 0.00360, time: 50.94628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   376 ----
[CW] collect: return: 122.93870, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 0.04990, qf2_loss: 0.04894, policy_loss: -24.73304, policy_entropy: -7.08167, alpha: 0.00373, time: 50.78045
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   377 ----
[CW] collect: return: 133.50509, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 0.04904, qf2_loss: 0.04936, policy_loss: -24.75401, policy_entropy: -7.35057, alpha: 0.00388, time: 50.80935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   378 ----
[CW] collect: return: 119.45405, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 0.04387, qf2_loss: 0.04344, policy_loss: -24.72064, policy_entropy: -7.08598, alpha: 0.00405, time: 50.77135
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   379 ----
[CW] collect: return: 47.40340, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 0.05535, qf2_loss: 0.05463, policy_loss: -24.68712, policy_entropy: -6.63091, alpha: 0.00417, time: 51.01010
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   380 ----
[CW] collect: return: 128.43505, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 0.04370, qf2_loss: 0.04389, policy_loss: -24.65551, policy_entropy: -6.58707, alpha: 0.00426, time: 50.74899
[CW] eval: return: 120.12036, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   381 ----
[CW] collect: return: 116.76150, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 0.04343, qf2_loss: 0.04326, policy_loss: -24.63000, policy_entropy: -6.25369, alpha: 0.00432, time: 51.08780
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   382 ----
[CW] collect: return: 120.21280, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 0.04206, qf2_loss: 0.04222, policy_loss: -24.62201, policy_entropy: -6.55917, alpha: 0.00438, time: 50.69826
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   383 ----
[CW] collect: return: 135.57717, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 0.05390, qf2_loss: 0.05357, policy_loss: -24.59670, policy_entropy: -6.27999, alpha: 0.00446, time: 50.80991
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   384 ----
[CW] collect: return: 126.82651, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 0.04674, qf2_loss: 0.04629, policy_loss: -24.61408, policy_entropy: -6.82910, alpha: 0.00455, time: 51.10494
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   385 ----
[CW] collect: return: 132.40484, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 0.04930, qf2_loss: 0.04947, policy_loss: -24.59501, policy_entropy: -6.33636, alpha: 0.00466, time: 50.81595
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   386 ----
[CW] collect: return: 125.53398, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 0.06385, qf2_loss: 0.06298, policy_loss: -24.59256, policy_entropy: -6.45391, alpha: 0.00471, time: 50.98524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   387 ----
[CW] collect: return: 127.18915, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 0.04708, qf2_loss: 0.04729, policy_loss: -24.60836, policy_entropy: -6.13943, alpha: 0.00481, time: 50.76019
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   388 ----
[CW] collect: return: 116.68064, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 0.05131, qf2_loss: 0.05051, policy_loss: -24.59725, policy_entropy: -6.11986, alpha: 0.00481, time: 51.06026
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   389 ----
[CW] collect: return: 122.59534, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 0.05619, qf2_loss: 0.05577, policy_loss: -24.59018, policy_entropy: -6.11035, alpha: 0.00483, time: 51.04863
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   390 ----
[CW] collect: return: 105.36749, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 0.05355, qf2_loss: 0.05318, policy_loss: -24.60832, policy_entropy: -6.33074, alpha: 0.00488, time: 51.45785
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   391 ----
[CW] collect: return: 130.83010, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 0.05490, qf2_loss: 0.05511, policy_loss: -24.61435, policy_entropy: -5.91603, alpha: 0.00493, time: 51.05427
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   392 ----
[CW] collect: return: 133.32062, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 0.05759, qf2_loss: 0.05780, policy_loss: -24.60490, policy_entropy: -5.80041, alpha: 0.00488, time: 50.90796
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   393 ----
[CW] collect: return: 55.27465, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 0.06645, qf2_loss: 0.06684, policy_loss: -24.58157, policy_entropy: -6.09366, alpha: 0.00487, time: 50.86412
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   394 ----
[CW] collect: return: 112.16452, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 0.06271, qf2_loss: 0.06114, policy_loss: -24.59899, policy_entropy: -5.85012, alpha: 0.00486, time: 50.78594
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   395 ----
[CW] collect: return: 67.08343, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 0.06077, qf2_loss: 0.06000, policy_loss: -24.58672, policy_entropy: -5.81408, alpha: 0.00483, time: 51.17343
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   396 ----
[CW] collect: return: 125.15189, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 0.05811, qf2_loss: 0.05750, policy_loss: -24.59423, policy_entropy: -5.83170, alpha: 0.00478, time: 50.93317
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   397 ----
[CW] collect: return: 56.78154, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 0.05585, qf2_loss: 0.05515, policy_loss: -24.53946, policy_entropy: -6.18271, alpha: 0.00479, time: 50.97383
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   398 ----
[CW] collect: return: 125.42355, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 0.05610, qf2_loss: 0.05574, policy_loss: -24.56950, policy_entropy: -6.11037, alpha: 0.00483, time: 50.55816
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   399 ----
[CW] collect: return: 126.25590, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 0.05418, qf2_loss: 0.05440, policy_loss: -24.56505, policy_entropy: -5.80118, alpha: 0.00482, time: 50.67692
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   400 ----
[CW] collect: return: 128.25463, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 0.05672, qf2_loss: 0.05667, policy_loss: -24.58384, policy_entropy: -5.82738, alpha: 0.00477, time: 50.89905
[CW] eval: return: 101.19022, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   401 ----
[CW] collect: return: 67.32158, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 0.05544, qf2_loss: 0.05494, policy_loss: -24.56777, policy_entropy: -5.79883, alpha: 0.00473, time: 51.03311
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   402 ----
[CW] collect: return: 143.35418, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 0.05404, qf2_loss: 0.05339, policy_loss: -24.57874, policy_entropy: -5.65971, alpha: 0.00465, time: 50.84526
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   403 ----
[CW] collect: return: 116.10956, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 0.07231, qf2_loss: 0.07042, policy_loss: -24.55837, policy_entropy: -5.95546, alpha: 0.00461, time: 51.23359
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   404 ----
[CW] collect: return: 119.97004, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 0.07763, qf2_loss: 0.07689, policy_loss: -24.57516, policy_entropy: -5.59569, alpha: 0.00457, time: 51.20123
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   405 ----
[CW] collect: return: 116.79337, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 0.05244, qf2_loss: 0.05196, policy_loss: -24.56782, policy_entropy: -5.89334, alpha: 0.00450, time: 50.84210
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   406 ----
[CW] collect: return: 110.44798, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 0.05622, qf2_loss: 0.05575, policy_loss: -24.56410, policy_entropy: -5.47591, alpha: 0.00444, time: 50.77141
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   407 ----
[CW] collect: return: 131.59098, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 0.05567, qf2_loss: 0.05552, policy_loss: -24.57576, policy_entropy: -5.95453, alpha: 0.00437, time: 50.72446
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   408 ----
[CW] collect: return: 79.27961, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 0.05262, qf2_loss: 0.05216, policy_loss: -24.56422, policy_entropy: -6.00662, alpha: 0.00438, time: 50.83401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   409 ----
[CW] collect: return: 47.70234, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 0.05658, qf2_loss: 0.05691, policy_loss: -24.52315, policy_entropy: -5.74383, alpha: 0.00436, time: 51.07393
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   410 ----
[CW] collect: return: 128.65237, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 0.07408, qf2_loss: 0.07296, policy_loss: -24.53623, policy_entropy: -5.52256, alpha: 0.00427, time: 51.07353
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   411 ----
[CW] collect: return: 71.82233, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 0.05988, qf2_loss: 0.05993, policy_loss: -24.54041, policy_entropy: -5.42142, alpha: 0.00416, time: 50.99712
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   412 ----
[CW] collect: return: 130.44747, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 0.05174, qf2_loss: 0.05111, policy_loss: -24.50648, policy_entropy: -5.58414, alpha: 0.00405, time: 50.65234
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   413 ----
[CW] collect: return: 142.96810, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 0.06371, qf2_loss: 0.06217, policy_loss: -24.48823, policy_entropy: -5.88301, alpha: 0.00400, time: 51.24533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   414 ----
[CW] collect: return: 141.60206, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 0.05388, qf2_loss: 0.05381, policy_loss: -24.48835, policy_entropy: -5.72147, alpha: 0.00398, time: 51.08823
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   415 ----
[CW] collect: return: 115.56098, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 0.05896, qf2_loss: 0.05846, policy_loss: -24.50781, policy_entropy: -5.68180, alpha: 0.00391, time: 50.96700
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   416 ----
[CW] collect: return: 138.98075, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 0.05520, qf2_loss: 0.05453, policy_loss: -24.49574, policy_entropy: -6.12477, alpha: 0.00390, time: 51.02033
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   417 ----
[CW] collect: return: 138.36937, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 0.06071, qf2_loss: 0.06083, policy_loss: -24.47808, policy_entropy: -6.24206, alpha: 0.00392, time: 51.02481
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   418 ----
[CW] collect: return: 136.48678, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 0.06268, qf2_loss: 0.06238, policy_loss: -24.47326, policy_entropy: -6.18558, alpha: 0.00398, time: 50.89941
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   419 ----
[CW] collect: return: 143.68499, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 0.05501, qf2_loss: 0.05468, policy_loss: -24.48200, policy_entropy: -5.95656, alpha: 0.00397, time: 51.02597
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   420 ----
[CW] collect: return: 140.41367, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 0.06111, qf2_loss: 0.05984, policy_loss: -24.48345, policy_entropy: -5.86177, alpha: 0.00397, time: 50.85940
[CW] eval: return: 135.31156, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   421 ----
[CW] collect: return: 149.43821, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 0.05938, qf2_loss: 0.05932, policy_loss: -24.45699, policy_entropy: -5.89866, alpha: 0.00394, time: 50.97161
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   422 ----
[CW] collect: return: 32.40015, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 0.05926, qf2_loss: 0.05806, policy_loss: -24.47127, policy_entropy: -6.09844, alpha: 0.00394, time: 50.87466
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   423 ----
[CW] collect: return: 139.48860, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 0.05658, qf2_loss: 0.05589, policy_loss: -24.44647, policy_entropy: -5.87088, alpha: 0.00395, time: 50.99073
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   424 ----
[CW] collect: return: 130.48109, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 0.05181, qf2_loss: 0.05196, policy_loss: -24.46156, policy_entropy: -5.84095, alpha: 0.00392, time: 50.74174
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   425 ----
[CW] collect: return: 133.06433, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 0.06046, qf2_loss: 0.05958, policy_loss: -24.45146, policy_entropy: -5.90698, alpha: 0.00388, time: 50.98310
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   426 ----
[CW] collect: return: 129.87933, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 0.05255, qf2_loss: 0.05234, policy_loss: -24.42315, policy_entropy: -6.15650, alpha: 0.00389, time: 51.37455
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   427 ----
[CW] collect: return: 141.79300, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 0.05433, qf2_loss: 0.05400, policy_loss: -24.40304, policy_entropy: -5.95588, alpha: 0.00390, time: 51.18973
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   428 ----
[CW] collect: return: 130.53507, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 0.06149, qf2_loss: 0.06108, policy_loss: -24.39675, policy_entropy: -6.14040, alpha: 0.00390, time: 51.05943
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   429 ----
[CW] collect: return: 133.37632, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 0.06347, qf2_loss: 0.06325, policy_loss: -24.41371, policy_entropy: -6.20151, alpha: 0.00395, time: 51.14403
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   430 ----
[CW] collect: return: 138.73354, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 0.06073, qf2_loss: 0.05983, policy_loss: -24.40504, policy_entropy: -6.00020, alpha: 0.00397, time: 51.41032
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   431 ----
[CW] collect: return: 114.45470, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 0.05805, qf2_loss: 0.05720, policy_loss: -24.40077, policy_entropy: -6.14571, alpha: 0.00398, time: 51.27876
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   432 ----
[CW] collect: return: 136.43460, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 0.05139, qf2_loss: 0.05205, policy_loss: -24.41371, policy_entropy: -6.33701, alpha: 0.00403, time: 50.63678
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   433 ----
[CW] collect: return: 136.15432, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 0.05692, qf2_loss: 0.05741, policy_loss: -24.39373, policy_entropy: -6.20928, alpha: 0.00410, time: 50.72134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   434 ----
[CW] collect: return: 132.03603, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 0.05242, qf2_loss: 0.05188, policy_loss: -24.38124, policy_entropy: -6.17533, alpha: 0.00414, time: 51.16461
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   435 ----
[CW] collect: return: 137.09315, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 0.05119, qf2_loss: 0.05103, policy_loss: -24.38014, policy_entropy: -6.06301, alpha: 0.00418, time: 50.82243
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   436 ----
[CW] collect: return: 142.20527, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 0.05612, qf2_loss: 0.05585, policy_loss: -24.40402, policy_entropy: -6.26581, alpha: 0.00421, time: 50.57758
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   437 ----
[CW] collect: return: 133.59382, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 0.06315, qf2_loss: 0.06251, policy_loss: -24.36952, policy_entropy: -6.08706, alpha: 0.00427, time: 52.21072
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   438 ----
[CW] collect: return: 162.40682, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 0.05883, qf2_loss: 0.05855, policy_loss: -24.35019, policy_entropy: -6.00259, alpha: 0.00428, time: 51.07727
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   439 ----
[CW] collect: return: 117.03549, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 0.05416, qf2_loss: 0.05446, policy_loss: -24.35770, policy_entropy: -6.17064, alpha: 0.00429, time: 51.13671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   440 ----
[CW] collect: return: 150.35186, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 0.04882, qf2_loss: 0.04870, policy_loss: -24.36560, policy_entropy: -6.03165, alpha: 0.00433, time: 50.92439
[CW] eval: return: 146.14290, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   441 ----
[CW] collect: return: 136.10712, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 0.05010, qf2_loss: 0.04985, policy_loss: -24.33259, policy_entropy: -5.98222, alpha: 0.00433, time: 51.13655
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   442 ----
[CW] collect: return: 161.18879, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 0.06144, qf2_loss: 0.06068, policy_loss: -24.34898, policy_entropy: -6.45400, alpha: 0.00437, time: 50.77837
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   443 ----
[CW] collect: return: 142.14031, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 0.05129, qf2_loss: 0.05066, policy_loss: -24.34324, policy_entropy: -6.22175, alpha: 0.00447, time: 50.99729
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   444 ----
[CW] collect: return: 138.45703, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 0.06128, qf2_loss: 0.06145, policy_loss: -24.34228, policy_entropy: -6.33316, alpha: 0.00453, time: 50.77234
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   445 ----
[CW] collect: return: 147.85768, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 0.05982, qf2_loss: 0.05972, policy_loss: -24.32251, policy_entropy: -6.23409, alpha: 0.00462, time: 50.51816
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   446 ----
[CW] collect: return: 150.15179, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 0.05451, qf2_loss: 0.05438, policy_loss: -24.33332, policy_entropy: -6.01361, alpha: 0.00467, time: 50.94847
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   447 ----
[CW] collect: return: 142.59296, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 0.05237, qf2_loss: 0.05257, policy_loss: -24.32316, policy_entropy: -5.79658, alpha: 0.00464, time: 51.11748
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   448 ----
[CW] collect: return: 157.87923, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 0.05422, qf2_loss: 0.05390, policy_loss: -24.37234, policy_entropy: -6.25907, alpha: 0.00464, time: 50.84177
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   449 ----
[CW] collect: return: 141.74268, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 0.05532, qf2_loss: 0.05478, policy_loss: -24.34977, policy_entropy: -6.05623, alpha: 0.00468, time: 50.42400
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   450 ----
[CW] collect: return: 133.56425, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 0.05393, qf2_loss: 0.05431, policy_loss: -24.30298, policy_entropy: -6.19449, alpha: 0.00473, time: 50.64525
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   451 ----
[CW] collect: return: 144.46598, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 0.06190, qf2_loss: 0.06126, policy_loss: -24.31802, policy_entropy: -6.03890, alpha: 0.00475, time: 50.99045
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   452 ----
[CW] collect: return: 128.02560, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 0.05544, qf2_loss: 0.05578, policy_loss: -24.36876, policy_entropy: -6.08712, alpha: 0.00479, time: 50.57651
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   453 ----
[CW] collect: return: 171.35510, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 0.06370, qf2_loss: 0.06276, policy_loss: -24.28881, policy_entropy: -5.73656, alpha: 0.00475, time: 51.01406
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   454 ----
[CW] collect: return: 149.90636, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 0.05926, qf2_loss: 0.05950, policy_loss: -24.32734, policy_entropy: -6.23579, alpha: 0.00475, time: 50.74832
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   455 ----
[CW] collect: return: 150.20125, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 0.05201, qf2_loss: 0.05184, policy_loss: -24.31540, policy_entropy: -5.92683, alpha: 0.00477, time: 50.83168
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   456 ----
[CW] collect: return: 130.36895, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 0.05311, qf2_loss: 0.05335, policy_loss: -24.31724, policy_entropy: -5.94085, alpha: 0.00476, time: 51.12298
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   457 ----
[CW] collect: return: 149.83624, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 0.06958, qf2_loss: 0.06881, policy_loss: -24.32239, policy_entropy: -5.82416, alpha: 0.00473, time: 51.38632
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   458 ----
[CW] collect: return: 169.71550, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 0.05478, qf2_loss: 0.05490, policy_loss: -24.28199, policy_entropy: -5.93125, alpha: 0.00467, time: 50.97258
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   459 ----
[CW] collect: return: 129.20294, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 0.05869, qf2_loss: 0.05875, policy_loss: -24.32290, policy_entropy: -6.09593, alpha: 0.00468, time: 50.96830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   460 ----
[CW] collect: return: 149.92283, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 0.06780, qf2_loss: 0.06779, policy_loss: -24.33087, policy_entropy: -6.28367, alpha: 0.00473, time: 50.86208
[CW] eval: return: 160.01486, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   461 ----
[CW] collect: return: 167.48583, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 0.05371, qf2_loss: 0.05372, policy_loss: -24.30575, policy_entropy: -6.04839, alpha: 0.00478, time: 51.08475
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   462 ----
[CW] collect: return: 187.50823, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 0.05735, qf2_loss: 0.05733, policy_loss: -24.36368, policy_entropy: -6.11953, alpha: 0.00481, time: 50.81911
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   463 ----
[CW] collect: return: 164.30120, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 0.06740, qf2_loss: 0.06814, policy_loss: -24.30370, policy_entropy: -5.83150, alpha: 0.00482, time: 50.62911
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   464 ----
[CW] collect: return: 154.85551, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 0.05794, qf2_loss: 0.05769, policy_loss: -24.33344, policy_entropy: -5.92336, alpha: 0.00476, time: 50.82735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   465 ----
[CW] collect: return: 144.26153, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 0.05728, qf2_loss: 0.05764, policy_loss: -24.33649, policy_entropy: -6.21083, alpha: 0.00479, time: 50.71990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   466 ----
[CW] collect: return: 176.38603, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 0.06183, qf2_loss: 0.06110, policy_loss: -24.31735, policy_entropy: -6.13390, alpha: 0.00482, time: 51.12134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   467 ----
[CW] collect: return: 148.74481, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 0.05810, qf2_loss: 0.05814, policy_loss: -24.37711, policy_entropy: -6.31581, alpha: 0.00491, time: 50.88228
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   468 ----
[CW] collect: return: 162.60065, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 0.07431, qf2_loss: 0.07364, policy_loss: -24.32046, policy_entropy: -6.12324, alpha: 0.00498, time: 50.90516
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   469 ----
[CW] collect: return: 163.21432, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 0.06740, qf2_loss: 0.06761, policy_loss: -24.34926, policy_entropy: -6.19402, alpha: 0.00503, time: 51.18506
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   470 ----
[CW] collect: return: 179.36261, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 0.05919, qf2_loss: 0.05866, policy_loss: -24.38075, policy_entropy: -6.02060, alpha: 0.00505, time: 51.32338
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   471 ----
[CW] collect: return: 172.58545, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 0.06144, qf2_loss: 0.06204, policy_loss: -24.38488, policy_entropy: -6.11089, alpha: 0.00505, time: 50.77031
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   472 ----
[CW] collect: return: 180.00917, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 0.05820, qf2_loss: 0.05789, policy_loss: -24.40942, policy_entropy: -6.15049, alpha: 0.00511, time: 50.97507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   473 ----
[CW] collect: return: 166.24760, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 0.08156, qf2_loss: 0.08144, policy_loss: -24.37662, policy_entropy: -5.83232, alpha: 0.00510, time: 50.78937
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   474 ----
[CW] collect: return: 142.62894, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 0.05857, qf2_loss: 0.05872, policy_loss: -24.40859, policy_entropy: -6.34548, alpha: 0.00513, time: 50.74172
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   475 ----
[CW] collect: return: 168.89289, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 0.06512, qf2_loss: 0.06488, policy_loss: -24.42464, policy_entropy: -6.51618, alpha: 0.00527, time: 51.14649
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   476 ----
[CW] collect: return: 138.19517, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 0.07080, qf2_loss: 0.07081, policy_loss: -24.43399, policy_entropy: -6.23828, alpha: 0.00540, time: 50.67099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   477 ----
[CW] collect: return: 133.60415, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 0.07051, qf2_loss: 0.07025, policy_loss: -24.50017, policy_entropy: -6.33559, alpha: 0.00549, time: 50.80542
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   478 ----
[CW] collect: return: 154.58513, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 0.06170, qf2_loss: 0.06242, policy_loss: -24.43517, policy_entropy: -6.10185, alpha: 0.00560, time: 50.54737
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   479 ----
[CW] collect: return: 154.67797, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 0.06530, qf2_loss: 0.06596, policy_loss: -24.43880, policy_entropy: -5.94373, alpha: 0.00558, time: 51.52248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   480 ----
[CW] collect: return: 141.58582, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 0.07891, qf2_loss: 0.07691, policy_loss: -24.47866, policy_entropy: -5.95523, alpha: 0.00555, time: 51.21096
[CW] eval: return: 162.86213, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   481 ----
[CW] collect: return: 145.70937, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 0.06990, qf2_loss: 0.07045, policy_loss: -24.40673, policy_entropy: -5.97314, alpha: 0.00555, time: 51.14371
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   482 ----
[CW] collect: return: 178.56040, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 0.06335, qf2_loss: 0.06359, policy_loss: -24.47809, policy_entropy: -6.03736, alpha: 0.00555, time: 51.13422
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   483 ----
[CW] collect: return: 145.94243, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 0.06701, qf2_loss: 0.06752, policy_loss: -24.47781, policy_entropy: -6.16196, alpha: 0.00558, time: 50.99264
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   484 ----
[CW] collect: return: 140.39372, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 0.07437, qf2_loss: 0.07316, policy_loss: -24.49822, policy_entropy: -6.24150, alpha: 0.00565, time: 50.74062
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   485 ----
[CW] collect: return: 147.31098, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 0.07375, qf2_loss: 0.07423, policy_loss: -24.51854, policy_entropy: -6.02272, alpha: 0.00569, time: 50.98430
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   486 ----
[CW] collect: return: 159.80167, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 0.07170, qf2_loss: 0.07118, policy_loss: -24.52418, policy_entropy: -5.95720, alpha: 0.00568, time: 51.39971
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   487 ----
[CW] collect: return: 182.49195, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 0.06277, qf2_loss: 0.06232, policy_loss: -24.51041, policy_entropy: -5.97254, alpha: 0.00568, time: 50.90589
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   488 ----
[CW] collect: return: 146.37635, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 0.06916, qf2_loss: 0.06989, policy_loss: -24.56369, policy_entropy: -6.20885, alpha: 0.00567, time: 51.11582
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   489 ----
[CW] collect: return: 139.08739, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 0.07117, qf2_loss: 0.07054, policy_loss: -24.52518, policy_entropy: -6.06847, alpha: 0.00576, time: 51.07112
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   490 ----
[CW] collect: return: 176.51976, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 0.07038, qf2_loss: 0.07035, policy_loss: -24.55458, policy_entropy: -6.10702, alpha: 0.00579, time: 50.92598
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   491 ----
[CW] collect: return: 159.93935, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 0.07691, qf2_loss: 0.07766, policy_loss: -24.56054, policy_entropy: -6.19488, alpha: 0.00581, time: 50.82514
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   492 ----
[CW] collect: return: 154.89561, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 0.06942, qf2_loss: 0.06917, policy_loss: -24.57970, policy_entropy: -6.07128, alpha: 0.00592, time: 51.35271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   493 ----
[CW] collect: return: 182.76855, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 0.06732, qf2_loss: 0.06759, policy_loss: -24.54655, policy_entropy: -5.49911, alpha: 0.00581, time: 51.26357
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   494 ----
[CW] collect: return: 179.60292, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 0.06723, qf2_loss: 0.06694, policy_loss: -24.64360, policy_entropy: -6.10411, alpha: 0.00575, time: 51.19029
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   495 ----
[CW] collect: return: 153.03538, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 0.06853, qf2_loss: 0.06869, policy_loss: -24.56500, policy_entropy: -5.68438, alpha: 0.00573, time: 50.93128
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   496 ----
[CW] collect: return: 171.00475, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 0.06344, qf2_loss: 0.06346, policy_loss: -24.62499, policy_entropy: -5.89944, alpha: 0.00564, time: 51.50559
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   497 ----
[CW] collect: return: 172.60335, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 0.07000, qf2_loss: 0.06991, policy_loss: -24.62369, policy_entropy: -5.53581, alpha: 0.00557, time: 50.84496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   498 ----
[CW] collect: return: 162.55536, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 0.08349, qf2_loss: 0.08207, policy_loss: -24.61473, policy_entropy: -5.78516, alpha: 0.00545, time: 50.55913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   499 ----
[CW] collect: return: 171.84557, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 0.07275, qf2_loss: 0.07342, policy_loss: -24.66396, policy_entropy: -5.91728, alpha: 0.00542, time: 52.20878
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   500 ----
[CW] collect: return: 158.36763, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 0.07357, qf2_loss: 0.07325, policy_loss: -24.65013, policy_entropy: -5.85781, alpha: 0.00538, time: 51.12087
[CW] eval: return: 177.63155, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   501 ----
[CW] collect: return: 163.19704, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 0.06508, qf2_loss: 0.06581, policy_loss: -24.64334, policy_entropy: -5.83932, alpha: 0.00533, time: 51.17249
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   502 ----
[CW] collect: return: 176.52376, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 0.06338, qf2_loss: 0.06402, policy_loss: -24.64018, policy_entropy: -6.00215, alpha: 0.00531, time: 51.34787
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   503 ----
[CW] collect: return: 154.87475, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 0.06711, qf2_loss: 0.06707, policy_loss: -24.61118, policy_entropy: -5.71217, alpha: 0.00528, time: 51.23189
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   504 ----
[CW] collect: return: 175.75511, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 0.06816, qf2_loss: 0.06773, policy_loss: -24.63975, policy_entropy: -6.21943, alpha: 0.00526, time: 51.17371
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   505 ----
[CW] collect: return: 175.83610, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 0.06886, qf2_loss: 0.06948, policy_loss: -24.62690, policy_entropy: -6.19562, alpha: 0.00531, time: 51.27174
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   506 ----
[CW] collect: return: 139.64749, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 0.07079, qf2_loss: 0.07045, policy_loss: -24.69687, policy_entropy: -6.21996, alpha: 0.00535, time: 51.85486
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   507 ----
[CW] collect: return: 174.32208, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 0.06657, qf2_loss: 0.06670, policy_loss: -24.68082, policy_entropy: -6.05209, alpha: 0.00544, time: 51.02539
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   508 ----
[CW] collect: return: 183.01295, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 0.09345, qf2_loss: 0.09293, policy_loss: -24.77869, policy_entropy: -6.31685, alpha: 0.00548, time: 50.89296
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   509 ----
[CW] collect: return: 169.23059, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 0.06825, qf2_loss: 0.06836, policy_loss: -24.68160, policy_entropy: -6.35586, alpha: 0.00557, time: 50.82492
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   510 ----
[CW] collect: return: 185.50933, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 0.07517, qf2_loss: 0.07437, policy_loss: -24.78242, policy_entropy: -6.29908, alpha: 0.00569, time: 50.96817
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   511 ----
[CW] collect: return: 182.34807, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 0.07631, qf2_loss: 0.07744, policy_loss: -24.76491, policy_entropy: -6.29545, alpha: 0.00579, time: 50.91559
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   512 ----
[CW] collect: return: 184.51217, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 0.07108, qf2_loss: 0.07103, policy_loss: -24.76239, policy_entropy: -5.78126, alpha: 0.00584, time: 50.78392
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   513 ----
[CW] collect: return: 183.79632, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 0.08147, qf2_loss: 0.08121, policy_loss: -24.78055, policy_entropy: -6.18655, alpha: 0.00581, time: 51.28885
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   514 ----
[CW] collect: return: 200.91367, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 0.08209, qf2_loss: 0.08218, policy_loss: -24.78116, policy_entropy: -5.99865, alpha: 0.00582, time: 50.94363
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   515 ----
[CW] collect: return: 208.46007, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 0.07030, qf2_loss: 0.07089, policy_loss: -24.79912, policy_entropy: -5.98638, alpha: 0.00585, time: 50.98689
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   516 ----
[CW] collect: return: 200.96660, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 0.07884, qf2_loss: 0.07808, policy_loss: -24.89400, policy_entropy: -6.03403, alpha: 0.00583, time: 51.15668
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   517 ----
[CW] collect: return: 196.93126, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 0.08256, qf2_loss: 0.08296, policy_loss: -24.82329, policy_entropy: -6.17744, alpha: 0.00588, time: 50.84283
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   518 ----
[CW] collect: return: 208.69590, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 0.07745, qf2_loss: 0.07799, policy_loss: -24.90109, policy_entropy: -5.71828, alpha: 0.00587, time: 51.72852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   519 ----
[CW] collect: return: 187.39656, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 0.06794, qf2_loss: 0.06884, policy_loss: -24.86620, policy_entropy: -5.80159, alpha: 0.00577, time: 51.09199
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   520 ----
[CW] collect: return: 159.88023, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 0.07468, qf2_loss: 0.07454, policy_loss: -24.86284, policy_entropy: -5.88523, alpha: 0.00570, time: 51.04921
[CW] eval: return: 179.68559, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   521 ----
[CW] collect: return: 155.65352, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 0.08053, qf2_loss: 0.08061, policy_loss: -24.90156, policy_entropy: -5.71124, alpha: 0.00566, time: 50.88719
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   522 ----
[CW] collect: return: 191.57234, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 0.07891, qf2_loss: 0.07922, policy_loss: -24.94048, policy_entropy: -5.98313, alpha: 0.00561, time: 50.92087
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   523 ----
[CW] collect: return: 195.03444, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 0.07828, qf2_loss: 0.07869, policy_loss: -24.94504, policy_entropy: -6.12101, alpha: 0.00560, time: 50.83579
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   524 ----
[CW] collect: return: 188.89963, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 0.07814, qf2_loss: 0.07886, policy_loss: -24.94731, policy_entropy: -6.36709, alpha: 0.00568, time: 50.75864
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   525 ----
[CW] collect: return: 205.10417, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 0.08672, qf2_loss: 0.08696, policy_loss: -24.97680, policy_entropy: -6.39595, alpha: 0.00580, time: 50.93073
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   526 ----
[CW] collect: return: 195.76819, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 0.08093, qf2_loss: 0.08138, policy_loss: -25.05067, policy_entropy: -6.37855, alpha: 0.00595, time: 50.96327
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   527 ----
[CW] collect: return: 176.82817, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 0.09839, qf2_loss: 0.09761, policy_loss: -25.07811, policy_entropy: -6.04359, alpha: 0.00603, time: 50.84375
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   528 ----
[CW] collect: return: 204.22792, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 0.09251, qf2_loss: 0.09404, policy_loss: -25.10740, policy_entropy: -6.00919, alpha: 0.00605, time: 51.06423
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   529 ----
[CW] collect: return: 187.50912, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 0.08329, qf2_loss: 0.08354, policy_loss: -25.12763, policy_entropy: -6.05245, alpha: 0.00606, time: 51.13578
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   530 ----
[CW] collect: return: 196.92461, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 0.07996, qf2_loss: 0.08059, policy_loss: -25.13126, policy_entropy: -5.94147, alpha: 0.00606, time: 50.97393
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   531 ----
[CW] collect: return: 184.39851, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 0.08081, qf2_loss: 0.08129, policy_loss: -25.13840, policy_entropy: -5.78205, alpha: 0.00603, time: 51.13545
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   532 ----
[CW] collect: return: 187.60483, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 0.08222, qf2_loss: 0.08282, policy_loss: -25.18979, policy_entropy: -5.78276, alpha: 0.00592, time: 51.44283
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   533 ----
[CW] collect: return: 195.40795, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 0.10447, qf2_loss: 0.10434, policy_loss: -25.21377, policy_entropy: -5.76533, alpha: 0.00584, time: 51.07565
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   534 ----
[CW] collect: return: 191.57826, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 0.08986, qf2_loss: 0.09021, policy_loss: -25.29171, policy_entropy: -6.13708, alpha: 0.00582, time: 50.90568
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   535 ----
[CW] collect: return: 193.41874, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 0.08857, qf2_loss: 0.08832, policy_loss: -25.30347, policy_entropy: -6.23398, alpha: 0.00589, time: 50.89297
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   536 ----
[CW] collect: return: 196.26196, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 0.08341, qf2_loss: 0.08354, policy_loss: -25.30297, policy_entropy: -6.03954, alpha: 0.00595, time: 51.18062
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   537 ----
[CW] collect: return: 172.23414, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 0.08490, qf2_loss: 0.08527, policy_loss: -25.32339, policy_entropy: -5.96812, alpha: 0.00595, time: 51.18598
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   538 ----
[CW] collect: return: 190.01498, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 0.09761, qf2_loss: 0.09888, policy_loss: -25.32420, policy_entropy: -6.24468, alpha: 0.00597, time: 50.75982
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   539 ----
[CW] collect: return: 185.84407, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 0.11608, qf2_loss: 0.11616, policy_loss: -25.39941, policy_entropy: -6.32689, alpha: 0.00607, time: 51.09491
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   540 ----
[CW] collect: return: 173.07995, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 0.10757, qf2_loss: 0.10745, policy_loss: -25.47453, policy_entropy: -6.23692, alpha: 0.00620, time: 50.77337
[CW] eval: return: 196.00757, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   541 ----
[CW] collect: return: 193.82409, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 0.09343, qf2_loss: 0.09385, policy_loss: -25.43732, policy_entropy: -6.19866, alpha: 0.00629, time: 51.42229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   542 ----
[CW] collect: return: 207.76262, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 0.09715, qf2_loss: 0.09832, policy_loss: -25.41492, policy_entropy: -5.88080, alpha: 0.00631, time: 50.87112
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   543 ----
[CW] collect: return: 207.18855, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 0.09367, qf2_loss: 0.09442, policy_loss: -25.53655, policy_entropy: -6.02697, alpha: 0.00628, time: 51.13942
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   544 ----
[CW] collect: return: 205.91879, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 0.09229, qf2_loss: 0.09249, policy_loss: -25.53485, policy_entropy: -5.86449, alpha: 0.00626, time: 51.11536
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   545 ----
[CW] collect: return: 223.35718, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 0.10411, qf2_loss: 0.10458, policy_loss: -25.47844, policy_entropy: -5.82016, alpha: 0.00621, time: 51.15227
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   546 ----
[CW] collect: return: 201.10824, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 0.09543, qf2_loss: 0.09591, policy_loss: -25.59836, policy_entropy: -6.00857, alpha: 0.00617, time: 50.84026
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   547 ----
[CW] collect: return: 217.03921, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 0.09472, qf2_loss: 0.09603, policy_loss: -25.58653, policy_entropy: -5.87985, alpha: 0.00614, time: 51.77799
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   548 ----
[CW] collect: return: 204.07819, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 0.10696, qf2_loss: 0.10694, policy_loss: -25.61524, policy_entropy: -6.19612, alpha: 0.00614, time: 51.56873
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   549 ----
[CW] collect: return: 208.49329, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 0.10046, qf2_loss: 0.10055, policy_loss: -25.62598, policy_entropy: -6.00271, alpha: 0.00618, time: 51.52499
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   550 ----
[CW] collect: return: 190.19203, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 0.09925, qf2_loss: 0.09957, policy_loss: -25.73186, policy_entropy: -5.86819, alpha: 0.00617, time: 51.53165
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   551 ----
[CW] collect: return: 198.54792, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 0.11333, qf2_loss: 0.11460, policy_loss: -25.75190, policy_entropy: -5.91462, alpha: 0.00613, time: 51.49993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   552 ----
[CW] collect: return: 202.42448, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 0.10620, qf2_loss: 0.10624, policy_loss: -25.72913, policy_entropy: -5.85700, alpha: 0.00607, time: 51.09698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   553 ----
[CW] collect: return: 216.62359, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 0.09833, qf2_loss: 0.09956, policy_loss: -25.78556, policy_entropy: -6.05602, alpha: 0.00606, time: 50.71547
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   554 ----
[CW] collect: return: 201.38451, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 0.10619, qf2_loss: 0.10612, policy_loss: -25.83024, policy_entropy: -6.13504, alpha: 0.00611, time: 51.17341
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   555 ----
[CW] collect: return: 201.84992, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 0.10358, qf2_loss: 0.10462, policy_loss: -25.79950, policy_entropy: -5.97061, alpha: 0.00613, time: 51.00965
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   556 ----
[CW] collect: return: 221.35705, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 0.10013, qf2_loss: 0.10156, policy_loss: -25.89958, policy_entropy: -6.10594, alpha: 0.00614, time: 50.77126
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   557 ----
[CW] collect: return: 192.05122, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 0.10268, qf2_loss: 0.10231, policy_loss: -25.86502, policy_entropy: -6.06679, alpha: 0.00615, time: 50.75359
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   558 ----
[CW] collect: return: 202.16892, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 0.10880, qf2_loss: 0.10923, policy_loss: -25.87712, policy_entropy: -6.04439, alpha: 0.00620, time: 50.45771
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   559 ----
[CW] collect: return: 186.80073, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 0.11217, qf2_loss: 0.11216, policy_loss: -25.93601, policy_entropy: -6.21995, alpha: 0.00625, time: 50.86900
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   560 ----
[CW] collect: return: 193.67964, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 0.11576, qf2_loss: 0.11633, policy_loss: -26.00915, policy_entropy: -6.23886, alpha: 0.00634, time: 50.85608
[CW] eval: return: 199.17731, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   561 ----
[CW] collect: return: 204.36250, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 0.11643, qf2_loss: 0.11748, policy_loss: -26.01652, policy_entropy: -6.02202, alpha: 0.00640, time: 51.33102
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   562 ----
[CW] collect: return: 193.11753, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 0.11024, qf2_loss: 0.11076, policy_loss: -25.95620, policy_entropy: -5.80010, alpha: 0.00637, time: 50.92670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   563 ----
[CW] collect: return: 191.51036, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 0.11334, qf2_loss: 0.11360, policy_loss: -26.07435, policy_entropy: -6.02041, alpha: 0.00631, time: 50.89999
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   564 ----
[CW] collect: return: 189.76782, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 0.11257, qf2_loss: 0.11298, policy_loss: -26.01651, policy_entropy: -6.01324, alpha: 0.00634, time: 51.25066
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   565 ----
[CW] collect: return: 185.63202, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 0.11438, qf2_loss: 0.11532, policy_loss: -26.07018, policy_entropy: -6.10195, alpha: 0.00636, time: 51.24723
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   566 ----
[CW] collect: return: 206.78845, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 0.12493, qf2_loss: 0.12543, policy_loss: -26.12223, policy_entropy: -5.87676, alpha: 0.00635, time: 51.07950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   567 ----
[CW] collect: return: 194.72460, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 0.12728, qf2_loss: 0.12842, policy_loss: -26.11717, policy_entropy: -5.99577, alpha: 0.00634, time: 50.86908
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   568 ----
[CW] collect: return: 187.84755, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 0.12489, qf2_loss: 0.12566, policy_loss: -26.18815, policy_entropy: -6.13721, alpha: 0.00635, time: 51.86521
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   569 ----
[CW] collect: return: 206.35928, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 0.11445, qf2_loss: 0.11500, policy_loss: -26.17187, policy_entropy: -5.98175, alpha: 0.00638, time: 51.30491
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   570 ----
[CW] collect: return: 203.38204, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 0.11580, qf2_loss: 0.11540, policy_loss: -26.23509, policy_entropy: -6.12047, alpha: 0.00640, time: 50.95121
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   571 ----
[CW] collect: return: 220.51196, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 0.11568, qf2_loss: 0.11671, policy_loss: -26.16828, policy_entropy: -6.06561, alpha: 0.00643, time: 50.85509
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   572 ----
[CW] collect: return: 204.46564, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 0.11042, qf2_loss: 0.11096, policy_loss: -26.27634, policy_entropy: -6.10566, alpha: 0.00647, time: 50.80332
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   573 ----
[CW] collect: return: 220.01248, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 0.11559, qf2_loss: 0.11671, policy_loss: -26.28744, policy_entropy: -6.11148, alpha: 0.00653, time: 50.91459
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   574 ----
[CW] collect: return: 196.44778, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 0.11361, qf2_loss: 0.11489, policy_loss: -26.32666, policy_entropy: -6.01776, alpha: 0.00656, time: 50.93609
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   575 ----
[CW] collect: return: 220.73469, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 0.13746, qf2_loss: 0.13847, policy_loss: -26.38570, policy_entropy: -6.30044, alpha: 0.00663, time: 51.43573
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   576 ----
[CW] collect: return: 197.38133, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 0.12345, qf2_loss: 0.12355, policy_loss: -26.35913, policy_entropy: -5.96090, alpha: 0.00669, time: 50.79183
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   577 ----
[CW] collect: return: 199.34446, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 0.12738, qf2_loss: 0.12837, policy_loss: -26.41652, policy_entropy: -6.05583, alpha: 0.00670, time: 51.12505
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   578 ----
[CW] collect: return: 223.70239, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 0.11330, qf2_loss: 0.11430, policy_loss: -26.39597, policy_entropy: -5.98113, alpha: 0.00673, time: 51.08009
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   579 ----
[CW] collect: return: 194.15953, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 0.12271, qf2_loss: 0.12300, policy_loss: -26.44017, policy_entropy: -5.75158, alpha: 0.00666, time: 50.98595
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   580 ----
[CW] collect: return: 228.96281, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 0.10995, qf2_loss: 0.11051, policy_loss: -26.53704, policy_entropy: -6.04980, alpha: 0.00659, time: 51.02404
[CW] eval: return: 207.46025, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   581 ----
[CW] collect: return: 188.15305, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 0.13103, qf2_loss: 0.13185, policy_loss: -26.49328, policy_entropy: -6.02748, alpha: 0.00663, time: 51.13262
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   582 ----
[CW] collect: return: 226.81604, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 0.12764, qf2_loss: 0.12717, policy_loss: -26.56239, policy_entropy: -6.28274, alpha: 0.00668, time: 50.64263
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   583 ----
[CW] collect: return: 202.60716, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 0.13084, qf2_loss: 0.13133, policy_loss: -26.50871, policy_entropy: -6.39575, alpha: 0.00685, time: 50.78222
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   584 ----
[CW] collect: return: 206.79924, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 0.11740, qf2_loss: 0.11807, policy_loss: -26.62961, policy_entropy: -6.08940, alpha: 0.00698, time: 51.14410
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   585 ----
[CW] collect: return: 226.72190, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 0.11244, qf2_loss: 0.11344, policy_loss: -26.57314, policy_entropy: -6.04532, alpha: 0.00703, time: 51.03157
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   586 ----
[CW] collect: return: 209.36960, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 0.12268, qf2_loss: 0.12305, policy_loss: -26.67288, policy_entropy: -6.05211, alpha: 0.00703, time: 50.96240
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   587 ----
[CW] collect: return: 213.90818, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 0.11728, qf2_loss: 0.11877, policy_loss: -26.60521, policy_entropy: -6.19101, alpha: 0.00709, time: 51.51628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   588 ----
[CW] collect: return: 223.30140, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 0.13061, qf2_loss: 0.13271, policy_loss: -26.66859, policy_entropy: -6.29788, alpha: 0.00722, time: 51.31913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   589 ----
[CW] collect: return: 227.42338, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 0.12417, qf2_loss: 0.12464, policy_loss: -26.76541, policy_entropy: -6.00726, alpha: 0.00733, time: 51.19567
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   590 ----
[CW] collect: return: 209.29477, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 0.12228, qf2_loss: 0.12245, policy_loss: -26.74528, policy_entropy: -6.02850, alpha: 0.00736, time: 51.40244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   591 ----
[CW] collect: return: 226.19322, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 0.12376, qf2_loss: 0.12567, policy_loss: -26.82593, policy_entropy: -6.03201, alpha: 0.00737, time: 51.23726
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   592 ----
[CW] collect: return: 198.19448, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 0.15021, qf2_loss: 0.14950, policy_loss: -26.81181, policy_entropy: -5.95277, alpha: 0.00737, time: 50.96887
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   593 ----
[CW] collect: return: 218.74114, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 0.12927, qf2_loss: 0.13019, policy_loss: -26.88320, policy_entropy: -6.13163, alpha: 0.00738, time: 51.28119
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   594 ----
[CW] collect: return: 215.68454, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 0.12488, qf2_loss: 0.12547, policy_loss: -26.85052, policy_entropy: -6.01770, alpha: 0.00743, time: 51.33052
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   595 ----
[CW] collect: return: 206.12327, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 0.14052, qf2_loss: 0.14122, policy_loss: -26.89709, policy_entropy: -5.97652, alpha: 0.00741, time: 51.23290
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   596 ----
[CW] collect: return: 203.74767, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 0.13864, qf2_loss: 0.14063, policy_loss: -27.02240, policy_entropy: -5.96566, alpha: 0.00741, time: 51.35921
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   597 ----
[CW] collect: return: 201.19861, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 0.12385, qf2_loss: 0.12477, policy_loss: -27.01168, policy_entropy: -6.05648, alpha: 0.00741, time: 51.24643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   598 ----
[CW] collect: return: 227.90811, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 0.13929, qf2_loss: 0.14020, policy_loss: -27.08275, policy_entropy: -5.93886, alpha: 0.00742, time: 50.54105
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   599 ----
[CW] collect: return: 202.96496, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 0.13324, qf2_loss: 0.13382, policy_loss: -27.12856, policy_entropy: -5.88497, alpha: 0.00737, time: 51.48032
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   600 ----
[CW] collect: return: 195.80389, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 0.12534, qf2_loss: 0.12633, policy_loss: -27.12651, policy_entropy: -6.00752, alpha: 0.00732, time: 51.01679
[CW] eval: return: 218.66283, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   601 ----
[CW] collect: return: 234.55574, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 0.13683, qf2_loss: 0.13744, policy_loss: -27.20563, policy_entropy: -6.01672, alpha: 0.00734, time: 51.65782
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   602 ----
[CW] collect: return: 212.47276, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 0.13959, qf2_loss: 0.14010, policy_loss: -27.17020, policy_entropy: -5.88300, alpha: 0.00731, time: 50.86719
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   603 ----
[CW] collect: return: 220.00962, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 0.14215, qf2_loss: 0.14289, policy_loss: -27.12523, policy_entropy: -5.97491, alpha: 0.00728, time: 51.47939
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   604 ----
[CW] collect: return: 213.05154, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 0.13631, qf2_loss: 0.13696, policy_loss: -27.13258, policy_entropy: -6.06864, alpha: 0.00728, time: 50.94370
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   605 ----
[CW] collect: return: 199.94441, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 0.12990, qf2_loss: 0.13089, policy_loss: -27.14943, policy_entropy: -5.99727, alpha: 0.00729, time: 50.95032
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   606 ----
[CW] collect: return: 225.33062, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 0.13719, qf2_loss: 0.13731, policy_loss: -27.25665, policy_entropy: -6.04652, alpha: 0.00732, time: 51.37093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   607 ----
[CW] collect: return: 202.42530, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 0.14439, qf2_loss: 0.14446, policy_loss: -27.42313, policy_entropy: -5.96199, alpha: 0.00733, time: 50.64605
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   608 ----
[CW] collect: return: 235.90203, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 0.13896, qf2_loss: 0.14032, policy_loss: -27.28958, policy_entropy: -5.73725, alpha: 0.00724, time: 51.12518
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   609 ----
[CW] collect: return: 224.18441, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 0.12825, qf2_loss: 0.12909, policy_loss: -27.39773, policy_entropy: -6.05732, alpha: 0.00715, time: 50.91362
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   610 ----
[CW] collect: return: 215.48887, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 0.13878, qf2_loss: 0.13858, policy_loss: -27.54159, policy_entropy: -6.23090, alpha: 0.00723, time: 50.99872
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   611 ----
[CW] collect: return: 212.03438, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 0.13580, qf2_loss: 0.13634, policy_loss: -27.45395, policy_entropy: -5.94652, alpha: 0.00731, time: 50.84192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   612 ----
[CW] collect: return: 218.05100, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 0.14386, qf2_loss: 0.14456, policy_loss: -27.54104, policy_entropy: -6.01319, alpha: 0.00729, time: 51.15060
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   613 ----
[CW] collect: return: 207.73222, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 0.13977, qf2_loss: 0.14081, policy_loss: -27.55484, policy_entropy: -5.81369, alpha: 0.00726, time: 51.46844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   614 ----
[CW] collect: return: 246.37307, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 0.14075, qf2_loss: 0.14087, policy_loss: -27.51883, policy_entropy: -5.92649, alpha: 0.00717, time: 51.09931
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   615 ----
[CW] collect: return: 223.12956, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 0.13577, qf2_loss: 0.13644, policy_loss: -27.57870, policy_entropy: -6.05200, alpha: 0.00715, time: 51.09064
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   616 ----
[CW] collect: return: 220.34338, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 0.13681, qf2_loss: 0.13680, policy_loss: -27.58880, policy_entropy: -6.11026, alpha: 0.00720, time: 51.21457
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   617 ----
[CW] collect: return: 205.57056, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 0.13782, qf2_loss: 0.13892, policy_loss: -27.75781, policy_entropy: -6.02846, alpha: 0.00722, time: 51.36557
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   618 ----
[CW] collect: return: 220.75914, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 0.14143, qf2_loss: 0.14110, policy_loss: -27.64597, policy_entropy: -5.92764, alpha: 0.00723, time: 51.15116
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   619 ----
[CW] collect: return: 224.03735, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 0.14361, qf2_loss: 0.14387, policy_loss: -27.75187, policy_entropy: -5.87552, alpha: 0.00715, time: 52.17729
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   620 ----
[CW] collect: return: 227.60831, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 0.14329, qf2_loss: 0.14453, policy_loss: -27.81109, policy_entropy: -6.09165, alpha: 0.00717, time: 51.66394
[CW] eval: return: 226.00242, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   621 ----
[CW] collect: return: 230.15927, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 0.14094, qf2_loss: 0.14082, policy_loss: -27.80490, policy_entropy: -5.92777, alpha: 0.00717, time: 51.08337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   622 ----
[CW] collect: return: 219.02961, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 0.13513, qf2_loss: 0.13661, policy_loss: -27.96287, policy_entropy: -6.09056, alpha: 0.00715, time: 51.32496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   623 ----
[CW] collect: return: 221.26616, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 0.14102, qf2_loss: 0.14117, policy_loss: -27.81174, policy_entropy: -6.15692, alpha: 0.00725, time: 51.33370
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   624 ----
[CW] collect: return: 232.57565, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 0.14082, qf2_loss: 0.14147, policy_loss: -27.93831, policy_entropy: -6.02166, alpha: 0.00729, time: 51.11340
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   625 ----
[CW] collect: return: 216.01353, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 0.13814, qf2_loss: 0.13806, policy_loss: -27.93864, policy_entropy: -5.89616, alpha: 0.00727, time: 51.00041
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   626 ----
[CW] collect: return: 227.51268, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 0.13760, qf2_loss: 0.13677, policy_loss: -28.00208, policy_entropy: -6.16570, alpha: 0.00727, time: 51.23881
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   627 ----
[CW] collect: return: 201.01106, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 0.14916, qf2_loss: 0.14863, policy_loss: -28.05459, policy_entropy: -6.20413, alpha: 0.00739, time: 51.19218
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   628 ----
[CW] collect: return: 241.91375, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 0.15445, qf2_loss: 0.15427, policy_loss: -28.00309, policy_entropy: -6.05848, alpha: 0.00745, time: 51.13750
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   629 ----
[CW] collect: return: 207.72549, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 0.14634, qf2_loss: 0.14781, policy_loss: -28.09242, policy_entropy: -6.14258, alpha: 0.00749, time: 51.05513
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   630 ----
[CW] collect: return: 191.00928, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 0.14238, qf2_loss: 0.14230, policy_loss: -28.11750, policy_entropy: -6.28175, alpha: 0.00763, time: 51.61193
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   631 ----
[CW] collect: return: 223.20322, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 0.14883, qf2_loss: 0.14831, policy_loss: -28.21779, policy_entropy: -6.18503, alpha: 0.00776, time: 50.94129
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   632 ----
[CW] collect: return: 221.83684, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 0.14428, qf2_loss: 0.14449, policy_loss: -28.27181, policy_entropy: -5.97821, alpha: 0.00782, time: 50.99971
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   633 ----
[CW] collect: return: 216.02011, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 0.14280, qf2_loss: 0.14339, policy_loss: -28.18368, policy_entropy: -5.95940, alpha: 0.00780, time: 51.04298
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   634 ----
[CW] collect: return: 221.32594, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 0.15575, qf2_loss: 0.15847, policy_loss: -28.38090, policy_entropy: -5.98243, alpha: 0.00776, time: 50.90249
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   635 ----
[CW] collect: return: 221.55531, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 0.16869, qf2_loss: 0.16693, policy_loss: -28.43473, policy_entropy: -6.19469, alpha: 0.00780, time: 51.04726
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   636 ----
[CW] collect: return: 211.69365, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 0.14393, qf2_loss: 0.14462, policy_loss: -28.30881, policy_entropy: -6.15271, alpha: 0.00797, time: 51.04214
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   637 ----
[CW] collect: return: 230.43471, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 0.15598, qf2_loss: 0.15616, policy_loss: -28.30462, policy_entropy: -5.92445, alpha: 0.00797, time: 51.15959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   638 ----
[CW] collect: return: 215.11179, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 0.14915, qf2_loss: 0.14948, policy_loss: -28.32356, policy_entropy: -5.93320, alpha: 0.00796, time: 50.97570
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   639 ----
[CW] collect: return: 228.57721, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 0.15158, qf2_loss: 0.15163, policy_loss: -28.45009, policy_entropy: -5.84461, alpha: 0.00786, time: 51.72702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   640 ----
[CW] collect: return: 240.89409, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 0.17128, qf2_loss: 0.17030, policy_loss: -28.50099, policy_entropy: -6.01694, alpha: 0.00778, time: 51.35608
[CW] eval: return: 232.24803, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   641 ----
[CW] collect: return: 247.80564, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 0.15113, qf2_loss: 0.15060, policy_loss: -28.47858, policy_entropy: -6.10578, alpha: 0.00786, time: 51.73205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   642 ----
[CW] collect: return: 238.52124, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 0.15911, qf2_loss: 0.15925, policy_loss: -28.40212, policy_entropy: -6.24866, alpha: 0.00795, time: 51.21483
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   643 ----
[CW] collect: return: 245.57281, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 0.15649, qf2_loss: 0.15757, policy_loss: -28.68559, policy_entropy: -6.13785, alpha: 0.00809, time: 50.91660
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   644 ----
[CW] collect: return: 240.15984, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 0.15820, qf2_loss: 0.15970, policy_loss: -28.62409, policy_entropy: -5.82166, alpha: 0.00809, time: 51.09882
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   645 ----
[CW] collect: return: 218.44184, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 0.14850, qf2_loss: 0.14886, policy_loss: -28.56843, policy_entropy: -5.66371, alpha: 0.00795, time: 51.19646
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   646 ----
[CW] collect: return: 261.19197, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 0.14839, qf2_loss: 0.14864, policy_loss: -28.69780, policy_entropy: -5.85337, alpha: 0.00776, time: 51.21342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   647 ----
[CW] collect: return: 219.05041, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 0.15198, qf2_loss: 0.15195, policy_loss: -28.69739, policy_entropy: -5.90323, alpha: 0.00771, time: 51.66022
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   648 ----
[CW] collect: return: 233.35404, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 0.14986, qf2_loss: 0.15008, policy_loss: -28.74223, policy_entropy: -6.01424, alpha: 0.00765, time: 51.06637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   649 ----
[CW] collect: return: 248.66709, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 0.15767, qf2_loss: 0.15721, policy_loss: -28.87728, policy_entropy: -6.04198, alpha: 0.00768, time: 51.19811
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   650 ----
[CW] collect: return: 251.99370, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 0.15186, qf2_loss: 0.15119, policy_loss: -28.91246, policy_entropy: -6.09157, alpha: 0.00772, time: 51.21657
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   651 ----
[CW] collect: return: 255.37072, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 0.15732, qf2_loss: 0.15853, policy_loss: -28.94348, policy_entropy: -6.00663, alpha: 0.00775, time: 51.03125
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   652 ----
[CW] collect: return: 236.85011, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 0.15923, qf2_loss: 0.15890, policy_loss: -28.91339, policy_entropy: -6.18415, alpha: 0.00779, time: 51.31146
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   653 ----
[CW] collect: return: 251.34424, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 0.17595, qf2_loss: 0.17511, policy_loss: -28.85506, policy_entropy: -6.16391, alpha: 0.00791, time: 51.15078
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   654 ----
[CW] collect: return: 251.47502, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 0.15140, qf2_loss: 0.15188, policy_loss: -29.05662, policy_entropy: -5.98720, alpha: 0.00796, time: 51.06010
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   655 ----
[CW] collect: return: 248.38562, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 0.14975, qf2_loss: 0.14929, policy_loss: -29.02498, policy_entropy: -6.03279, alpha: 0.00798, time: 51.05607
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   656 ----
[CW] collect: return: 240.94999, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 0.15802, qf2_loss: 0.15836, policy_loss: -29.03335, policy_entropy: -6.15507, alpha: 0.00803, time: 51.02844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   657 ----
[CW] collect: return: 215.47799, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 0.15096, qf2_loss: 0.15054, policy_loss: -29.15275, policy_entropy: -6.02350, alpha: 0.00812, time: 51.11227
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   658 ----
[CW] collect: return: 234.27898, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 0.16916, qf2_loss: 0.16957, policy_loss: -29.18617, policy_entropy: -5.84520, alpha: 0.00804, time: 50.69632
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   659 ----
[CW] collect: return: 238.54888, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 0.16495, qf2_loss: 0.16324, policy_loss: -29.16986, policy_entropy: -5.96092, alpha: 0.00796, time: 51.03001
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   660 ----
[CW] collect: return: 255.45392, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 0.15993, qf2_loss: 0.15890, policy_loss: -29.27482, policy_entropy: -6.04225, alpha: 0.00799, time: 51.70460
[CW] eval: return: 240.64046, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   661 ----
[CW] collect: return: 218.60423, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 0.15917, qf2_loss: 0.15923, policy_loss: -29.22933, policy_entropy: -5.99284, alpha: 0.00800, time: 51.35321
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   662 ----
[CW] collect: return: 210.54961, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 0.16606, qf2_loss: 0.16499, policy_loss: -29.37653, policy_entropy: -6.06592, alpha: 0.00799, time: 51.05995
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   663 ----
[CW] collect: return: 249.33951, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 0.15955, qf2_loss: 0.16032, policy_loss: -29.24386, policy_entropy: -6.00233, alpha: 0.00802, time: 50.93099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   664 ----
[CW] collect: return: 228.27337, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 0.18261, qf2_loss: 0.18342, policy_loss: -29.23762, policy_entropy: -6.09332, alpha: 0.00806, time: 51.48179
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   665 ----
[CW] collect: return: 247.80669, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 0.16536, qf2_loss: 0.16524, policy_loss: -29.33265, policy_entropy: -6.08121, alpha: 0.00811, time: 50.88225
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   666 ----
[CW] collect: return: 238.69179, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 0.16292, qf2_loss: 0.16342, policy_loss: -29.49104, policy_entropy: -6.00544, alpha: 0.00817, time: 51.61137
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   667 ----
[CW] collect: return: 237.38365, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 0.17602, qf2_loss: 0.17593, policy_loss: -29.36947, policy_entropy: -5.95379, alpha: 0.00813, time: 51.16118
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   668 ----
[CW] collect: return: 245.73752, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 0.17009, qf2_loss: 0.16844, policy_loss: -29.51559, policy_entropy: -5.91555, alpha: 0.00811, time: 51.10599
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   669 ----
[CW] collect: return: 262.40303, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 0.16438, qf2_loss: 0.16446, policy_loss: -29.44923, policy_entropy: -5.89835, alpha: 0.00802, time: 51.06079
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   670 ----
[CW] collect: return: 233.90757, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 0.16766, qf2_loss: 0.16672, policy_loss: -29.67431, policy_entropy: -5.92322, alpha: 0.00798, time: 51.09332
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   671 ----
[CW] collect: return: 240.47866, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 0.16169, qf2_loss: 0.16285, policy_loss: -29.74261, policy_entropy: -6.03301, alpha: 0.00795, time: 51.31706
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   672 ----
[CW] collect: return: 247.21206, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 0.16248, qf2_loss: 0.16264, policy_loss: -29.66564, policy_entropy: -6.02942, alpha: 0.00795, time: 51.22541
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   673 ----
[CW] collect: return: 248.97535, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 0.16582, qf2_loss: 0.16580, policy_loss: -29.56294, policy_entropy: -5.96622, alpha: 0.00798, time: 51.01000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   674 ----
[CW] collect: return: 208.73096, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 0.16996, qf2_loss: 0.16868, policy_loss: -29.77480, policy_entropy: -5.93970, alpha: 0.00795, time: 50.87746
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   675 ----
[CW] collect: return: 284.03114, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 0.16945, qf2_loss: 0.16835, policy_loss: -29.73319, policy_entropy: -5.97932, alpha: 0.00792, time: 51.22698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   676 ----
[CW] collect: return: 237.90103, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 0.16902, qf2_loss: 0.16870, policy_loss: -29.85631, policy_entropy: -6.04497, alpha: 0.00792, time: 51.45407
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   677 ----
[CW] collect: return: 233.96288, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 0.17995, qf2_loss: 0.17978, policy_loss: -29.79761, policy_entropy: -6.01696, alpha: 0.00797, time: 51.35859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   678 ----
[CW] collect: return: 224.64932, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 0.17235, qf2_loss: 0.17327, policy_loss: -29.98757, policy_entropy: -6.02204, alpha: 0.00794, time: 50.81646
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   679 ----
[CW] collect: return: 244.90732, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 0.16877, qf2_loss: 0.16869, policy_loss: -29.69787, policy_entropy: -5.92458, alpha: 0.00793, time: 50.97802
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   680 ----
[CW] collect: return: 221.83875, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 0.17217, qf2_loss: 0.17270, policy_loss: -29.84081, policy_entropy: -5.96327, alpha: 0.00790, time: 51.25221
[CW] eval: return: 249.84092, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   681 ----
[CW] collect: return: 262.04315, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 0.17234, qf2_loss: 0.17209, policy_loss: -29.96562, policy_entropy: -5.90654, alpha: 0.00784, time: 51.13330
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   682 ----
[CW] collect: return: 258.64841, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 0.17217, qf2_loss: 0.17121, policy_loss: -30.02904, policy_entropy: -6.21946, alpha: 0.00790, time: 51.00572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   683 ----
[CW] collect: return: 221.18725, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 0.16974, qf2_loss: 0.17003, policy_loss: -30.12861, policy_entropy: -6.27834, alpha: 0.00807, time: 51.00715
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   684 ----
[CW] collect: return: 240.99909, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 0.16958, qf2_loss: 0.16802, policy_loss: -30.08997, policy_entropy: -6.10460, alpha: 0.00820, time: 50.89828
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   685 ----
[CW] collect: return: 233.04261, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 0.19883, qf2_loss: 0.19889, policy_loss: -30.15745, policy_entropy: -5.87698, alpha: 0.00823, time: 51.26606
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   686 ----
[CW] collect: return: 250.11295, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 0.17467, qf2_loss: 0.17467, policy_loss: -29.95771, policy_entropy: -5.93795, alpha: 0.00813, time: 51.03047
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   687 ----
[CW] collect: return: 241.41507, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 0.17591, qf2_loss: 0.17487, policy_loss: -30.14506, policy_entropy: -6.09672, alpha: 0.00816, time: 51.02941
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   688 ----
[CW] collect: return: 274.67085, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 0.17058, qf2_loss: 0.17031, policy_loss: -30.21096, policy_entropy: -6.18706, alpha: 0.00821, time: 51.09667
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   689 ----
[CW] collect: return: 261.61594, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 0.17621, qf2_loss: 0.17635, policy_loss: -30.29175, policy_entropy: -6.18838, alpha: 0.00836, time: 51.07621
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   690 ----
[CW] collect: return: 221.70412, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 0.18660, qf2_loss: 0.18723, policy_loss: -30.39147, policy_entropy: -6.19320, alpha: 0.00854, time: 51.02599
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   691 ----
[CW] collect: return: 247.30582, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 0.17499, qf2_loss: 0.17520, policy_loss: -30.39294, policy_entropy: -6.05396, alpha: 0.00862, time: 51.42481
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   692 ----
[CW] collect: return: 236.19708, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 0.19692, qf2_loss: 0.19633, policy_loss: -30.52478, policy_entropy: -6.05039, alpha: 0.00864, time: 51.26496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   693 ----
[CW] collect: return: 254.03905, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 0.17480, qf2_loss: 0.17362, policy_loss: -30.52406, policy_entropy: -6.01378, alpha: 0.00864, time: 51.34458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   694 ----
[CW] collect: return: 241.49387, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 0.18322, qf2_loss: 0.18379, policy_loss: -30.43111, policy_entropy: -6.14902, alpha: 0.00875, time: 51.34897
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   695 ----
[CW] collect: return: 245.15411, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 0.18031, qf2_loss: 0.17886, policy_loss: -30.47767, policy_entropy: -6.13078, alpha: 0.00884, time: 51.02817
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   696 ----
[CW] collect: return: 248.19861, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 0.17970, qf2_loss: 0.17928, policy_loss: -30.67377, policy_entropy: -5.88882, alpha: 0.00881, time: 51.20701
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   697 ----
[CW] collect: return: 255.95048, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 0.23116, qf2_loss: 0.23224, policy_loss: -30.61162, policy_entropy: -6.04195, alpha: 0.00883, time: 51.54336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   698 ----
[CW] collect: return: 256.16701, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 0.18250, qf2_loss: 0.18196, policy_loss: -30.69305, policy_entropy: -5.95159, alpha: 0.00879, time: 50.98090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   699 ----
[CW] collect: return: 253.98084, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 0.17357, qf2_loss: 0.17396, policy_loss: -30.71188, policy_entropy: -5.99928, alpha: 0.00882, time: 51.55104
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   700 ----
[CW] collect: return: 251.31856, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 0.19802, qf2_loss: 0.19502, policy_loss: -30.71012, policy_entropy: -6.02551, alpha: 0.00881, time: 51.41731
[CW] eval: return: 250.66061, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   701 ----
[CW] collect: return: 247.81236, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 0.17667, qf2_loss: 0.17557, policy_loss: -30.65367, policy_entropy: -5.94971, alpha: 0.00882, time: 52.19569
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   702 ----
[CW] collect: return: 263.05425, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 0.17740, qf2_loss: 0.17671, policy_loss: -30.73804, policy_entropy: -5.91746, alpha: 0.00874, time: 51.36260
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   703 ----
[CW] collect: return: 262.03687, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 0.18456, qf2_loss: 0.18468, policy_loss: -30.78581, policy_entropy: -5.90718, alpha: 0.00870, time: 51.13519
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   704 ----
[CW] collect: return: 264.08143, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 0.20830, qf2_loss: 0.20890, policy_loss: -30.85479, policy_entropy: -5.89820, alpha: 0.00862, time: 50.95850
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   705 ----
[CW] collect: return: 264.45545, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 0.19553, qf2_loss: 0.19493, policy_loss: -30.77365, policy_entropy: -5.92932, alpha: 0.00856, time: 51.29879
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   706 ----
[CW] collect: return: 264.11036, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 0.19536, qf2_loss: 0.19466, policy_loss: -30.93771, policy_entropy: -5.90193, alpha: 0.00848, time: 51.36589
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   707 ----
[CW] collect: return: 243.29534, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 0.19222, qf2_loss: 0.19190, policy_loss: -30.87696, policy_entropy: -5.94581, alpha: 0.00841, time: 51.13328
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   708 ----
[CW] collect: return: 252.79465, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 0.18897, qf2_loss: 0.18851, policy_loss: -31.11653, policy_entropy: -6.10063, alpha: 0.00844, time: 51.09697
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   709 ----
[CW] collect: return: 273.67575, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 0.20325, qf2_loss: 0.20403, policy_loss: -31.09534, policy_entropy: -6.02487, alpha: 0.00850, time: 51.21733
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   710 ----
[CW] collect: return: 266.83249, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 0.19448, qf2_loss: 0.19195, policy_loss: -31.22491, policy_entropy: -6.05113, alpha: 0.00852, time: 51.00401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   711 ----
[CW] collect: return: 250.70134, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 0.19265, qf2_loss: 0.19405, policy_loss: -31.20890, policy_entropy: -5.96045, alpha: 0.00853, time: 50.70836
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   712 ----
[CW] collect: return: 253.92852, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 0.19685, qf2_loss: 0.19611, policy_loss: -31.24391, policy_entropy: -6.08725, alpha: 0.00853, time: 51.89369
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   713 ----
[CW] collect: return: 261.18319, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 0.20062, qf2_loss: 0.19955, policy_loss: -31.34256, policy_entropy: -6.15095, alpha: 0.00865, time: 51.43054
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   714 ----
[CW] collect: return: 236.71992, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 0.19007, qf2_loss: 0.19004, policy_loss: -31.25667, policy_entropy: -5.96208, alpha: 0.00869, time: 51.36405
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   715 ----
[CW] collect: return: 289.52187, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 0.19730, qf2_loss: 0.19697, policy_loss: -31.24774, policy_entropy: -5.97237, alpha: 0.00866, time: 51.42392
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   716 ----
[CW] collect: return: 250.03345, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 0.20580, qf2_loss: 0.20616, policy_loss: -31.41566, policy_entropy: -6.09581, alpha: 0.00871, time: 50.93399
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   717 ----
[CW] collect: return: 235.64855, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 0.18406, qf2_loss: 0.18324, policy_loss: -31.44897, policy_entropy: -5.91780, alpha: 0.00871, time: 50.98596
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   718 ----
[CW] collect: return: 246.09878, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 0.18880, qf2_loss: 0.18912, policy_loss: -31.38875, policy_entropy: -5.95442, alpha: 0.00862, time: 50.95577
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   719 ----
[CW] collect: return: 287.05947, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 0.20288, qf2_loss: 0.20155, policy_loss: -31.59034, policy_entropy: -6.19108, alpha: 0.00868, time: 51.05391
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   720 ----
[CW] collect: return: 257.76577, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 0.21390, qf2_loss: 0.21245, policy_loss: -31.47486, policy_entropy: -5.72228, alpha: 0.00868, time: 51.53302
[CW] eval: return: 253.60915, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   721 ----
[CW] collect: return: 264.31430, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 0.19220, qf2_loss: 0.19267, policy_loss: -31.47901, policy_entropy: -5.87969, alpha: 0.00849, time: 51.28860
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   722 ----
[CW] collect: return: 268.98306, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 0.21050, qf2_loss: 0.20916, policy_loss: -31.58010, policy_entropy: -5.90324, alpha: 0.00844, time: 51.03270
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   723 ----
[CW] collect: return: 262.04938, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 0.19372, qf2_loss: 0.19334, policy_loss: -31.67219, policy_entropy: -6.14144, alpha: 0.00841, time: 51.09502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   724 ----
[CW] collect: return: 272.91426, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 0.19433, qf2_loss: 0.19413, policy_loss: -31.58751, policy_entropy: -5.99772, alpha: 0.00850, time: 50.97065
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   725 ----
[CW] collect: return: 203.03685, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 0.20888, qf2_loss: 0.20817, policy_loss: -31.70471, policy_entropy: -6.00408, alpha: 0.00848, time: 50.99071
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   726 ----
[CW] collect: return: 269.60611, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 0.20764, qf2_loss: 0.20806, policy_loss: -31.62018, policy_entropy: -5.91285, alpha: 0.00845, time: 51.14439
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   727 ----
[CW] collect: return: 273.93736, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 0.18947, qf2_loss: 0.18900, policy_loss: -31.82122, policy_entropy: -6.06115, alpha: 0.00844, time: 51.16365
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   728 ----
[CW] collect: return: 209.55319, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 0.20192, qf2_loss: 0.20170, policy_loss: -31.81066, policy_entropy: -6.08783, alpha: 0.00851, time: 51.24390
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   729 ----
[CW] collect: return: 242.32024, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 0.22071, qf2_loss: 0.22058, policy_loss: -31.92413, policy_entropy: -6.10969, alpha: 0.00860, time: 51.33780
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   730 ----
[CW] collect: return: 255.94723, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 0.21064, qf2_loss: 0.21056, policy_loss: -31.84563, policy_entropy: -6.04232, alpha: 0.00865, time: 51.35480
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   731 ----
[CW] collect: return: 270.92398, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 0.20899, qf2_loss: 0.20757, policy_loss: -31.91328, policy_entropy: -5.99242, alpha: 0.00865, time: 51.09238
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   732 ----
[CW] collect: return: 286.51916, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 0.19536, qf2_loss: 0.19535, policy_loss: -32.01879, policy_entropy: -6.06316, alpha: 0.00870, time: 51.12240
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   733 ----
[CW] collect: return: 246.32765, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 0.21792, qf2_loss: 0.21930, policy_loss: -31.97965, policy_entropy: -6.03768, alpha: 0.00872, time: 50.99283
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   734 ----
[CW] collect: return: 277.54052, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 0.19597, qf2_loss: 0.19557, policy_loss: -32.10692, policy_entropy: -6.14726, alpha: 0.00881, time: 51.41663
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   735 ----
[CW] collect: return: 259.04991, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 0.20862, qf2_loss: 0.20726, policy_loss: -32.17922, policy_entropy: -6.09444, alpha: 0.00889, time: 51.80144
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   736 ----
[CW] collect: return: 275.65314, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 0.22102, qf2_loss: 0.22156, policy_loss: -32.17950, policy_entropy: -6.15231, alpha: 0.00904, time: 51.41004
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   737 ----
[CW] collect: return: 255.88300, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 0.23768, qf2_loss: 0.23653, policy_loss: -32.23695, policy_entropy: -5.96103, alpha: 0.00910, time: 51.06814
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   738 ----
[CW] collect: return: 255.04626, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 0.21632, qf2_loss: 0.21711, policy_loss: -32.14131, policy_entropy: -5.90052, alpha: 0.00901, time: 51.13130
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   739 ----
[CW] collect: return: 266.68973, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 0.20179, qf2_loss: 0.20187, policy_loss: -32.31869, policy_entropy: -6.20024, alpha: 0.00904, time: 51.13210
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   740 ----
[CW] collect: return: 261.52088, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 0.22065, qf2_loss: 0.22085, policy_loss: -32.30296, policy_entropy: -6.14428, alpha: 0.00923, time: 51.17741
[CW] eval: return: 268.76837, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   741 ----
[CW] collect: return: 275.54311, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 0.20928, qf2_loss: 0.20952, policy_loss: -32.40516, policy_entropy: -6.04587, alpha: 0.00931, time: 51.12221
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   742 ----
[CW] collect: return: 254.05635, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 0.20695, qf2_loss: 0.20669, policy_loss: -32.29072, policy_entropy: -6.09346, alpha: 0.00935, time: 51.12492
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   743 ----
[CW] collect: return: 279.43047, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 0.21971, qf2_loss: 0.21890, policy_loss: -32.44576, policy_entropy: -5.99633, alpha: 0.00942, time: 51.19049
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   744 ----
[CW] collect: return: 308.85620, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 0.19764, qf2_loss: 0.19779, policy_loss: -32.41897, policy_entropy: -5.92523, alpha: 0.00940, time: 51.28831
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   745 ----
[CW] collect: return: 253.20573, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 0.21254, qf2_loss: 0.21066, policy_loss: -32.48713, policy_entropy: -5.96333, alpha: 0.00933, time: 51.00896
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   746 ----
[CW] collect: return: 251.78977, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 0.21868, qf2_loss: 0.21936, policy_loss: -32.67762, policy_entropy: -5.81958, alpha: 0.00927, time: 51.30161
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   747 ----
[CW] collect: return: 258.42561, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 0.21014, qf2_loss: 0.21076, policy_loss: -32.50938, policy_entropy: -5.88562, alpha: 0.00910, time: 51.16753
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   748 ----
[CW] collect: return: 256.70668, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 0.21893, qf2_loss: 0.21925, policy_loss: -32.69326, policy_entropy: -5.97944, alpha: 0.00903, time: 50.65899
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   749 ----
[CW] collect: return: 267.00304, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 0.21067, qf2_loss: 0.21000, policy_loss: -32.77792, policy_entropy: -6.19903, alpha: 0.00908, time: 50.74488
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   750 ----
[CW] collect: return: 264.69674, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 0.21036, qf2_loss: 0.21099, policy_loss: -32.91426, policy_entropy: -6.28208, alpha: 0.00929, time: 51.03462
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   751 ----
[CW] collect: return: 279.42288, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 0.26819, qf2_loss: 0.26889, policy_loss: -32.85448, policy_entropy: -6.15412, alpha: 0.00952, time: 50.93057
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   752 ----
[CW] collect: return: 267.66423, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 0.23425, qf2_loss: 0.23441, policy_loss: -32.75952, policy_entropy: -6.23102, alpha: 0.00976, time: 50.95373
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   753 ----
[CW] collect: return: 273.60339, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 0.21739, qf2_loss: 0.21594, policy_loss: -32.95481, policy_entropy: -5.97252, alpha: 0.00983, time: 51.06582
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   754 ----
[CW] collect: return: 260.48751, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 0.21230, qf2_loss: 0.21238, policy_loss: -33.00194, policy_entropy: -5.82642, alpha: 0.00972, time: 51.30092
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   755 ----
[CW] collect: return: 268.87990, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 0.21958, qf2_loss: 0.22006, policy_loss: -32.98272, policy_entropy: -5.71628, alpha: 0.00956, time: 51.32256
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   756 ----
[CW] collect: return: 261.66566, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 0.24037, qf2_loss: 0.24023, policy_loss: -33.03560, policy_entropy: -5.94177, alpha: 0.00933, time: 50.91962
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   757 ----
[CW] collect: return: 264.08362, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 0.22112, qf2_loss: 0.22269, policy_loss: -33.06838, policy_entropy: -6.15497, alpha: 0.00937, time: 51.11164
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   758 ----
[CW] collect: return: 289.69490, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 0.20767, qf2_loss: 0.20785, policy_loss: -33.17244, policy_entropy: -6.14295, alpha: 0.00953, time: 50.75000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   759 ----
[CW] collect: return: 252.44563, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 0.21263, qf2_loss: 0.21255, policy_loss: -33.32833, policy_entropy: -6.02607, alpha: 0.00960, time: 51.45908
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   760 ----
[CW] collect: return: 278.67785, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 0.21139, qf2_loss: 0.21170, policy_loss: -33.21531, policy_entropy: -5.94775, alpha: 0.00959, time: 51.44497
[CW] eval: return: 278.36177, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   761 ----
[CW] collect: return: 281.39600, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 0.22067, qf2_loss: 0.22071, policy_loss: -33.28715, policy_entropy: -5.88143, alpha: 0.00952, time: 51.17335
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   762 ----
[CW] collect: return: 280.52014, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 0.22721, qf2_loss: 0.22636, policy_loss: -33.35022, policy_entropy: -6.01463, alpha: 0.00946, time: 50.84170
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   763 ----
[CW] collect: return: 269.76131, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 0.22668, qf2_loss: 0.22779, policy_loss: -33.27275, policy_entropy: -5.87470, alpha: 0.00944, time: 50.79376
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   764 ----
[CW] collect: return: 285.41206, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 0.23882, qf2_loss: 0.23859, policy_loss: -33.44323, policy_entropy: -5.72778, alpha: 0.00923, time: 50.93745
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   765 ----
[CW] collect: return: 279.38056, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 0.22950, qf2_loss: 0.23055, policy_loss: -33.36753, policy_entropy: -5.92433, alpha: 0.00908, time: 50.81331
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   766 ----
[CW] collect: return: 265.34562, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 0.24148, qf2_loss: 0.23948, policy_loss: -33.56508, policy_entropy: -5.92711, alpha: 0.00903, time: 51.88519
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   767 ----
[CW] collect: return: 241.12758, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 0.21562, qf2_loss: 0.21445, policy_loss: -33.44552, policy_entropy: -5.85626, alpha: 0.00897, time: 52.02259
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   768 ----
[CW] collect: return: 274.88870, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 0.23616, qf2_loss: 0.23617, policy_loss: -33.53766, policy_entropy: -5.99147, alpha: 0.00890, time: 51.68562
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   769 ----
[CW] collect: return: 278.49719, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 0.24240, qf2_loss: 0.24384, policy_loss: -33.68909, policy_entropy: -6.01278, alpha: 0.00889, time: 51.13323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   770 ----
[CW] collect: return: 263.45650, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 0.22288, qf2_loss: 0.22292, policy_loss: -33.67127, policy_entropy: -6.03147, alpha: 0.00890, time: 51.39206
[CW] ---------------------------

============================= JOB FEEDBACK =============================

NodeName=uc2n518
Job ID: 21914427
Array Job ID: 21914427_0
Cluster: uc2
User/Group: uprnr/stud
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 4
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 2-00:01:12 core-walltime
Job Wall-clock time: 12:00:18
Memory Utilized: 7.31 GB
Memory Efficiency: 12.48% of 58.59 GB
