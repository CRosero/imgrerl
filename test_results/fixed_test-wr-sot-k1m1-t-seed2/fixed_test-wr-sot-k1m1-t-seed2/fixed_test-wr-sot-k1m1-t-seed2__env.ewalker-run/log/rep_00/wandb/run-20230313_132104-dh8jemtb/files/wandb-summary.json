{"collect/return": 263.45649708248675, "collect/steps": 1000.0, "collect/total_steps": 776000.0, "train/qf1_loss": 0.22288306042551995, "train/qf2_loss": 0.22291639521718026, "train/policy_loss": -33.671271858215334, "train/policy_entropy": -6.031465744972229, "train/alpha": 0.008903043111786246, "train/time": 51.39206027984619, "eval/return": 278.36177033979914, "eval/steps": 1000.0, "_timestamp": 1678753245.8597858, "_runtime": 43181.52985191345, "_step": 770}