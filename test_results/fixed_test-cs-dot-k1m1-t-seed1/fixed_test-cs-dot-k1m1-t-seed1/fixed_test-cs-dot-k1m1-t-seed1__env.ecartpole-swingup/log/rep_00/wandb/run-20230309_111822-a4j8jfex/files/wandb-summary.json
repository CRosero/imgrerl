{"collect/return": 844.4419714193791, "collect/steps": 1000.0, "collect/total_steps": 832000.0, "train/qf1_loss": 37.47820476531982, "train/qf2_loss": 37.31171764373779, "train/policy_loss": -612.936982421875, "train/policy_entropy": -0.9789101701974868, "train/alpha": 0.43703696101903916, "train/time": 33.23136639595032, "eval/return": 830.5808593019331, "eval/steps": 1000.0, "_timestamp": 1678385782.3514132, "_runtime": 28680.10188817978, "_step": 826}