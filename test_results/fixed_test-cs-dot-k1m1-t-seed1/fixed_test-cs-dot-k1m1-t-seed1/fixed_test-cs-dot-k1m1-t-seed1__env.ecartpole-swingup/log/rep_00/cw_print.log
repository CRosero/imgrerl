[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 87.68394, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.55946, qf2_loss: 1.56076, policy_loss: -2.60332, policy_entropy: 0.68217, alpha: 0.98504, time: 37.91205
[CW] eval: return: 116.46328, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 191.68063, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.13060, qf2_loss: 0.13007, policy_loss: -3.13303, policy_entropy: 0.67809, alpha: 0.95629, time: 32.34366
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 178.39592, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.10763, qf2_loss: 0.10720, policy_loss: -3.72431, policy_entropy: 0.67219, alpha: 0.92881, time: 33.16498
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 161.55783, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.11160, qf2_loss: 0.10975, policy_loss: -4.48767, policy_entropy: 0.66511, alpha: 0.90253, time: 33.20031
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 252.88730, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.14034, qf2_loss: 0.13870, policy_loss: -5.38646, policy_entropy: 0.65624, alpha: 0.87738, time: 33.35221
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 74.02062, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.16176, qf2_loss: 0.16081, policy_loss: -5.91582, policy_entropy: 0.65210, alpha: 0.85327, time: 33.52865
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 243.12928, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.22559, qf2_loss: 0.22522, policy_loss: -6.81444, policy_entropy: 0.64348, alpha: 0.83013, time: 33.47581
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 128.93900, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.28426, qf2_loss: 0.28391, policy_loss: -7.47985, policy_entropy: 0.63786, alpha: 0.80792, time: 33.44130
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 151.66564, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.30463, qf2_loss: 0.30328, policy_loss: -8.26579, policy_entropy: 0.63332, alpha: 0.78653, time: 33.54369
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 161.99666, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.43292, qf2_loss: 0.43399, policy_loss: -8.98565, policy_entropy: 0.62169, alpha: 0.76597, time: 33.58321
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 115.38253, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.41675, qf2_loss: 0.41749, policy_loss: -9.59436, policy_entropy: 0.61262, alpha: 0.74622, time: 33.60626
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 47.87470, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.50775, qf2_loss: 0.50767, policy_loss: -10.10330, policy_entropy: 0.60649, alpha: 0.72719, time: 33.63541
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 331.84685, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.65123, qf2_loss: 0.65373, policy_loss: -11.29072, policy_entropy: 0.58327, alpha: 0.70890, time: 33.67284
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 247.37895, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.69471, qf2_loss: 0.70029, policy_loss: -12.25621, policy_entropy: 0.56299, alpha: 0.69138, time: 33.60091
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 175.25071, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.86713, qf2_loss: 0.86753, policy_loss: -13.31999, policy_entropy: 0.54607, alpha: 0.67456, time: 33.67195
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 61.23732, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.78815, qf2_loss: 0.78903, policy_loss: -13.70595, policy_entropy: 0.53828, alpha: 0.65832, time: 33.68884
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 86.28818, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.90310, qf2_loss: 0.90191, policy_loss: -14.50878, policy_entropy: 0.51586, alpha: 0.64264, time: 33.53754
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 315.87576, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 1.25684, qf2_loss: 1.24879, policy_loss: -15.40882, policy_entropy: 0.49569, alpha: 0.62758, time: 33.66091
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 279.01348, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 1.27545, qf2_loss: 1.26634, policy_loss: -16.63426, policy_entropy: 0.46211, alpha: 0.61310, time: 33.64928
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 156.84968, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 1.15521, qf2_loss: 1.14013, policy_loss: -17.30618, policy_entropy: 0.44197, alpha: 0.59921, time: 33.53815
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 186.93978, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 1.20912, qf2_loss: 1.19884, policy_loss: -18.32714, policy_entropy: 0.41847, alpha: 0.58579, time: 33.43577
[CW] eval: return: 233.84395, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 191.76910, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 1.43821, qf2_loss: 1.43147, policy_loss: -19.12798, policy_entropy: 0.40366, alpha: 0.57285, time: 33.54415
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 197.94013, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 1.73968, qf2_loss: 1.72710, policy_loss: -20.11866, policy_entropy: 0.37717, alpha: 0.56030, time: 33.56614
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 193.72018, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 1.49088, qf2_loss: 1.47773, policy_loss: -20.77043, policy_entropy: 0.34738, alpha: 0.54824, time: 33.61226
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 167.69985, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 1.54157, qf2_loss: 1.53271, policy_loss: -22.00008, policy_entropy: 0.33559, alpha: 0.53656, time: 33.57770
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 270.07463, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 2.63937, qf2_loss: 2.63285, policy_loss: -23.00014, policy_entropy: 0.31944, alpha: 0.52518, time: 33.31281
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 224.68746, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 1.63468, qf2_loss: 1.61488, policy_loss: -23.85507, policy_entropy: 0.30059, alpha: 0.51412, time: 33.60926
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 264.50006, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 1.77074, qf2_loss: 1.75898, policy_loss: -24.69241, policy_entropy: 0.27219, alpha: 0.50344, time: 33.62864
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 194.12890, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 1.72553, qf2_loss: 1.72356, policy_loss: -25.72759, policy_entropy: 0.25983, alpha: 0.49306, time: 33.58482
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 258.16263, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 1.79431, qf2_loss: 1.79077, policy_loss: -26.52892, policy_entropy: 0.24809, alpha: 0.48293, time: 33.59144
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 237.65671, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 1.86764, qf2_loss: 1.85617, policy_loss: -27.79693, policy_entropy: 0.23321, alpha: 0.47305, time: 33.43483
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 261.50009, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 2.01472, qf2_loss: 2.01287, policy_loss: -28.56010, policy_entropy: 0.23031, alpha: 0.46336, time: 33.51055
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 261.28920, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 3.55694, qf2_loss: 3.53584, policy_loss: -29.57456, policy_entropy: 0.21248, alpha: 0.45391, time: 33.46761
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 343.64253, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 2.14227, qf2_loss: 2.13292, policy_loss: -31.00952, policy_entropy: 0.17668, alpha: 0.44473, time: 33.41491
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 195.17184, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 1.96592, qf2_loss: 1.96738, policy_loss: -31.50213, policy_entropy: 0.13402, alpha: 0.43597, time: 33.37505
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 297.77497, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 2.09208, qf2_loss: 2.10138, policy_loss: -32.49429, policy_entropy: 0.10610, alpha: 0.42757, time: 33.57322
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 218.01854, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 2.31874, qf2_loss: 2.31392, policy_loss: -33.78012, policy_entropy: 0.07385, alpha: 0.41946, time: 33.41369
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 212.44702, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 2.32919, qf2_loss: 2.32389, policy_loss: -34.51423, policy_entropy: 0.04248, alpha: 0.41165, time: 33.46706
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 316.56709, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 2.41083, qf2_loss: 2.41300, policy_loss: -35.68363, policy_entropy: 0.00193, alpha: 0.40412, time: 33.54785
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 213.98235, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 2.31558, qf2_loss: 2.30699, policy_loss: -36.36510, policy_entropy: -0.01568, alpha: 0.39690, time: 33.50694
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 202.67645, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 2.30809, qf2_loss: 2.27988, policy_loss: -37.14876, policy_entropy: -0.03918, alpha: 0.38980, time: 33.31009
[CW] eval: return: 267.83081, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 316.01335, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 2.36334, qf2_loss: 2.36304, policy_loss: -38.59872, policy_entropy: -0.05253, alpha: 0.38288, time: 33.29647
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 253.23657, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 2.50657, qf2_loss: 2.51052, policy_loss: -39.39922, policy_entropy: -0.06265, alpha: 0.37607, time: 33.54913
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 321.16715, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 2.68589, qf2_loss: 2.68467, policy_loss: -40.45882, policy_entropy: -0.07707, alpha: 0.36939, time: 33.52065
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 217.75147, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 2.55523, qf2_loss: 2.54504, policy_loss: -41.72990, policy_entropy: -0.07878, alpha: 0.36273, time: 33.52049
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 309.05417, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 3.07962, qf2_loss: 3.08891, policy_loss: -42.41841, policy_entropy: -0.09209, alpha: 0.35621, time: 33.58712
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 224.83584, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 2.79134, qf2_loss: 2.78575, policy_loss: -43.73661, policy_entropy: -0.10057, alpha: 0.34975, time: 33.55067
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 261.56562, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 3.29426, qf2_loss: 3.29342, policy_loss: -44.81402, policy_entropy: -0.12081, alpha: 0.34349, time: 33.54240
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 264.53845, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 3.01291, qf2_loss: 3.01388, policy_loss: -45.70690, policy_entropy: -0.12619, alpha: 0.33727, time: 33.36442
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 315.80037, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 2.63784, qf2_loss: 2.62533, policy_loss: -46.52784, policy_entropy: -0.15279, alpha: 0.33122, time: 33.48029
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 297.57887, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 2.55720, qf2_loss: 2.56488, policy_loss: -47.86799, policy_entropy: -0.17303, alpha: 0.32535, time: 33.48556
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 259.11038, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 2.95761, qf2_loss: 2.96264, policy_loss: -48.56298, policy_entropy: -0.20516, alpha: 0.31967, time: 33.35033
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 285.32930, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 2.90273, qf2_loss: 2.91400, policy_loss: -49.61619, policy_entropy: -0.21088, alpha: 0.31416, time: 33.26781
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 305.21497, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 2.86572, qf2_loss: 2.86661, policy_loss: -51.19993, policy_entropy: -0.22874, alpha: 0.30870, time: 33.39657
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 334.80211, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 2.88436, qf2_loss: 2.89352, policy_loss: -52.09910, policy_entropy: -0.25852, alpha: 0.30344, time: 33.42794
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 237.43055, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 3.06164, qf2_loss: 3.03973, policy_loss: -53.25771, policy_entropy: -0.27592, alpha: 0.29835, time: 33.38915
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 265.60238, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 3.30908, qf2_loss: 3.31576, policy_loss: -54.09872, policy_entropy: -0.29530, alpha: 0.29337, time: 33.67752
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 213.51480, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 2.93816, qf2_loss: 2.95906, policy_loss: -55.34715, policy_entropy: -0.30809, alpha: 0.28852, time: 33.48920
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 388.75672, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 3.14296, qf2_loss: 3.13999, policy_loss: -56.08397, policy_entropy: -0.32974, alpha: 0.28376, time: 33.36909
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 297.49708, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 3.33938, qf2_loss: 3.34146, policy_loss: -57.41577, policy_entropy: -0.36121, alpha: 0.27918, time: 33.51009
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 323.71908, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 3.36879, qf2_loss: 3.36914, policy_loss: -58.41747, policy_entropy: -0.38226, alpha: 0.27476, time: 33.55383
[CW] eval: return: 253.88504, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 297.83558, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 3.11382, qf2_loss: 3.13365, policy_loss: -59.54869, policy_entropy: -0.41080, alpha: 0.27049, time: 33.31947
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 207.96544, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 3.26268, qf2_loss: 3.26152, policy_loss: -60.68627, policy_entropy: -0.42863, alpha: 0.26639, time: 33.26578
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 320.74372, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 9.80212, qf2_loss: 9.75687, policy_loss: -61.24612, policy_entropy: -0.45025, alpha: 0.26235, time: 33.33462
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 185.30497, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 3.63001, qf2_loss: 3.62176, policy_loss: -62.65576, policy_entropy: -0.47524, alpha: 0.25848, time: 33.17117
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 269.46673, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 3.31241, qf2_loss: 3.32558, policy_loss: -63.63524, policy_entropy: -0.51131, alpha: 0.25483, time: 33.40759
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 189.10094, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 3.17318, qf2_loss: 3.18107, policy_loss: -64.77749, policy_entropy: -0.54802, alpha: 0.25134, time: 33.41240
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 178.83348, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 3.27443, qf2_loss: 3.26943, policy_loss: -65.47114, policy_entropy: -0.57755, alpha: 0.24806, time: 33.47591
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 246.86340, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 3.47866, qf2_loss: 3.46649, policy_loss: -66.47276, policy_entropy: -0.59980, alpha: 0.24498, time: 33.51848
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 217.40960, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 3.59160, qf2_loss: 3.56752, policy_loss: -67.31223, policy_entropy: -0.62808, alpha: 0.24197, time: 33.55631
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 343.81507, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 3.58502, qf2_loss: 3.59921, policy_loss: -67.98923, policy_entropy: -0.64725, alpha: 0.23915, time: 33.53259
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 211.28423, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 3.41955, qf2_loss: 3.41554, policy_loss: -68.93098, policy_entropy: -0.65836, alpha: 0.23639, time: 33.67827
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 295.55387, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 3.72047, qf2_loss: 3.74361, policy_loss: -70.32498, policy_entropy: -0.67866, alpha: 0.23373, time: 33.70513
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 241.44398, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 3.98485, qf2_loss: 4.00308, policy_loss: -71.18956, policy_entropy: -0.68807, alpha: 0.23110, time: 33.64966
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 289.70459, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 4.21194, qf2_loss: 4.21026, policy_loss: -71.90054, policy_entropy: -0.66653, alpha: 0.22836, time: 33.26301
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 408.00086, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 4.59762, qf2_loss: 4.56208, policy_loss: -73.00215, policy_entropy: -0.68958, alpha: 0.22562, time: 33.59382
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 194.37261, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 4.03170, qf2_loss: 3.99919, policy_loss: -73.93911, policy_entropy: -0.68615, alpha: 0.22288, time: 33.31571
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 186.11825, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 3.72276, qf2_loss: 3.71736, policy_loss: -74.49861, policy_entropy: -0.70068, alpha: 0.22015, time: 33.46378
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 268.35414, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 3.92131, qf2_loss: 3.91723, policy_loss: -75.44038, policy_entropy: -0.69605, alpha: 0.21744, time: 33.71901
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 246.07966, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 4.16453, qf2_loss: 4.11110, policy_loss: -76.12272, policy_entropy: -0.70255, alpha: 0.21468, time: 33.65851
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 261.34004, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 4.58447, qf2_loss: 4.55007, policy_loss: -77.52265, policy_entropy: -0.69890, alpha: 0.21189, time: 33.60277
[CW] eval: return: 274.33354, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 291.20284, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 3.88973, qf2_loss: 3.88666, policy_loss: -78.71176, policy_entropy: -0.68912, alpha: 0.20900, time: 33.48659
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 304.63767, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 4.18038, qf2_loss: 4.18334, policy_loss: -79.83457, policy_entropy: -0.69994, alpha: 0.20607, time: 33.50255
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 215.16495, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 4.07528, qf2_loss: 4.07104, policy_loss: -80.29774, policy_entropy: -0.69580, alpha: 0.20324, time: 33.54061
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 215.95998, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 4.04747, qf2_loss: 4.04303, policy_loss: -80.70332, policy_entropy: -0.68811, alpha: 0.20021, time: 33.57024
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 273.60566, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 4.70938, qf2_loss: 4.69668, policy_loss: -82.18278, policy_entropy: -0.69679, alpha: 0.19721, time: 33.57795
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 241.50434, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 6.30857, qf2_loss: 6.30329, policy_loss: -83.20341, policy_entropy: -0.71867, alpha: 0.19428, time: 33.59276
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 296.80620, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 4.41450, qf2_loss: 4.40057, policy_loss: -83.92979, policy_entropy: -0.72429, alpha: 0.19151, time: 33.30726
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 282.75381, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 15.09941, qf2_loss: 15.20807, policy_loss: -84.73114, policy_entropy: -0.70392, alpha: 0.18868, time: 33.21919
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 234.81348, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 4.62261, qf2_loss: 4.60376, policy_loss: -85.57532, policy_entropy: -0.74564, alpha: 0.18588, time: 33.34027
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 222.06871, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 4.22618, qf2_loss: 4.20183, policy_loss: -86.74556, policy_entropy: -0.76968, alpha: 0.18339, time: 33.44457
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 258.97641, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 4.40224, qf2_loss: 4.37172, policy_loss: -87.40475, policy_entropy: -0.78283, alpha: 0.18105, time: 33.45872
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 349.95842, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 4.59848, qf2_loss: 4.61292, policy_loss: -88.11333, policy_entropy: -0.79363, alpha: 0.17885, time: 33.43956
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 306.47745, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 4.55927, qf2_loss: 4.53168, policy_loss: -89.54416, policy_entropy: -0.81361, alpha: 0.17671, time: 33.65541
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 323.54213, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 4.52748, qf2_loss: 4.53225, policy_loss: -90.17607, policy_entropy: -0.83871, alpha: 0.17486, time: 33.36991
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 384.83908, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 4.40518, qf2_loss: 4.41029, policy_loss: -91.18091, policy_entropy: -0.83502, alpha: 0.17302, time: 33.57435
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 275.39103, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 4.70534, qf2_loss: 4.66401, policy_loss: -92.20346, policy_entropy: -0.85072, alpha: 0.17120, time: 33.43259
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 299.96346, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 5.30168, qf2_loss: 5.29034, policy_loss: -93.51603, policy_entropy: -0.87881, alpha: 0.16961, time: 33.42937
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 307.53833, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 8.39467, qf2_loss: 8.37965, policy_loss: -94.32280, policy_entropy: -0.90299, alpha: 0.16826, time: 33.39479
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 200.57302, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 6.48018, qf2_loss: 6.43721, policy_loss: -94.86092, policy_entropy: -0.89266, alpha: 0.16698, time: 33.48443
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 327.14128, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 5.11759, qf2_loss: 5.10330, policy_loss: -96.17940, policy_entropy: -0.92089, alpha: 0.16575, time: 33.41882
[CW] eval: return: 311.08400, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 257.05162, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 4.84223, qf2_loss: 4.83834, policy_loss: -96.83085, policy_entropy: -0.93989, alpha: 0.16489, time: 33.28461
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 249.38138, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 5.41470, qf2_loss: 5.39195, policy_loss: -97.99307, policy_entropy: -0.95992, alpha: 0.16414, time: 33.42979
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 268.48988, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 5.29075, qf2_loss: 5.24435, policy_loss: -99.02307, policy_entropy: -0.96876, alpha: 0.16367, time: 33.34990
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 368.03282, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 5.88727, qf2_loss: 5.88544, policy_loss: -99.19083, policy_entropy: -0.95228, alpha: 0.16311, time: 33.40735
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 276.69078, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 5.34230, qf2_loss: 5.28676, policy_loss: -100.60483, policy_entropy: -0.97326, alpha: 0.16238, time: 33.28393
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 391.02667, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 6.04889, qf2_loss: 6.02996, policy_loss: -101.98403, policy_entropy: -0.97357, alpha: 0.16209, time: 33.38102
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 435.65098, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 5.94235, qf2_loss: 5.94670, policy_loss: -102.35719, policy_entropy: -0.98196, alpha: 0.16160, time: 33.46524
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 388.46696, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 6.09341, qf2_loss: 6.04459, policy_loss: -103.04504, policy_entropy: -1.00061, alpha: 0.16153, time: 33.46269
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 374.00285, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 6.10294, qf2_loss: 6.07961, policy_loss: -104.61882, policy_entropy: -1.01060, alpha: 0.16159, time: 33.55702
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 376.97621, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 6.38865, qf2_loss: 6.33586, policy_loss: -106.08584, policy_entropy: -1.00610, alpha: 0.16183, time: 33.60749
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 376.68529, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 7.03044, qf2_loss: 7.00936, policy_loss: -106.81375, policy_entropy: -1.01391, alpha: 0.16196, time: 34.34543
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 268.55161, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 8.63667, qf2_loss: 8.58165, policy_loss: -108.00023, policy_entropy: -1.02013, alpha: 0.16222, time: 33.53090
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 158.99108, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 6.40642, qf2_loss: 6.42837, policy_loss: -108.94828, policy_entropy: -1.00446, alpha: 0.16257, time: 33.66048
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 169.48649, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 6.52848, qf2_loss: 6.48967, policy_loss: -108.76830, policy_entropy: -1.00627, alpha: 0.16270, time: 33.51977
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 227.60180, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 7.30023, qf2_loss: 7.24572, policy_loss: -110.06487, policy_entropy: -0.99427, alpha: 0.16278, time: 33.39686
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 171.41325, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 7.30384, qf2_loss: 7.23360, policy_loss: -111.14105, policy_entropy: -1.01188, alpha: 0.16287, time: 33.51377
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 240.32766, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 8.88152, qf2_loss: 8.85547, policy_loss: -112.14724, policy_entropy: -0.99303, alpha: 0.16294, time: 33.70762
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 170.39194, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 7.57437, qf2_loss: 7.53226, policy_loss: -112.45491, policy_entropy: -1.00015, alpha: 0.16284, time: 33.70609
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 217.50206, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 6.95778, qf2_loss: 6.90868, policy_loss: -113.10397, policy_entropy: -1.00162, alpha: 0.16291, time: 33.51299
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 204.74288, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 7.16258, qf2_loss: 7.06674, policy_loss: -114.42777, policy_entropy: -1.00092, alpha: 0.16289, time: 33.45275
[CW] eval: return: 190.90536, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 224.84055, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 8.80813, qf2_loss: 8.76781, policy_loss: -115.69210, policy_entropy: -0.99503, alpha: 0.16275, time: 33.46428
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 192.76014, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 8.28837, qf2_loss: 8.21899, policy_loss: -116.27052, policy_entropy: -0.97654, alpha: 0.16250, time: 33.58747
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 259.72651, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 8.08270, qf2_loss: 8.05738, policy_loss: -117.22180, policy_entropy: -0.98530, alpha: 0.16173, time: 33.56571
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 182.75128, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 7.62399, qf2_loss: 7.58699, policy_loss: -118.00176, policy_entropy: -0.98693, alpha: 0.16116, time: 33.69180
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 258.95538, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 7.75915, qf2_loss: 7.68629, policy_loss: -118.92745, policy_entropy: -0.98764, alpha: 0.16084, time: 33.51478
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 200.82061, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 7.76052, qf2_loss: 7.68265, policy_loss: -119.48864, policy_entropy: -0.98766, alpha: 0.16019, time: 33.49043
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 216.72494, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 7.28671, qf2_loss: 7.23874, policy_loss: -120.46131, policy_entropy: -1.00979, alpha: 0.16019, time: 33.51261
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 305.66764, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 8.23979, qf2_loss: 8.21674, policy_loss: -121.07809, policy_entropy: -1.00097, alpha: 0.16039, time: 33.34345
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 285.27290, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 14.45000, qf2_loss: 14.24849, policy_loss: -121.94186, policy_entropy: -0.97049, alpha: 0.15987, time: 33.56477
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 231.84680, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 10.77459, qf2_loss: 10.70718, policy_loss: -123.28645, policy_entropy: -0.98396, alpha: 0.15877, time: 33.56556
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 278.77576, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 7.92395, qf2_loss: 7.82032, policy_loss: -124.12966, policy_entropy: -0.99600, alpha: 0.15828, time: 33.60198
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 261.36653, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 7.86417, qf2_loss: 7.82452, policy_loss: -124.32505, policy_entropy: -0.99306, alpha: 0.15797, time: 33.64758
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 247.83165, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 7.67550, qf2_loss: 7.64858, policy_loss: -125.03454, policy_entropy: -1.02429, alpha: 0.15844, time: 33.69849
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 260.68218, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 9.04092, qf2_loss: 8.97450, policy_loss: -126.47383, policy_entropy: -1.00190, alpha: 0.15910, time: 33.45323
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 406.55959, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 8.49195, qf2_loss: 8.44532, policy_loss: -126.60340, policy_entropy: -1.02076, alpha: 0.15954, time: 33.54601
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 325.22519, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 9.61459, qf2_loss: 9.56518, policy_loss: -128.14376, policy_entropy: -1.02790, alpha: 0.16079, time: 33.42546
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 454.75490, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 9.19485, qf2_loss: 9.08573, policy_loss: -128.28003, policy_entropy: -1.01494, alpha: 0.16203, time: 33.61334
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 390.58839, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 8.75474, qf2_loss: 8.76214, policy_loss: -130.47351, policy_entropy: -1.02318, alpha: 0.16346, time: 33.33514
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 381.72185, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 9.32438, qf2_loss: 9.26856, policy_loss: -130.34529, policy_entropy: -1.00906, alpha: 0.16447, time: 33.42137
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 428.88589, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 10.02512, qf2_loss: 10.03566, policy_loss: -131.32767, policy_entropy: -1.03517, alpha: 0.16585, time: 33.46394
[CW] eval: return: 371.94719, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 332.47768, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 10.12845, qf2_loss: 10.02317, policy_loss: -132.59579, policy_entropy: -1.01434, alpha: 0.16749, time: 33.17069
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 320.46660, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 10.44751, qf2_loss: 10.42928, policy_loss: -133.07642, policy_entropy: -1.02888, alpha: 0.16909, time: 33.54428
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 371.07408, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 10.00394, qf2_loss: 10.05695, policy_loss: -133.88683, policy_entropy: -1.03247, alpha: 0.17112, time: 33.50674
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 292.04408, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 10.20193, qf2_loss: 10.19887, policy_loss: -135.12960, policy_entropy: -1.01912, alpha: 0.17290, time: 33.48094
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 280.90468, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 10.16741, qf2_loss: 10.11155, policy_loss: -135.87917, policy_entropy: -1.01180, alpha: 0.17437, time: 33.50951
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 339.40554, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 10.74885, qf2_loss: 10.73781, policy_loss: -136.66738, policy_entropy: -1.01707, alpha: 0.17601, time: 33.47645
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 418.29874, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 10.37640, qf2_loss: 10.29058, policy_loss: -137.75647, policy_entropy: -1.01691, alpha: 0.17691, time: 33.43932
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 353.80960, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 10.19640, qf2_loss: 10.25233, policy_loss: -137.95966, policy_entropy: -1.01983, alpha: 0.17896, time: 33.49159
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 262.55171, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 9.82043, qf2_loss: 9.70879, policy_loss: -138.89424, policy_entropy: -1.02045, alpha: 0.18019, time: 33.42793
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 276.18135, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 11.98540, qf2_loss: 11.82791, policy_loss: -139.69596, policy_entropy: -1.00150, alpha: 0.18150, time: 33.44820
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 340.05508, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 11.17342, qf2_loss: 11.10887, policy_loss: -139.84477, policy_entropy: -1.01812, alpha: 0.18244, time: 33.50111
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 326.86770, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 9.67393, qf2_loss: 9.57554, policy_loss: -141.04644, policy_entropy: -1.01355, alpha: 0.18389, time: 33.34186
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 368.71673, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 9.50711, qf2_loss: 9.50840, policy_loss: -142.26536, policy_entropy: -1.01391, alpha: 0.18501, time: 33.54868
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 334.12851, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 10.19584, qf2_loss: 10.12558, policy_loss: -143.20562, policy_entropy: -1.01084, alpha: 0.18607, time: 33.29597
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 356.00054, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 10.81822, qf2_loss: 10.78119, policy_loss: -143.75846, policy_entropy: -1.01078, alpha: 0.18762, time: 33.40552
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 393.59518, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 11.14265, qf2_loss: 11.13714, policy_loss: -144.35327, policy_entropy: -0.98886, alpha: 0.18710, time: 33.46947
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 308.87943, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 10.53571, qf2_loss: 10.48596, policy_loss: -145.95549, policy_entropy: -1.02085, alpha: 0.18776, time: 33.49807
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 333.62553, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 10.26028, qf2_loss: 10.20007, policy_loss: -146.39361, policy_entropy: -1.01234, alpha: 0.18913, time: 33.37108
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 423.24621, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 11.41281, qf2_loss: 11.26211, policy_loss: -146.80021, policy_entropy: -0.99928, alpha: 0.19054, time: 33.37675
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 356.28259, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 11.18884, qf2_loss: 11.06673, policy_loss: -147.60552, policy_entropy: -0.99267, alpha: 0.18955, time: 33.42187
[CW] eval: return: 355.60745, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 310.62624, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 10.58261, qf2_loss: 10.49448, policy_loss: -149.07310, policy_entropy: -1.01980, alpha: 0.19033, time: 33.50339
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 402.21774, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 10.68473, qf2_loss: 10.62308, policy_loss: -149.05885, policy_entropy: -1.00140, alpha: 0.19120, time: 33.47043
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 478.90288, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 11.87723, qf2_loss: 11.81933, policy_loss: -149.16693, policy_entropy: -0.98799, alpha: 0.19092, time: 33.34231
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 454.93355, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 11.16555, qf2_loss: 11.09528, policy_loss: -151.14041, policy_entropy: -1.01097, alpha: 0.19085, time: 33.34999
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 312.85126, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 10.71798, qf2_loss: 10.61397, policy_loss: -151.88043, policy_entropy: -1.00671, alpha: 0.19190, time: 33.41354
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 451.27066, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 10.37232, qf2_loss: 10.32109, policy_loss: -151.90017, policy_entropy: -1.00319, alpha: 0.19228, time: 33.37764
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 402.43199, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 11.14531, qf2_loss: 11.00213, policy_loss: -153.50441, policy_entropy: -1.00020, alpha: 0.19216, time: 33.36717
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 406.55905, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 10.98017, qf2_loss: 10.83492, policy_loss: -153.76525, policy_entropy: -1.00488, alpha: 0.19235, time: 33.43824
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 352.74791, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 10.99894, qf2_loss: 11.00603, policy_loss: -154.39626, policy_entropy: -1.00875, alpha: 0.19354, time: 33.52802
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 409.37549, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 11.46623, qf2_loss: 11.38423, policy_loss: -155.21644, policy_entropy: -0.99568, alpha: 0.19404, time: 33.20526
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 346.77643, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 10.72688, qf2_loss: 10.66742, policy_loss: -156.28199, policy_entropy: -1.01464, alpha: 0.19406, time: 33.52277
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 324.04004, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 11.14651, qf2_loss: 11.10952, policy_loss: -156.73548, policy_entropy: -1.00129, alpha: 0.19481, time: 33.21248
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 295.44014, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 12.16226, qf2_loss: 12.01643, policy_loss: -157.29978, policy_entropy: -0.99654, alpha: 0.19497, time: 33.38252
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 370.97845, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 11.08379, qf2_loss: 11.02521, policy_loss: -158.27702, policy_entropy: -1.01076, alpha: 0.19482, time: 33.39018
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 432.64846, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 11.52771, qf2_loss: 11.37653, policy_loss: -158.32909, policy_entropy: -0.99816, alpha: 0.19552, time: 33.36227
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 409.24793, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 10.96276, qf2_loss: 10.91208, policy_loss: -159.57578, policy_entropy: -1.01392, alpha: 0.19613, time: 33.23323
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 375.78476, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 11.03356, qf2_loss: 10.96398, policy_loss: -160.93122, policy_entropy: -1.02351, alpha: 0.19793, time: 33.21200
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 369.11577, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 11.65009, qf2_loss: 11.61883, policy_loss: -159.80814, policy_entropy: -1.01094, alpha: 0.19969, time: 33.25147
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 433.22295, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 11.03740, qf2_loss: 11.04526, policy_loss: -161.46718, policy_entropy: -1.01958, alpha: 0.20120, time: 33.26119
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 411.30942, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 10.66292, qf2_loss: 10.63791, policy_loss: -162.15232, policy_entropy: -0.99649, alpha: 0.20276, time: 33.33007
[CW] eval: return: 428.68306, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 486.87631, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 10.93256, qf2_loss: 10.69329, policy_loss: -163.55061, policy_entropy: -1.00685, alpha: 0.20248, time: 33.39699
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 482.94466, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 10.79858, qf2_loss: 10.76704, policy_loss: -164.71246, policy_entropy: -1.01278, alpha: 0.20393, time: 33.37455
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 450.07988, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 12.70110, qf2_loss: 12.71863, policy_loss: -165.10283, policy_entropy: -0.99247, alpha: 0.20459, time: 33.30839
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 457.11913, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 10.46073, qf2_loss: 10.36623, policy_loss: -165.53792, policy_entropy: -1.01139, alpha: 0.20422, time: 33.24191
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 422.91849, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 10.61939, qf2_loss: 10.56721, policy_loss: -166.78409, policy_entropy: -1.01716, alpha: 0.20566, time: 33.22024
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 442.43599, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 10.94679, qf2_loss: 10.87464, policy_loss: -166.95298, policy_entropy: -1.00879, alpha: 0.20710, time: 34.25950
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 298.14947, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 11.14971, qf2_loss: 10.94024, policy_loss: -167.25343, policy_entropy: -1.00342, alpha: 0.20760, time: 33.09228
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 470.57063, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 10.73785, qf2_loss: 10.61102, policy_loss: -169.75387, policy_entropy: -1.01258, alpha: 0.20889, time: 33.20124
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 519.07242, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 11.93352, qf2_loss: 11.72215, policy_loss: -169.20500, policy_entropy: -1.00509, alpha: 0.20995, time: 33.26542
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 465.49823, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 11.71971, qf2_loss: 11.57398, policy_loss: -170.13239, policy_entropy: -0.99153, alpha: 0.20991, time: 33.01005
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 385.59336, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 11.40848, qf2_loss: 11.26467, policy_loss: -170.68924, policy_entropy: -1.00463, alpha: 0.20965, time: 33.18965
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 508.56671, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 10.90174, qf2_loss: 10.88138, policy_loss: -171.42501, policy_entropy: -0.99256, alpha: 0.20940, time: 33.32422
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 474.98635, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 10.81357, qf2_loss: 10.78449, policy_loss: -172.15389, policy_entropy: -1.00653, alpha: 0.20930, time: 33.43248
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 459.34960, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 11.35136, qf2_loss: 11.25612, policy_loss: -172.75706, policy_entropy: -1.00035, alpha: 0.21017, time: 33.52787
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 422.20464, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 11.69613, qf2_loss: 11.50018, policy_loss: -174.10194, policy_entropy: -1.00347, alpha: 0.21005, time: 33.33993
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 531.45906, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 10.67676, qf2_loss: 10.58893, policy_loss: -174.65101, policy_entropy: -1.01447, alpha: 0.21143, time: 33.28536
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 413.37403, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 10.48895, qf2_loss: 10.35553, policy_loss: -174.81009, policy_entropy: -1.00319, alpha: 0.21228, time: 33.25567
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 495.85284, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 11.30279, qf2_loss: 11.13374, policy_loss: -176.18096, policy_entropy: -1.01998, alpha: 0.21290, time: 33.10321
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 435.30050, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 12.29838, qf2_loss: 12.25007, policy_loss: -176.21497, policy_entropy: -1.01058, alpha: 0.21538, time: 33.24418
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 491.70471, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 11.07919, qf2_loss: 10.96835, policy_loss: -177.37890, policy_entropy: -1.00573, alpha: 0.21619, time: 33.22640
[CW] eval: return: 459.10618, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 507.95341, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 11.33163, qf2_loss: 11.23623, policy_loss: -179.24435, policy_entropy: -1.02361, alpha: 0.21707, time: 32.90713
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 494.01056, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 11.51928, qf2_loss: 11.39896, policy_loss: -179.68746, policy_entropy: -1.02950, alpha: 0.22140, time: 33.00730
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 378.87019, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 12.12606, qf2_loss: 11.98632, policy_loss: -179.92494, policy_entropy: -1.00456, alpha: 0.22350, time: 33.26638
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 356.89325, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 12.52738, qf2_loss: 12.58287, policy_loss: -179.73691, policy_entropy: -1.00776, alpha: 0.22400, time: 33.38149
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 515.52625, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 11.31813, qf2_loss: 11.09014, policy_loss: -181.54133, policy_entropy: -1.01396, alpha: 0.22528, time: 33.31027
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 466.48810, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 13.65139, qf2_loss: 13.53221, policy_loss: -181.75967, policy_entropy: -1.00407, alpha: 0.22678, time: 33.42071
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 459.66615, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 13.64155, qf2_loss: 13.45700, policy_loss: -182.39046, policy_entropy: -1.00012, alpha: 0.22664, time: 33.38739
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 390.56022, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 10.42746, qf2_loss: 10.27341, policy_loss: -183.10732, policy_entropy: -1.00807, alpha: 0.22706, time: 32.97895
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 453.84163, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 11.00270, qf2_loss: 10.85432, policy_loss: -184.11025, policy_entropy: -1.00119, alpha: 0.22813, time: 33.18210
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 424.38542, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 11.42605, qf2_loss: 11.37746, policy_loss: -185.49101, policy_entropy: -1.00807, alpha: 0.22872, time: 33.30570
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 452.13180, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 10.94949, qf2_loss: 10.85095, policy_loss: -184.79667, policy_entropy: -0.99555, alpha: 0.22901, time: 33.31512
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 444.55464, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 11.66040, qf2_loss: 11.48973, policy_loss: -186.01554, policy_entropy: -1.00588, alpha: 0.22885, time: 33.37509
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 424.99068, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 12.49680, qf2_loss: 12.41521, policy_loss: -187.35469, policy_entropy: -1.01734, alpha: 0.23033, time: 33.33483
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 520.95050, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 11.88580, qf2_loss: 11.83127, policy_loss: -187.96427, policy_entropy: -1.00044, alpha: 0.23166, time: 33.23654
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 426.60931, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 12.37474, qf2_loss: 12.25562, policy_loss: -189.02685, policy_entropy: -0.99912, alpha: 0.23152, time: 33.32821
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 375.00596, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 12.49817, qf2_loss: 12.33467, policy_loss: -189.96072, policy_entropy: -1.00712, alpha: 0.23167, time: 33.77173
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 454.28979, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 11.25365, qf2_loss: 11.07764, policy_loss: -189.29744, policy_entropy: -0.99513, alpha: 0.23264, time: 33.54821
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 410.83143, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 13.33375, qf2_loss: 13.30315, policy_loss: -191.48082, policy_entropy: -1.00869, alpha: 0.23293, time: 33.41848
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 485.83226, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 12.76095, qf2_loss: 12.68512, policy_loss: -191.82774, policy_entropy: -1.00143, alpha: 0.23319, time: 33.57842
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 472.30633, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 13.24713, qf2_loss: 13.10813, policy_loss: -191.77840, policy_entropy: -0.99501, alpha: 0.23301, time: 36.86766
[CW] eval: return: 476.67257, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 554.77818, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 12.40354, qf2_loss: 12.28108, policy_loss: -192.38954, policy_entropy: -1.00167, alpha: 0.23246, time: 33.55125
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 433.03286, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 11.97539, qf2_loss: 11.94071, policy_loss: -194.45711, policy_entropy: -1.00301, alpha: 0.23322, time: 33.58695
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 485.78808, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 12.80414, qf2_loss: 12.70570, policy_loss: -195.33143, policy_entropy: -1.01434, alpha: 0.23382, time: 34.56424
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 484.69130, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 13.39019, qf2_loss: 13.27482, policy_loss: -195.79143, policy_entropy: -1.00377, alpha: 0.23555, time: 33.69200
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 488.50108, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 15.31791, qf2_loss: 15.19550, policy_loss: -194.85298, policy_entropy: -0.98188, alpha: 0.23438, time: 33.54564
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 442.21308, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 12.43035, qf2_loss: 12.27434, policy_loss: -196.60386, policy_entropy: -1.00326, alpha: 0.23273, time: 33.58961
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 453.34065, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 11.89257, qf2_loss: 11.87847, policy_loss: -197.95873, policy_entropy: -1.01647, alpha: 0.23467, time: 33.65901
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 540.53940, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 11.93160, qf2_loss: 11.75691, policy_loss: -198.65539, policy_entropy: -1.00148, alpha: 0.23641, time: 33.65496
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 510.45193, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 12.37197, qf2_loss: 12.22079, policy_loss: -199.52603, policy_entropy: -1.01435, alpha: 0.23738, time: 33.67282
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 540.97188, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 14.04605, qf2_loss: 13.90659, policy_loss: -198.83705, policy_entropy: -1.00338, alpha: 0.23859, time: 33.58414
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 522.34423, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 13.38082, qf2_loss: 13.24552, policy_loss: -200.38163, policy_entropy: -1.00633, alpha: 0.23890, time: 33.51810
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 478.25562, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 14.38483, qf2_loss: 14.24492, policy_loss: -201.68925, policy_entropy: -1.01199, alpha: 0.24069, time: 33.66861
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 468.82191, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 13.06450, qf2_loss: 13.02230, policy_loss: -202.05310, policy_entropy: -1.00900, alpha: 0.24203, time: 33.58814
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 435.63015, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 13.19752, qf2_loss: 13.09150, policy_loss: -202.45261, policy_entropy: -1.01379, alpha: 0.24388, time: 33.68381
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 513.65312, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 13.30576, qf2_loss: 13.06434, policy_loss: -203.56623, policy_entropy: -1.01141, alpha: 0.24569, time: 33.55415
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 474.59787, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 12.41705, qf2_loss: 12.31468, policy_loss: -203.76309, policy_entropy: -1.01420, alpha: 0.24776, time: 33.54019
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 494.69674, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 13.14366, qf2_loss: 13.09322, policy_loss: -204.41275, policy_entropy: -1.00937, alpha: 0.24968, time: 33.38016
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 503.79271, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 12.63400, qf2_loss: 12.53309, policy_loss: -206.43396, policy_entropy: -1.01358, alpha: 0.25183, time: 33.63444
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 516.66816, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 12.91576, qf2_loss: 12.85800, policy_loss: -207.10609, policy_entropy: -1.01988, alpha: 0.25385, time: 33.60811
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 540.89744, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 14.05769, qf2_loss: 13.99688, policy_loss: -206.61652, policy_entropy: -0.99932, alpha: 0.25675, time: 33.62847
[CW] eval: return: 500.34686, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 505.67809, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 15.28605, qf2_loss: 15.12363, policy_loss: -207.39184, policy_entropy: -0.99252, alpha: 0.25518, time: 33.64082
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 529.45402, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 12.95695, qf2_loss: 12.85671, policy_loss: -208.95657, policy_entropy: -1.00060, alpha: 0.25443, time: 33.25084
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 560.02678, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 13.13309, qf2_loss: 13.00436, policy_loss: -209.81483, policy_entropy: -1.00807, alpha: 0.25516, time: 33.38184
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 553.82507, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 14.08988, qf2_loss: 14.08274, policy_loss: -211.19915, policy_entropy: -1.01843, alpha: 0.25765, time: 33.60901
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 589.18476, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 12.98643, qf2_loss: 12.83850, policy_loss: -211.69178, policy_entropy: -1.01757, alpha: 0.26010, time: 33.61971
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 497.12476, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 13.55801, qf2_loss: 13.39388, policy_loss: -211.11650, policy_entropy: -1.00079, alpha: 0.26180, time: 33.53928
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 470.15657, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 13.33125, qf2_loss: 13.11488, policy_loss: -211.90000, policy_entropy: -1.01411, alpha: 0.26323, time: 33.67462
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 396.41862, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 14.42319, qf2_loss: 14.22178, policy_loss: -212.73879, policy_entropy: -1.00496, alpha: 0.26435, time: 33.56807
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 484.87677, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 14.96693, qf2_loss: 15.04709, policy_loss: -213.61541, policy_entropy: -1.00471, alpha: 0.26544, time: 33.73967
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 493.54273, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 13.26682, qf2_loss: 13.08177, policy_loss: -215.69785, policy_entropy: -1.00807, alpha: 0.26590, time: 33.42985
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 472.27998, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 13.37672, qf2_loss: 13.24950, policy_loss: -215.89763, policy_entropy: -1.00776, alpha: 0.26804, time: 33.72845
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 504.95417, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 14.35365, qf2_loss: 14.20868, policy_loss: -216.34409, policy_entropy: -1.00609, alpha: 0.26936, time: 33.61560
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 526.89115, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 13.61875, qf2_loss: 13.45154, policy_loss: -216.61969, policy_entropy: -1.00335, alpha: 0.27005, time: 33.59523
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 463.93124, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 14.38999, qf2_loss: 14.40319, policy_loss: -216.76301, policy_entropy: -1.00946, alpha: 0.27067, time: 33.64803
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 421.21475, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 14.68917, qf2_loss: 14.48219, policy_loss: -217.89900, policy_entropy: -1.01048, alpha: 0.27236, time: 33.50950
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 531.37066, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 13.95711, qf2_loss: 13.76110, policy_loss: -218.38816, policy_entropy: -1.01776, alpha: 0.27488, time: 33.59023
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 423.45073, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 12.95001, qf2_loss: 12.74944, policy_loss: -221.36393, policy_entropy: -1.00881, alpha: 0.27764, time: 33.60548
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 479.83547, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 12.63000, qf2_loss: 12.54328, policy_loss: -221.03715, policy_entropy: -1.00655, alpha: 0.27902, time: 33.62902
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 551.02391, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 16.01941, qf2_loss: 15.91717, policy_loss: -221.69279, policy_entropy: -1.00324, alpha: 0.28003, time: 33.51793
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 483.70527, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 13.03202, qf2_loss: 12.88828, policy_loss: -220.25363, policy_entropy: -1.00511, alpha: 0.28060, time: 33.39591
[CW] eval: return: 538.63104, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 544.58148, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 14.26699, qf2_loss: 14.03446, policy_loss: -222.24612, policy_entropy: -1.00986, alpha: 0.28216, time: 33.47698
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 547.39297, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 13.63405, qf2_loss: 13.52407, policy_loss: -223.45841, policy_entropy: -1.00221, alpha: 0.28307, time: 33.51810
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 541.73392, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 13.63452, qf2_loss: 13.52145, policy_loss: -223.45141, policy_entropy: -1.00228, alpha: 0.28307, time: 33.57098
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 477.54086, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 13.47172, qf2_loss: 13.38982, policy_loss: -225.15539, policy_entropy: -1.01334, alpha: 0.28555, time: 33.36142
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 549.12764, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 13.00011, qf2_loss: 12.91550, policy_loss: -225.18390, policy_entropy: -0.99960, alpha: 0.28623, time: 33.56987
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 475.11759, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 12.52178, qf2_loss: 12.37392, policy_loss: -227.07109, policy_entropy: -1.01929, alpha: 0.28753, time: 33.50143
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 597.37712, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 13.25313, qf2_loss: 13.05268, policy_loss: -227.40530, policy_entropy: -1.00833, alpha: 0.29045, time: 33.67582
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 570.24344, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 13.80925, qf2_loss: 13.80904, policy_loss: -227.57294, policy_entropy: -1.00774, alpha: 0.29156, time: 33.56119
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 622.57016, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 14.12680, qf2_loss: 13.92388, policy_loss: -227.33473, policy_entropy: -1.00569, alpha: 0.29336, time: 33.51258
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 473.89525, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 13.28186, qf2_loss: 13.17469, policy_loss: -230.41546, policy_entropy: -1.01407, alpha: 0.29503, time: 33.25056
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 579.34161, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 13.24917, qf2_loss: 13.18446, policy_loss: -230.89322, policy_entropy: -1.00987, alpha: 0.29791, time: 34.06494
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 562.49002, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 13.57205, qf2_loss: 13.46223, policy_loss: -230.90015, policy_entropy: -1.00411, alpha: 0.29846, time: 33.72783
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 603.76441, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 13.80487, qf2_loss: 13.70149, policy_loss: -230.63948, policy_entropy: -1.00155, alpha: 0.29895, time: 33.48676
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 521.95398, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 14.18829, qf2_loss: 13.99108, policy_loss: -231.80853, policy_entropy: -0.99993, alpha: 0.29971, time: 33.54384
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 554.81778, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 14.24407, qf2_loss: 14.17798, policy_loss: -233.41145, policy_entropy: -1.01131, alpha: 0.30087, time: 33.57963
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 588.04551, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 13.68833, qf2_loss: 13.55068, policy_loss: -233.30011, policy_entropy: -1.00058, alpha: 0.30147, time: 33.54891
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 604.65522, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 13.42000, qf2_loss: 13.27541, policy_loss: -233.97063, policy_entropy: -0.99687, alpha: 0.30180, time: 33.36352
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 479.69890, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 12.39557, qf2_loss: 12.26646, policy_loss: -235.12215, policy_entropy: -1.00778, alpha: 0.30200, time: 33.49297
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 550.81232, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 13.22561, qf2_loss: 13.12594, policy_loss: -235.09335, policy_entropy: -0.99754, alpha: 0.30321, time: 33.47784
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 620.90384, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 12.20480, qf2_loss: 12.17048, policy_loss: -234.88357, policy_entropy: -1.00411, alpha: 0.30265, time: 33.57711
[CW] eval: return: 563.20293, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 558.73817, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 12.72275, qf2_loss: 12.65521, policy_loss: -236.73101, policy_entropy: -1.00868, alpha: 0.30442, time: 33.44004
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 596.27934, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 13.68096, qf2_loss: 13.67683, policy_loss: -238.40609, policy_entropy: -1.00445, alpha: 0.30524, time: 33.44413
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 614.79611, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 13.83672, qf2_loss: 13.54753, policy_loss: -238.09794, policy_entropy: -1.00017, alpha: 0.30567, time: 33.55337
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 566.51690, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 14.57762, qf2_loss: 14.53116, policy_loss: -239.27466, policy_entropy: -1.00084, alpha: 0.30605, time: 33.68468
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 635.28422, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 15.50682, qf2_loss: 15.44643, policy_loss: -239.97324, policy_entropy: -0.99952, alpha: 0.30674, time: 33.45291
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 511.81379, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 14.09996, qf2_loss: 13.93638, policy_loss: -242.06917, policy_entropy: -1.00800, alpha: 0.30665, time: 33.61450
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 552.32177, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 12.75972, qf2_loss: 12.70098, policy_loss: -241.42971, policy_entropy: -1.00884, alpha: 0.30844, time: 33.25917
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 540.83675, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 13.71633, qf2_loss: 13.42254, policy_loss: -241.06618, policy_entropy: -0.99794, alpha: 0.30920, time: 33.48226
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 623.64550, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 13.44927, qf2_loss: 13.32706, policy_loss: -243.59630, policy_entropy: -1.01414, alpha: 0.31015, time: 33.26070
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 548.71988, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 13.50172, qf2_loss: 13.45395, policy_loss: -242.82831, policy_entropy: -0.99654, alpha: 0.31154, time: 33.51201
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 561.14479, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 14.08017, qf2_loss: 14.04163, policy_loss: -244.09451, policy_entropy: -0.99730, alpha: 0.31123, time: 33.29516
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 473.83038, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 13.74984, qf2_loss: 13.60849, policy_loss: -244.61348, policy_entropy: -0.99907, alpha: 0.31014, time: 33.46557
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 511.23402, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 13.01058, qf2_loss: 12.86811, policy_loss: -244.62357, policy_entropy: -1.00314, alpha: 0.31137, time: 33.51005
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 607.55043, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 13.21315, qf2_loss: 13.22865, policy_loss: -245.41437, policy_entropy: -0.99890, alpha: 0.31066, time: 33.40199
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 530.28200, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 14.58060, qf2_loss: 14.44554, policy_loss: -246.11316, policy_entropy: -1.00933, alpha: 0.31148, time: 33.10566
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 463.24129, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 14.52296, qf2_loss: 14.36584, policy_loss: -247.56139, policy_entropy: -1.00757, alpha: 0.31372, time: 33.45778
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 511.91250, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 13.92507, qf2_loss: 13.88216, policy_loss: -248.42491, policy_entropy: -1.00672, alpha: 0.31509, time: 33.42918
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 552.28463, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 13.33761, qf2_loss: 13.27584, policy_loss: -249.56201, policy_entropy: -1.00115, alpha: 0.31631, time: 33.41376
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 546.89861, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 13.06418, qf2_loss: 12.99030, policy_loss: -248.79628, policy_entropy: -1.00792, alpha: 0.31665, time: 33.45159
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 540.75388, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 12.86366, qf2_loss: 12.78393, policy_loss: -251.23616, policy_entropy: -1.00859, alpha: 0.31916, time: 33.30365
[CW] eval: return: 539.42160, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 631.10578, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 15.53784, qf2_loss: 15.35404, policy_loss: -251.47468, policy_entropy: -0.99803, alpha: 0.32087, time: 33.35863
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 606.47096, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 14.91290, qf2_loss: 14.93612, policy_loss: -252.77855, policy_entropy: -0.99695, alpha: 0.31965, time: 33.49465
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 612.59209, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 13.56072, qf2_loss: 13.56907, policy_loss: -253.57699, policy_entropy: -1.01137, alpha: 0.31997, time: 33.39773
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 492.03603, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 13.13824, qf2_loss: 13.00626, policy_loss: -254.11833, policy_entropy: -1.01160, alpha: 0.32274, time: 33.49114
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 608.88110, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 13.37204, qf2_loss: 13.30498, policy_loss: -254.49332, policy_entropy: -0.99869, alpha: 0.32453, time: 33.28239
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 619.71012, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 13.28688, qf2_loss: 13.22777, policy_loss: -255.98317, policy_entropy: -1.01486, alpha: 0.32562, time: 33.41331
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 603.98480, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 14.26288, qf2_loss: 14.26075, policy_loss: -256.02827, policy_entropy: -1.00610, alpha: 0.32857, time: 33.32742
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 573.43363, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 13.02964, qf2_loss: 13.02629, policy_loss: -256.72267, policy_entropy: -0.99465, alpha: 0.32881, time: 33.25556
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 602.50900, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 12.92016, qf2_loss: 12.91441, policy_loss: -256.98777, policy_entropy: -1.00125, alpha: 0.32815, time: 33.46136
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 571.28842, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 14.50829, qf2_loss: 14.44693, policy_loss: -257.07636, policy_entropy: -1.00453, alpha: 0.32929, time: 33.32442
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 553.07858, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 14.73881, qf2_loss: 14.70328, policy_loss: -257.56215, policy_entropy: -0.99847, alpha: 0.32877, time: 33.42119
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 592.28976, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 13.25671, qf2_loss: 13.15471, policy_loss: -260.12002, policy_entropy: -1.00490, alpha: 0.32913, time: 33.32630
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 594.44485, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 14.20291, qf2_loss: 14.15995, policy_loss: -259.34431, policy_entropy: -1.00328, alpha: 0.33052, time: 33.38733
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 577.28706, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 14.68473, qf2_loss: 14.61887, policy_loss: -260.30914, policy_entropy: -1.01096, alpha: 0.33273, time: 33.31621
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 555.66263, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 16.01548, qf2_loss: 15.90669, policy_loss: -259.44701, policy_entropy: -0.99210, alpha: 0.33250, time: 33.36688
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 579.44367, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 14.05238, qf2_loss: 13.92358, policy_loss: -261.86638, policy_entropy: -1.00946, alpha: 0.33274, time: 33.34190
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 548.89245, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 14.48206, qf2_loss: 14.43735, policy_loss: -262.92500, policy_entropy: -1.00940, alpha: 0.33471, time: 33.28541
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 620.30343, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 14.25631, qf2_loss: 14.13789, policy_loss: -262.85273, policy_entropy: -1.01221, alpha: 0.33765, time: 33.45329
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 628.01498, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 14.26889, qf2_loss: 14.03779, policy_loss: -263.95038, policy_entropy: -1.00640, alpha: 0.34080, time: 33.16931
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 607.80419, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 14.94542, qf2_loss: 14.91438, policy_loss: -264.79140, policy_entropy: -1.00026, alpha: 0.34129, time: 33.34094
[CW] eval: return: 598.02506, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 609.71713, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 14.28386, qf2_loss: 14.16634, policy_loss: -263.84897, policy_entropy: -1.00594, alpha: 0.34193, time: 33.10186
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 489.41570, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 14.93430, qf2_loss: 14.82679, policy_loss: -267.46234, policy_entropy: -0.99745, alpha: 0.34232, time: 33.30615
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 626.91387, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 12.99506, qf2_loss: 12.96301, policy_loss: -267.76976, policy_entropy: -1.01448, alpha: 0.34464, time: 33.37203
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 671.41486, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 13.12144, qf2_loss: 13.06447, policy_loss: -268.45843, policy_entropy: -0.99848, alpha: 0.34620, time: 33.28964
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 684.24138, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 18.01883, qf2_loss: 18.03423, policy_loss: -267.94571, policy_entropy: -1.00482, alpha: 0.34651, time: 33.40416
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 686.52900, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 14.40328, qf2_loss: 14.24225, policy_loss: -269.08163, policy_entropy: -1.00654, alpha: 0.34857, time: 33.30343
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 610.41141, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 14.49247, qf2_loss: 14.30777, policy_loss: -269.96401, policy_entropy: -1.00617, alpha: 0.34968, time: 33.35599
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 620.97638, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 13.78158, qf2_loss: 13.68255, policy_loss: -270.47473, policy_entropy: -1.00550, alpha: 0.35149, time: 33.79414
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 617.34491, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 15.56080, qf2_loss: 15.54896, policy_loss: -270.70615, policy_entropy: -1.01261, alpha: 0.35393, time: 33.37138
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 692.75322, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 14.49029, qf2_loss: 14.58615, policy_loss: -271.67176, policy_entropy: -1.00395, alpha: 0.35582, time: 33.36501
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 552.87232, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 13.47080, qf2_loss: 13.39023, policy_loss: -272.58603, policy_entropy: -1.00182, alpha: 0.35672, time: 33.26881
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 632.89073, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 14.01506, qf2_loss: 13.90920, policy_loss: -273.30344, policy_entropy: -1.00363, alpha: 0.35690, time: 33.30082
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 763.91385, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 13.83511, qf2_loss: 13.70022, policy_loss: -275.72731, policy_entropy: -1.00092, alpha: 0.35907, time: 34.68720
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 537.22685, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 16.66178, qf2_loss: 16.67129, policy_loss: -276.18012, policy_entropy: -1.01221, alpha: 0.35975, time: 33.21814
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 693.56344, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 16.83424, qf2_loss: 16.83425, policy_loss: -276.59350, policy_entropy: -1.01832, alpha: 0.36387, time: 35.92968
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 617.45761, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 14.52542, qf2_loss: 14.48704, policy_loss: -277.43572, policy_entropy: -1.00207, alpha: 0.36749, time: 34.39247
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 542.40837, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 12.47131, qf2_loss: 12.43527, policy_loss: -279.44053, policy_entropy: -1.01842, alpha: 0.36960, time: 33.38041
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 677.61659, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 14.38422, qf2_loss: 14.07729, policy_loss: -280.15884, policy_entropy: -1.01004, alpha: 0.37408, time: 33.28141
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 624.89248, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 14.66185, qf2_loss: 14.48012, policy_loss: -278.04848, policy_entropy: -1.01174, alpha: 0.37659, time: 33.25116
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 607.40486, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 14.94919, qf2_loss: 14.89473, policy_loss: -279.23412, policy_entropy: -0.99464, alpha: 0.37735, time: 33.32308
[CW] eval: return: 681.04092, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 672.30286, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 15.42908, qf2_loss: 15.36104, policy_loss: -281.37915, policy_entropy: -0.99950, alpha: 0.37806, time: 33.14994
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 686.24867, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 14.26334, qf2_loss: 14.19193, policy_loss: -281.03260, policy_entropy: -1.00267, alpha: 0.37778, time: 33.36290
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 674.04730, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 13.90205, qf2_loss: 13.80038, policy_loss: -281.73426, policy_entropy: -1.00455, alpha: 0.37912, time: 33.42900
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 808.26142, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 15.34376, qf2_loss: 15.29379, policy_loss: -282.57083, policy_entropy: -1.00938, alpha: 0.38072, time: 33.30046
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 678.89592, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 15.30685, qf2_loss: 15.18229, policy_loss: -283.65806, policy_entropy: -1.01784, alpha: 0.38482, time: 33.24049
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 745.68910, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 17.52129, qf2_loss: 17.59701, policy_loss: -284.06985, policy_entropy: -0.98893, alpha: 0.38769, time: 33.32170
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 592.85479, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 15.84378, qf2_loss: 15.75002, policy_loss: -285.01627, policy_entropy: -1.01295, alpha: 0.38665, time: 33.42146
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 681.01561, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 14.52280, qf2_loss: 14.50532, policy_loss: -287.14787, policy_entropy: -1.00867, alpha: 0.38944, time: 33.30070
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 620.81893, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 15.34906, qf2_loss: 15.25102, policy_loss: -286.43709, policy_entropy: -1.00988, alpha: 0.39262, time: 33.08540
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 623.99773, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 15.73236, qf2_loss: 15.67358, policy_loss: -288.80883, policy_entropy: -1.00544, alpha: 0.39474, time: 33.04281
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 683.28040, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 15.49278, qf2_loss: 15.28913, policy_loss: -288.97392, policy_entropy: -1.00859, alpha: 0.39602, time: 33.22615
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 632.15411, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 15.41616, qf2_loss: 15.32910, policy_loss: -287.86789, policy_entropy: -0.99542, alpha: 0.39634, time: 33.11804
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 614.86183, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 14.78689, qf2_loss: 14.71181, policy_loss: -289.56559, policy_entropy: -1.00002, alpha: 0.39579, time: 33.25503
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 747.41421, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 16.93393, qf2_loss: 16.91386, policy_loss: -290.56914, policy_entropy: -1.00906, alpha: 0.39706, time: 33.34787
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 617.22966, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 17.05020, qf2_loss: 17.01777, policy_loss: -291.91014, policy_entropy: -0.99471, alpha: 0.39848, time: 33.47290
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 763.27549, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 15.82359, qf2_loss: 15.72825, policy_loss: -291.98992, policy_entropy: -1.01342, alpha: 0.39920, time: 33.26836
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 673.80829, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 15.90240, qf2_loss: 15.96414, policy_loss: -291.54314, policy_entropy: -1.00670, alpha: 0.40171, time: 33.29874
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 727.34739, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 15.43749, qf2_loss: 15.28331, policy_loss: -293.62399, policy_entropy: -1.01047, alpha: 0.40514, time: 33.23546
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 576.25377, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 17.53520, qf2_loss: 17.41471, policy_loss: -293.42893, policy_entropy: -1.00002, alpha: 0.40647, time: 32.95564
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 833.47912, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 17.64870, qf2_loss: 17.62797, policy_loss: -295.75446, policy_entropy: -1.01004, alpha: 0.40738, time: 33.21980
[CW] eval: return: 686.51443, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 656.43331, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 16.87089, qf2_loss: 16.78338, policy_loss: -297.61773, policy_entropy: -1.00549, alpha: 0.41113, time: 33.45324
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 620.67047, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 17.67205, qf2_loss: 17.57381, policy_loss: -295.52996, policy_entropy: -1.00760, alpha: 0.41166, time: 33.21287
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 834.15072, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 16.31737, qf2_loss: 16.37357, policy_loss: -298.13579, policy_entropy: -1.01297, alpha: 0.41442, time: 33.33867
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 763.32298, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 20.52206, qf2_loss: 20.45144, policy_loss: -298.28465, policy_entropy: -0.99511, alpha: 0.41676, time: 33.35565
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 619.01790, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 16.90503, qf2_loss: 16.70868, policy_loss: -299.43488, policy_entropy: -1.00757, alpha: 0.41603, time: 33.30943
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 699.03573, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 16.16937, qf2_loss: 16.04708, policy_loss: -300.77896, policy_entropy: -1.02657, alpha: 0.42118, time: 33.36365
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 682.76700, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 18.83844, qf2_loss: 18.92193, policy_loss: -300.75945, policy_entropy: -1.00200, alpha: 0.42667, time: 33.24967
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 687.40249, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 18.36358, qf2_loss: 18.16219, policy_loss: -302.28444, policy_entropy: -1.00525, alpha: 0.42614, time: 33.26694
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 806.94580, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 17.03720, qf2_loss: 16.87660, policy_loss: -302.18287, policy_entropy: -0.99948, alpha: 0.42851, time: 33.36954
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 679.79295, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 16.33956, qf2_loss: 16.18165, policy_loss: -304.72184, policy_entropy: -1.01291, alpha: 0.42846, time: 33.26559
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 600.19331, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 15.85063, qf2_loss: 15.82518, policy_loss: -303.41660, policy_entropy: -1.01248, alpha: 0.43334, time: 33.17292
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 763.04376, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 17.80075, qf2_loss: 17.97479, policy_loss: -304.30182, policy_entropy: -1.00050, alpha: 0.43621, time: 32.86819
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 612.39757, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 19.27481, qf2_loss: 19.03958, policy_loss: -304.75097, policy_entropy: -0.99330, alpha: 0.43339, time: 33.21577
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 816.90122, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 18.76285, qf2_loss: 18.83498, policy_loss: -306.22697, policy_entropy: -1.00926, alpha: 0.43496, time: 33.37510
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 815.70729, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 20.46026, qf2_loss: 20.42199, policy_loss: -307.63319, policy_entropy: -0.99832, alpha: 0.43568, time: 33.19586
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 814.92314, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 18.46181, qf2_loss: 18.35424, policy_loss: -308.24862, policy_entropy: -1.01264, alpha: 0.43731, time: 33.29519
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 829.05856, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 17.60947, qf2_loss: 17.65848, policy_loss: -307.97300, policy_entropy: -1.01521, alpha: 0.44134, time: 33.32556
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 842.71924, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 18.51439, qf2_loss: 18.34405, policy_loss: -307.38887, policy_entropy: -1.00769, alpha: 0.44570, time: 33.25062
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 677.25302, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 16.91050, qf2_loss: 16.93495, policy_loss: -310.01102, policy_entropy: -1.01020, alpha: 0.44816, time: 33.34036
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 801.94419, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 18.29080, qf2_loss: 18.13361, policy_loss: -311.17252, policy_entropy: -1.01222, alpha: 0.44984, time: 33.03585
[CW] eval: return: 791.29426, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 833.59041, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 18.60421, qf2_loss: 18.61254, policy_loss: -311.64355, policy_entropy: -1.01454, alpha: 0.45436, time: 32.86255
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 834.09742, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 25.33831, qf2_loss: 25.41063, policy_loss: -313.77055, policy_entropy: -0.99612, alpha: 0.45767, time: 33.29756
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 539.84278, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 26.70183, qf2_loss: 26.45575, policy_loss: -314.32402, policy_entropy: -0.98629, alpha: 0.45391, time: 33.30521
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 814.10640, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 17.69295, qf2_loss: 17.58955, policy_loss: -315.24554, policy_entropy: -1.01062, alpha: 0.45353, time: 33.14219
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 667.72588, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 17.21138, qf2_loss: 17.18521, policy_loss: -315.74570, policy_entropy: -1.01188, alpha: 0.45769, time: 32.83605
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 827.69967, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 19.07422, qf2_loss: 18.76569, policy_loss: -316.75476, policy_entropy: -1.01113, alpha: 0.46062, time: 33.29120
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 634.20966, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 18.79720, qf2_loss: 18.81787, policy_loss: -316.66227, policy_entropy: -1.00338, alpha: 0.46422, time: 33.11778
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 822.29377, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 19.18104, qf2_loss: 19.20780, policy_loss: -319.21382, policy_entropy: -1.00963, alpha: 0.46532, time: 33.61108
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 822.76162, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 20.21770, qf2_loss: 20.10726, policy_loss: -318.66681, policy_entropy: -1.00361, alpha: 0.46687, time: 41.10904
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 721.80389, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 21.04028, qf2_loss: 20.91956, policy_loss: -320.24818, policy_entropy: -0.99790, alpha: 0.46819, time: 33.01070
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 826.26793, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 18.34935, qf2_loss: 18.30584, policy_loss: -321.50628, policy_entropy: -1.00865, alpha: 0.46750, time: 33.36380
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 836.46368, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 19.53868, qf2_loss: 19.38931, policy_loss: -322.10623, policy_entropy: -1.01622, alpha: 0.47201, time: 33.22483
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 747.44023, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 24.88960, qf2_loss: 24.81829, policy_loss: -321.85029, policy_entropy: -0.99521, alpha: 0.47496, time: 32.95239
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 732.76761, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 25.26601, qf2_loss: 25.12526, policy_loss: -323.89329, policy_entropy: -1.00580, alpha: 0.47292, time: 33.29105
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 619.59081, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 21.85940, qf2_loss: 21.84015, policy_loss: -323.81076, policy_entropy: -1.00264, alpha: 0.47499, time: 33.23252
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 607.65455, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 20.08317, qf2_loss: 19.89681, policy_loss: -323.41603, policy_entropy: -1.01468, alpha: 0.47687, time: 33.29204
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 831.55485, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 19.71361, qf2_loss: 19.61072, policy_loss: -327.73439, policy_entropy: -1.02008, alpha: 0.48267, time: 32.90829
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 839.90867, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 19.40483, qf2_loss: 19.29774, policy_loss: -326.41970, policy_entropy: -1.00624, alpha: 0.48686, time: 32.93333
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 810.26150, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 18.91286, qf2_loss: 18.81706, policy_loss: -327.05642, policy_entropy: -1.00406, alpha: 0.48948, time: 33.25331
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 829.75785, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 22.18870, qf2_loss: 22.24046, policy_loss: -328.32076, policy_entropy: -1.00678, alpha: 0.49093, time: 33.26179
[CW] eval: return: 738.55217, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 746.36537, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 23.80634, qf2_loss: 23.87872, policy_loss: -329.72610, policy_entropy: -1.00492, alpha: 0.49144, time: 33.36477
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 818.83677, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 20.99728, qf2_loss: 20.91974, policy_loss: -330.97009, policy_entropy: -1.00352, alpha: 0.49266, time: 33.31586
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 835.78386, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 20.91233, qf2_loss: 20.99942, policy_loss: -332.99085, policy_entropy: -1.01446, alpha: 0.49612, time: 33.33655
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 804.45727, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 23.80390, qf2_loss: 23.65008, policy_loss: -333.62050, policy_entropy: -1.00348, alpha: 0.49928, time: 33.17609
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 687.53486, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 23.45739, qf2_loss: 23.19187, policy_loss: -334.32065, policy_entropy: -1.01122, alpha: 0.50109, time: 33.55520
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 827.16823, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 27.04046, qf2_loss: 26.90005, policy_loss: -334.57667, policy_entropy: -1.00533, alpha: 0.50476, time: 33.30398
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 800.34975, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 21.09892, qf2_loss: 20.94580, policy_loss: -334.22070, policy_entropy: -1.00373, alpha: 0.50642, time: 33.28860
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 802.76946, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 22.42546, qf2_loss: 22.20299, policy_loss: -336.69647, policy_entropy: -1.00177, alpha: 0.50717, time: 33.28412
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 835.37406, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 23.37630, qf2_loss: 23.54787, policy_loss: -336.14266, policy_entropy: -0.99533, alpha: 0.50636, time: 33.02041
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 602.73301, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 25.50765, qf2_loss: 25.23001, policy_loss: -337.19000, policy_entropy: -0.99300, alpha: 0.50596, time: 33.11394
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 839.21856, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 22.60616, qf2_loss: 22.63052, policy_loss: -337.10269, policy_entropy: -1.00742, alpha: 0.50478, time: 33.37675
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 835.91612, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 24.48958, qf2_loss: 24.44528, policy_loss: -339.00359, policy_entropy: -0.99806, alpha: 0.50587, time: 33.13725
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 841.61315, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 24.93782, qf2_loss: 24.98040, policy_loss: -340.58435, policy_entropy: -1.00894, alpha: 0.50697, time: 33.31375
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 815.12372, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 23.09407, qf2_loss: 23.05470, policy_loss: -340.16449, policy_entropy: -1.00614, alpha: 0.50936, time: 33.18709
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 845.20026, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 24.51811, qf2_loss: 24.45899, policy_loss: -342.42771, policy_entropy: -1.01096, alpha: 0.51214, time: 33.12961
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 825.06909, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 23.62470, qf2_loss: 23.67590, policy_loss: -343.15831, policy_entropy: -1.00074, alpha: 0.51461, time: 33.02468
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 827.67517, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 23.85584, qf2_loss: 23.67938, policy_loss: -345.59488, policy_entropy: -1.00321, alpha: 0.51475, time: 33.28056
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 831.62905, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 23.54227, qf2_loss: 23.37679, policy_loss: -340.74929, policy_entropy: -0.99145, alpha: 0.51413, time: 32.91806
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 848.32792, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 23.78473, qf2_loss: 23.72603, policy_loss: -344.38829, policy_entropy: -1.00409, alpha: 0.51453, time: 33.00093
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 810.65300, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 30.92996, qf2_loss: 31.02741, policy_loss: -345.74937, policy_entropy: -0.99857, alpha: 0.51383, time: 33.15511
[CW] eval: return: 803.04860, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 835.22303, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 27.11979, qf2_loss: 26.98802, policy_loss: -347.41830, policy_entropy: -1.00821, alpha: 0.51461, time: 33.22604
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 827.50254, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 27.45914, qf2_loss: 27.35494, policy_loss: -349.76144, policy_entropy: -1.00902, alpha: 0.51742, time: 33.38094
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 815.98085, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 24.82653, qf2_loss: 24.59288, policy_loss: -350.38706, policy_entropy: -1.01454, alpha: 0.52042, time: 33.31559
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 846.57315, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 23.29889, qf2_loss: 23.05415, policy_loss: -352.02190, policy_entropy: -0.99940, alpha: 0.52440, time: 32.95958
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 662.50584, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 23.70652, qf2_loss: 23.74886, policy_loss: -351.10453, policy_entropy: -1.00775, alpha: 0.52374, time: 32.90937
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 760.05410, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 25.12841, qf2_loss: 25.11049, policy_loss: -352.89846, policy_entropy: -1.00461, alpha: 0.52788, time: 33.02982
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 740.02731, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 26.29611, qf2_loss: 26.28660, policy_loss: -351.97055, policy_entropy: -0.99414, alpha: 0.52721, time: 33.15303
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 845.98218, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 27.93955, qf2_loss: 28.28190, policy_loss: -354.91490, policy_entropy: -1.01195, alpha: 0.52839, time: 33.18935
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 842.39786, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 25.91137, qf2_loss: 25.93389, policy_loss: -354.25634, policy_entropy: -1.01138, alpha: 0.53330, time: 32.98727
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 814.02391, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 28.38153, qf2_loss: 28.12725, policy_loss: -356.98946, policy_entropy: -1.01675, alpha: 0.53832, time: 33.06515
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 812.16591, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 26.75547, qf2_loss: 26.49327, policy_loss: -356.07172, policy_entropy: -0.99054, alpha: 0.53889, time: 33.17848
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 846.19037, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 25.88603, qf2_loss: 25.74848, policy_loss: -358.59827, policy_entropy: -1.00952, alpha: 0.53771, time: 33.11620
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 844.70782, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 25.78811, qf2_loss: 25.72791, policy_loss: -358.38000, policy_entropy: -0.99954, alpha: 0.54090, time: 33.03719
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 829.35264, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 34.93123, qf2_loss: 35.05319, policy_loss: -359.48650, policy_entropy: -1.00739, alpha: 0.54083, time: 32.93576
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 827.14386, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 28.93280, qf2_loss: 28.76274, policy_loss: -361.68418, policy_entropy: -1.00622, alpha: 0.54426, time: 33.12111
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 603.91750, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 25.00773, qf2_loss: 24.94156, policy_loss: -362.12955, policy_entropy: -1.00584, alpha: 0.54574, time: 33.00377
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 829.40126, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 25.41796, qf2_loss: 25.16828, policy_loss: -362.75924, policy_entropy: -0.99928, alpha: 0.54618, time: 33.08671
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 827.08674, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 27.71539, qf2_loss: 27.51918, policy_loss: -363.78327, policy_entropy: -1.00475, alpha: 0.54725, time: 33.00031
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 835.89267, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 31.82558, qf2_loss: 31.96820, policy_loss: -365.00875, policy_entropy: -1.00059, alpha: 0.54906, time: 33.05156
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 842.55348, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 30.84042, qf2_loss: 30.46122, policy_loss: -365.65737, policy_entropy: -0.99386, alpha: 0.54742, time: 33.05532
[CW] eval: return: 796.20273, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 850.41002, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 42.59414, qf2_loss: 42.56664, policy_loss: -365.18047, policy_entropy: -1.00721, alpha: 0.54634, time: 35.38926
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 816.17659, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 31.17094, qf2_loss: 31.48134, policy_loss: -368.56766, policy_entropy: -1.00044, alpha: 0.54929, time: 33.30851
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 829.78325, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 25.04715, qf2_loss: 24.88866, policy_loss: -370.30848, policy_entropy: -1.00575, alpha: 0.54880, time: 33.05040
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 844.36744, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 25.54942, qf2_loss: 25.41478, policy_loss: -368.19777, policy_entropy: -0.99584, alpha: 0.55062, time: 33.22843
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 603.69437, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 26.49136, qf2_loss: 26.26111, policy_loss: -370.75927, policy_entropy: -1.00879, alpha: 0.55043, time: 33.15792
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 822.02602, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 27.30449, qf2_loss: 27.29054, policy_loss: -373.88005, policy_entropy: -1.00516, alpha: 0.55491, time: 33.84424
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 854.25864, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 28.60988, qf2_loss: 28.56389, policy_loss: -371.29610, policy_entropy: -1.01035, alpha: 0.55646, time: 33.28295
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 844.09125, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 26.23165, qf2_loss: 26.00380, policy_loss: -375.04654, policy_entropy: -1.00727, alpha: 0.55910, time: 33.14928
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 675.37593, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 28.45081, qf2_loss: 28.32569, policy_loss: -375.64289, policy_entropy: -1.00375, alpha: 0.56057, time: 33.13409
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 829.60506, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 28.80223, qf2_loss: 28.54026, policy_loss: -373.83301, policy_entropy: -1.00110, alpha: 0.56201, time: 35.03159
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 845.85804, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 29.18371, qf2_loss: 29.11715, policy_loss: -376.46129, policy_entropy: -1.00480, alpha: 0.56309, time: 33.31731
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 850.12685, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 31.00597, qf2_loss: 30.72607, policy_loss: -376.19021, policy_entropy: -1.00654, alpha: 0.56400, time: 33.03637
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 850.14869, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 30.60790, qf2_loss: 30.61214, policy_loss: -378.61872, policy_entropy: -1.00087, alpha: 0.56666, time: 32.97325
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 540.00352, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 33.30941, qf2_loss: 33.08997, policy_loss: -378.91323, policy_entropy: -1.00300, alpha: 0.56716, time: 33.08428
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 815.35376, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 34.27595, qf2_loss: 34.34659, policy_loss: -380.28563, policy_entropy: -1.00344, alpha: 0.56704, time: 33.18257
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 828.53266, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 29.37756, qf2_loss: 29.26013, policy_loss: -379.79502, policy_entropy: -0.99034, alpha: 0.56901, time: 33.15272
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 723.97608, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 28.71450, qf2_loss: 28.71517, policy_loss: -381.46974, policy_entropy: -1.00418, alpha: 0.56641, time: 33.16557
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 850.47881, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 32.05676, qf2_loss: 31.92900, policy_loss: -380.44997, policy_entropy: -1.00480, alpha: 0.56758, time: 33.07426
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 554.58899, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 51.06860, qf2_loss: 51.38477, policy_loss: -381.20541, policy_entropy: -1.00089, alpha: 0.56995, time: 33.13948
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 658.06082, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 49.84944, qf2_loss: 49.72866, policy_loss: -384.69348, policy_entropy: -1.00423, alpha: 0.57017, time: 33.08979
[CW] eval: return: 717.86653, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 594.89478, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 32.52904, qf2_loss: 32.28921, policy_loss: -385.38937, policy_entropy: -0.99093, alpha: 0.56933, time: 32.91668
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 847.32391, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 28.08149, qf2_loss: 27.83956, policy_loss: -385.33998, policy_entropy: -1.00249, alpha: 0.56811, time: 32.97749
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 729.98361, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 28.32010, qf2_loss: 28.47588, policy_loss: -387.20507, policy_entropy: -1.00926, alpha: 0.57003, time: 33.00342
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 847.51399, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 30.27191, qf2_loss: 29.90805, policy_loss: -386.70131, policy_entropy: -1.01876, alpha: 0.57549, time: 33.22391
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 851.69065, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 32.88928, qf2_loss: 32.86316, policy_loss: -388.60037, policy_entropy: -1.00881, alpha: 0.58099, time: 33.06502
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 840.44338, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 43.39845, qf2_loss: 43.78374, policy_loss: -388.29789, policy_entropy: -0.99453, alpha: 0.58235, time: 33.09839
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 851.17790, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 33.29189, qf2_loss: 33.27080, policy_loss: -389.60664, policy_entropy: -0.99434, alpha: 0.58095, time: 33.04472
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 850.95304, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 31.10954, qf2_loss: 30.80068, policy_loss: -391.47179, policy_entropy: -1.00067, alpha: 0.57909, time: 33.24996
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 843.59948, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 29.95692, qf2_loss: 29.98136, policy_loss: -388.52376, policy_entropy: -0.99911, alpha: 0.57682, time: 33.21678
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 845.23336, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 38.36953, qf2_loss: 38.33734, policy_loss: -392.73910, policy_entropy: -1.01285, alpha: 0.58075, time: 33.20252
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 690.79715, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 55.38408, qf2_loss: 55.14463, policy_loss: -393.93452, policy_entropy: -1.00629, alpha: 0.58480, time: 33.20939
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 755.76625, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 32.55041, qf2_loss: 32.57945, policy_loss: -394.68702, policy_entropy: -1.00329, alpha: 0.58644, time: 32.85147
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 545.92745, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 30.86179, qf2_loss: 30.81261, policy_loss: -395.44040, policy_entropy: -1.00394, alpha: 0.58740, time: 33.10793
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 844.16892, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 31.30699, qf2_loss: 31.22478, policy_loss: -394.52270, policy_entropy: -1.00283, alpha: 0.58891, time: 33.10500
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 845.46865, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 32.63308, qf2_loss: 32.53480, policy_loss: -398.09216, policy_entropy: -1.00944, alpha: 0.59063, time: 33.10755
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 852.53627, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 33.70630, qf2_loss: 33.56929, policy_loss: -397.72818, policy_entropy: -1.00435, alpha: 0.59566, time: 32.86204
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 619.83490, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 37.79349, qf2_loss: 37.92004, policy_loss: -396.75872, policy_entropy: -0.99364, alpha: 0.59395, time: 32.72809
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 845.21184, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 39.48489, qf2_loss: 39.25724, policy_loss: -400.93475, policy_entropy: -0.98990, alpha: 0.59060, time: 33.16341
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 843.76960, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 34.71121, qf2_loss: 35.09780, policy_loss: -399.37725, policy_entropy: -0.99600, alpha: 0.58858, time: 33.26527
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 761.11340, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 34.63657, qf2_loss: 34.69915, policy_loss: -402.46848, policy_entropy: -1.01847, alpha: 0.59077, time: 33.27881
[CW] eval: return: 794.04621, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 851.83253, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 36.25905, qf2_loss: 36.39546, policy_loss: -402.03876, policy_entropy: -0.98853, alpha: 0.59247, time: 33.12731
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 828.73135, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 36.77857, qf2_loss: 36.57827, policy_loss: -401.58756, policy_entropy: -0.99970, alpha: 0.59041, time: 33.19438
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 850.68608, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 31.74657, qf2_loss: 31.58318, policy_loss: -405.97997, policy_entropy: -1.00033, alpha: 0.58921, time: 33.13310
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 835.52356, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 35.13558, qf2_loss: 35.18678, policy_loss: -402.76667, policy_entropy: -0.99907, alpha: 0.58917, time: 33.17190
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 619.17850, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 34.05969, qf2_loss: 33.98468, policy_loss: -405.88499, policy_entropy: -0.99669, alpha: 0.58923, time: 33.12885
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 841.13998, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 40.11200, qf2_loss: 40.08487, policy_loss: -408.54504, policy_entropy: -1.00126, alpha: 0.58912, time: 32.94230
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 847.14871, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 36.82298, qf2_loss: 36.85696, policy_loss: -405.87317, policy_entropy: -0.99370, alpha: 0.58857, time: 33.09195
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 835.34368, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 34.07635, qf2_loss: 34.16027, policy_loss: -406.74467, policy_entropy: -1.00275, alpha: 0.58736, time: 33.06786
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 842.60805, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 36.12723, qf2_loss: 36.05695, policy_loss: -409.75816, policy_entropy: -1.01154, alpha: 0.58713, time: 33.15293
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 853.28282, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 37.74659, qf2_loss: 37.80038, policy_loss: -409.98215, policy_entropy: -1.00553, alpha: 0.59211, time: 33.24699
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 832.38090, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 40.21593, qf2_loss: 39.63656, policy_loss: -408.93537, policy_entropy: -0.99906, alpha: 0.59365, time: 33.06207
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 853.20802, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 42.34797, qf2_loss: 42.61573, policy_loss: -413.15303, policy_entropy: -1.00622, alpha: 0.59518, time: 33.22788
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 845.41776, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 35.52112, qf2_loss: 34.94430, policy_loss: -412.44462, policy_entropy: -0.99103, alpha: 0.59379, time: 33.05908
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 845.72040, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 37.11596, qf2_loss: 36.72117, policy_loss: -413.92793, policy_entropy: -0.99431, alpha: 0.59136, time: 33.19950
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 845.08789, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 34.33910, qf2_loss: 34.40873, policy_loss: -416.63053, policy_entropy: -0.98754, alpha: 0.58741, time: 33.35791
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 841.84230, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 32.93776, qf2_loss: 32.93253, policy_loss: -416.21171, policy_entropy: -1.00311, alpha: 0.58615, time: 33.20479
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 843.72815, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 35.68168, qf2_loss: 35.90465, policy_loss: -416.52768, policy_entropy: -1.01005, alpha: 0.58903, time: 33.69490
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 849.85432, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 45.83794, qf2_loss: 45.91790, policy_loss: -419.54397, policy_entropy: -1.00596, alpha: 0.59198, time: 33.04820
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 808.22709, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 56.93675, qf2_loss: 56.74473, policy_loss: -415.78806, policy_entropy: -1.00120, alpha: 0.59045, time: 32.89500
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 817.02926, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 54.53809, qf2_loss: 54.96122, policy_loss: -417.69028, policy_entropy: -1.00012, alpha: 0.59398, time: 33.01580
[CW] eval: return: 807.52264, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 830.93546, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 38.23761, qf2_loss: 38.09951, policy_loss: -420.01142, policy_entropy: -1.00175, alpha: 0.59385, time: 32.90256
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 842.36347, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 33.06664, qf2_loss: 33.21065, policy_loss: -420.88645, policy_entropy: -0.99214, alpha: 0.59141, time: 33.34678
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 831.34147, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 33.02286, qf2_loss: 32.93940, policy_loss: -423.17906, policy_entropy: -0.99791, alpha: 0.58982, time: 33.16195
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 845.77619, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 37.88098, qf2_loss: 37.50531, policy_loss: -418.49824, policy_entropy: -0.99835, alpha: 0.58970, time: 32.99861
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 851.79482, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 35.05063, qf2_loss: 34.71110, policy_loss: -422.57294, policy_entropy: -0.98634, alpha: 0.58685, time: 35.26109
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 839.10231, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 39.90811, qf2_loss: 39.77156, policy_loss: -426.06795, policy_entropy: -1.01941, alpha: 0.58638, time: 33.35221
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 848.73482, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 38.35555, qf2_loss: 38.61579, policy_loss: -423.61355, policy_entropy: -1.00443, alpha: 0.59249, time: 33.01891
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 834.72508, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 37.27607, qf2_loss: 36.59307, policy_loss: -425.70311, policy_entropy: -1.00677, alpha: 0.59481, time: 33.08266
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 550.10984, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 35.41656, qf2_loss: 35.21620, policy_loss: -425.50892, policy_entropy: -1.00482, alpha: 0.59511, time: 33.33134
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 840.66868, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 34.93541, qf2_loss: 34.80050, policy_loss: -427.61961, policy_entropy: -1.00660, alpha: 0.59937, time: 33.27913
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 843.56932, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 38.33787, qf2_loss: 37.97331, policy_loss: -426.19061, policy_entropy: -1.00934, alpha: 0.60091, time: 33.20859
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 846.35862, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 38.05335, qf2_loss: 38.25540, policy_loss: -431.60573, policy_entropy: -0.99552, alpha: 0.60389, time: 33.17898
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 702.07224, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 40.63437, qf2_loss: 40.80565, policy_loss: -428.50392, policy_entropy: -0.99109, alpha: 0.59981, time: 33.36597
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 848.75971, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 39.24747, qf2_loss: 39.16999, policy_loss: -431.40252, policy_entropy: -0.99423, alpha: 0.59742, time: 33.47550
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 769.67714, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 44.47379, qf2_loss: 44.36990, policy_loss: -432.82982, policy_entropy: -1.00441, alpha: 0.59809, time: 33.21453
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 845.79224, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 39.71870, qf2_loss: 39.75538, policy_loss: -434.49439, policy_entropy: -1.00241, alpha: 0.59809, time: 33.33503
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 841.00983, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 33.85099, qf2_loss: 33.74556, policy_loss: -433.91105, policy_entropy: -1.00765, alpha: 0.59912, time: 33.30284
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 851.88160, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 36.09019, qf2_loss: 35.92639, policy_loss: -430.41567, policy_entropy: -0.99657, alpha: 0.60206, time: 33.23623
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 853.00129, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 41.52035, qf2_loss: 41.33738, policy_loss: -433.57047, policy_entropy: -1.00561, alpha: 0.60222, time: 33.15405
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 687.39822, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 39.88237, qf2_loss: 39.76744, policy_loss: -436.97492, policy_entropy: -0.99147, alpha: 0.60219, time: 33.14128
[CW] eval: return: 801.19011, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 689.06108, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 39.11847, qf2_loss: 39.36911, policy_loss: -438.27001, policy_entropy: -1.00307, alpha: 0.60071, time: 33.34477
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 851.62257, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 38.79381, qf2_loss: 38.72052, policy_loss: -435.68920, policy_entropy: -1.00074, alpha: 0.60040, time: 33.11964
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 690.39208, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 59.42930, qf2_loss: 59.25441, policy_loss: -437.30941, policy_entropy: -1.00189, alpha: 0.60183, time: 33.06067
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 635.05891, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 43.63201, qf2_loss: 43.52889, policy_loss: -437.83724, policy_entropy: -0.99949, alpha: 0.60282, time: 33.21975
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 851.42982, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 36.89178, qf2_loss: 36.71778, policy_loss: -439.23694, policy_entropy: -0.98932, alpha: 0.59989, time: 33.22261
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 849.16249, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 36.18788, qf2_loss: 36.41798, policy_loss: -445.19407, policy_entropy: -0.99797, alpha: 0.59676, time: 33.20249
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 765.86717, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 39.60722, qf2_loss: 39.85617, policy_loss: -440.16127, policy_entropy: -0.99357, alpha: 0.59619, time: 33.09832
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 842.68988, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 36.08710, qf2_loss: 36.38579, policy_loss: -441.98642, policy_entropy: -0.99936, alpha: 0.59497, time: 33.40439
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 852.16797, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 38.50480, qf2_loss: 38.17424, policy_loss: -442.94223, policy_entropy: -1.00148, alpha: 0.59591, time: 33.16484
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 850.75277, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 37.73442, qf2_loss: 37.73042, policy_loss: -444.32501, policy_entropy: -0.99551, alpha: 0.59256, time: 33.22165
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 850.88411, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 42.25160, qf2_loss: 42.35115, policy_loss: -445.51451, policy_entropy: -1.00531, alpha: 0.59335, time: 32.97188
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 757.49067, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 38.76532, qf2_loss: 38.59557, policy_loss: -446.92192, policy_entropy: -1.01790, alpha: 0.59873, time: 33.01259
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 826.24721, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 39.22154, qf2_loss: 39.15053, policy_loss: -446.73937, policy_entropy: -1.00725, alpha: 0.60294, time: 33.31916
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 847.57135, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 46.14825, qf2_loss: 46.02107, policy_loss: -446.37801, policy_entropy: -0.99329, alpha: 0.60434, time: 33.09553
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 756.71316, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 51.19204, qf2_loss: 51.67622, policy_loss: -447.29136, policy_entropy: -1.00057, alpha: 0.60058, time: 33.09498
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 846.89095, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 46.92569, qf2_loss: 46.84551, policy_loss: -449.01387, policy_entropy: -0.98700, alpha: 0.59968, time: 33.22217
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 834.67346, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 47.51783, qf2_loss: 47.64153, policy_loss: -448.32719, policy_entropy: -1.00398, alpha: 0.59889, time: 33.17233
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 844.75585, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 41.14767, qf2_loss: 40.98545, policy_loss: -451.23237, policy_entropy: -0.99166, alpha: 0.59858, time: 33.27806
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 846.76287, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 49.97382, qf2_loss: 50.50508, policy_loss: -449.05188, policy_entropy: -1.00428, alpha: 0.59745, time: 33.10778
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 852.26588, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 76.16298, qf2_loss: 75.77621, policy_loss: -449.56555, policy_entropy: -0.99128, alpha: 0.59466, time: 33.20114
[CW] eval: return: 821.86742, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 824.86488, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 65.26674, qf2_loss: 64.99088, policy_loss: -451.58611, policy_entropy: -0.98173, alpha: 0.58938, time: 33.27134
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 845.44876, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 43.76590, qf2_loss: 43.72102, policy_loss: -452.14322, policy_entropy: -1.00132, alpha: 0.58831, time: 33.26015
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 844.80789, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 45.54420, qf2_loss: 45.31181, policy_loss: -454.08983, policy_entropy: -1.00034, alpha: 0.58941, time: 33.22167
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 848.51482, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 36.54444, qf2_loss: 36.52597, policy_loss: -458.43377, policy_entropy: -0.98712, alpha: 0.58661, time: 33.04868
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 846.48004, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 37.74461, qf2_loss: 37.54470, policy_loss: -451.41005, policy_entropy: -1.01016, alpha: 0.58472, time: 33.09525
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 830.18020, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 37.01590, qf2_loss: 37.66831, policy_loss: -455.41926, policy_entropy: -0.98972, alpha: 0.58438, time: 33.10951
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 841.21559, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 43.28040, qf2_loss: 43.16699, policy_loss: -457.68904, policy_entropy: -0.99042, alpha: 0.58003, time: 33.32437
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 835.12841, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 61.35038, qf2_loss: 61.94571, policy_loss: -459.25431, policy_entropy: -1.01077, alpha: 0.57967, time: 33.25188
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 844.32847, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 41.59245, qf2_loss: 41.30403, policy_loss: -459.99966, policy_entropy: -0.98602, alpha: 0.58281, time: 33.08962
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 837.25170, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 34.50125, qf2_loss: 34.17141, policy_loss: -459.80855, policy_entropy: -0.98845, alpha: 0.57560, time: 33.29822
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 837.82637, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 35.49126, qf2_loss: 35.49505, policy_loss: -462.49548, policy_entropy: -0.98906, alpha: 0.57327, time: 33.12589
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 854.21501, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 41.48861, qf2_loss: 41.28538, policy_loss: -461.74132, policy_entropy: -0.99860, alpha: 0.57130, time: 33.21712
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 846.97895, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 39.06579, qf2_loss: 39.15448, policy_loss: -461.15312, policy_entropy: -1.01463, alpha: 0.57139, time: 33.31848
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 828.09927, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 34.24774, qf2_loss: 34.01930, policy_loss: -463.03858, policy_entropy: -0.98847, alpha: 0.57281, time: 35.68944
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 832.22716, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 38.63248, qf2_loss: 38.37184, policy_loss: -465.33183, policy_entropy: -0.99039, alpha: 0.57126, time: 33.31230
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 836.27196, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 43.63607, qf2_loss: 44.92063, policy_loss: -464.82994, policy_entropy: -1.01791, alpha: 0.56909, time: 33.18645
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 823.45325, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 47.14316, qf2_loss: 46.71657, policy_loss: -465.38290, policy_entropy: -1.00713, alpha: 0.57640, time: 33.29928
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 783.53802, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 42.98299, qf2_loss: 42.60984, policy_loss: -465.56037, policy_entropy: -0.98083, alpha: 0.57346, time: 33.16718
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 825.26979, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 46.94914, qf2_loss: 46.38742, policy_loss: -467.21092, policy_entropy: -1.00784, alpha: 0.57155, time: 33.85427
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 680.82239, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 44.67482, qf2_loss: 44.37094, policy_loss: -469.16690, policy_entropy: -1.00677, alpha: 0.57362, time: 32.97402
[CW] eval: return: 713.10452, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 771.04841, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 41.93834, qf2_loss: 42.08896, policy_loss: -467.48246, policy_entropy: -0.98834, alpha: 0.57288, time: 33.71323
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 844.49687, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 43.71253, qf2_loss: 42.89846, policy_loss: -472.62900, policy_entropy: -1.00117, alpha: 0.57053, time: 37.64471
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 832.88051, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 37.58984, qf2_loss: 37.50871, policy_loss: -472.46954, policy_entropy: -0.99335, alpha: 0.57088, time: 32.58199
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 761.60174, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 53.39591, qf2_loss: 53.41104, policy_loss: -471.59066, policy_entropy: -1.01321, alpha: 0.57015, time: 32.91796
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 840.60263, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 54.20096, qf2_loss: 54.91618, policy_loss: -473.69733, policy_entropy: -1.00470, alpha: 0.57424, time: 33.16730
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 838.37993, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 52.08720, qf2_loss: 52.91556, policy_loss: -472.70523, policy_entropy: -0.99741, alpha: 0.57534, time: 33.24630
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 844.47014, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 42.32728, qf2_loss: 41.75875, policy_loss: -468.59454, policy_entropy: -0.98961, alpha: 0.57340, time: 33.26354
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 834.15917, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 40.97297, qf2_loss: 40.29359, policy_loss: -474.36899, policy_entropy: -0.98956, alpha: 0.56852, time: 33.15744
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 836.45858, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 42.30792, qf2_loss: 41.95713, policy_loss: -473.20204, policy_entropy: -0.99949, alpha: 0.56701, time: 33.21500
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 845.42844, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 36.73146, qf2_loss: 36.74711, policy_loss: -474.22628, policy_entropy: -0.97956, alpha: 0.56420, time: 33.14959
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 841.40821, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 46.21897, qf2_loss: 45.57933, policy_loss: -476.23045, policy_entropy: -0.99024, alpha: 0.55801, time: 33.24136
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 826.10109, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 40.78127, qf2_loss: 40.84996, policy_loss: -476.06736, policy_entropy: -0.99224, alpha: 0.55610, time: 33.25342
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 844.90647, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 39.08391, qf2_loss: 38.56931, policy_loss: -481.46034, policy_entropy: -0.98986, alpha: 0.55203, time: 33.18839
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 845.60162, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 38.04243, qf2_loss: 37.98131, policy_loss: -478.65001, policy_entropy: -1.00952, alpha: 0.55231, time: 33.38296
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 601.56303, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 40.77512, qf2_loss: 40.67939, policy_loss: -478.82085, policy_entropy: -0.99592, alpha: 0.55298, time: 33.37125
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 840.76247, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 42.93807, qf2_loss: 43.51750, policy_loss: -478.91541, policy_entropy: -0.99114, alpha: 0.55296, time: 33.25385
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 838.78024, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 46.28291, qf2_loss: 46.66026, policy_loss: -478.63073, policy_entropy: -1.00222, alpha: 0.55034, time: 33.08676
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 830.46447, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 43.64787, qf2_loss: 43.36197, policy_loss: -481.06693, policy_entropy: -1.01557, alpha: 0.55303, time: 32.98687
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 764.89996, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 43.84494, qf2_loss: 43.88971, policy_loss: -479.70866, policy_entropy: -1.01392, alpha: 0.55726, time: 32.86347
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 840.41664, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 47.31300, qf2_loss: 47.09705, policy_loss: -480.33091, policy_entropy: -0.99607, alpha: 0.56027, time: 33.05712
[CW] eval: return: 809.99084, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 839.19379, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 51.38735, qf2_loss: 50.73231, policy_loss: -483.38289, policy_entropy: -1.02710, alpha: 0.56246, time: 33.38139
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 833.38918, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 51.46694, qf2_loss: 51.64398, policy_loss: -485.35803, policy_entropy: -1.00001, alpha: 0.56844, time: 33.18924
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 755.14615, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 42.49059, qf2_loss: 42.28312, policy_loss: -481.85759, policy_entropy: -1.00183, alpha: 0.56920, time: 33.26670
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 834.73973, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 39.38470, qf2_loss: 39.11276, policy_loss: -487.71951, policy_entropy: -0.98412, alpha: 0.56553, time: 33.05690
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 827.61989, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 46.97734, qf2_loss: 46.73912, policy_loss: -489.02146, policy_entropy: -0.99917, alpha: 0.56284, time: 33.28675
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 828.82166, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 42.95893, qf2_loss: 43.02701, policy_loss: -487.72331, policy_entropy: -1.00635, alpha: 0.56210, time: 33.39367
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 838.55242, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 39.88639, qf2_loss: 40.04660, policy_loss: -487.18154, policy_entropy: -0.98463, alpha: 0.56202, time: 33.25127
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 847.32472, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 43.61945, qf2_loss: 43.66116, policy_loss: -486.33495, policy_entropy: -1.00550, alpha: 0.56134, time: 33.30231
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 822.05212, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 39.95315, qf2_loss: 39.72213, policy_loss: -490.35098, policy_entropy: -0.98989, alpha: 0.56084, time: 33.36978
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 843.77066, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 39.65870, qf2_loss: 39.68230, policy_loss: -487.49375, policy_entropy: -1.00649, alpha: 0.56035, time: 33.25477
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 840.98295, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 40.77227, qf2_loss: 40.60956, policy_loss: -490.91783, policy_entropy: -1.00089, alpha: 0.56172, time: 33.31678
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 840.12168, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 46.79221, qf2_loss: 46.65818, policy_loss: -488.35654, policy_entropy: -1.00662, alpha: 0.56233, time: 33.18896
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 834.82095, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 44.90176, qf2_loss: 45.37568, policy_loss: -494.78136, policy_entropy: -0.99842, alpha: 0.56468, time: 33.27229
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 829.24933, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 43.04716, qf2_loss: 43.04196, policy_loss: -493.39261, policy_entropy: -0.99284, alpha: 0.56013, time: 33.32918
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 836.05457, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 56.21556, qf2_loss: 55.77777, policy_loss: -495.38350, policy_entropy: -1.00891, alpha: 0.56133, time: 33.20808
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 828.20752, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 51.22938, qf2_loss: 51.37103, policy_loss: -495.99348, policy_entropy: -1.00181, alpha: 0.56500, time: 33.16460
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 833.16284, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 41.48093, qf2_loss: 41.14184, policy_loss: -497.35060, policy_entropy: -0.99556, alpha: 0.56232, time: 33.22022
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 840.11091, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 41.74723, qf2_loss: 41.30132, policy_loss: -494.12844, policy_entropy: -0.99403, alpha: 0.56148, time: 33.50104
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 842.40944, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 45.14982, qf2_loss: 45.17349, policy_loss: -496.59461, policy_entropy: -1.00644, alpha: 0.56091, time: 33.22366
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 834.42927, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 45.03639, qf2_loss: 45.38179, policy_loss: -491.55032, policy_entropy: -1.00281, alpha: 0.56226, time: 32.89466
[CW] eval: return: 840.91060, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 845.97298, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 45.79528, qf2_loss: 46.27281, policy_loss: -495.98106, policy_entropy: -1.00294, alpha: 0.56363, time: 33.25399
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 832.85453, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 44.87136, qf2_loss: 44.54059, policy_loss: -496.52977, policy_entropy: -1.00449, alpha: 0.56501, time: 33.39512
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 849.83549, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 47.53008, qf2_loss: 47.36419, policy_loss: -500.15328, policy_entropy: -0.99404, alpha: 0.56598, time: 33.31199
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 833.74473, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 47.64900, qf2_loss: 47.52785, policy_loss: -501.18341, policy_entropy: -0.99396, alpha: 0.56349, time: 33.23486
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 836.02531, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 56.58494, qf2_loss: 56.01882, policy_loss: -497.01855, policy_entropy: -1.01728, alpha: 0.56362, time: 33.44761
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 836.44072, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 40.67345, qf2_loss: 40.21948, policy_loss: -499.79721, policy_entropy: -0.99680, alpha: 0.56809, time: 32.80785
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 823.80124, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 40.87072, qf2_loss: 40.84419, policy_loss: -499.39682, policy_entropy: -0.98625, alpha: 0.56440, time: 33.27274
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 763.16428, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 41.06392, qf2_loss: 41.41763, policy_loss: -504.30249, policy_entropy: -0.98716, alpha: 0.56023, time: 33.27392
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 837.91269, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 45.33850, qf2_loss: 46.03057, policy_loss: -502.06140, policy_entropy: -1.00814, alpha: 0.55771, time: 33.26330
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 757.22934, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 61.71077, qf2_loss: 61.47815, policy_loss: -502.40333, policy_entropy: -1.02827, alpha: 0.56539, time: 33.72483
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 828.47360, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 44.76364, qf2_loss: 43.61968, policy_loss: -506.12803, policy_entropy: -0.97288, alpha: 0.56499, time: 33.16277
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 840.09038, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 48.75412, qf2_loss: 48.42347, policy_loss: -503.42418, policy_entropy: -0.99756, alpha: 0.56023, time: 33.34152
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 840.56774, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 46.06889, qf2_loss: 45.58451, policy_loss: -506.14317, policy_entropy: -0.99711, alpha: 0.55937, time: 33.19445
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 823.33172, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 42.18438, qf2_loss: 41.95429, policy_loss: -508.83682, policy_entropy: -0.99073, alpha: 0.55820, time: 32.87561
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 621.48529, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 43.31097, qf2_loss: 42.69652, policy_loss: -505.90908, policy_entropy: -1.00691, alpha: 0.55886, time: 33.10732
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 833.95647, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 55.77278, qf2_loss: 56.35451, policy_loss: -508.45054, policy_entropy: -1.02489, alpha: 0.56030, time: 33.08210
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 823.49418, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 74.48651, qf2_loss: 74.57240, policy_loss: -509.20588, policy_entropy: -1.01137, alpha: 0.56764, time: 33.25384
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 611.86147, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 93.91203, qf2_loss: 94.83670, policy_loss: -507.60227, policy_entropy: -1.00334, alpha: 0.56966, time: 35.60661
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 767.28092, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 48.34356, qf2_loss: 47.77033, policy_loss: -507.64961, policy_entropy: -0.99293, alpha: 0.57144, time: 33.04875
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 847.34301, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 40.29453, qf2_loss: 40.17052, policy_loss: -507.83424, policy_entropy: -0.96928, alpha: 0.56467, time: 33.78877
[CW] eval: return: 758.25497, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 525.92557, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 39.55447, qf2_loss: 39.24114, policy_loss: -507.81446, policy_entropy: -0.99172, alpha: 0.55765, time: 32.76798
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 832.94956, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 39.38303, qf2_loss: 39.37457, policy_loss: -509.84240, policy_entropy: -0.97430, alpha: 0.55337, time: 33.47487
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 458.61804, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 40.51817, qf2_loss: 40.03639, policy_loss: -507.81618, policy_entropy: -0.99409, alpha: 0.54901, time: 33.07144
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 836.07687, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 40.94278, qf2_loss: 40.85585, policy_loss: -511.08602, policy_entropy: -0.98457, alpha: 0.54661, time: 33.26923
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 847.87611, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 41.26240, qf2_loss: 41.25942, policy_loss: -511.96496, policy_entropy: -0.99695, alpha: 0.54432, time: 32.97983
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 837.37189, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 47.75023, qf2_loss: 47.23271, policy_loss: -511.56589, policy_entropy: -0.99268, alpha: 0.54320, time: 32.82908
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 839.62443, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 45.27296, qf2_loss: 45.26741, policy_loss: -514.57307, policy_entropy: -0.99995, alpha: 0.54216, time: 33.24769
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 844.49335, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 46.78560, qf2_loss: 46.57579, policy_loss: -513.43937, policy_entropy: -0.99846, alpha: 0.54100, time: 32.74078
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 839.22196, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 43.32694, qf2_loss: 42.65760, policy_loss: -512.50427, policy_entropy: -0.99811, alpha: 0.54094, time: 33.02391
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 827.24338, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 42.90589, qf2_loss: 42.97538, policy_loss: -514.63610, policy_entropy: -0.99363, alpha: 0.54016, time: 33.23122
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 834.85494, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 51.67057, qf2_loss: 51.76669, policy_loss: -513.88528, policy_entropy: -0.98488, alpha: 0.53653, time: 33.22687
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 834.02716, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 39.91404, qf2_loss: 39.40953, policy_loss: -521.51108, policy_entropy: -0.99184, alpha: 0.53513, time: 33.29625
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 831.21850, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 43.62791, qf2_loss: 43.51535, policy_loss: -517.12359, policy_entropy: -1.01558, alpha: 0.53402, time: 32.77507
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 846.91959, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 42.59376, qf2_loss: 42.26566, policy_loss: -523.48270, policy_entropy: -1.02204, alpha: 0.53977, time: 33.03778
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 839.78315, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 51.84796, qf2_loss: 51.22815, policy_loss: -522.40932, policy_entropy: -1.02499, alpha: 0.54491, time: 33.17023
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 829.69573, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 54.95265, qf2_loss: 54.50957, policy_loss: -520.36801, policy_entropy: -0.99821, alpha: 0.54968, time: 33.16887
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 844.27385, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 40.38459, qf2_loss: 40.36202, policy_loss: -521.19805, policy_entropy: -0.99475, alpha: 0.54846, time: 33.35289
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 838.01353, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 43.17685, qf2_loss: 42.83041, policy_loss: -517.23496, policy_entropy: -1.01339, alpha: 0.54851, time: 33.17245
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 837.63359, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 48.44308, qf2_loss: 48.43785, policy_loss: -521.16470, policy_entropy: -1.01075, alpha: 0.55222, time: 33.35992
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 841.57616, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 47.22343, qf2_loss: 46.80217, policy_loss: -521.25969, policy_entropy: -1.01287, alpha: 0.55469, time: 32.93181
[CW] eval: return: 825.42038, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 842.08096, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 41.15120, qf2_loss: 40.91572, policy_loss: -522.75653, policy_entropy: -0.99017, alpha: 0.55602, time: 33.18045
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 849.68426, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 45.94201, qf2_loss: 45.92806, policy_loss: -523.30267, policy_entropy: -1.00485, alpha: 0.55557, time: 33.11809
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 844.00809, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 42.97014, qf2_loss: 42.98053, policy_loss: -528.68965, policy_entropy: -0.99448, alpha: 0.55656, time: 33.22392
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 828.66324, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 48.41987, qf2_loss: 48.32820, policy_loss: -527.39810, policy_entropy: -0.99088, alpha: 0.55314, time: 33.21460
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 835.32670, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 44.30897, qf2_loss: 43.85298, policy_loss: -523.81596, policy_entropy: -0.99997, alpha: 0.55222, time: 33.19096
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 844.93157, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 40.13214, qf2_loss: 39.78176, policy_loss: -527.58388, policy_entropy: -1.00206, alpha: 0.55354, time: 33.43608
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 844.00730, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 40.40330, qf2_loss: 39.77903, policy_loss: -526.77143, policy_entropy: -1.01040, alpha: 0.55530, time: 33.49928
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 839.88879, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 39.20105, qf2_loss: 38.95604, policy_loss: -528.05051, policy_entropy: -0.99709, alpha: 0.55495, time: 33.60983
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 756.40308, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 42.78440, qf2_loss: 42.86719, policy_loss: -527.56658, policy_entropy: -0.99926, alpha: 0.55504, time: 33.50860
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 838.85348, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 40.28814, qf2_loss: 40.14539, policy_loss: -529.83922, policy_entropy: -0.98855, alpha: 0.55382, time: 33.35427
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 844.65071, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 43.94369, qf2_loss: 43.40405, policy_loss: -529.34424, policy_entropy: -0.99238, alpha: 0.55076, time: 33.34314
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 847.34811, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 43.39421, qf2_loss: 43.33783, policy_loss: -533.69489, policy_entropy: -0.98305, alpha: 0.54668, time: 33.42612
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 837.06734, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 57.44836, qf2_loss: 57.06798, policy_loss: -527.99129, policy_entropy: -1.03696, alpha: 0.54745, time: 33.50722
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 685.30663, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 59.50855, qf2_loss: 59.24970, policy_loss: -529.30561, policy_entropy: -0.99855, alpha: 0.55577, time: 33.39351
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 844.90996, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 48.45691, qf2_loss: 48.75450, policy_loss: -529.01451, policy_entropy: -0.99369, alpha: 0.55347, time: 33.36381
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 827.93312, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 45.43942, qf2_loss: 45.30924, policy_loss: -533.21000, policy_entropy: -0.98579, alpha: 0.55121, time: 33.38997
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 833.55106, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 43.40079, qf2_loss: 42.99458, policy_loss: -529.73566, policy_entropy: -0.98252, alpha: 0.54583, time: 33.52376
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 833.91110, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 42.09316, qf2_loss: 41.94849, policy_loss: -533.42645, policy_entropy: -0.99275, alpha: 0.54368, time: 33.47504
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 679.96739, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 46.07296, qf2_loss: 45.90051, policy_loss: -535.19768, policy_entropy: -0.98783, alpha: 0.53959, time: 33.33289
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 836.16566, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 41.23437, qf2_loss: 41.17452, policy_loss: -537.22576, policy_entropy: -1.00250, alpha: 0.53864, time: 33.11948
[CW] eval: return: 836.61682, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 835.19386, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 39.44578, qf2_loss: 39.24974, policy_loss: -535.12969, policy_entropy: -1.00375, alpha: 0.53872, time: 33.11657
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 822.41321, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 38.91681, qf2_loss: 38.98110, policy_loss: -533.39343, policy_entropy: -1.00626, alpha: 0.54121, time: 33.28356
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 828.15894, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 43.42297, qf2_loss: 42.85882, policy_loss: -537.85231, policy_entropy: -0.99122, alpha: 0.54112, time: 33.11322
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 847.47067, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 77.15139, qf2_loss: 77.47624, policy_loss: -533.48134, policy_entropy: -1.04059, alpha: 0.54324, time: 33.16240
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 833.24966, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 59.73019, qf2_loss: 59.58152, policy_loss: -536.85876, policy_entropy: -1.01113, alpha: 0.55200, time: 33.36350
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 839.43408, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 41.20136, qf2_loss: 40.85302, policy_loss: -541.26141, policy_entropy: -0.96836, alpha: 0.55006, time: 33.31679
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 838.88820, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 40.57389, qf2_loss: 40.19390, policy_loss: -538.40254, policy_entropy: -0.98700, alpha: 0.54401, time: 33.43770
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 835.46779, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 42.20769, qf2_loss: 42.57557, policy_loss: -541.25489, policy_entropy: -0.96385, alpha: 0.53815, time: 33.29606
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 839.28150, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 41.73608, qf2_loss: 42.09688, policy_loss: -537.64036, policy_entropy: -0.98698, alpha: 0.53154, time: 33.25433
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 843.66159, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 37.86208, qf2_loss: 37.80320, policy_loss: -540.66390, policy_entropy: -0.99958, alpha: 0.52949, time: 33.39066
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 838.96750, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 41.60959, qf2_loss: 41.57715, policy_loss: -540.70311, policy_entropy: -0.98836, alpha: 0.52939, time: 33.57009
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 844.11720, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 38.94679, qf2_loss: 38.66924, policy_loss: -540.17242, policy_entropy: -0.99264, alpha: 0.52624, time: 33.39059
[CW] ---------------------------
[CW] ---- Iteration:   673 ----
[CW] collect: return: 840.03980, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 44.29781, qf2_loss: 44.78711, policy_loss: -539.31812, policy_entropy: -1.01305, alpha: 0.52593, time: 33.38453
[CW] ---------------------------
[CW] ---- Iteration:   674 ----
[CW] collect: return: 848.17647, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 41.66748, qf2_loss: 41.48986, policy_loss: -546.22791, policy_entropy: -1.00693, alpha: 0.52989, time: 33.46401
[CW] ---------------------------
[CW] ---- Iteration:   675 ----
[CW] collect: return: 690.63943, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 43.48146, qf2_loss: 43.63219, policy_loss: -544.23866, policy_entropy: -0.98918, alpha: 0.52843, time: 38.13033
[CW] ---------------------------
[CW] ---- Iteration:   676 ----
[CW] collect: return: 839.15466, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 41.85761, qf2_loss: 41.56065, policy_loss: -546.33662, policy_entropy: -0.99762, alpha: 0.52746, time: 33.17448
[CW] ---------------------------
[CW] ---- Iteration:   677 ----
[CW] collect: return: 750.81514, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 58.05686, qf2_loss: 59.53693, policy_loss: -542.60789, policy_entropy: -1.02570, alpha: 0.52932, time: 33.38951
[CW] ---------------------------
[CW] ---- Iteration:   678 ----
[CW] collect: return: 824.89650, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 48.15083, qf2_loss: 47.98861, policy_loss: -544.49411, policy_entropy: -0.99141, alpha: 0.53193, time: 33.29781
[CW] ---------------------------
[CW] ---- Iteration:   679 ----
[CW] collect: return: 831.14981, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 39.57915, qf2_loss: 39.47046, policy_loss: -546.46394, policy_entropy: -0.98647, alpha: 0.52921, time: 33.29797
[CW] ---------------------------
[CW] ---- Iteration:   680 ----
[CW] collect: return: 836.68287, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 42.88643, qf2_loss: 42.53574, policy_loss: -544.92215, policy_entropy: -0.98979, alpha: 0.52636, time: 33.15092
[CW] eval: return: 837.32715, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   681 ----
[CW] collect: return: 839.07292, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 39.50933, qf2_loss: 39.44651, policy_loss: -548.15161, policy_entropy: -0.98333, alpha: 0.52352, time: 33.26562
[CW] ---------------------------
[CW] ---- Iteration:   682 ----
[CW] collect: return: 841.76423, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 47.41477, qf2_loss: 47.52316, policy_loss: -546.42728, policy_entropy: -0.99635, alpha: 0.52171, time: 33.29122
[CW] ---------------------------
[CW] ---- Iteration:   683 ----
[CW] collect: return: 823.88032, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 43.28581, qf2_loss: 43.26876, policy_loss: -547.90641, policy_entropy: -0.98695, alpha: 0.51892, time: 33.25274
[CW] ---------------------------
[CW] ---- Iteration:   684 ----
[CW] collect: return: 836.57097, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 40.86688, qf2_loss: 40.64638, policy_loss: -549.84861, policy_entropy: -0.98404, alpha: 0.51574, time: 33.27251
[CW] ---------------------------
[CW] ---- Iteration:   685 ----
[CW] collect: return: 753.88583, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 40.88261, qf2_loss: 40.30982, policy_loss: -547.21894, policy_entropy: -0.98931, alpha: 0.51241, time: 33.30959
[CW] ---------------------------
[CW] ---- Iteration:   686 ----
[CW] collect: return: 841.13141, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 39.01947, qf2_loss: 38.71859, policy_loss: -549.13102, policy_entropy: -0.98778, alpha: 0.50978, time: 33.37769
[CW] ---------------------------
[CW] ---- Iteration:   687 ----
[CW] collect: return: 847.82571, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 41.84566, qf2_loss: 41.73107, policy_loss: -549.31974, policy_entropy: -0.99429, alpha: 0.50732, time: 33.23052
[CW] ---------------------------
[CW] ---- Iteration:   688 ----
[CW] collect: return: 847.24968, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 44.88712, qf2_loss: 44.64749, policy_loss: -549.61963, policy_entropy: -0.98361, alpha: 0.50682, time: 33.13638
[CW] ---------------------------
[CW] ---- Iteration:   689 ----
[CW] collect: return: 842.10688, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 41.06461, qf2_loss: 40.28307, policy_loss: -551.01599, policy_entropy: -0.99792, alpha: 0.50395, time: 32.99095
[CW] ---------------------------
[CW] ---- Iteration:   690 ----
[CW] collect: return: 846.53495, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 42.24320, qf2_loss: 42.49054, policy_loss: -553.07267, policy_entropy: -0.99986, alpha: 0.50291, time: 33.31783
[CW] ---------------------------
[CW] ---- Iteration:   691 ----
[CW] collect: return: 843.27026, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 41.02102, qf2_loss: 40.81932, policy_loss: -554.82128, policy_entropy: -0.98322, alpha: 0.50210, time: 33.21593
[CW] ---------------------------
[CW] ---- Iteration:   692 ----
[CW] collect: return: 756.79167, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 39.34659, qf2_loss: 38.93071, policy_loss: -553.16299, policy_entropy: -0.99880, alpha: 0.50045, time: 33.21455
[CW] ---------------------------
[CW] ---- Iteration:   693 ----
[CW] collect: return: 839.94831, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 45.18278, qf2_loss: 45.29382, policy_loss: -551.10133, policy_entropy: -1.00343, alpha: 0.50060, time: 33.16310
[CW] ---------------------------
[CW] ---- Iteration:   694 ----
[CW] collect: return: 830.90325, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 44.23045, qf2_loss: 43.63227, policy_loss: -550.46278, policy_entropy: -1.01001, alpha: 0.50085, time: 33.00482
[CW] ---------------------------
[CW] ---- Iteration:   695 ----
[CW] collect: return: 837.69870, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 44.39378, qf2_loss: 44.58913, policy_loss: -552.11769, policy_entropy: -1.01301, alpha: 0.50403, time: 33.06029
[CW] ---------------------------
[CW] ---- Iteration:   696 ----
[CW] collect: return: 843.54749, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 40.67072, qf2_loss: 40.23788, policy_loss: -551.20937, policy_entropy: -1.00738, alpha: 0.50565, time: 33.12061
[CW] ---------------------------
[CW] ---- Iteration:   697 ----
[CW] collect: return: 836.61046, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 42.97980, qf2_loss: 42.55724, policy_loss: -554.11759, policy_entropy: -1.00176, alpha: 0.50698, time: 33.27149
[CW] ---------------------------
[CW] ---- Iteration:   698 ----
[CW] collect: return: 839.32624, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 43.97416, qf2_loss: 44.02523, policy_loss: -552.28321, policy_entropy: -1.00194, alpha: 0.50780, time: 32.79505
[CW] ---------------------------
[CW] ---- Iteration:   699 ----
[CW] collect: return: 844.27802, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 39.79001, qf2_loss: 39.82905, policy_loss: -555.50076, policy_entropy: -0.98896, alpha: 0.50684, time: 33.20436
[CW] ---------------------------
[CW] ---- Iteration:   700 ----
[CW] collect: return: 752.70390, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 40.42899, qf2_loss: 40.34253, policy_loss: -557.61257, policy_entropy: -0.99381, alpha: 0.50499, time: 32.93153
[CW] eval: return: 839.45682, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   701 ----
[CW] collect: return: 844.30575, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 42.81937, qf2_loss: 42.51144, policy_loss: -550.45265, policy_entropy: -1.00155, alpha: 0.50433, time: 33.11363
[CW] ---------------------------
[CW] ---- Iteration:   702 ----
[CW] collect: return: 824.41855, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 41.92221, qf2_loss: 41.89973, policy_loss: -557.31522, policy_entropy: -0.99997, alpha: 0.50352, time: 33.31629
[CW] ---------------------------
[CW] ---- Iteration:   703 ----
[CW] collect: return: 842.96051, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 38.43854, qf2_loss: 38.39928, policy_loss: -556.47994, policy_entropy: -0.99675, alpha: 0.50400, time: 33.13884
[CW] ---------------------------
[CW] ---- Iteration:   704 ----
[CW] collect: return: 837.24318, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 109.53379, qf2_loss: 109.55232, policy_loss: -558.81050, policy_entropy: -1.02627, alpha: 0.50516, time: 33.30721
[CW] ---------------------------
[CW] ---- Iteration:   705 ----
[CW] collect: return: 541.69852, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 83.23137, qf2_loss: 83.15726, policy_loss: -561.43036, policy_entropy: -1.00636, alpha: 0.51015, time: 33.11647
[CW] ---------------------------
[CW] ---- Iteration:   706 ----
[CW] collect: return: 840.40239, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 46.81942, qf2_loss: 47.01095, policy_loss: -558.92712, policy_entropy: -0.98429, alpha: 0.50885, time: 33.42816
[CW] ---------------------------
[CW] ---- Iteration:   707 ----
[CW] collect: return: 597.49243, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 43.17252, qf2_loss: 42.87472, policy_loss: -562.11631, policy_entropy: -0.99007, alpha: 0.50633, time: 33.03027
[CW] ---------------------------
[CW] ---- Iteration:   708 ----
[CW] collect: return: 577.28228, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 39.30162, qf2_loss: 39.23025, policy_loss: -558.33441, policy_entropy: -0.98654, alpha: 0.50405, time: 33.13097
[CW] ---------------------------
[CW] ---- Iteration:   709 ----
[CW] collect: return: 830.04658, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 36.91985, qf2_loss: 36.69274, policy_loss: -562.94363, policy_entropy: -0.99141, alpha: 0.50092, time: 33.19546
[CW] ---------------------------
[CW] ---- Iteration:   710 ----
[CW] collect: return: 840.40926, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 36.64300, qf2_loss: 36.52473, policy_loss: -561.41939, policy_entropy: -0.98683, alpha: 0.50002, time: 33.21320
[CW] ---------------------------
[CW] ---- Iteration:   711 ----
[CW] collect: return: 841.35482, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 36.46426, qf2_loss: 36.51443, policy_loss: -561.74903, policy_entropy: -0.99388, alpha: 0.49755, time: 33.18437
[CW] ---------------------------
[CW] ---- Iteration:   712 ----
[CW] collect: return: 835.42037, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 36.98481, qf2_loss: 36.57387, policy_loss: -563.91422, policy_entropy: -0.99594, alpha: 0.49629, time: 33.06549
[CW] ---------------------------
[CW] ---- Iteration:   713 ----
[CW] collect: return: 839.20869, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 38.17792, qf2_loss: 37.70799, policy_loss: -561.38717, policy_entropy: -1.01860, alpha: 0.49723, time: 33.09139
[CW] ---------------------------
[CW] ---- Iteration:   714 ----
[CW] collect: return: 843.15943, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 41.41444, qf2_loss: 40.77016, policy_loss: -564.67695, policy_entropy: -0.99910, alpha: 0.49832, time: 33.07172
[CW] ---------------------------
[CW] ---- Iteration:   715 ----
[CW] collect: return: 822.92138, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 37.73907, qf2_loss: 37.77851, policy_loss: -566.61731, policy_entropy: -0.99327, alpha: 0.49800, time: 33.19901
[CW] ---------------------------
[CW] ---- Iteration:   716 ----
[CW] collect: return: 669.64448, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 42.07222, qf2_loss: 42.67755, policy_loss: -561.83626, policy_entropy: -1.00373, alpha: 0.49768, time: 33.16134
[CW] ---------------------------
[CW] ---- Iteration:   717 ----
[CW] collect: return: 829.57520, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 39.14556, qf2_loss: 38.76647, policy_loss: -569.09458, policy_entropy: -0.99245, alpha: 0.49727, time: 33.08288
[CW] ---------------------------
[CW] ---- Iteration:   718 ----
[CW] collect: return: 827.95181, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 40.03440, qf2_loss: 39.76543, policy_loss: -563.85640, policy_entropy: -1.00359, alpha: 0.49714, time: 33.20129
[CW] ---------------------------
[CW] ---- Iteration:   719 ----
[CW] collect: return: 831.40125, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 33.50152, qf2_loss: 33.39407, policy_loss: -569.27413, policy_entropy: -0.99472, alpha: 0.49731, time: 33.10946
[CW] ---------------------------
[CW] ---- Iteration:   720 ----
[CW] collect: return: 840.15419, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 38.53509, qf2_loss: 38.06933, policy_loss: -564.93710, policy_entropy: -0.99977, alpha: 0.49685, time: 32.97808
[CW] eval: return: 779.33343, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   721 ----
[CW] collect: return: 823.47386, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 40.15186, qf2_loss: 39.60475, policy_loss: -566.02221, policy_entropy: -0.99902, alpha: 0.49671, time: 33.10377
[CW] ---------------------------
[CW] ---- Iteration:   722 ----
[CW] collect: return: 832.39811, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 38.88823, qf2_loss: 38.98139, policy_loss: -568.81368, policy_entropy: -1.01593, alpha: 0.49737, time: 33.22303
[CW] ---------------------------
[CW] ---- Iteration:   723 ----
[CW] collect: return: 825.24209, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 40.81151, qf2_loss: 40.95339, policy_loss: -566.58187, policy_entropy: -1.00736, alpha: 0.49993, time: 33.17988
[CW] ---------------------------
[CW] ---- Iteration:   724 ----
[CW] collect: return: 840.50353, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 41.80627, qf2_loss: 41.69867, policy_loss: -570.70046, policy_entropy: -0.99195, alpha: 0.49931, time: 33.28860
[CW] ---------------------------
[CW] ---- Iteration:   725 ----
[CW] collect: return: 827.48739, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 35.57592, qf2_loss: 35.76699, policy_loss: -570.94286, policy_entropy: -0.98839, alpha: 0.49898, time: 36.18965
[CW] ---------------------------
[CW] ---- Iteration:   726 ----
[CW] collect: return: 842.18280, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 39.62217, qf2_loss: 40.15955, policy_loss: -572.46826, policy_entropy: -0.99541, alpha: 0.49688, time: 33.13631
[CW] ---------------------------
[CW] ---- Iteration:   727 ----
[CW] collect: return: 848.22663, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 38.52714, qf2_loss: 38.21481, policy_loss: -572.85227, policy_entropy: -0.99723, alpha: 0.49561, time: 33.20947
[CW] ---------------------------
[CW] ---- Iteration:   728 ----
[CW] collect: return: 835.89096, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 55.96785, qf2_loss: 55.97087, policy_loss: -569.67438, policy_entropy: -1.02406, alpha: 0.49578, time: 33.38830
[CW] ---------------------------
[CW] ---- Iteration:   729 ----
[CW] collect: return: 848.29471, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 60.28748, qf2_loss: 60.50408, policy_loss: -573.29315, policy_entropy: -0.98875, alpha: 0.50045, time: 33.16547
[CW] ---------------------------
[CW] ---- Iteration:   730 ----
[CW] collect: return: 842.14564, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 39.38653, qf2_loss: 39.19702, policy_loss: -572.43503, policy_entropy: -0.98925, alpha: 0.49655, time: 33.25398
[CW] ---------------------------
[CW] ---- Iteration:   731 ----
[CW] collect: return: 846.72540, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 39.54380, qf2_loss: 39.87044, policy_loss: -572.62735, policy_entropy: -0.99048, alpha: 0.49447, time: 35.14652
[CW] ---------------------------
[CW] ---- Iteration:   732 ----
[CW] collect: return: 841.54276, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 35.52938, qf2_loss: 35.45009, policy_loss: -573.38954, policy_entropy: -0.99871, alpha: 0.49372, time: 33.38507
[CW] ---------------------------
[CW] ---- Iteration:   733 ----
[CW] collect: return: 764.66729, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 32.24856, qf2_loss: 32.26186, policy_loss: -576.43470, policy_entropy: -0.98612, alpha: 0.49300, time: 33.19549
[CW] ---------------------------
[CW] ---- Iteration:   734 ----
[CW] collect: return: 842.89080, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 37.29184, qf2_loss: 36.67169, policy_loss: -572.21629, policy_entropy: -1.00330, alpha: 0.49136, time: 33.03887
[CW] ---------------------------
[CW] ---- Iteration:   735 ----
[CW] collect: return: 732.65106, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 37.94225, qf2_loss: 38.08733, policy_loss: -575.31961, policy_entropy: -1.00769, alpha: 0.49309, time: 33.74299
[CW] ---------------------------
[CW] ---- Iteration:   736 ----
[CW] collect: return: 841.83012, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 40.13469, qf2_loss: 39.57827, policy_loss: -573.63139, policy_entropy: -0.97909, alpha: 0.49233, time: 32.88690
[CW] ---------------------------
[CW] ---- Iteration:   737 ----
[CW] collect: return: 838.01322, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 35.87547, qf2_loss: 35.77701, policy_loss: -577.17500, policy_entropy: -0.96916, alpha: 0.48551, time: 33.26949
[CW] ---------------------------
[CW] ---- Iteration:   738 ----
[CW] collect: return: 840.79388, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 38.60857, qf2_loss: 38.58770, policy_loss: -573.01487, policy_entropy: -1.01333, alpha: 0.48325, time: 33.16776
[CW] ---------------------------
[CW] ---- Iteration:   739 ----
[CW] collect: return: 841.21653, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 36.62638, qf2_loss: 36.43684, policy_loss: -576.73422, policy_entropy: -0.99973, alpha: 0.48499, time: 33.18582
[CW] ---------------------------
[CW] ---- Iteration:   740 ----
[CW] collect: return: 829.96604, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 39.58028, qf2_loss: 39.26890, policy_loss: -575.46744, policy_entropy: -0.99341, alpha: 0.48407, time: 33.23749
[CW] eval: return: 798.22099, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   741 ----
[CW] collect: return: 841.56410, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 41.97767, qf2_loss: 41.87910, policy_loss: -576.23425, policy_entropy: -0.98110, alpha: 0.48249, time: 33.28364
[CW] ---------------------------
[CW] ---- Iteration:   742 ----
[CW] collect: return: 838.05765, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 37.59922, qf2_loss: 37.30842, policy_loss: -578.01403, policy_entropy: -0.99340, alpha: 0.47905, time: 33.20562
[CW] ---------------------------
[CW] ---- Iteration:   743 ----
[CW] collect: return: 751.28180, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 40.60638, qf2_loss: 40.66536, policy_loss: -574.05841, policy_entropy: -1.00598, alpha: 0.47831, time: 33.25382
[CW] ---------------------------
[CW] ---- Iteration:   744 ----
[CW] collect: return: 753.26597, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 38.20962, qf2_loss: 38.14039, policy_loss: -579.70045, policy_entropy: -1.00099, alpha: 0.47950, time: 33.10024
[CW] ---------------------------
[CW] ---- Iteration:   745 ----
[CW] collect: return: 580.44781, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 39.78823, qf2_loss: 39.24598, policy_loss: -578.66592, policy_entropy: -0.99193, alpha: 0.47855, time: 33.21150
[CW] ---------------------------
[CW] ---- Iteration:   746 ----
[CW] collect: return: 832.21709, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 38.23746, qf2_loss: 37.98337, policy_loss: -579.23414, policy_entropy: -0.97739, alpha: 0.47598, time: 33.20191
[CW] ---------------------------
[CW] ---- Iteration:   747 ----
[CW] collect: return: 840.73270, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 43.04708, qf2_loss: 43.28681, policy_loss: -578.97584, policy_entropy: -0.98928, alpha: 0.47296, time: 33.17131
[CW] ---------------------------
[CW] ---- Iteration:   748 ----
[CW] collect: return: 842.37314, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 43.48105, qf2_loss: 43.10387, policy_loss: -576.58353, policy_entropy: -1.00798, alpha: 0.47199, time: 33.06391
[CW] ---------------------------
[CW] ---- Iteration:   749 ----
[CW] collect: return: 818.19704, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 44.87914, qf2_loss: 44.89427, policy_loss: -583.19770, policy_entropy: -0.99372, alpha: 0.47262, time: 33.29749
[CW] ---------------------------
[CW] ---- Iteration:   750 ----
[CW] collect: return: 837.94588, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 38.71136, qf2_loss: 38.48641, policy_loss: -580.90307, policy_entropy: -0.99309, alpha: 0.47187, time: 33.18074
[CW] ---------------------------
[CW] ---- Iteration:   751 ----
[CW] collect: return: 829.47948, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 43.78859, qf2_loss: 43.20521, policy_loss: -579.22792, policy_entropy: -1.02198, alpha: 0.47292, time: 32.98599
[CW] ---------------------------
[CW] ---- Iteration:   752 ----
[CW] collect: return: 838.71000, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 39.68782, qf2_loss: 39.85043, policy_loss: -581.62862, policy_entropy: -1.00070, alpha: 0.47519, time: 33.19096
[CW] ---------------------------
[CW] ---- Iteration:   753 ----
[CW] collect: return: 845.86099, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 40.04884, qf2_loss: 39.97763, policy_loss: -582.69501, policy_entropy: -0.99569, alpha: 0.47466, time: 33.11318
[CW] ---------------------------
[CW] ---- Iteration:   754 ----
[CW] collect: return: 837.94522, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 40.08479, qf2_loss: 39.86593, policy_loss: -581.06668, policy_entropy: -1.00201, alpha: 0.47416, time: 33.14384
[CW] ---------------------------
[CW] ---- Iteration:   755 ----
[CW] collect: return: 837.77936, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 41.35593, qf2_loss: 41.04334, policy_loss: -583.95820, policy_entropy: -0.98455, alpha: 0.47286, time: 33.16645
[CW] ---------------------------
[CW] ---- Iteration:   756 ----
[CW] collect: return: 835.03847, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 49.61989, qf2_loss: 49.40869, policy_loss: -584.02522, policy_entropy: -0.98937, alpha: 0.47074, time: 33.01686
[CW] ---------------------------
[CW] ---- Iteration:   757 ----
[CW] collect: return: 840.15450, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 43.59629, qf2_loss: 43.56889, policy_loss: -581.54977, policy_entropy: -1.01146, alpha: 0.47061, time: 33.20401
[CW] ---------------------------
[CW] ---- Iteration:   758 ----
[CW] collect: return: 682.09874, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 39.95999, qf2_loss: 39.82895, policy_loss: -582.00953, policy_entropy: -1.01039, alpha: 0.47298, time: 33.26064
[CW] ---------------------------
[CW] ---- Iteration:   759 ----
[CW] collect: return: 848.13784, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 39.74098, qf2_loss: 39.73248, policy_loss: -581.31666, policy_entropy: -1.01557, alpha: 0.47533, time: 33.24627
[CW] ---------------------------
[CW] ---- Iteration:   760 ----
[CW] collect: return: 728.67624, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 38.05968, qf2_loss: 37.98544, policy_loss: -582.83118, policy_entropy: -0.97448, alpha: 0.47448, time: 33.11203
[CW] eval: return: 784.07710, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   761 ----
[CW] collect: return: 827.93902, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 39.46275, qf2_loss: 39.16842, policy_loss: -584.35074, policy_entropy: -0.99109, alpha: 0.47080, time: 33.11306
[CW] ---------------------------
[CW] ---- Iteration:   762 ----
[CW] collect: return: 830.36070, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 37.36993, qf2_loss: 37.15031, policy_loss: -586.45907, policy_entropy: -0.97174, alpha: 0.46887, time: 33.10944
[CW] ---------------------------
[CW] ---- Iteration:   763 ----
[CW] collect: return: 827.46178, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 43.77512, qf2_loss: 44.17418, policy_loss: -582.22443, policy_entropy: -0.99764, alpha: 0.46501, time: 33.03223
[CW] ---------------------------
[CW] ---- Iteration:   764 ----
[CW] collect: return: 822.75545, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 41.14646, qf2_loss: 40.90382, policy_loss: -584.74179, policy_entropy: -0.99525, alpha: 0.46458, time: 33.05384
[CW] ---------------------------
[CW] ---- Iteration:   765 ----
[CW] collect: return: 828.75165, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 40.55739, qf2_loss: 40.26540, policy_loss: -588.48118, policy_entropy: -0.98699, alpha: 0.46302, time: 32.68768
[CW] ---------------------------
[CW] ---- Iteration:   766 ----
[CW] collect: return: 769.45555, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 41.11232, qf2_loss: 41.02942, policy_loss: -584.75818, policy_entropy: -1.00583, alpha: 0.46180, time: 32.96332
[CW] ---------------------------
[CW] ---- Iteration:   767 ----
[CW] collect: return: 842.99543, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 39.52966, qf2_loss: 39.59218, policy_loss: -587.39930, policy_entropy: -0.99050, alpha: 0.46093, time: 33.15975
[CW] ---------------------------
[CW] ---- Iteration:   768 ----
[CW] collect: return: 832.60403, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 39.10122, qf2_loss: 39.05891, policy_loss: -589.04123, policy_entropy: -0.99068, alpha: 0.45993, time: 33.08880
[CW] ---------------------------
[CW] ---- Iteration:   769 ----
[CW] collect: return: 828.77971, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 49.50098, qf2_loss: 49.68778, policy_loss: -589.20175, policy_entropy: -1.01402, alpha: 0.46037, time: 33.08135
[CW] ---------------------------
[CW] ---- Iteration:   770 ----
[CW] collect: return: 758.44287, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 36.58964, qf2_loss: 36.38728, policy_loss: -591.85390, policy_entropy: -0.96398, alpha: 0.45903, time: 32.97030
[CW] ---------------------------
[CW] ---- Iteration:   771 ----
[CW] collect: return: 837.63061, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 36.69479, qf2_loss: 36.72440, policy_loss: -591.23506, policy_entropy: -0.99293, alpha: 0.45441, time: 33.04620
[CW] ---------------------------
[CW] ---- Iteration:   772 ----
[CW] collect: return: 827.28711, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 39.21163, qf2_loss: 38.73069, policy_loss: -588.29203, policy_entropy: -0.99904, alpha: 0.45390, time: 33.15431
[CW] ---------------------------
[CW] ---- Iteration:   773 ----
[CW] collect: return: 843.21305, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 35.55989, qf2_loss: 35.20057, policy_loss: -593.94869, policy_entropy: -1.00099, alpha: 0.45359, time: 33.08369
[CW] ---------------------------
[CW] ---- Iteration:   774 ----
[CW] collect: return: 818.99472, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 38.38678, qf2_loss: 38.05921, policy_loss: -589.82908, policy_entropy: -0.98725, alpha: 0.45363, time: 33.11427
[CW] ---------------------------
[CW] ---- Iteration:   775 ----
[CW] collect: return: 827.57455, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 38.42806, qf2_loss: 38.66584, policy_loss: -590.38314, policy_entropy: -0.99713, alpha: 0.45162, time: 33.32343
[CW] ---------------------------
[CW] ---- Iteration:   776 ----
[CW] collect: return: 831.45132, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 38.00928, qf2_loss: 38.36998, policy_loss: -589.78893, policy_entropy: -1.00304, alpha: 0.45183, time: 33.33527
[CW] ---------------------------
[CW] ---- Iteration:   777 ----
[CW] collect: return: 836.72798, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 39.24658, qf2_loss: 38.71673, policy_loss: -591.36877, policy_entropy: -1.00948, alpha: 0.45268, time: 33.17919
[CW] ---------------------------
[CW] ---- Iteration:   778 ----
[CW] collect: return: 821.95613, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 41.32873, qf2_loss: 41.18077, policy_loss: -594.08031, policy_entropy: -0.99484, alpha: 0.45318, time: 33.11239
[CW] ---------------------------
[CW] ---- Iteration:   779 ----
[CW] collect: return: 664.80696, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 41.52015, qf2_loss: 41.28820, policy_loss: -592.71725, policy_entropy: -1.00753, alpha: 0.45261, time: 32.92828
[CW] ---------------------------
[CW] ---- Iteration:   780 ----
[CW] collect: return: 843.31514, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 37.62124, qf2_loss: 37.87781, policy_loss: -591.19440, policy_entropy: -1.01384, alpha: 0.45518, time: 33.28973
[CW] eval: return: 820.17504, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   781 ----
[CW] collect: return: 831.46275, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 43.12057, qf2_loss: 42.42550, policy_loss: -590.02627, policy_entropy: -1.00786, alpha: 0.45692, time: 32.88277
[CW] ---------------------------
[CW] ---- Iteration:   782 ----
[CW] collect: return: 832.97204, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 41.98470, qf2_loss: 41.86101, policy_loss: -590.51795, policy_entropy: -1.01346, alpha: 0.45840, time: 32.77079
[CW] ---------------------------
[CW] ---- Iteration:   783 ----
[CW] collect: return: 839.14538, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 41.94817, qf2_loss: 41.81995, policy_loss: -595.51661, policy_entropy: -1.00106, alpha: 0.46044, time: 33.15225
[CW] ---------------------------
[CW] ---- Iteration:   784 ----
[CW] collect: return: 830.83574, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 39.53298, qf2_loss: 39.34070, policy_loss: -597.00338, policy_entropy: -0.99462, alpha: 0.45948, time: 33.50083
[CW] ---------------------------
[CW] ---- Iteration:   785 ----
[CW] collect: return: 820.45491, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 37.29970, qf2_loss: 37.23212, policy_loss: -597.98680, policy_entropy: -0.98267, alpha: 0.45704, time: 32.76195
[CW] ---------------------------
[CW] ---- Iteration:   786 ----
[CW] collect: return: 838.27110, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 36.33978, qf2_loss: 36.01996, policy_loss: -598.03286, policy_entropy: -0.99816, alpha: 0.45629, time: 33.20353
[CW] ---------------------------
[CW] ---- Iteration:   787 ----
[CW] collect: return: 690.96831, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 38.29906, qf2_loss: 38.11274, policy_loss: -599.12221, policy_entropy: -0.97856, alpha: 0.45400, time: 33.08223
[CW] ---------------------------
[CW] ---- Iteration:   788 ----
[CW] collect: return: 823.81687, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 41.25866, qf2_loss: 41.02228, policy_loss: -599.78522, policy_entropy: -0.99057, alpha: 0.45127, time: 36.29559
[CW] ---------------------------
[CW] ---- Iteration:   789 ----
[CW] collect: return: 837.96661, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 32.18004, qf2_loss: 32.28177, policy_loss: -599.24998, policy_entropy: -1.00123, alpha: 0.44967, time: 33.11529
[CW] ---------------------------
[CW] ---- Iteration:   790 ----
[CW] collect: return: 833.84064, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 34.43050, qf2_loss: 34.57674, policy_loss: -599.15643, policy_entropy: -0.99712, alpha: 0.45061, time: 33.36236
[CW] ---------------------------
[CW] ---- Iteration:   791 ----
[CW] collect: return: 843.22225, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 34.88575, qf2_loss: 34.48768, policy_loss: -597.70493, policy_entropy: -1.00888, alpha: 0.45101, time: 32.73609
[CW] ---------------------------
[CW] ---- Iteration:   792 ----
[CW] collect: return: 827.19540, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 38.07996, qf2_loss: 37.88433, policy_loss: -599.54160, policy_entropy: -1.00627, alpha: 0.45183, time: 32.95977
[CW] ---------------------------
[CW] ---- Iteration:   793 ----
[CW] collect: return: 831.18070, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 41.27530, qf2_loss: 40.65099, policy_loss: -601.72027, policy_entropy: -0.98958, alpha: 0.45207, time: 33.09424
[CW] ---------------------------
[CW] ---- Iteration:   794 ----
[CW] collect: return: 750.16899, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 38.98051, qf2_loss: 38.66113, policy_loss: -600.37518, policy_entropy: -0.99522, alpha: 0.45077, time: 32.87830
[CW] ---------------------------
[CW] ---- Iteration:   795 ----
[CW] collect: return: 834.17888, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 40.38043, qf2_loss: 39.91116, policy_loss: -598.73780, policy_entropy: -0.99620, alpha: 0.45016, time: 33.00003
[CW] ---------------------------
[CW] ---- Iteration:   796 ----
[CW] collect: return: 838.14496, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 71.08677, qf2_loss: 70.98229, policy_loss: -598.91489, policy_entropy: -1.03428, alpha: 0.45105, time: 32.98069
[CW] ---------------------------
[CW] ---- Iteration:   797 ----
[CW] collect: return: 821.43555, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 56.88711, qf2_loss: 56.55976, policy_loss: -602.23691, policy_entropy: -0.98358, alpha: 0.45422, time: 32.99336
[CW] ---------------------------
[CW] ---- Iteration:   798 ----
[CW] collect: return: 840.09750, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 35.16380, qf2_loss: 35.28852, policy_loss: -602.80943, policy_entropy: -0.97932, alpha: 0.45129, time: 33.13146
[CW] ---------------------------
[CW] ---- Iteration:   799 ----
[CW] collect: return: 825.29415, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 35.05736, qf2_loss: 34.90415, policy_loss: -600.72132, policy_entropy: -0.98507, alpha: 0.44814, time: 33.23158
[CW] ---------------------------
[CW] ---- Iteration:   800 ----
[CW] collect: return: 674.33574, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 33.71784, qf2_loss: 33.76195, policy_loss: -602.97325, policy_entropy: -1.00703, alpha: 0.44746, time: 33.21017
[CW] eval: return: 799.06365, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   801 ----
[CW] collect: return: 839.04023, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 35.37535, qf2_loss: 35.32999, policy_loss: -598.76753, policy_entropy: -1.01857, alpha: 0.44858, time: 32.96628
[CW] ---------------------------
[CW] ---- Iteration:   802 ----
[CW] collect: return: 833.62896, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 36.63303, qf2_loss: 36.43315, policy_loss: -604.68006, policy_entropy: -1.00288, alpha: 0.45055, time: 33.04755
[CW] ---------------------------
[CW] ---- Iteration:   803 ----
[CW] collect: return: 740.82833, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 32.07350, qf2_loss: 31.88752, policy_loss: -607.06200, policy_entropy: -0.97915, alpha: 0.45063, time: 33.20483
[CW] ---------------------------
[CW] ---- Iteration:   804 ----
[CW] collect: return: 839.24413, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 34.88840, qf2_loss: 34.96695, policy_loss: -600.21216, policy_entropy: -1.00493, alpha: 0.44815, time: 33.29152
[CW] ---------------------------
[CW] ---- Iteration:   805 ----
[CW] collect: return: 829.74787, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 36.90149, qf2_loss: 36.84957, policy_loss: -602.18860, policy_entropy: -0.99548, alpha: 0.44855, time: 32.79037
[CW] ---------------------------
[CW] ---- Iteration:   806 ----
[CW] collect: return: 842.30590, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 34.28267, qf2_loss: 34.18204, policy_loss: -603.90779, policy_entropy: -0.99192, alpha: 0.44741, time: 32.95412
[CW] ---------------------------
[CW] ---- Iteration:   807 ----
[CW] collect: return: 832.48474, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 37.96851, qf2_loss: 37.64951, policy_loss: -606.74014, policy_entropy: -0.98160, alpha: 0.44529, time: 33.14895
[CW] ---------------------------
[CW] ---- Iteration:   808 ----
[CW] collect: return: 839.70801, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 39.48433, qf2_loss: 39.29589, policy_loss: -602.95955, policy_entropy: -0.98926, alpha: 0.44297, time: 33.12924
[CW] ---------------------------
[CW] ---- Iteration:   809 ----
[CW] collect: return: 739.44105, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 41.34832, qf2_loss: 41.23388, policy_loss: -605.47162, policy_entropy: -1.00231, alpha: 0.44304, time: 33.37059
[CW] ---------------------------
[CW] ---- Iteration:   810 ----
[CW] collect: return: 823.34374, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 37.49625, qf2_loss: 37.65043, policy_loss: -606.78735, policy_entropy: -1.00494, alpha: 0.44257, time: 33.46474
[CW] ---------------------------
[CW] ---- Iteration:   811 ----
[CW] collect: return: 846.33766, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 33.38474, qf2_loss: 33.26687, policy_loss: -610.67810, policy_entropy: -0.97721, alpha: 0.44181, time: 33.38934
[CW] ---------------------------
[CW] ---- Iteration:   812 ----
[CW] collect: return: 834.50108, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 34.85380, qf2_loss: 34.60434, policy_loss: -608.91256, policy_entropy: -1.00231, alpha: 0.43956, time: 33.19597
[CW] ---------------------------
[CW] ---- Iteration:   813 ----
[CW] collect: return: 824.91248, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 38.21760, qf2_loss: 37.88285, policy_loss: -608.57474, policy_entropy: -1.00544, alpha: 0.44052, time: 32.86596
[CW] ---------------------------
[CW] ---- Iteration:   814 ----
[CW] collect: return: 835.06563, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 34.75071, qf2_loss: 34.30063, policy_loss: -610.98577, policy_entropy: -0.99916, alpha: 0.44111, time: 32.86504
[CW] ---------------------------
[CW] ---- Iteration:   815 ----
[CW] collect: return: 839.83205, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 34.07216, qf2_loss: 33.56130, policy_loss: -609.70772, policy_entropy: -1.02419, alpha: 0.44280, time: 32.91835
[CW] ---------------------------
[CW] ---- Iteration:   816 ----
[CW] collect: return: 841.00530, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 34.26867, qf2_loss: 34.39824, policy_loss: -611.61532, policy_entropy: -0.99471, alpha: 0.44556, time: 33.26531
[CW] ---------------------------
[CW] ---- Iteration:   817 ----
[CW] collect: return: 840.64100, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 36.38622, qf2_loss: 36.48004, policy_loss: -609.86208, policy_entropy: -0.97978, alpha: 0.44333, time: 32.98699
[CW] ---------------------------
[CW] ---- Iteration:   818 ----
[CW] collect: return: 838.41332, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 39.16486, qf2_loss: 39.52937, policy_loss: -607.41304, policy_entropy: -0.99784, alpha: 0.44029, time: 33.16877
[CW] ---------------------------
[CW] ---- Iteration:   819 ----
[CW] collect: return: 826.62839, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 36.94843, qf2_loss: 36.69982, policy_loss: -611.39153, policy_entropy: -1.00308, alpha: 0.44040, time: 32.84526
[CW] ---------------------------
[CW] ---- Iteration:   820 ----
[CW] collect: return: 829.91307, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 40.20634, qf2_loss: 39.94773, policy_loss: -606.88048, policy_entropy: -1.01282, alpha: 0.44204, time: 33.02641
[CW] eval: return: 830.58086, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   821 ----
[CW] collect: return: 841.72386, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 38.14025, qf2_loss: 37.82794, policy_loss: -611.84663, policy_entropy: -0.99525, alpha: 0.44262, time: 33.25777
[CW] ---------------------------
[CW] ---- Iteration:   822 ----
[CW] collect: return: 835.07418, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 37.01809, qf2_loss: 36.86534, policy_loss: -609.28398, policy_entropy: -1.00361, alpha: 0.44270, time: 33.44910
[CW] ---------------------------
[CW] ---- Iteration:   823 ----
[CW] collect: return: 749.06891, steps: 1000.00000, total_steps: 829000.00000
[CW] train: qf1_loss: 36.75182, qf2_loss: 36.55459, policy_loss: -610.99764, policy_entropy: -1.00921, alpha: 0.44349, time: 32.86802
[CW] ---------------------------
[CW] ---- Iteration:   824 ----
[CW] collect: return: 815.46328, steps: 1000.00000, total_steps: 830000.00000
[CW] train: qf1_loss: 37.01992, qf2_loss: 36.50991, policy_loss: -609.69700, policy_entropy: -0.99349, alpha: 0.44457, time: 32.73222
[CW] ---------------------------
[CW] ---- Iteration:   825 ----
[CW] collect: return: 830.98155, steps: 1000.00000, total_steps: 831000.00000
[CW] train: qf1_loss: 37.71222, qf2_loss: 37.41865, policy_loss: -613.34331, policy_entropy: -0.96939, alpha: 0.44089, time: 32.98998
[CW] ---------------------------
[CW] ---- Iteration:   826 ----
[CW] collect: return: 844.44197, steps: 1000.00000, total_steps: 832000.00000
[CW] train: qf1_loss: 37.47820, qf2_loss: 37.31172, policy_loss: -612.93698, policy_entropy: -0.97891, alpha: 0.43704, time: 33.23137
[CW] ---------------------------
