{"collect/return": 594.6583531238139, "collect/steps": 1000.0, "collect/total_steps": 648000.0, "train/qf1_loss": 6.7568755316734315, "train/qf2_loss": 6.865918984413147, "train/policy_loss": -154.57595054626464, "train/policy_entropy": -6.011509046554566, "train/alpha": 0.04064879458397627, "train/time": 51.856083393096924, "eval/return": 639.2485674382187, "eval/steps": 1000.0, "_timestamp": 1678735353.9064384, "_runtime": 35937.66093540192, "_step": 642}