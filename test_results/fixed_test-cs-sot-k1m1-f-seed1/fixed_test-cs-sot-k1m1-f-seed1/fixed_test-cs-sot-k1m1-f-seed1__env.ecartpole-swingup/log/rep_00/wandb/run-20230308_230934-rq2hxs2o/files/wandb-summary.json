{"collect/return": 731.8993775861054, "collect/steps": 1000.0, "collect/total_steps": 545000.0, "train/qf1_loss": 52.654846115112306, "train/qf2_loss": 52.744704265594486, "train/policy_loss": -460.4605096435547, "train/policy_entropy": -1.0005644315481186, "train/alpha": 0.5821376371383667, "train/time": 50.94443917274475, "eval/return": 811.3928421634448, "eval/steps": 1000.0, "_timestamp": 1678342168.667279, "_runtime": 28793.704054117203, "_step": 539}