{"collect/return": 846.3577505609137, "collect/steps": 1000.0, "collect/total_steps": 833000.0, "train/qf1_loss": 32.67388725280762, "train/qf2_loss": 32.80316785812378, "train/policy_loss": -652.5058673095704, "train/policy_entropy": -1.0195478838682175, "train/alpha": 0.33046332448720933, "train/time": 33.143763303756714, "eval/return": 839.953161735824, "eval/steps": 1000.0, "_timestamp": 1678385770.05562, "_runtime": 28667.815587997437, "_step": 827}