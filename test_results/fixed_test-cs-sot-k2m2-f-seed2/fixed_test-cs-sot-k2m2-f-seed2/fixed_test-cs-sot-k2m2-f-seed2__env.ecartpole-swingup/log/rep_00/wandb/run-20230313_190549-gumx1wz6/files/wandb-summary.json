{"collect/return": 828.8182984261657, "collect/steps": 1000.0, "collect/total_steps": 417000.0, "train/qf1_loss": 56.78290939331055, "train/qf2_loss": 56.69344772338867, "train/policy_loss": -411.6349453735352, "train/policy_entropy": -0.9951853710412979, "train/alpha": 0.6869418090581894, "train/time": 67.81362056732178, "eval/return": 823.6109241775073, "eval/steps": 1000.0, "_timestamp": 1678759402.8201544, "_runtime": 28653.640959501266, "_step": 411}