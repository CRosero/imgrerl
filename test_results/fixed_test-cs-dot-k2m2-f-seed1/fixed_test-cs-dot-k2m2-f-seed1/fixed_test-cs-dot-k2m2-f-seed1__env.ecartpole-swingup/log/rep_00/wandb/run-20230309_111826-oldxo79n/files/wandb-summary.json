{"collect/return": 832.7197640308623, "collect/steps": 1000.0, "collect/total_steps": 640000.0, "train/qf1_loss": 146.01131092071535, "train/qf2_loss": 145.37249687194824, "train/policy_loss": -589.2966662597656, "train/policy_entropy": -1.0087392407655715, "train/alpha": 0.43101495295763015, "train/time": 43.74722766876221, "eval/return": 828.5330220480313, "eval/steps": 1000.0, "_timestamp": 1678385784.4729908, "_runtime": 28677.70477581024, "_step": 634}