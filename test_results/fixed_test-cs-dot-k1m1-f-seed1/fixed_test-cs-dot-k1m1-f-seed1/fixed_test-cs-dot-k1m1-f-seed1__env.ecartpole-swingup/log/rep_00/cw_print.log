[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 66.85565, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.14737, qf2_loss: 1.14653, policy_loss: -2.64784, policy_entropy: 0.68215, alpha: 0.98504, time: 37.72981
[CW] eval: return: 132.92429, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 109.06478, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.09916, qf2_loss: 0.09856, policy_loss: -3.12884, policy_entropy: 0.67931, alpha: 0.95627, time: 32.39186
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 120.43974, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.09574, qf2_loss: 0.09521, policy_loss: -3.66411, policy_entropy: 0.67390, alpha: 0.92878, time: 33.18224
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 124.77554, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.11601, qf2_loss: 0.11766, policy_loss: -4.29396, policy_entropy: 0.66865, alpha: 0.90248, time: 33.22833
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 49.92785, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.12619, qf2_loss: 0.13085, policy_loss: -4.78468, policy_entropy: 0.66370, alpha: 0.87728, time: 33.15947
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 66.99549, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.15985, qf2_loss: 0.16100, policy_loss: -5.29014, policy_entropy: 0.66223, alpha: 0.85310, time: 33.57290
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 191.28699, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.18042, qf2_loss: 0.18061, policy_loss: -6.00834, policy_entropy: 0.65727, alpha: 0.82987, time: 33.77336
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 76.29565, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.21117, qf2_loss: 0.21193, policy_loss: -6.62025, policy_entropy: 0.65567, alpha: 0.80754, time: 33.39573
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 252.32436, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.34619, qf2_loss: 0.34611, policy_loss: -7.55816, policy_entropy: 0.64671, alpha: 0.78606, time: 33.61381
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 62.55006, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.33484, qf2_loss: 0.33610, policy_loss: -8.18901, policy_entropy: 0.64350, alpha: 0.76541, time: 33.21852
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 136.58313, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.39886, qf2_loss: 0.40083, policy_loss: -8.73595, policy_entropy: 0.63954, alpha: 0.74552, time: 33.58433
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 178.17512, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.55893, qf2_loss: 0.56168, policy_loss: -9.55568, policy_entropy: 0.63573, alpha: 0.72633, time: 33.30004
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 99.69886, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.50087, qf2_loss: 0.50578, policy_loss: -10.24757, policy_entropy: 0.62630, alpha: 0.70783, time: 33.32753
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 194.65909, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.64986, qf2_loss: 0.65388, policy_loss: -11.09719, policy_entropy: 0.60988, alpha: 0.69005, time: 33.16383
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 80.94598, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.66974, qf2_loss: 0.67229, policy_loss: -11.44035, policy_entropy: 0.58735, alpha: 0.67300, time: 33.02373
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 217.75435, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 1.01363, qf2_loss: 1.01389, policy_loss: -12.46267, policy_entropy: 0.55512, alpha: 0.65670, time: 33.40346
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 169.57262, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.82158, qf2_loss: 0.82375, policy_loss: -13.28630, policy_entropy: 0.52656, alpha: 0.64113, time: 33.59416
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 235.38186, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 1.12942, qf2_loss: 1.12914, policy_loss: -14.09691, policy_entropy: 0.49165, alpha: 0.62626, time: 33.63198
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 193.06867, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 1.88837, qf2_loss: 1.89122, policy_loss: -15.54126, policy_entropy: 0.46223, alpha: 0.61202, time: 33.29376
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 214.92885, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 1.30185, qf2_loss: 1.30044, policy_loss: -16.34942, policy_entropy: 0.41067, alpha: 0.59841, time: 33.78711
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 205.24493, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 1.24963, qf2_loss: 1.24876, policy_loss: -17.23778, policy_entropy: 0.36148, alpha: 0.58552, time: 33.23344
[CW] eval: return: 220.53425, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 224.10778, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 1.27568, qf2_loss: 1.28491, policy_loss: -18.10160, policy_entropy: 0.32826, alpha: 0.57322, time: 33.60788
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 275.41264, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 1.43939, qf2_loss: 1.44165, policy_loss: -19.18236, policy_entropy: 0.29762, alpha: 0.56134, time: 33.33138
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 194.93013, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 1.35224, qf2_loss: 1.35637, policy_loss: -19.88433, policy_entropy: 0.25772, alpha: 0.54994, time: 33.31481
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 246.07300, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 1.43633, qf2_loss: 1.45497, policy_loss: -20.90534, policy_entropy: 0.22375, alpha: 0.53897, time: 33.43936
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 202.96240, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 1.37777, qf2_loss: 1.38766, policy_loss: -21.95481, policy_entropy: 0.18590, alpha: 0.52843, time: 33.17554
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 238.95982, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 1.39818, qf2_loss: 1.41212, policy_loss: -22.88146, policy_entropy: 0.15776, alpha: 0.51826, time: 33.68206
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 234.53299, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 1.64920, qf2_loss: 1.68309, policy_loss: -23.95232, policy_entropy: 0.13385, alpha: 0.50839, time: 33.31057
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 279.46167, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 1.42598, qf2_loss: 1.44218, policy_loss: -25.06930, policy_entropy: 0.10763, alpha: 0.49881, time: 33.36129
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 197.64763, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 1.49602, qf2_loss: 1.50702, policy_loss: -25.78844, policy_entropy: 0.08042, alpha: 0.48948, time: 33.52652
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 199.98395, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 1.56706, qf2_loss: 1.57657, policy_loss: -27.01647, policy_entropy: 0.04645, alpha: 0.48046, time: 33.11102
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 279.36689, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 1.64749, qf2_loss: 1.66401, policy_loss: -27.91977, policy_entropy: 0.00436, alpha: 0.47179, time: 32.98059
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 212.26405, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 1.61366, qf2_loss: 1.62609, policy_loss: -28.56924, policy_entropy: -0.02414, alpha: 0.46346, time: 33.36586
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 220.65754, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 1.71407, qf2_loss: 1.72864, policy_loss: -29.75872, policy_entropy: -0.06111, alpha: 0.45541, time: 33.37798
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 231.04737, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 1.77772, qf2_loss: 1.80598, policy_loss: -30.75208, policy_entropy: -0.08942, alpha: 0.44762, time: 33.46894
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 254.94987, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 1.73272, qf2_loss: 1.74228, policy_loss: -31.73494, policy_entropy: -0.14473, alpha: 0.44018, time: 33.30171
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 215.95896, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 1.94739, qf2_loss: 1.95316, policy_loss: -33.15978, policy_entropy: -0.16291, alpha: 0.43302, time: 33.81972
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 241.88767, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 1.81309, qf2_loss: 1.82503, policy_loss: -33.91514, policy_entropy: -0.21265, alpha: 0.42612, time: 33.44177
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 207.62047, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 1.97638, qf2_loss: 1.99568, policy_loss: -34.71563, policy_entropy: -0.24166, alpha: 0.41951, time: 33.29091
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 291.35984, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 1.87348, qf2_loss: 1.89707, policy_loss: -36.09127, policy_entropy: -0.28013, alpha: 0.41316, time: 33.28717
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 283.17038, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 1.92947, qf2_loss: 1.94450, policy_loss: -37.19813, policy_entropy: -0.31929, alpha: 0.40714, time: 33.25905
[CW] eval: return: 238.39591, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 244.14202, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 2.02626, qf2_loss: 2.03904, policy_loss: -38.12521, policy_entropy: -0.34215, alpha: 0.40128, time: 33.46980
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 201.18576, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 1.99269, qf2_loss: 2.00986, policy_loss: -38.96619, policy_entropy: -0.38451, alpha: 0.39565, time: 33.14656
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 250.23730, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 2.27932, qf2_loss: 2.29871, policy_loss: -40.10292, policy_entropy: -0.40549, alpha: 0.39025, time: 33.25128
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 227.71491, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 2.23936, qf2_loss: 2.25071, policy_loss: -41.07564, policy_entropy: -0.39803, alpha: 0.38487, time: 33.22493
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 300.10463, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 1.96751, qf2_loss: 1.98659, policy_loss: -42.10537, policy_entropy: -0.40385, alpha: 0.37939, time: 33.26935
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 211.37339, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 1.82438, qf2_loss: 1.84906, policy_loss: -43.11308, policy_entropy: -0.41259, alpha: 0.37396, time: 32.95849
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 227.95070, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 1.84285, qf2_loss: 1.85813, policy_loss: -44.03670, policy_entropy: -0.41059, alpha: 0.36850, time: 33.41867
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 264.66315, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 1.84374, qf2_loss: 1.85632, policy_loss: -45.03751, policy_entropy: -0.40305, alpha: 0.36293, time: 33.16997
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 299.11088, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 1.98689, qf2_loss: 1.99381, policy_loss: -46.06911, policy_entropy: -0.35773, alpha: 0.35713, time: 33.23143
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 280.10409, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 1.98173, qf2_loss: 1.99879, policy_loss: -46.90668, policy_entropy: -0.35291, alpha: 0.35108, time: 33.74335
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 269.73845, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 1.96772, qf2_loss: 1.97754, policy_loss: -47.95780, policy_entropy: -0.34776, alpha: 0.34494, time: 33.42590
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 210.49404, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 1.86482, qf2_loss: 1.87569, policy_loss: -48.69615, policy_entropy: -0.33552, alpha: 0.33880, time: 33.30775
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 247.87880, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 1.96968, qf2_loss: 1.99084, policy_loss: -49.58590, policy_entropy: -0.33564, alpha: 0.33253, time: 33.34037
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 331.42989, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 2.01097, qf2_loss: 2.02360, policy_loss: -50.55219, policy_entropy: -0.32930, alpha: 0.32635, time: 33.61367
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 343.39756, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 1.88790, qf2_loss: 1.90003, policy_loss: -51.70058, policy_entropy: -0.31533, alpha: 0.32015, time: 33.72587
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 265.27011, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 2.05811, qf2_loss: 2.07016, policy_loss: -52.40248, policy_entropy: -0.34263, alpha: 0.31399, time: 33.33775
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 286.02088, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 2.06436, qf2_loss: 2.07510, policy_loss: -53.73412, policy_entropy: -0.31568, alpha: 0.30794, time: 33.59601
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 348.97892, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 2.18698, qf2_loss: 2.20172, policy_loss: -54.58430, policy_entropy: -0.33129, alpha: 0.30191, time: 33.21085
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 405.65396, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 2.20133, qf2_loss: 2.20232, policy_loss: -55.65706, policy_entropy: -0.35606, alpha: 0.29608, time: 33.30697
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 209.71807, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 2.01871, qf2_loss: 2.02982, policy_loss: -56.35455, policy_entropy: -0.38030, alpha: 0.29055, time: 33.30195
[CW] eval: return: 230.12161, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 192.04632, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 2.05832, qf2_loss: 2.06939, policy_loss: -57.15135, policy_entropy: -0.36156, alpha: 0.28510, time: 33.26713
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 198.49938, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 2.24117, qf2_loss: 2.25735, policy_loss: -57.89539, policy_entropy: -0.37754, alpha: 0.27966, time: 33.39546
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 230.45298, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 1.98106, qf2_loss: 1.99178, policy_loss: -59.06300, policy_entropy: -0.38263, alpha: 0.27436, time: 33.89845
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 210.52556, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 2.01427, qf2_loss: 2.01738, policy_loss: -59.40213, policy_entropy: -0.38952, alpha: 0.26917, time: 33.26009
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 253.16112, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 2.46671, qf2_loss: 2.49358, policy_loss: -60.54286, policy_entropy: -0.38992, alpha: 0.26404, time: 33.23850
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 238.42790, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 2.31854, qf2_loss: 2.34103, policy_loss: -61.52151, policy_entropy: -0.41044, alpha: 0.25901, time: 33.39071
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 267.90928, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 2.63979, qf2_loss: 2.63444, policy_loss: -62.42658, policy_entropy: -0.41602, alpha: 0.25420, time: 33.37562
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 203.78370, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 2.43698, qf2_loss: 2.46269, policy_loss: -63.48671, policy_entropy: -0.45480, alpha: 0.24956, time: 33.29486
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 302.29834, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 2.15456, qf2_loss: 2.16858, policy_loss: -63.86201, policy_entropy: -0.47320, alpha: 0.24521, time: 33.18673
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 295.64015, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 2.44946, qf2_loss: 2.45930, policy_loss: -65.19548, policy_entropy: -0.49369, alpha: 0.24095, time: 33.21830
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 244.22417, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 2.63981, qf2_loss: 2.64736, policy_loss: -66.13074, policy_entropy: -0.52060, alpha: 0.23690, time: 33.70919
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 329.84957, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 2.71215, qf2_loss: 2.73877, policy_loss: -66.97749, policy_entropy: -0.54230, alpha: 0.23309, time: 33.36346
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 249.72673, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 2.67479, qf2_loss: 2.68942, policy_loss: -68.41463, policy_entropy: -0.59958, alpha: 0.22954, time: 33.69799
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 299.53660, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 2.65125, qf2_loss: 2.66290, policy_loss: -69.16598, policy_entropy: -0.63927, alpha: 0.22637, time: 33.50911
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 314.67851, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 2.96549, qf2_loss: 2.93927, policy_loss: -69.80299, policy_entropy: -0.67469, alpha: 0.22346, time: 33.38214
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 368.19356, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 2.81507, qf2_loss: 2.80274, policy_loss: -70.84164, policy_entropy: -0.71488, alpha: 0.22081, time: 33.77179
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 285.31102, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 2.58433, qf2_loss: 2.59837, policy_loss: -72.11599, policy_entropy: -0.75221, alpha: 0.21852, time: 33.39271
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 244.90172, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 3.19210, qf2_loss: 3.22102, policy_loss: -72.89876, policy_entropy: -0.77116, alpha: 0.21640, time: 33.59999
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 281.25259, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 2.78565, qf2_loss: 2.79763, policy_loss: -73.90010, policy_entropy: -0.78224, alpha: 0.21433, time: 33.39011
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 287.21095, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 2.72106, qf2_loss: 2.72222, policy_loss: -74.87466, policy_entropy: -0.81785, alpha: 0.21252, time: 33.27371
[CW] eval: return: 276.70712, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 346.70339, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 2.77778, qf2_loss: 2.79834, policy_loss: -75.55260, policy_entropy: -0.81305, alpha: 0.21071, time: 33.15032
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 399.00021, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 3.13676, qf2_loss: 3.15054, policy_loss: -76.74470, policy_entropy: -0.83144, alpha: 0.20889, time: 33.30690
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 224.08809, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 3.33423, qf2_loss: 3.32613, policy_loss: -77.76456, policy_entropy: -0.84417, alpha: 0.20729, time: 33.27146
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 228.70827, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 3.25599, qf2_loss: 3.28460, policy_loss: -78.57902, policy_entropy: -0.84510, alpha: 0.20570, time: 33.01247
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 279.20888, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 3.16211, qf2_loss: 3.18615, policy_loss: -79.47276, policy_entropy: -0.81667, alpha: 0.20379, time: 33.13347
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 372.27420, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 3.04024, qf2_loss: 3.05516, policy_loss: -80.80622, policy_entropy: -0.83798, alpha: 0.20189, time: 33.33785
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 231.38620, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 3.43104, qf2_loss: 3.44411, policy_loss: -81.47025, policy_entropy: -0.85260, alpha: 0.20006, time: 33.31255
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 319.68180, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 4.16872, qf2_loss: 4.17590, policy_loss: -82.62175, policy_entropy: -0.85841, alpha: 0.19845, time: 33.15311
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 315.32061, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 4.00628, qf2_loss: 4.02480, policy_loss: -83.46958, policy_entropy: -0.86667, alpha: 0.19678, time: 33.22881
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 273.99904, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 3.36441, qf2_loss: 3.35588, policy_loss: -84.27672, policy_entropy: -0.87447, alpha: 0.19516, time: 33.51194
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 359.85311, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 3.73022, qf2_loss: 3.74344, policy_loss: -85.39519, policy_entropy: -0.87379, alpha: 0.19352, time: 33.22546
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 305.71773, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 3.48824, qf2_loss: 3.47751, policy_loss: -86.47432, policy_entropy: -0.89307, alpha: 0.19201, time: 33.25949
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 304.80242, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 3.48432, qf2_loss: 3.48763, policy_loss: -86.99400, policy_entropy: -0.90301, alpha: 0.19058, time: 33.39891
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 347.15419, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 4.71419, qf2_loss: 4.74396, policy_loss: -88.42118, policy_entropy: -0.90323, alpha: 0.18933, time: 33.21076
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 311.85839, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 4.29864, qf2_loss: 4.26050, policy_loss: -88.90694, policy_entropy: -0.90099, alpha: 0.18777, time: 33.17790
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 323.07639, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 3.68698, qf2_loss: 3.68326, policy_loss: -89.91803, policy_entropy: -0.89529, alpha: 0.18631, time: 33.51992
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 371.92957, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 3.74319, qf2_loss: 3.73429, policy_loss: -90.88473, policy_entropy: -0.94158, alpha: 0.18498, time: 33.15055
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 242.62622, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 3.92425, qf2_loss: 3.89738, policy_loss: -91.70152, policy_entropy: -0.92874, alpha: 0.18392, time: 33.07938
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 311.32090, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 3.95638, qf2_loss: 3.96048, policy_loss: -92.69314, policy_entropy: -0.94699, alpha: 0.18292, time: 33.12253
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 338.06316, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 4.34660, qf2_loss: 4.33213, policy_loss: -93.46621, policy_entropy: -0.94758, alpha: 0.18205, time: 32.93985
[CW] eval: return: 315.13584, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 420.88885, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 3.95041, qf2_loss: 3.95639, policy_loss: -94.86382, policy_entropy: -0.95800, alpha: 0.18129, time: 33.26123
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 400.35599, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 5.50396, qf2_loss: 5.46038, policy_loss: -95.16120, policy_entropy: -0.97156, alpha: 0.18053, time: 33.06606
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 340.33462, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 5.13905, qf2_loss: 5.16984, policy_loss: -96.28860, policy_entropy: -0.96973, alpha: 0.17994, time: 33.19006
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 421.52768, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 4.51494, qf2_loss: 4.47602, policy_loss: -97.40132, policy_entropy: -0.97102, alpha: 0.17946, time: 33.35557
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 298.51251, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 4.17572, qf2_loss: 4.15101, policy_loss: -98.68957, policy_entropy: -0.97803, alpha: 0.17893, time: 33.20744
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 363.94856, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 5.52918, qf2_loss: 5.56207, policy_loss: -99.29305, policy_entropy: -0.97164, alpha: 0.17832, time: 33.04386
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 442.00308, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 3.96353, qf2_loss: 3.97834, policy_loss: -100.14745, policy_entropy: -0.98912, alpha: 0.17792, time: 33.70035
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 351.51377, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 4.17109, qf2_loss: 4.19381, policy_loss: -101.28128, policy_entropy: -0.99849, alpha: 0.17777, time: 33.51244
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 312.89815, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 4.95032, qf2_loss: 4.94398, policy_loss: -102.34581, policy_entropy: -0.99035, alpha: 0.17777, time: 33.34315
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 339.58840, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 4.34506, qf2_loss: 4.32570, policy_loss: -102.57992, policy_entropy: -0.99759, alpha: 0.17736, time: 33.55998
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 292.90282, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 4.30163, qf2_loss: 4.29162, policy_loss: -103.70581, policy_entropy: -1.02012, alpha: 0.17769, time: 33.50227
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 349.00317, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 4.48913, qf2_loss: 4.50492, policy_loss: -104.49999, policy_entropy: -0.99401, alpha: 0.17801, time: 33.83291
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 329.67957, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 4.66623, qf2_loss: 4.66324, policy_loss: -105.76354, policy_entropy: -0.99135, alpha: 0.17778, time: 33.57141
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 316.73592, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 4.31476, qf2_loss: 4.31218, policy_loss: -106.46400, policy_entropy: -0.99148, alpha: 0.17726, time: 33.69571
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 326.71717, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 4.51611, qf2_loss: 4.52269, policy_loss: -107.25534, policy_entropy: -1.00655, alpha: 0.17718, time: 33.40891
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 394.44844, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 4.91338, qf2_loss: 4.92210, policy_loss: -108.30023, policy_entropy: -1.01115, alpha: 0.17766, time: 33.34478
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 415.61100, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 6.03278, qf2_loss: 6.05734, policy_loss: -108.96616, policy_entropy: -0.99900, alpha: 0.17786, time: 33.09079
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 329.17969, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 8.17550, qf2_loss: 8.17713, policy_loss: -109.75887, policy_entropy: -1.00479, alpha: 0.17790, time: 33.23702
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 450.70825, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 4.47934, qf2_loss: 4.46279, policy_loss: -111.25767, policy_entropy: -1.01630, alpha: 0.17833, time: 32.93420
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 338.48982, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 4.17599, qf2_loss: 4.20340, policy_loss: -112.36280, policy_entropy: -1.01183, alpha: 0.17901, time: 33.22879
[CW] eval: return: 391.25540, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 395.48562, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 4.45676, qf2_loss: 4.43597, policy_loss: -112.87581, policy_entropy: -1.03659, alpha: 0.17983, time: 33.38435
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 362.49662, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 4.62666, qf2_loss: 4.64005, policy_loss: -114.19525, policy_entropy: -1.02813, alpha: 0.18112, time: 33.06040
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 261.09713, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 4.58511, qf2_loss: 4.59254, policy_loss: -114.86510, policy_entropy: -1.02015, alpha: 0.18208, time: 33.22869
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 325.58657, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 4.78167, qf2_loss: 4.79557, policy_loss: -115.78485, policy_entropy: -1.00293, alpha: 0.18288, time: 33.08074
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 382.69918, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 4.95479, qf2_loss: 4.96815, policy_loss: -116.91575, policy_entropy: -1.00378, alpha: 0.18297, time: 33.05348
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 299.04396, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 5.32320, qf2_loss: 5.27650, policy_loss: -117.61548, policy_entropy: -1.00429, alpha: 0.18316, time: 33.19277
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 315.22582, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 5.15644, qf2_loss: 5.16178, policy_loss: -118.23241, policy_entropy: -0.99719, alpha: 0.18316, time: 33.26101
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 371.34749, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 5.64626, qf2_loss: 5.64020, policy_loss: -119.02574, policy_entropy: -1.01884, alpha: 0.18365, time: 33.19829
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 301.49267, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 5.35891, qf2_loss: 5.32596, policy_loss: -120.35074, policy_entropy: -1.01760, alpha: 0.18462, time: 33.17612
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 421.00485, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 4.68193, qf2_loss: 4.68076, policy_loss: -121.39381, policy_entropy: -1.02627, alpha: 0.18593, time: 33.14627
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 431.13545, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 4.99998, qf2_loss: 4.99788, policy_loss: -121.87379, policy_entropy: -1.01139, alpha: 0.18688, time: 33.02014
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 376.91499, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 5.63696, qf2_loss: 5.66690, policy_loss: -122.93670, policy_entropy: -1.01753, alpha: 0.18792, time: 33.66878
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 340.18001, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 6.12899, qf2_loss: 6.14372, policy_loss: -123.67958, policy_entropy: -1.02332, alpha: 0.18924, time: 33.38901
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 382.08059, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 6.02600, qf2_loss: 5.95471, policy_loss: -124.65192, policy_entropy: -1.01708, alpha: 0.19073, time: 33.54512
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 355.79817, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 5.69788, qf2_loss: 5.74324, policy_loss: -125.59022, policy_entropy: -1.03252, alpha: 0.19228, time: 33.22996
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 390.24055, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 4.96534, qf2_loss: 4.96218, policy_loss: -126.21121, policy_entropy: -1.01332, alpha: 0.19387, time: 33.57136
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 336.57392, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 5.04571, qf2_loss: 5.03666, policy_loss: -127.58086, policy_entropy: -1.04609, alpha: 0.19578, time: 33.54580
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 333.55341, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 5.55874, qf2_loss: 5.55007, policy_loss: -128.47502, policy_entropy: -1.02066, alpha: 0.19865, time: 33.34560
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 363.92123, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 5.68194, qf2_loss: 5.69614, policy_loss: -129.43187, policy_entropy: -1.02768, alpha: 0.20041, time: 33.30598
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 354.20755, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 5.37292, qf2_loss: 5.40050, policy_loss: -129.98987, policy_entropy: -1.01913, alpha: 0.20249, time: 33.76596
[CW] eval: return: 405.14644, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 421.10932, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 5.78555, qf2_loss: 5.78374, policy_loss: -130.95448, policy_entropy: -1.02264, alpha: 0.20453, time: 33.08712
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 381.98399, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 5.16274, qf2_loss: 5.18223, policy_loss: -132.48404, policy_entropy: -1.03789, alpha: 0.20706, time: 33.26138
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 291.84083, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 6.13052, qf2_loss: 6.11878, policy_loss: -132.93227, policy_entropy: -1.00821, alpha: 0.20884, time: 33.39538
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 322.19499, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 5.36845, qf2_loss: 5.36750, policy_loss: -133.97141, policy_entropy: -1.01375, alpha: 0.20961, time: 33.42107
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 372.47920, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 5.42354, qf2_loss: 5.41665, policy_loss: -134.49563, policy_entropy: -1.03420, alpha: 0.21192, time: 33.14904
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 330.76215, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 5.34165, qf2_loss: 5.39114, policy_loss: -135.56650, policy_entropy: -1.02930, alpha: 0.21561, time: 33.46413
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 399.35278, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 5.47204, qf2_loss: 5.41522, policy_loss: -136.56509, policy_entropy: -1.00453, alpha: 0.21733, time: 33.72184
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 450.50235, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 5.11012, qf2_loss: 5.16208, policy_loss: -137.05911, policy_entropy: -1.01016, alpha: 0.21771, time: 33.67426
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 398.88379, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 5.81427, qf2_loss: 5.78495, policy_loss: -137.89156, policy_entropy: -1.00461, alpha: 0.21832, time: 32.96603
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 362.52932, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 5.21858, qf2_loss: 5.26271, policy_loss: -138.95566, policy_entropy: -1.01219, alpha: 0.21940, time: 33.12329
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 455.12448, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 5.39109, qf2_loss: 5.34153, policy_loss: -139.82825, policy_entropy: -1.01071, alpha: 0.22022, time: 33.37336
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 410.32137, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 5.49024, qf2_loss: 5.53772, policy_loss: -141.18480, policy_entropy: -0.98903, alpha: 0.22091, time: 33.11573
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 403.83480, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 4.91843, qf2_loss: 4.93147, policy_loss: -141.72328, policy_entropy: -1.00777, alpha: 0.22070, time: 33.04024
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 441.80099, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 6.00941, qf2_loss: 5.97881, policy_loss: -142.62769, policy_entropy: -0.99914, alpha: 0.22120, time: 33.42298
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 391.40394, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 5.00485, qf2_loss: 5.01885, policy_loss: -142.98860, policy_entropy: -1.00726, alpha: 0.22133, time: 33.18717
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 419.95022, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 5.27482, qf2_loss: 5.30849, policy_loss: -144.73934, policy_entropy: -1.01062, alpha: 0.22234, time: 33.11622
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 419.60184, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 5.37834, qf2_loss: 5.44125, policy_loss: -145.11071, policy_entropy: -0.99366, alpha: 0.22278, time: 33.41497
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 473.29437, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 5.93860, qf2_loss: 6.00283, policy_loss: -146.43534, policy_entropy: -0.98463, alpha: 0.22160, time: 33.45039
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 446.42266, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 5.56212, qf2_loss: 5.53332, policy_loss: -147.17695, policy_entropy: -1.00771, alpha: 0.22132, time: 33.20798
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 391.15040, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 5.34898, qf2_loss: 5.33782, policy_loss: -148.11794, policy_entropy: -1.01437, alpha: 0.22199, time: 33.56054
[CW] eval: return: 437.50644, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 345.81684, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 5.78527, qf2_loss: 5.79288, policy_loss: -149.16328, policy_entropy: -1.01643, alpha: 0.22370, time: 33.05282
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 462.23985, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 5.83321, qf2_loss: 5.86489, policy_loss: -149.63932, policy_entropy: -0.99165, alpha: 0.22442, time: 33.26356
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 458.03957, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 5.19459, qf2_loss: 5.20205, policy_loss: -150.22136, policy_entropy: -1.00212, alpha: 0.22361, time: 33.32345
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 436.15795, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 7.53265, qf2_loss: 7.51784, policy_loss: -151.59019, policy_entropy: -1.01029, alpha: 0.22447, time: 33.50110
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 381.38387, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 5.66232, qf2_loss: 5.67636, policy_loss: -152.53780, policy_entropy: -1.00223, alpha: 0.22558, time: 33.28911
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 427.09834, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 5.30630, qf2_loss: 5.30003, policy_loss: -153.42996, policy_entropy: -0.99233, alpha: 0.22525, time: 33.68614
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 385.29134, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 5.76752, qf2_loss: 5.76626, policy_loss: -154.58963, policy_entropy: -1.02192, alpha: 0.22593, time: 33.14630
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 395.49365, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 6.70915, qf2_loss: 6.63392, policy_loss: -154.94543, policy_entropy: -0.99407, alpha: 0.22677, time: 33.42368
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 322.41650, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 5.39320, qf2_loss: 5.41302, policy_loss: -156.24802, policy_entropy: -1.01779, alpha: 0.22760, time: 33.47425
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 431.17795, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 5.20932, qf2_loss: 5.23947, policy_loss: -156.67181, policy_entropy: -1.01954, alpha: 0.22952, time: 33.06070
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 396.46404, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 5.58678, qf2_loss: 5.61483, policy_loss: -157.30377, policy_entropy: -1.00261, alpha: 0.23115, time: 33.44660
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 374.59753, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 5.68061, qf2_loss: 5.66379, policy_loss: -158.48210, policy_entropy: -1.00083, alpha: 0.23152, time: 33.38134
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 458.71234, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 5.91294, qf2_loss: 5.90068, policy_loss: -159.40549, policy_entropy: -1.01202, alpha: 0.23177, time: 33.40805
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 586.04412, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 5.68204, qf2_loss: 5.64970, policy_loss: -160.60388, policy_entropy: -1.02731, alpha: 0.23439, time: 33.29619
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 467.28436, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 6.14891, qf2_loss: 6.15856, policy_loss: -161.31566, policy_entropy: -1.01162, alpha: 0.23693, time: 33.42453
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 451.31371, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 7.05469, qf2_loss: 7.03431, policy_loss: -161.70113, policy_entropy: -1.00526, alpha: 0.23843, time: 33.38298
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 469.58112, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 5.87951, qf2_loss: 5.88748, policy_loss: -163.22214, policy_entropy: -1.01960, alpha: 0.23979, time: 33.40148
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 350.81524, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 5.96140, qf2_loss: 5.97188, policy_loss: -164.28882, policy_entropy: -1.01060, alpha: 0.24183, time: 33.60945
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 461.31805, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 5.67296, qf2_loss: 5.68178, policy_loss: -165.35920, policy_entropy: -1.01770, alpha: 0.24339, time: 33.27546
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 462.77661, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 5.71162, qf2_loss: 5.65656, policy_loss: -165.67156, policy_entropy: -1.02471, alpha: 0.24601, time: 33.49474
[CW] eval: return: 485.06455, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 466.31795, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 6.83956, qf2_loss: 6.88675, policy_loss: -166.61951, policy_entropy: -1.01242, alpha: 0.24889, time: 33.07538
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 459.62943, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 6.79917, qf2_loss: 6.84538, policy_loss: -168.00170, policy_entropy: -1.00561, alpha: 0.24996, time: 33.31998
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 438.68126, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 6.42856, qf2_loss: 6.43760, policy_loss: -168.63653, policy_entropy: -1.00521, alpha: 0.25054, time: 33.20926
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 527.93874, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 6.19154, qf2_loss: 6.15553, policy_loss: -169.92019, policy_entropy: -1.00835, alpha: 0.25138, time: 33.17355
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 492.51990, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 5.83554, qf2_loss: 5.86728, policy_loss: -170.68823, policy_entropy: -1.01213, alpha: 0.25321, time: 33.32393
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 462.09253, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 5.93245, qf2_loss: 5.95661, policy_loss: -171.43720, policy_entropy: -1.00371, alpha: 0.25382, time: 33.52051
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 538.91223, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 6.99866, qf2_loss: 6.95068, policy_loss: -172.03647, policy_entropy: -1.00048, alpha: 0.25452, time: 33.41762
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 465.52428, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 6.47775, qf2_loss: 6.53740, policy_loss: -173.38087, policy_entropy: -1.00212, alpha: 0.25495, time: 33.83561
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 542.52994, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 6.68322, qf2_loss: 6.73917, policy_loss: -174.41263, policy_entropy: -1.01790, alpha: 0.25605, time: 33.38656
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 492.64974, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 7.40831, qf2_loss: 7.39011, policy_loss: -175.17764, policy_entropy: -1.01212, alpha: 0.25770, time: 33.24544
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 525.62907, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 6.93583, qf2_loss: 6.98966, policy_loss: -175.90174, policy_entropy: -1.00987, alpha: 0.25963, time: 33.42441
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 453.75303, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 7.47621, qf2_loss: 7.44067, policy_loss: -177.02875, policy_entropy: -0.99827, alpha: 0.26042, time: 33.60924
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 387.36685, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 6.66959, qf2_loss: 6.64226, policy_loss: -177.93824, policy_entropy: -1.01053, alpha: 0.26076, time: 33.58289
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 476.48525, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 6.69774, qf2_loss: 6.74153, policy_loss: -178.69170, policy_entropy: -1.01290, alpha: 0.26218, time: 33.54823
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 458.50858, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 6.84477, qf2_loss: 6.88619, policy_loss: -179.91129, policy_entropy: -1.00031, alpha: 0.26412, time: 33.37761
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 468.52704, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 6.80074, qf2_loss: 6.87880, policy_loss: -180.21877, policy_entropy: -1.00090, alpha: 0.26368, time: 33.54896
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 472.94949, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 6.93650, qf2_loss: 6.99237, policy_loss: -181.89649, policy_entropy: -1.01677, alpha: 0.26491, time: 33.43441
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 465.58960, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 8.85591, qf2_loss: 8.75987, policy_loss: -182.87309, policy_entropy: -0.99195, alpha: 0.26653, time: 33.49917
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 458.08824, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 7.49716, qf2_loss: 7.49778, policy_loss: -183.68958, policy_entropy: -1.02133, alpha: 0.26688, time: 33.22493
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 424.01192, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 6.94939, qf2_loss: 6.91910, policy_loss: -184.34210, policy_entropy: -1.01714, alpha: 0.26978, time: 33.12974
[CW] eval: return: 457.35046, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 446.02027, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 7.33810, qf2_loss: 7.33689, policy_loss: -185.50730, policy_entropy: -1.00810, alpha: 0.27223, time: 33.15323
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 541.74755, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 7.46639, qf2_loss: 7.47729, policy_loss: -186.23565, policy_entropy: -1.01081, alpha: 0.27258, time: 33.16361
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 439.59453, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 8.37016, qf2_loss: 8.31565, policy_loss: -186.97255, policy_entropy: -1.01274, alpha: 0.27498, time: 33.19196
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 526.21012, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 8.02151, qf2_loss: 7.97427, policy_loss: -187.71416, policy_entropy: -1.01035, alpha: 0.27695, time: 33.35851
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 521.21445, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 7.47180, qf2_loss: 7.47871, policy_loss: -188.91778, policy_entropy: -1.02037, alpha: 0.27958, time: 32.91597
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 520.57229, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 7.90730, qf2_loss: 7.90749, policy_loss: -190.07461, policy_entropy: -1.01933, alpha: 0.28254, time: 33.16757
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 505.20635, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 7.40747, qf2_loss: 7.43715, policy_loss: -191.14918, policy_entropy: -1.01367, alpha: 0.28515, time: 33.21643
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 531.65388, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 7.44019, qf2_loss: 7.40209, policy_loss: -191.84208, policy_entropy: -1.00967, alpha: 0.28748, time: 33.39784
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 504.69900, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 7.27820, qf2_loss: 7.29959, policy_loss: -192.69843, policy_entropy: -1.00124, alpha: 0.28806, time: 33.11469
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 511.49827, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 10.65774, qf2_loss: 10.67925, policy_loss: -193.77299, policy_entropy: -1.00194, alpha: 0.28804, time: 32.99593
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 421.55291, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 8.89773, qf2_loss: 8.87769, policy_loss: -194.26419, policy_entropy: -1.00338, alpha: 0.28839, time: 33.10600
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 454.01592, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 7.77713, qf2_loss: 7.76649, policy_loss: -195.11676, policy_entropy: -1.01312, alpha: 0.29004, time: 32.97396
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 429.60817, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 7.77239, qf2_loss: 7.78029, policy_loss: -196.59077, policy_entropy: -1.00346, alpha: 0.29152, time: 33.28867
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 456.46286, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 8.06432, qf2_loss: 8.11063, policy_loss: -197.08206, policy_entropy: -1.00952, alpha: 0.29320, time: 33.20022
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 502.08240, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 7.64819, qf2_loss: 7.64771, policy_loss: -198.44483, policy_entropy: -1.00763, alpha: 0.29428, time: 33.04135
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 544.25848, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 12.88077, qf2_loss: 12.80518, policy_loss: -198.37606, policy_entropy: -0.99543, alpha: 0.29557, time: 33.44374
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 431.59649, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 9.62669, qf2_loss: 9.67185, policy_loss: -200.05913, policy_entropy: -0.99414, alpha: 0.29422, time: 32.92620
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 545.20063, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 8.34880, qf2_loss: 8.35584, policy_loss: -200.97639, policy_entropy: -1.00819, alpha: 0.29323, time: 33.09914
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 520.88801, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 7.48866, qf2_loss: 7.53173, policy_loss: -202.29460, policy_entropy: -1.00941, alpha: 0.29479, time: 33.35452
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 439.95536, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 7.28616, qf2_loss: 7.25145, policy_loss: -203.17342, policy_entropy: -1.02069, alpha: 0.29824, time: 33.00528
[CW] eval: return: 506.13837, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 522.42810, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 9.03910, qf2_loss: 9.05045, policy_loss: -203.35331, policy_entropy: -1.00543, alpha: 0.30123, time: 34.36578
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 472.92348, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 10.76569, qf2_loss: 10.75883, policy_loss: -205.99432, policy_entropy: -0.98512, alpha: 0.29950, time: 34.50419
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 517.93313, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 8.40574, qf2_loss: 8.38334, policy_loss: -206.06870, policy_entropy: -0.99606, alpha: 0.29849, time: 33.49786
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 531.98258, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 7.92325, qf2_loss: 7.96890, policy_loss: -205.51062, policy_entropy: -1.00664, alpha: 0.29779, time: 34.36489
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 499.86846, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 8.50418, qf2_loss: 8.55511, policy_loss: -208.56514, policy_entropy: -1.01367, alpha: 0.29990, time: 33.15895
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 515.18046, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 7.92547, qf2_loss: 8.00023, policy_loss: -208.36235, policy_entropy: -0.99004, alpha: 0.30007, time: 33.19191
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 471.12877, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 8.63277, qf2_loss: 8.62368, policy_loss: -209.12645, policy_entropy: -1.00212, alpha: 0.29961, time: 33.28313
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 540.19901, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 9.54337, qf2_loss: 9.57134, policy_loss: -210.15919, policy_entropy: -1.01006, alpha: 0.30053, time: 33.34053
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 457.16464, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 10.32392, qf2_loss: 10.29273, policy_loss: -212.61474, policy_entropy: -1.00779, alpha: 0.30210, time: 33.11755
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 438.71226, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 9.17911, qf2_loss: 9.11521, policy_loss: -212.02616, policy_entropy: -1.01329, alpha: 0.30341, time: 34.53932
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 531.46535, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 8.82860, qf2_loss: 8.77223, policy_loss: -213.35255, policy_entropy: -1.00885, alpha: 0.30558, time: 33.45917
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 519.65305, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 8.29101, qf2_loss: 8.34001, policy_loss: -214.27912, policy_entropy: -1.00923, alpha: 0.30701, time: 33.44165
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 468.35585, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 12.29237, qf2_loss: 12.28857, policy_loss: -214.89510, policy_entropy: -0.99391, alpha: 0.30868, time: 33.57719
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 508.81508, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 11.99635, qf2_loss: 11.96664, policy_loss: -215.76909, policy_entropy: -1.00594, alpha: 0.30756, time: 33.31027
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 501.25808, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 11.85042, qf2_loss: 11.86914, policy_loss: -215.99881, policy_entropy: -1.00077, alpha: 0.30808, time: 33.40403
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 546.40322, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 8.83511, qf2_loss: 8.83249, policy_loss: -218.34422, policy_entropy: -1.01230, alpha: 0.30924, time: 33.37806
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 442.27902, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 8.17040, qf2_loss: 8.18379, policy_loss: -218.87485, policy_entropy: -1.00786, alpha: 0.31061, time: 33.25363
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 574.60513, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 8.70681, qf2_loss: 8.67699, policy_loss: -220.10302, policy_entropy: -1.01180, alpha: 0.31288, time: 33.05818
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 536.47934, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 8.31855, qf2_loss: 8.29498, policy_loss: -221.16366, policy_entropy: -1.00062, alpha: 0.31413, time: 33.23534
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 524.27546, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 9.63031, qf2_loss: 9.64168, policy_loss: -221.61934, policy_entropy: -1.00288, alpha: 0.31400, time: 33.42622
[CW] eval: return: 563.41603, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 544.03913, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 10.98321, qf2_loss: 10.94460, policy_loss: -223.14966, policy_entropy: -1.00757, alpha: 0.31603, time: 33.24281
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 530.72684, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 10.45034, qf2_loss: 10.36068, policy_loss: -224.38328, policy_entropy: -1.00203, alpha: 0.31686, time: 33.08910
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 539.49806, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 9.64148, qf2_loss: 9.60288, policy_loss: -224.59886, policy_entropy: -1.01359, alpha: 0.31772, time: 33.69264
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 571.99333, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 9.37556, qf2_loss: 9.43170, policy_loss: -226.03149, policy_entropy: -1.01077, alpha: 0.32041, time: 33.44918
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 438.89978, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 9.62864, qf2_loss: 9.60486, policy_loss: -226.54516, policy_entropy: -1.01177, alpha: 0.32309, time: 33.49035
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 604.11754, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 8.87308, qf2_loss: 8.82797, policy_loss: -227.69422, policy_entropy: -1.00361, alpha: 0.32484, time: 33.30946
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 522.11335, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 10.84374, qf2_loss: 10.73310, policy_loss: -229.36238, policy_entropy: -1.00330, alpha: 0.32509, time: 33.04017
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 531.43458, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 9.96157, qf2_loss: 10.04566, policy_loss: -229.52365, policy_entropy: -1.02660, alpha: 0.32816, time: 33.42923
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 617.40392, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 11.53700, qf2_loss: 11.48459, policy_loss: -230.74989, policy_entropy: -1.00592, alpha: 0.33086, time: 32.98246
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 613.12412, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 9.99183, qf2_loss: 9.95188, policy_loss: -231.77186, policy_entropy: -0.99876, alpha: 0.33203, time: 33.46005
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 477.16463, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 10.52060, qf2_loss: 10.39518, policy_loss: -233.27783, policy_entropy: -1.00315, alpha: 0.33235, time: 33.15482
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 544.21504, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 9.59448, qf2_loss: 9.60694, policy_loss: -233.38000, policy_entropy: -1.00577, alpha: 0.33280, time: 33.42976
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 542.44413, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 10.63617, qf2_loss: 10.67611, policy_loss: -235.33736, policy_entropy: -1.00636, alpha: 0.33420, time: 33.44830
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 543.94952, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 11.08510, qf2_loss: 11.01238, policy_loss: -234.92410, policy_entropy: -1.00857, alpha: 0.33553, time: 33.04223
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 540.27293, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 10.07645, qf2_loss: 10.12626, policy_loss: -235.15873, policy_entropy: -1.00474, alpha: 0.33699, time: 33.13808
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 543.11801, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 8.91399, qf2_loss: 8.98054, policy_loss: -237.99640, policy_entropy: -1.00036, alpha: 0.33779, time: 33.03928
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 522.03140, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 9.73223, qf2_loss: 9.74923, policy_loss: -239.09909, policy_entropy: -1.00343, alpha: 0.33829, time: 33.02257
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 457.73635, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 10.07497, qf2_loss: 10.07910, policy_loss: -239.02496, policy_entropy: -1.02085, alpha: 0.34101, time: 33.38581
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 528.00004, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 11.24499, qf2_loss: 11.24754, policy_loss: -240.74511, policy_entropy: -0.99794, alpha: 0.34329, time: 33.37550
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 505.58048, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 9.60537, qf2_loss: 9.63928, policy_loss: -240.87594, policy_entropy: -1.00669, alpha: 0.34306, time: 32.92075
[CW] eval: return: 522.21979, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 475.68429, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 10.20924, qf2_loss: 10.17535, policy_loss: -242.22245, policy_entropy: -1.01629, alpha: 0.34579, time: 33.03419
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 595.21004, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 10.79959, qf2_loss: 10.80934, policy_loss: -241.94728, policy_entropy: -1.00484, alpha: 0.34840, time: 33.22377
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 524.92852, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 9.98280, qf2_loss: 9.99334, policy_loss: -243.92201, policy_entropy: -0.99559, alpha: 0.34842, time: 33.55870
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 519.70320, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 11.51096, qf2_loss: 11.39132, policy_loss: -245.12277, policy_entropy: -1.00327, alpha: 0.34821, time: 33.22763
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 542.81066, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 11.55024, qf2_loss: 11.49355, policy_loss: -245.74542, policy_entropy: -1.00549, alpha: 0.34941, time: 33.06062
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 545.45507, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 9.48288, qf2_loss: 9.42455, policy_loss: -247.12544, policy_entropy: -1.01522, alpha: 0.35160, time: 33.52679
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 546.14481, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 10.02241, qf2_loss: 10.06315, policy_loss: -246.66596, policy_entropy: -1.01182, alpha: 0.35433, time: 32.91883
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 479.92596, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 10.34916, qf2_loss: 10.33343, policy_loss: -248.65872, policy_entropy: -0.99734, alpha: 0.35578, time: 33.35087
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 549.34703, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 11.59995, qf2_loss: 11.46798, policy_loss: -249.62485, policy_entropy: -1.00981, alpha: 0.35607, time: 33.21855
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 626.12343, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 11.32843, qf2_loss: 11.31608, policy_loss: -250.74989, policy_entropy: -1.00243, alpha: 0.35814, time: 33.70171
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 539.62368, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 14.27646, qf2_loss: 14.25188, policy_loss: -251.09712, policy_entropy: -0.99584, alpha: 0.35914, time: 33.23978
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 614.89783, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 10.32102, qf2_loss: 10.33307, policy_loss: -252.76159, policy_entropy: -1.00786, alpha: 0.35839, time: 33.05505
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 540.64791, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 9.35800, qf2_loss: 9.30993, policy_loss: -253.22715, policy_entropy: -1.01227, alpha: 0.36045, time: 33.40814
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 534.36279, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 10.40779, qf2_loss: 10.44081, policy_loss: -255.39050, policy_entropy: -1.00815, alpha: 0.36293, time: 32.91585
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 615.94344, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 11.36099, qf2_loss: 11.23450, policy_loss: -255.81359, policy_entropy: -0.99993, alpha: 0.36494, time: 32.98850
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 540.31729, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 11.35601, qf2_loss: 11.31137, policy_loss: -257.31897, policy_entropy: -1.01737, alpha: 0.36601, time: 33.13680
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 605.67788, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 10.91603, qf2_loss: 10.88920, policy_loss: -257.27254, policy_entropy: -1.00000, alpha: 0.36758, time: 33.02831
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 548.06732, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 11.34644, qf2_loss: 11.25247, policy_loss: -258.48062, policy_entropy: -1.01169, alpha: 0.36991, time: 32.85534
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 607.91135, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 10.49200, qf2_loss: 10.50748, policy_loss: -258.89668, policy_entropy: -1.00567, alpha: 0.37172, time: 33.16920
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 608.15794, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 10.30539, qf2_loss: 10.28358, policy_loss: -261.19840, policy_entropy: -1.01141, alpha: 0.37357, time: 33.21985
[CW] eval: return: 532.86805, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 612.39534, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 10.76345, qf2_loss: 10.71093, policy_loss: -260.73860, policy_entropy: -1.01174, alpha: 0.37645, time: 32.84770
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 531.22363, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 13.03001, qf2_loss: 13.01193, policy_loss: -261.92561, policy_entropy: -1.00449, alpha: 0.37942, time: 33.10295
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 521.65775, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 14.64916, qf2_loss: 14.39436, policy_loss: -264.76546, policy_entropy: -0.98878, alpha: 0.37802, time: 33.08964
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 681.96931, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 12.28299, qf2_loss: 12.28495, policy_loss: -263.57455, policy_entropy: -1.00983, alpha: 0.37728, time: 33.00145
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 631.13172, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 10.44581, qf2_loss: 10.51908, policy_loss: -264.84221, policy_entropy: -1.01047, alpha: 0.37991, time: 33.43905
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 528.19082, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 11.26687, qf2_loss: 11.20862, policy_loss: -266.57203, policy_entropy: -1.01668, alpha: 0.38345, time: 33.20458
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 532.32937, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 13.18618, qf2_loss: 13.22380, policy_loss: -266.97595, policy_entropy: -1.00847, alpha: 0.38713, time: 33.24139
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 543.29670, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 11.01664, qf2_loss: 10.92901, policy_loss: -269.58823, policy_entropy: -1.00354, alpha: 0.38884, time: 32.89572
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 606.73881, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 11.81724, qf2_loss: 11.80003, policy_loss: -270.14168, policy_entropy: -1.01341, alpha: 0.39156, time: 33.52284
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 604.01501, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 12.26447, qf2_loss: 12.29781, policy_loss: -269.74409, policy_entropy: -0.99645, alpha: 0.39324, time: 32.98222
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 535.25753, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 10.52417, qf2_loss: 10.44549, policy_loss: -272.23570, policy_entropy: -1.01102, alpha: 0.39312, time: 32.99882
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 686.03614, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 11.06728, qf2_loss: 11.16790, policy_loss: -272.17237, policy_entropy: -1.00786, alpha: 0.39662, time: 32.83535
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 677.67122, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 10.99009, qf2_loss: 10.94185, policy_loss: -274.36833, policy_entropy: -1.00841, alpha: 0.39837, time: 32.84144
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 605.19570, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 11.19112, qf2_loss: 11.22870, policy_loss: -274.61378, policy_entropy: -1.01559, alpha: 0.40104, time: 33.14889
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 559.66069, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 12.85150, qf2_loss: 12.76373, policy_loss: -274.50287, policy_entropy: -1.00612, alpha: 0.40456, time: 32.77764
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 534.88381, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 12.38661, qf2_loss: 12.41816, policy_loss: -276.40667, policy_entropy: -1.00160, alpha: 0.40589, time: 33.20209
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 650.22241, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 15.66541, qf2_loss: 15.75919, policy_loss: -277.13865, policy_entropy: -0.99000, alpha: 0.40496, time: 32.97585
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 547.61361, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 11.68552, qf2_loss: 11.64364, policy_loss: -278.66461, policy_entropy: -1.00286, alpha: 0.40269, time: 32.93907
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 592.20275, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 12.89395, qf2_loss: 12.74349, policy_loss: -278.90285, policy_entropy: -1.00627, alpha: 0.40480, time: 33.14875
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 653.04626, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 11.69328, qf2_loss: 11.72647, policy_loss: -279.43164, policy_entropy: -1.01326, alpha: 0.40773, time: 32.91025
[CW] eval: return: 608.24843, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 672.97335, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 12.44777, qf2_loss: 12.30245, policy_loss: -281.11242, policy_entropy: -1.00441, alpha: 0.40975, time: 32.98736
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 590.84627, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 12.63778, qf2_loss: 12.64772, policy_loss: -280.77096, policy_entropy: -1.00092, alpha: 0.41070, time: 32.74751
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 691.86775, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 13.34276, qf2_loss: 13.32058, policy_loss: -282.41953, policy_entropy: -1.00092, alpha: 0.41091, time: 32.96466
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 614.84668, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 13.21599, qf2_loss: 13.29866, policy_loss: -284.56444, policy_entropy: -0.99486, alpha: 0.41104, time: 33.04607
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 606.56755, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 12.01680, qf2_loss: 11.96283, policy_loss: -284.30669, policy_entropy: -1.00463, alpha: 0.41022, time: 33.14156
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 638.73258, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 12.89106, qf2_loss: 12.90039, policy_loss: -286.16702, policy_entropy: -1.00899, alpha: 0.41223, time: 33.02090
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 597.42318, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 12.67904, qf2_loss: 12.60010, policy_loss: -287.47756, policy_entropy: -0.99877, alpha: 0.41372, time: 33.23899
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 678.92140, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 12.87967, qf2_loss: 12.80068, policy_loss: -287.95424, policy_entropy: -1.00189, alpha: 0.41331, time: 33.09010
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 632.32905, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 14.43140, qf2_loss: 14.31300, policy_loss: -287.04238, policy_entropy: -0.99944, alpha: 0.41373, time: 33.03763
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 533.65897, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 13.12913, qf2_loss: 13.09765, policy_loss: -289.65126, policy_entropy: -1.00501, alpha: 0.41407, time: 32.98755
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 598.68324, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 12.09755, qf2_loss: 12.20253, policy_loss: -290.30977, policy_entropy: -1.00479, alpha: 0.41556, time: 33.04350
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 613.33897, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 12.37048, qf2_loss: 12.25696, policy_loss: -291.14898, policy_entropy: -1.00150, alpha: 0.41694, time: 33.12217
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 621.30486, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 12.48560, qf2_loss: 12.49971, policy_loss: -291.47625, policy_entropy: -1.01075, alpha: 0.41832, time: 33.21965
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 680.50043, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 13.42254, qf2_loss: 13.50375, policy_loss: -293.55317, policy_entropy: -1.00308, alpha: 0.42019, time: 33.31651
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 688.99972, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 14.08401, qf2_loss: 13.89251, policy_loss: -294.62908, policy_entropy: -1.00431, alpha: 0.42121, time: 33.30178
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 679.63232, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 13.84907, qf2_loss: 13.77419, policy_loss: -295.04945, policy_entropy: -0.99860, alpha: 0.42217, time: 33.05405
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 614.55668, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 13.73613, qf2_loss: 13.64309, policy_loss: -294.15776, policy_entropy: -1.00448, alpha: 0.42219, time: 33.45000
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 661.29006, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 12.74817, qf2_loss: 12.68504, policy_loss: -296.23124, policy_entropy: -1.00462, alpha: 0.42392, time: 33.15601
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 672.62234, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 13.88822, qf2_loss: 13.80186, policy_loss: -297.36215, policy_entropy: -0.99724, alpha: 0.42500, time: 32.88610
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 676.56846, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 13.98497, qf2_loss: 14.03745, policy_loss: -299.39976, policy_entropy: -1.00138, alpha: 0.42450, time: 32.96796
[CW] eval: return: 693.20217, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 680.91701, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 13.11780, qf2_loss: 13.14934, policy_loss: -298.43954, policy_entropy: -0.99842, alpha: 0.42367, time: 32.95437
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 681.61770, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 13.79436, qf2_loss: 13.71805, policy_loss: -300.47605, policy_entropy: -1.00594, alpha: 0.42419, time: 33.04128
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 733.20966, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 14.28884, qf2_loss: 14.16270, policy_loss: -300.35639, policy_entropy: -1.02228, alpha: 0.42921, time: 33.26657
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 606.76662, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 13.43594, qf2_loss: 13.40985, policy_loss: -301.34339, policy_entropy: -1.00179, alpha: 0.43231, time: 33.03462
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 689.25367, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 14.29823, qf2_loss: 14.43954, policy_loss: -303.14905, policy_entropy: -0.99517, alpha: 0.43250, time: 33.00856
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 644.99148, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 18.16519, qf2_loss: 18.06387, policy_loss: -304.41149, policy_entropy: -0.99555, alpha: 0.43002, time: 33.29635
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 825.33994, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 12.83485, qf2_loss: 12.78399, policy_loss: -305.07006, policy_entropy: -0.99850, alpha: 0.42964, time: 32.71526
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 757.17120, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 13.45113, qf2_loss: 13.41558, policy_loss: -306.41602, policy_entropy: -1.02037, alpha: 0.43225, time: 32.70191
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 839.44473, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 12.62191, qf2_loss: 12.56087, policy_loss: -308.79841, policy_entropy: -1.00347, alpha: 0.43561, time: 32.88284
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 677.29405, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 13.09064, qf2_loss: 13.00041, policy_loss: -309.00279, policy_entropy: -1.00773, alpha: 0.43692, time: 33.01111
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 750.53654, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 13.19453, qf2_loss: 13.20034, policy_loss: -308.25579, policy_entropy: -1.00527, alpha: 0.43838, time: 32.75804
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 683.68076, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 13.66332, qf2_loss: 13.65316, policy_loss: -309.52262, policy_entropy: -1.00354, alpha: 0.43990, time: 33.13206
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 668.59986, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 15.85099, qf2_loss: 15.88997, policy_loss: -312.01084, policy_entropy: -0.99247, alpha: 0.43987, time: 32.80315
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 666.73753, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 14.14386, qf2_loss: 13.96470, policy_loss: -313.09455, policy_entropy: -0.98856, alpha: 0.43691, time: 33.02541
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 746.09952, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 14.24373, qf2_loss: 14.00870, policy_loss: -313.01248, policy_entropy: -1.00463, alpha: 0.43634, time: 36.19652
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 677.16490, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 15.24720, qf2_loss: 15.11571, policy_loss: -313.29622, policy_entropy: -0.99885, alpha: 0.43768, time: 33.22070
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 827.94591, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 14.66126, qf2_loss: 14.63065, policy_loss: -316.83476, policy_entropy: -1.00604, alpha: 0.43678, time: 33.76171
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 840.34810, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 12.99928, qf2_loss: 13.07455, policy_loss: -316.51626, policy_entropy: -1.01026, alpha: 0.44021, time: 33.09011
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 829.57958, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 18.20916, qf2_loss: 18.27740, policy_loss: -317.05535, policy_entropy: -1.00327, alpha: 0.44255, time: 33.54689
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 617.09426, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 15.88815, qf2_loss: 15.82294, policy_loss: -317.75441, policy_entropy: -0.99357, alpha: 0.44069, time: 32.91986
[CW] eval: return: 784.93200, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 826.11767, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 16.10842, qf2_loss: 16.11143, policy_loss: -318.01431, policy_entropy: -1.00828, alpha: 0.44204, time: 32.91826
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 839.84558, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 14.09159, qf2_loss: 14.02314, policy_loss: -320.78531, policy_entropy: -1.01501, alpha: 0.44447, time: 33.00982
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 837.77722, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 13.10381, qf2_loss: 13.09080, policy_loss: -320.55234, policy_entropy: -1.00898, alpha: 0.44936, time: 32.84779
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 842.07032, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 13.73221, qf2_loss: 13.72179, policy_loss: -322.38453, policy_entropy: -1.02099, alpha: 0.45381, time: 32.74878
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 844.91581, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 15.27451, qf2_loss: 15.22129, policy_loss: -323.50196, policy_entropy: -0.99428, alpha: 0.45673, time: 33.51918
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 731.18485, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 16.14079, qf2_loss: 16.14178, policy_loss: -324.16315, policy_entropy: -1.00395, alpha: 0.45588, time: 33.13700
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 838.81368, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 18.01982, qf2_loss: 18.03093, policy_loss: -326.00571, policy_entropy: -0.99568, alpha: 0.45698, time: 33.14901
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 836.76895, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 13.78234, qf2_loss: 13.81215, policy_loss: -326.56233, policy_entropy: -1.00659, alpha: 0.45609, time: 32.90271
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 839.12824, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 15.68702, qf2_loss: 15.51710, policy_loss: -327.39329, policy_entropy: -1.00861, alpha: 0.45908, time: 33.04406
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 843.42529, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 14.99942, qf2_loss: 14.91925, policy_loss: -328.31441, policy_entropy: -1.00881, alpha: 0.46255, time: 32.72208
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 842.46767, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 16.37970, qf2_loss: 16.38879, policy_loss: -328.05981, policy_entropy: -1.00234, alpha: 0.46412, time: 32.81279
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 839.35300, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 15.90543, qf2_loss: 15.82201, policy_loss: -330.35704, policy_entropy: -1.00925, alpha: 0.46490, time: 32.84957
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 680.44590, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 16.26818, qf2_loss: 16.09166, policy_loss: -332.16850, policy_entropy: -1.00912, alpha: 0.46915, time: 33.50805
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 751.09259, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 16.69971, qf2_loss: 16.64113, policy_loss: -334.11823, policy_entropy: -1.00164, alpha: 0.47048, time: 33.27318
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 797.48173, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 14.03677, qf2_loss: 14.13703, policy_loss: -333.15931, policy_entropy: -1.01742, alpha: 0.47434, time: 32.79962
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 838.87312, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 18.68775, qf2_loss: 18.67165, policy_loss: -334.83115, policy_entropy: -0.99390, alpha: 0.47692, time: 33.00154
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 838.26622, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 19.62329, qf2_loss: 19.54744, policy_loss: -334.82742, policy_entropy: -0.99436, alpha: 0.47446, time: 32.86731
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 838.92334, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 15.87072, qf2_loss: 15.87405, policy_loss: -336.82864, policy_entropy: -1.00218, alpha: 0.47337, time: 32.80544
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 825.49569, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 15.99821, qf2_loss: 15.83579, policy_loss: -336.64381, policy_entropy: -1.00277, alpha: 0.47560, time: 33.12674
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 835.37080, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 15.52137, qf2_loss: 15.31621, policy_loss: -339.16918, policy_entropy: -1.01192, alpha: 0.47778, time: 32.86071
[CW] eval: return: 805.33319, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 597.06583, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 16.52325, qf2_loss: 16.56622, policy_loss: -340.53338, policy_entropy: -1.01323, alpha: 0.48331, time: 32.82563
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 827.74403, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 17.12132, qf2_loss: 16.91915, policy_loss: -342.76143, policy_entropy: -1.01160, alpha: 0.48617, time: 33.10057
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 830.92284, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 17.91602, qf2_loss: 17.96210, policy_loss: -341.71898, policy_entropy: -0.99450, alpha: 0.48588, time: 33.40960
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 756.88894, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 16.75158, qf2_loss: 16.70040, policy_loss: -343.43973, policy_entropy: -1.01269, alpha: 0.48810, time: 32.78203
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 844.09997, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 16.50695, qf2_loss: 16.60911, policy_loss: -342.28904, policy_entropy: -1.00116, alpha: 0.49110, time: 33.10092
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 839.05634, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 20.30195, qf2_loss: 20.09015, policy_loss: -345.35554, policy_entropy: -0.99609, alpha: 0.49124, time: 33.19271
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 833.17238, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 18.57100, qf2_loss: 18.41856, policy_loss: -345.69991, policy_entropy: -1.00048, alpha: 0.49050, time: 32.72707
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 844.33700, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 22.42392, qf2_loss: 22.20293, policy_loss: -348.13368, policy_entropy: -1.00007, alpha: 0.48996, time: 33.02613
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 668.41147, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 20.75246, qf2_loss: 20.60157, policy_loss: -348.52635, policy_entropy: -0.99860, alpha: 0.48893, time: 32.91584
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 844.96086, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 16.69546, qf2_loss: 16.78410, policy_loss: -351.10340, policy_entropy: -1.01078, alpha: 0.49050, time: 33.12389
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 832.49936, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 16.81820, qf2_loss: 16.87324, policy_loss: -351.17568, policy_entropy: -1.01112, alpha: 0.49630, time: 33.59103
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 831.97437, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 17.20649, qf2_loss: 17.23631, policy_loss: -350.50963, policy_entropy: -1.00352, alpha: 0.49820, time: 33.17381
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 841.52163, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 19.97180, qf2_loss: 20.08713, policy_loss: -355.03560, policy_entropy: -0.99767, alpha: 0.49860, time: 33.24820
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 842.19132, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 20.13873, qf2_loss: 19.85843, policy_loss: -353.78460, policy_entropy: -0.98563, alpha: 0.49522, time: 32.98923
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 819.61415, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 28.58669, qf2_loss: 28.62359, policy_loss: -354.81790, policy_entropy: -0.99599, alpha: 0.49502, time: 33.47747
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 760.90754, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 22.94256, qf2_loss: 22.76517, policy_loss: -357.61764, policy_entropy: -0.99333, alpha: 0.48965, time: 32.91787
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 839.80568, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 18.14375, qf2_loss: 18.19728, policy_loss: -356.89560, policy_entropy: -1.01631, alpha: 0.49168, time: 32.82177
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 842.05077, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 18.30010, qf2_loss: 18.28912, policy_loss: -360.98424, policy_entropy: -1.01464, alpha: 0.49764, time: 32.85197
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 836.48560, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 31.28880, qf2_loss: 30.87680, policy_loss: -360.53929, policy_entropy: -0.99126, alpha: 0.50093, time: 33.23192
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 838.69398, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 20.85642, qf2_loss: 20.90160, policy_loss: -362.68106, policy_entropy: -0.99878, alpha: 0.49514, time: 32.85129
[CW] eval: return: 838.12705, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 835.67086, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 17.75038, qf2_loss: 17.76525, policy_loss: -362.49328, policy_entropy: -1.01479, alpha: 0.49826, time: 33.01627
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 838.79193, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 18.27564, qf2_loss: 18.14630, policy_loss: -363.44303, policy_entropy: -1.00033, alpha: 0.50117, time: 33.30746
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 834.98891, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 18.23752, qf2_loss: 18.28143, policy_loss: -365.46102, policy_entropy: -1.00695, alpha: 0.50281, time: 34.02802
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 762.49096, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 20.05103, qf2_loss: 20.01322, policy_loss: -366.21447, policy_entropy: -1.00055, alpha: 0.50489, time: 32.85576
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 844.30917, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 19.96106, qf2_loss: 19.91149, policy_loss: -368.63400, policy_entropy: -1.00685, alpha: 0.50674, time: 32.92216
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 745.97455, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 21.02668, qf2_loss: 21.13904, policy_loss: -367.34848, policy_entropy: -1.01088, alpha: 0.50869, time: 32.95062
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 837.95951, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 21.27702, qf2_loss: 21.12705, policy_loss: -368.18762, policy_entropy: -1.00831, alpha: 0.51263, time: 32.94428
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 832.92972, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 21.25148, qf2_loss: 21.25751, policy_loss: -371.56532, policy_entropy: -0.99485, alpha: 0.51436, time: 32.93898
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 825.77767, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 21.45645, qf2_loss: 21.25579, policy_loss: -372.19645, policy_entropy: -0.99687, alpha: 0.51058, time: 33.56244
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 842.52669, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 24.82987, qf2_loss: 24.89978, policy_loss: -373.76337, policy_entropy: -0.99103, alpha: 0.50959, time: 33.01450
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 672.66660, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 24.87461, qf2_loss: 24.73283, policy_loss: -375.65702, policy_entropy: -0.99662, alpha: 0.50739, time: 32.96436
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 832.07066, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 22.89635, qf2_loss: 22.70561, policy_loss: -374.37648, policy_entropy: -1.01619, alpha: 0.50915, time: 33.63802
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 827.60628, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 23.15256, qf2_loss: 23.21632, policy_loss: -378.18884, policy_entropy: -1.00898, alpha: 0.51343, time: 32.82108
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 836.61392, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 21.40628, qf2_loss: 21.35820, policy_loss: -376.52749, policy_entropy: -0.99780, alpha: 0.51544, time: 32.96174
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 833.56718, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 22.35993, qf2_loss: 22.34343, policy_loss: -378.30034, policy_entropy: -0.99433, alpha: 0.51344, time: 32.80846
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 834.78200, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 24.29603, qf2_loss: 24.33571, policy_loss: -379.80805, policy_entropy: -0.99462, alpha: 0.51207, time: 32.96391
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 835.49350, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 23.93213, qf2_loss: 23.77257, policy_loss: -380.92266, policy_entropy: -0.99945, alpha: 0.50970, time: 33.34821
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 830.52004, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 24.20549, qf2_loss: 24.14268, policy_loss: -380.65983, policy_entropy: -1.00753, alpha: 0.51156, time: 33.11022
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 842.65078, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 24.40295, qf2_loss: 24.51672, policy_loss: -385.20194, policy_entropy: -1.00727, alpha: 0.51417, time: 33.01909
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 834.83454, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 25.10962, qf2_loss: 25.02041, policy_loss: -384.82910, policy_entropy: -1.01271, alpha: 0.51728, time: 32.82314
[CW] eval: return: 828.12357, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 835.46391, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 23.05864, qf2_loss: 22.91586, policy_loss: -386.69461, policy_entropy: -1.00417, alpha: 0.52150, time: 32.71681
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 832.42006, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 23.41722, qf2_loss: 23.33117, policy_loss: -388.84357, policy_entropy: -0.99318, alpha: 0.52004, time: 33.04784
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 835.28991, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 27.44204, qf2_loss: 27.33782, policy_loss: -387.84342, policy_entropy: -1.01295, alpha: 0.52128, time: 33.00821
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 839.69563, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 32.29795, qf2_loss: 31.67663, policy_loss: -389.23280, policy_entropy: -1.00931, alpha: 0.52390, time: 33.01424
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 836.68035, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 24.40650, qf2_loss: 24.39603, policy_loss: -392.01446, policy_entropy: -1.00882, alpha: 0.52774, time: 35.29280
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 838.33496, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 22.50962, qf2_loss: 22.57765, policy_loss: -392.49151, policy_entropy: -1.01171, alpha: 0.53120, time: 32.83092
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 839.92083, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 22.78563, qf2_loss: 22.71499, policy_loss: -394.21619, policy_entropy: -0.98521, alpha: 0.53250, time: 32.86466
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 761.61237, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 25.33583, qf2_loss: 25.39677, policy_loss: -394.54630, policy_entropy: -0.99862, alpha: 0.52810, time: 32.89586
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 677.17362, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 37.25757, qf2_loss: 37.38892, policy_loss: -396.40037, policy_entropy: -0.99243, alpha: 0.52678, time: 32.96936
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 833.16289, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 30.87605, qf2_loss: 30.61906, policy_loss: -397.82846, policy_entropy: -0.98537, alpha: 0.52378, time: 33.19289
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 680.29232, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 24.22957, qf2_loss: 24.19947, policy_loss: -399.10522, policy_entropy: -1.01485, alpha: 0.52271, time: 32.99185
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 834.38560, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 26.01940, qf2_loss: 25.91408, policy_loss: -395.97181, policy_entropy: -1.02538, alpha: 0.52962, time: 33.64017
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 835.15685, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 28.33480, qf2_loss: 28.16449, policy_loss: -400.20697, policy_entropy: -0.99508, alpha: 0.53265, time: 32.78234
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 752.92622, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 32.96024, qf2_loss: 32.48982, policy_loss: -401.46019, policy_entropy: -0.99654, alpha: 0.53220, time: 33.07162
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 830.60330, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 24.80373, qf2_loss: 24.86616, policy_loss: -404.55611, policy_entropy: -1.01210, alpha: 0.53233, time: 32.94037
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 593.64902, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 25.38987, qf2_loss: 25.30719, policy_loss: -404.42511, policy_entropy: -0.99100, alpha: 0.53255, time: 33.05982
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 837.41669, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 25.65257, qf2_loss: 25.54779, policy_loss: -405.71174, policy_entropy: -1.00400, alpha: 0.53288, time: 32.96310
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 825.69312, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 30.29410, qf2_loss: 30.51087, policy_loss: -407.72746, policy_entropy: -1.00598, alpha: 0.53354, time: 32.91041
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 827.56906, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 30.54439, qf2_loss: 30.42685, policy_loss: -406.72137, policy_entropy: -0.97958, alpha: 0.53316, time: 33.18031
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 827.09367, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 31.15896, qf2_loss: 31.20567, policy_loss: -408.60046, policy_entropy: -1.00606, alpha: 0.52938, time: 32.95006
[CW] eval: return: 821.85316, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 828.96326, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 27.59501, qf2_loss: 27.40674, policy_loss: -410.39961, policy_entropy: -0.99034, alpha: 0.52936, time: 32.91695
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 671.88877, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 27.69252, qf2_loss: 27.91339, policy_loss: -411.33319, policy_entropy: -1.00558, alpha: 0.52847, time: 33.20419
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 751.10849, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 26.53439, qf2_loss: 26.63230, policy_loss: -410.77863, policy_entropy: -1.01291, alpha: 0.53167, time: 32.99418
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 832.57580, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 27.83591, qf2_loss: 27.91138, policy_loss: -415.34328, policy_entropy: -0.99923, alpha: 0.53394, time: 32.97658
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 833.47164, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 29.08802, qf2_loss: 29.14613, policy_loss: -414.12762, policy_entropy: -1.00822, alpha: 0.53414, time: 32.88444
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 836.78210, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 28.95952, qf2_loss: 28.73413, policy_loss: -415.08784, policy_entropy: -0.99953, alpha: 0.53588, time: 32.78151
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 830.55375, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 34.71258, qf2_loss: 34.38028, policy_loss: -415.07925, policy_entropy: -1.00478, alpha: 0.53665, time: 33.05164
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 822.33155, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 36.82438, qf2_loss: 36.67697, policy_loss: -417.38038, policy_entropy: -0.98985, alpha: 0.53687, time: 33.20341
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 830.89752, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 33.78077, qf2_loss: 33.67728, policy_loss: -415.88506, policy_entropy: -1.00509, alpha: 0.53458, time: 33.07151
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 834.29456, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 27.92553, qf2_loss: 27.70182, policy_loss: -417.35606, policy_entropy: -1.00604, alpha: 0.53510, time: 32.88619
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 838.33591, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 27.77353, qf2_loss: 27.79523, policy_loss: -422.49683, policy_entropy: -0.99844, alpha: 0.53646, time: 32.85829
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 832.31606, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 27.04974, qf2_loss: 27.07734, policy_loss: -420.45277, policy_entropy: -0.99980, alpha: 0.53667, time: 33.21147
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 836.49847, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 38.11549, qf2_loss: 38.05864, policy_loss: -421.50398, policy_entropy: -1.00911, alpha: 0.53865, time: 33.08286
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 540.18874, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 30.51508, qf2_loss: 30.44699, policy_loss: -424.80156, policy_entropy: -0.99845, alpha: 0.53920, time: 33.09656
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 764.72426, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 28.45197, qf2_loss: 28.30797, policy_loss: -422.51657, policy_entropy: -1.00639, alpha: 0.53949, time: 32.89597
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 762.93441, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 30.85790, qf2_loss: 30.54980, policy_loss: -425.68141, policy_entropy: -1.00353, alpha: 0.54129, time: 32.80165
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 835.98295, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 29.15369, qf2_loss: 28.99694, policy_loss: -427.78843, policy_entropy: -0.99445, alpha: 0.54157, time: 33.16409
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 752.90621, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 31.70717, qf2_loss: 31.20196, policy_loss: -429.52380, policy_entropy: -1.00043, alpha: 0.53996, time: 33.33234
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 767.63031, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 31.52859, qf2_loss: 31.51004, policy_loss: -426.49611, policy_entropy: -1.00456, alpha: 0.54031, time: 33.04330
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 829.16716, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 55.70880, qf2_loss: 55.36466, policy_loss: -432.99204, policy_entropy: -0.97691, alpha: 0.53933, time: 33.50076
[CW] eval: return: 747.07519, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 754.75823, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 33.06030, qf2_loss: 33.23193, policy_loss: -434.85392, policy_entropy: -0.97116, alpha: 0.53136, time: 32.74378
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 835.74950, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 26.54702, qf2_loss: 26.49784, policy_loss: -433.76664, policy_entropy: -0.99269, alpha: 0.52640, time: 33.43849
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 829.31267, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 30.60169, qf2_loss: 30.43123, policy_loss: -432.88077, policy_entropy: -1.01104, alpha: 0.52710, time: 32.88119
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 839.34260, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 27.35592, qf2_loss: 27.43861, policy_loss: -437.28764, policy_entropy: -1.00138, alpha: 0.52855, time: 32.77712
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 834.89204, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 28.14062, qf2_loss: 28.20866, policy_loss: -438.15634, policy_entropy: -1.01063, alpha: 0.52904, time: 33.04707
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 835.23791, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 29.53304, qf2_loss: 29.46323, policy_loss: -436.10592, policy_entropy: -1.00454, alpha: 0.53226, time: 33.00580
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 824.46197, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 31.06331, qf2_loss: 31.17244, policy_loss: -440.69984, policy_entropy: -1.00748, alpha: 0.53411, time: 32.83349
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 836.30064, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 31.51139, qf2_loss: 31.53436, policy_loss: -437.23172, policy_entropy: -1.00788, alpha: 0.53660, time: 35.49493
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 839.41399, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 31.54173, qf2_loss: 31.32969, policy_loss: -440.60358, policy_entropy: -0.99688, alpha: 0.53712, time: 33.02782
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 758.74424, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 31.71353, qf2_loss: 31.76799, policy_loss: -444.25962, policy_entropy: -0.99067, alpha: 0.53463, time: 33.17489
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 836.90761, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 31.85536, qf2_loss: 32.24774, policy_loss: -444.01233, policy_entropy: -1.00189, alpha: 0.53410, time: 33.60030
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 830.20579, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 32.78577, qf2_loss: 32.77540, policy_loss: -444.81638, policy_entropy: -1.00701, alpha: 0.53541, time: 32.82820
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 832.36154, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 29.88953, qf2_loss: 29.63287, policy_loss: -447.20958, policy_entropy: -1.00367, alpha: 0.53690, time: 32.85017
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 839.88790, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 32.18979, qf2_loss: 32.24364, policy_loss: -447.36180, policy_entropy: -1.00633, alpha: 0.53780, time: 32.99544
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 840.42002, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 33.59332, qf2_loss: 33.47886, policy_loss: -444.78734, policy_entropy: -1.02459, alpha: 0.54177, time: 32.79904
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 841.02460, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 35.99778, qf2_loss: 36.07145, policy_loss: -448.84277, policy_entropy: -1.00942, alpha: 0.54570, time: 33.51788
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 836.05403, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 32.18126, qf2_loss: 31.67307, policy_loss: -452.12770, policy_entropy: -0.97718, alpha: 0.54447, time: 33.35986
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 841.62086, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 31.24994, qf2_loss: 31.28821, policy_loss: -451.43958, policy_entropy: -1.00119, alpha: 0.54112, time: 32.82974
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 763.50300, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 31.79855, qf2_loss: 31.54417, policy_loss: -449.74164, policy_entropy: -1.01398, alpha: 0.54267, time: 32.82054
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 832.37105, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 33.77535, qf2_loss: 33.89787, policy_loss: -454.08615, policy_entropy: -0.98605, alpha: 0.54344, time: 32.99534
[CW] eval: return: 766.63929, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 606.41898, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 31.61254, qf2_loss: 31.38873, policy_loss: -454.30177, policy_entropy: -0.99975, alpha: 0.54182, time: 32.74445
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 676.72883, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 31.94796, qf2_loss: 32.14658, policy_loss: -452.75006, policy_entropy: -1.01015, alpha: 0.54257, time: 32.91628
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 831.92195, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 38.37692, qf2_loss: 38.50920, policy_loss: -454.57976, policy_entropy: -0.99479, alpha: 0.54373, time: 33.25032
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 744.58958, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 40.42379, qf2_loss: 40.19101, policy_loss: -457.87142, policy_entropy: -0.99580, alpha: 0.54256, time: 32.94073
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 748.00352, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 37.80803, qf2_loss: 38.01628, policy_loss: -457.13158, policy_entropy: -1.00534, alpha: 0.54248, time: 33.41870
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 824.87438, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 32.61649, qf2_loss: 32.42911, policy_loss: -460.18191, policy_entropy: -0.99578, alpha: 0.54290, time: 32.98382
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 561.94045, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 30.96220, qf2_loss: 30.89206, policy_loss: -460.78249, policy_entropy: -0.99763, alpha: 0.54189, time: 32.92475
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 838.66560, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 28.56827, qf2_loss: 28.64336, policy_loss: -461.44184, policy_entropy: -0.99422, alpha: 0.54062, time: 33.14938
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 844.17753, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 30.08514, qf2_loss: 29.94240, policy_loss: -463.64829, policy_entropy: -1.00441, alpha: 0.54002, time: 33.11838
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 832.53018, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 29.14392, qf2_loss: 29.09457, policy_loss: -466.61993, policy_entropy: -0.98216, alpha: 0.53993, time: 32.99283
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 835.97532, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 33.48445, qf2_loss: 33.48638, policy_loss: -464.98193, policy_entropy: -1.00309, alpha: 0.53771, time: 32.84069
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 832.85679, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 34.70637, qf2_loss: 34.71569, policy_loss: -464.28089, policy_entropy: -0.99310, alpha: 0.53607, time: 33.11698
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 838.29397, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 32.89718, qf2_loss: 32.88461, policy_loss: -465.56559, policy_entropy: -0.99145, alpha: 0.53514, time: 33.20894
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 753.58656, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 33.44443, qf2_loss: 33.07225, policy_loss: -467.78427, policy_entropy: -0.99963, alpha: 0.53388, time: 32.92842
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 841.51574, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 30.01741, qf2_loss: 30.21617, policy_loss: -472.32823, policy_entropy: -0.98988, alpha: 0.53249, time: 32.83124
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 823.45629, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 38.24292, qf2_loss: 37.85846, policy_loss: -470.33282, policy_entropy: -0.98885, alpha: 0.52971, time: 32.98507
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 604.05111, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 34.14204, qf2_loss: 34.04134, policy_loss: -468.53071, policy_entropy: -1.00420, alpha: 0.52904, time: 33.03471
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 835.68074, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 32.23927, qf2_loss: 31.95711, policy_loss: -473.54822, policy_entropy: -1.00691, alpha: 0.53015, time: 32.88909
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 753.30507, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 31.63336, qf2_loss: 31.44965, policy_loss: -473.40793, policy_entropy: -1.01103, alpha: 0.53305, time: 32.93239
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 752.87082, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 35.99680, qf2_loss: 36.02467, policy_loss: -472.95398, policy_entropy: -0.98937, alpha: 0.53288, time: 33.10875
[CW] eval: return: 835.01711, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 837.69194, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 32.87615, qf2_loss: 32.89421, policy_loss: -472.27762, policy_entropy: -1.00786, alpha: 0.53224, time: 32.91702
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 826.49836, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 34.75078, qf2_loss: 34.76953, policy_loss: -476.63956, policy_entropy: -0.99979, alpha: 0.53300, time: 33.14973
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 817.90996, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 35.10122, qf2_loss: 34.82390, policy_loss: -476.87897, policy_entropy: -0.99318, alpha: 0.53260, time: 32.75368
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 766.33981, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 40.84934, qf2_loss: 40.62169, policy_loss: -476.36632, policy_entropy: -0.99193, alpha: 0.53167, time: 33.07072
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 827.82318, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 37.93392, qf2_loss: 37.57351, policy_loss: -478.15260, policy_entropy: -1.00543, alpha: 0.53029, time: 33.20706
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 752.39134, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 39.17295, qf2_loss: 38.97381, policy_loss: -476.08574, policy_entropy: -1.01080, alpha: 0.53292, time: 33.10826
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 829.36380, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 30.98954, qf2_loss: 30.67320, policy_loss: -481.60332, policy_entropy: -0.99288, alpha: 0.53420, time: 33.11864
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 828.67959, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 33.58560, qf2_loss: 33.27098, policy_loss: -481.92116, policy_entropy: -0.99396, alpha: 0.53128, time: 32.90997
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 838.17313, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 32.06427, qf2_loss: 32.11067, policy_loss: -483.73291, policy_entropy: -1.00287, alpha: 0.53106, time: 32.94273
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 751.70658, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 34.44968, qf2_loss: 34.53046, policy_loss: -484.46576, policy_entropy: -0.99019, alpha: 0.53023, time: 33.04934
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 832.35081, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 40.78383, qf2_loss: 40.55716, policy_loss: -482.49639, policy_entropy: -1.01325, alpha: 0.53043, time: 33.13012
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 829.23918, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 31.26322, qf2_loss: 31.59349, policy_loss: -486.07455, policy_entropy: -1.00516, alpha: 0.53243, time: 33.26491
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 834.93458, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 33.66989, qf2_loss: 33.60921, policy_loss: -485.89492, policy_entropy: -1.00582, alpha: 0.53376, time: 33.26481
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 675.67337, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 36.74829, qf2_loss: 36.54240, policy_loss: -488.13947, policy_entropy: -0.99380, alpha: 0.53384, time: 32.89604
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 836.49890, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 40.94383, qf2_loss: 41.05417, policy_loss: -487.25592, policy_entropy: -1.01159, alpha: 0.53353, time: 33.07843
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 832.22751, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 36.10052, qf2_loss: 35.96224, policy_loss: -488.86753, policy_entropy: -1.00840, alpha: 0.53665, time: 32.88859
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 833.63481, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 32.61702, qf2_loss: 32.28457, policy_loss: -489.37303, policy_entropy: -0.99963, alpha: 0.53785, time: 33.20832
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 830.43128, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 32.65609, qf2_loss: 32.44791, policy_loss: -490.79511, policy_entropy: -0.98140, alpha: 0.53607, time: 33.02391
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 822.20860, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 38.05250, qf2_loss: 38.23556, policy_loss: -491.13787, policy_entropy: -1.00506, alpha: 0.53353, time: 33.03494
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 824.26501, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 38.70800, qf2_loss: 38.36665, policy_loss: -495.69420, policy_entropy: -0.98002, alpha: 0.53258, time: 33.11131
[CW] eval: return: 769.38761, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 823.54124, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 35.50443, qf2_loss: 34.92774, policy_loss: -494.21163, policy_entropy: -0.99942, alpha: 0.53032, time: 32.89632
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 838.37639, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 36.58710, qf2_loss: 36.76772, policy_loss: -494.21639, policy_entropy: -1.00430, alpha: 0.53002, time: 32.65302
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 823.72427, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 34.76315, qf2_loss: 34.53852, policy_loss: -494.85659, policy_entropy: -1.01447, alpha: 0.53219, time: 35.05017
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 838.82302, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 31.96744, qf2_loss: 31.88402, policy_loss: -495.29261, policy_entropy: -0.98972, alpha: 0.53323, time: 32.87886
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 817.77755, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 32.29951, qf2_loss: 32.07255, policy_loss: -499.52119, policy_entropy: -0.98895, alpha: 0.53127, time: 33.27840
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 747.17259, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 33.08030, qf2_loss: 33.03446, policy_loss: -498.12189, policy_entropy: -1.01283, alpha: 0.53044, time: 32.92327
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 821.25833, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 39.97759, qf2_loss: 39.54192, policy_loss: -503.03733, policy_entropy: -0.98404, alpha: 0.53076, time: 33.16351
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 826.28909, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 38.48560, qf2_loss: 38.10871, policy_loss: -502.70440, policy_entropy: -0.98540, alpha: 0.52803, time: 32.94384
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 747.61635, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 33.19910, qf2_loss: 33.29277, policy_loss: -501.45370, policy_entropy: -0.99786, alpha: 0.52535, time: 33.29817
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 821.35455, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 36.77913, qf2_loss: 36.92487, policy_loss: -503.09164, policy_entropy: -0.99306, alpha: 0.52569, time: 33.41310
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 810.42640, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 37.54173, qf2_loss: 37.52578, policy_loss: -501.71751, policy_entropy: -1.00523, alpha: 0.52461, time: 32.97080
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 823.59624, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 45.44931, qf2_loss: 45.11939, policy_loss: -501.45946, policy_entropy: -0.99888, alpha: 0.52522, time: 32.84174
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 827.38637, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 42.55729, qf2_loss: 42.64255, policy_loss: -507.19600, policy_entropy: -1.00023, alpha: 0.52465, time: 32.67242
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 832.74222, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 57.67464, qf2_loss: 57.49272, policy_loss: -503.57913, policy_entropy: -0.99622, alpha: 0.52362, time: 33.05254
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 810.96738, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 36.48074, qf2_loss: 36.50589, policy_loss: -504.96012, policy_entropy: -0.99910, alpha: 0.52315, time: 33.23655
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 827.37409, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 31.92610, qf2_loss: 31.90671, policy_loss: -506.79891, policy_entropy: -0.99068, alpha: 0.52239, time: 32.81938
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 748.45001, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 36.40655, qf2_loss: 36.02099, policy_loss: -508.53436, policy_entropy: -0.99024, alpha: 0.52020, time: 33.15157
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 818.56121, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 33.46877, qf2_loss: 33.73448, policy_loss: -510.19365, policy_entropy: -0.99034, alpha: 0.51903, time: 32.91394
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 824.68948, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 32.03519, qf2_loss: 32.09297, policy_loss: -511.10913, policy_entropy: -0.99500, alpha: 0.51802, time: 32.72676
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 831.36877, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 37.60029, qf2_loss: 36.91442, policy_loss: -512.21908, policy_entropy: -0.98854, alpha: 0.51540, time: 33.10976
[CW] eval: return: 789.88059, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 821.63975, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 35.05322, qf2_loss: 35.34579, policy_loss: -510.04211, policy_entropy: -0.99985, alpha: 0.51593, time: 32.73153
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 832.14365, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 33.66254, qf2_loss: 33.52482, policy_loss: -514.06491, policy_entropy: -0.99318, alpha: 0.51361, time: 33.31153
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 829.93610, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 31.39596, qf2_loss: 31.28514, policy_loss: -515.49291, policy_entropy: -0.99844, alpha: 0.51408, time: 33.14557
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 829.11517, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 33.18552, qf2_loss: 32.81960, policy_loss: -516.28998, policy_entropy: -0.99364, alpha: 0.51254, time: 33.45379
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 755.51132, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 33.68802, qf2_loss: 33.91667, policy_loss: -514.21457, policy_entropy: -1.01077, alpha: 0.51313, time: 33.14604
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 829.21200, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 36.51097, qf2_loss: 36.29177, policy_loss: -516.48715, policy_entropy: -1.00649, alpha: 0.51440, time: 32.87126
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 827.49263, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 34.06894, qf2_loss: 33.84216, policy_loss: -517.13189, policy_entropy: -0.99319, alpha: 0.51377, time: 33.00938
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 817.67716, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 31.75872, qf2_loss: 31.69041, policy_loss: -520.32137, policy_entropy: -0.98950, alpha: 0.51330, time: 32.79416
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 830.63226, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 37.15477, qf2_loss: 37.14418, policy_loss: -519.51690, policy_entropy: -0.99154, alpha: 0.51169, time: 33.19444
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 680.41101, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 35.08800, qf2_loss: 35.17006, policy_loss: -522.05214, policy_entropy: -0.99892, alpha: 0.50970, time: 33.33348
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 830.75602, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 42.68940, qf2_loss: 42.49627, policy_loss: -520.18033, policy_entropy: -1.00059, alpha: 0.51034, time: 32.81985
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 815.36450, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 43.28171, qf2_loss: 42.97806, policy_loss: -523.73136, policy_entropy: -0.97965, alpha: 0.50803, time: 33.29076
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 828.00857, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 33.76640, qf2_loss: 33.83693, policy_loss: -523.36592, policy_entropy: -1.00649, alpha: 0.50558, time: 32.83288
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 818.80559, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 32.78254, qf2_loss: 32.74161, policy_loss: -526.07517, policy_entropy: -0.98316, alpha: 0.50636, time: 33.00757
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 821.50714, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 33.55512, qf2_loss: 33.26332, policy_loss: -527.43539, policy_entropy: -0.96844, alpha: 0.50210, time: 32.92280
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 825.35310, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 31.93924, qf2_loss: 31.63218, policy_loss: -526.41830, policy_entropy: -0.98419, alpha: 0.49695, time: 33.15348
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 743.50478, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 33.72241, qf2_loss: 33.90509, policy_loss: -524.51111, policy_entropy: -1.00742, alpha: 0.49682, time: 32.96324
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 828.76246, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 37.23729, qf2_loss: 36.90978, policy_loss: -530.05444, policy_entropy: -1.00794, alpha: 0.49791, time: 32.83364
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 812.80435, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 35.50724, qf2_loss: 35.70785, policy_loss: -528.09264, policy_entropy: -0.99878, alpha: 0.49763, time: 33.09224
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 817.85547, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 34.91522, qf2_loss: 34.87125, policy_loss: -525.70637, policy_entropy: -1.01074, alpha: 0.49764, time: 32.85602
[CW] eval: return: 823.36011, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 824.20425, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 36.78809, qf2_loss: 36.83178, policy_loss: -530.60975, policy_entropy: -0.99303, alpha: 0.49885, time: 32.90542
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 684.42866, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 36.98607, qf2_loss: 36.47246, policy_loss: -528.71810, policy_entropy: -1.00938, alpha: 0.50005, time: 33.07116
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 818.35604, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 41.40922, qf2_loss: 40.97913, policy_loss: -532.47724, policy_entropy: -1.00430, alpha: 0.50059, time: 32.98431
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 747.87911, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 32.61699, qf2_loss: 32.50494, policy_loss: -531.86595, policy_entropy: -0.99116, alpha: 0.50126, time: 33.14709
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 829.64281, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 31.45310, qf2_loss: 31.14031, policy_loss: -532.95293, policy_entropy: -0.99623, alpha: 0.49891, time: 33.07266
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 748.37580, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 31.94383, qf2_loss: 31.66202, policy_loss: -531.88298, policy_entropy: -1.01654, alpha: 0.50105, time: 33.26513
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 820.06569, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 32.02394, qf2_loss: 31.83402, policy_loss: -533.03609, policy_entropy: -0.98874, alpha: 0.50188, time: 32.84732
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 816.96720, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 32.71771, qf2_loss: 32.32747, policy_loss: -536.20948, policy_entropy: -1.00485, alpha: 0.50049, time: 33.03653
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 825.89030, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 33.83621, qf2_loss: 34.09867, policy_loss: -537.81803, policy_entropy: -1.00087, alpha: 0.50062, time: 33.01198
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 836.51963, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 36.24921, qf2_loss: 36.64233, policy_loss: -536.90903, policy_entropy: -0.98258, alpha: 0.50035, time: 33.01200
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 823.03478, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 36.85153, qf2_loss: 36.48058, policy_loss: -538.09282, policy_entropy: -0.98712, alpha: 0.49629, time: 33.87193
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 831.77142, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 32.68667, qf2_loss: 32.75190, policy_loss: -536.75660, policy_entropy: -0.98360, alpha: 0.49384, time: 33.14658
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 755.40664, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 32.02667, qf2_loss: 31.81849, policy_loss: -540.88360, policy_entropy: -0.97937, alpha: 0.49048, time: 33.43398
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 746.90535, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 36.26118, qf2_loss: 36.54706, policy_loss: -540.28435, policy_entropy: -0.99400, alpha: 0.48929, time: 32.96852
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 813.99761, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 37.94815, qf2_loss: 37.95229, policy_loss: -539.60610, policy_entropy: -1.00236, alpha: 0.48830, time: 33.04039
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 817.95540, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 34.43601, qf2_loss: 34.54279, policy_loss: -538.92007, policy_entropy: -1.00921, alpha: 0.48839, time: 33.42345
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 735.86896, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 33.61668, qf2_loss: 33.35720, policy_loss: -544.79514, policy_entropy: -0.98164, alpha: 0.48832, time: 33.00161
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 669.15784, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 29.37870, qf2_loss: 29.44591, policy_loss: -544.11513, policy_entropy: -0.98476, alpha: 0.48561, time: 33.49625
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 738.38654, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 33.06634, qf2_loss: 33.21309, policy_loss: -544.07521, policy_entropy: -1.00017, alpha: 0.48389, time: 33.12452
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 744.19706, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 38.62790, qf2_loss: 38.64042, policy_loss: -543.36356, policy_entropy: -1.02352, alpha: 0.48540, time: 33.17696
[CW] eval: return: 772.52984, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 826.05993, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 36.01119, qf2_loss: 35.94893, policy_loss: -543.06507, policy_entropy: -1.01459, alpha: 0.48910, time: 33.65869
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 829.67046, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 39.70210, qf2_loss: 39.13016, policy_loss: -546.06011, policy_entropy: -0.97887, alpha: 0.48869, time: 40.63673
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 815.68181, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 40.19305, qf2_loss: 40.79414, policy_loss: -546.03737, policy_entropy: -0.99046, alpha: 0.48622, time: 33.19551
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 818.16389, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 33.34532, qf2_loss: 33.22835, policy_loss: -547.10026, policy_entropy: -0.98931, alpha: 0.48408, time: 34.24250
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 830.62482, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 32.49646, qf2_loss: 32.72288, policy_loss: -548.20713, policy_entropy: -0.99363, alpha: 0.48222, time: 33.08015
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 830.91909, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 29.46824, qf2_loss: 29.69647, policy_loss: -549.69003, policy_entropy: -1.00881, alpha: 0.48307, time: 32.97009
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 822.75013, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 33.14558, qf2_loss: 32.94958, policy_loss: -547.24698, policy_entropy: -1.01690, alpha: 0.48502, time: 33.36688
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 812.66672, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 29.81162, qf2_loss: 29.82152, policy_loss: -552.09506, policy_entropy: -0.99451, alpha: 0.48650, time: 33.17428
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 825.98466, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 32.98731, qf2_loss: 32.85499, policy_loss: -550.53395, policy_entropy: -0.99600, alpha: 0.48700, time: 33.10464
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 812.05996, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 31.84919, qf2_loss: 31.69268, policy_loss: -551.49177, policy_entropy: -1.00062, alpha: 0.48494, time: 33.47722
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 819.71795, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 32.23456, qf2_loss: 32.22175, policy_loss: -553.06558, policy_entropy: -0.98617, alpha: 0.48493, time: 33.05980
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 737.32128, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 31.44692, qf2_loss: 31.35297, policy_loss: -552.28401, policy_entropy: -0.98766, alpha: 0.48223, time: 32.97566
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 827.04484, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 32.72161, qf2_loss: 32.50198, policy_loss: -555.53521, policy_entropy: -0.98806, alpha: 0.47932, time: 33.13163
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 752.84021, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 34.17608, qf2_loss: 34.50835, policy_loss: -554.26171, policy_entropy: -0.99466, alpha: 0.47781, time: 33.00716
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 821.27117, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 34.15154, qf2_loss: 34.13085, policy_loss: -557.68392, policy_entropy: -0.98999, alpha: 0.47759, time: 33.08544
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 830.41509, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 35.27089, qf2_loss: 35.30525, policy_loss: -557.18069, policy_entropy: -0.98882, alpha: 0.47498, time: 33.11323
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 832.74687, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 33.08947, qf2_loss: 33.27185, policy_loss: -556.66508, policy_entropy: -0.99609, alpha: 0.47323, time: 33.11270
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 818.35653, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 33.28291, qf2_loss: 33.21694, policy_loss: -556.48948, policy_entropy: -1.02024, alpha: 0.47437, time: 32.94727
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 829.77120, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 32.39734, qf2_loss: 32.05655, policy_loss: -560.09006, policy_entropy: -0.97927, alpha: 0.47573, time: 33.31741
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 833.88960, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 34.74462, qf2_loss: 34.69055, policy_loss: -557.63087, policy_entropy: -0.99173, alpha: 0.47286, time: 33.12986
[CW] eval: return: 786.15941, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 823.94986, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 31.31521, qf2_loss: 31.05068, policy_loss: -557.97327, policy_entropy: -1.01736, alpha: 0.47314, time: 32.85202
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 830.31034, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 33.96138, qf2_loss: 33.80460, policy_loss: -559.38787, policy_entropy: -0.99630, alpha: 0.47430, time: 32.83508
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 813.15861, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 32.72696, qf2_loss: 32.98232, policy_loss: -558.37747, policy_entropy: -1.00079, alpha: 0.47353, time: 33.09711
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 597.60887, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 31.55678, qf2_loss: 31.43587, policy_loss: -561.88460, policy_entropy: -1.00224, alpha: 0.47483, time: 33.35447
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 822.92275, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 36.44101, qf2_loss: 36.41867, policy_loss: -560.15558, policy_entropy: -0.99678, alpha: 0.47463, time: 33.25358
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 814.12143, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 33.51312, qf2_loss: 33.49575, policy_loss: -563.38422, policy_entropy: -0.99004, alpha: 0.47313, time: 32.88101
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 819.12780, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 32.35673, qf2_loss: 32.23043, policy_loss: -563.23384, policy_entropy: -0.99145, alpha: 0.47225, time: 33.18174
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 811.77763, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 31.83465, qf2_loss: 31.98165, policy_loss: -566.10088, policy_entropy: -0.99348, alpha: 0.47026, time: 33.04018
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 819.96375, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 32.31631, qf2_loss: 32.77528, policy_loss: -567.02328, policy_entropy: -0.98957, alpha: 0.46876, time: 33.11717
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 820.03776, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 30.71761, qf2_loss: 30.75293, policy_loss: -564.48138, policy_entropy: -1.00749, alpha: 0.46882, time: 33.14019
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 830.46442, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 33.03995, qf2_loss: 32.51162, policy_loss: -564.27233, policy_entropy: -1.01030, alpha: 0.47038, time: 32.85759
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 820.87159, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 42.69417, qf2_loss: 42.77351, policy_loss: -568.76877, policy_entropy: -1.00847, alpha: 0.47072, time: 33.06592
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 833.80673, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 35.13406, qf2_loss: 34.94089, policy_loss: -567.07388, policy_entropy: -1.00040, alpha: 0.47346, time: 33.00060
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 819.59587, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 34.95359, qf2_loss: 34.90732, policy_loss: -569.59209, policy_entropy: -0.99847, alpha: 0.47290, time: 33.03241
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 832.02008, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 32.27613, qf2_loss: 31.89066, policy_loss: -569.99029, policy_entropy: -0.98148, alpha: 0.47123, time: 32.98100
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 826.96419, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 55.31741, qf2_loss: 54.76425, policy_loss: -570.93057, policy_entropy: -0.98485, alpha: 0.46933, time: 33.14119
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 820.12565, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 32.69962, qf2_loss: 32.77760, policy_loss: -570.43879, policy_entropy: -0.98758, alpha: 0.46598, time: 32.96183
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 818.57072, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 32.69769, qf2_loss: 32.84710, policy_loss: -568.64947, policy_entropy: -0.99438, alpha: 0.46402, time: 33.14777
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 755.07385, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 31.64967, qf2_loss: 31.87341, policy_loss: -570.60264, policy_entropy: -1.00018, alpha: 0.46360, time: 33.32620
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 831.04445, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 29.23028, qf2_loss: 29.07937, policy_loss: -573.25558, policy_entropy: -1.00165, alpha: 0.46398, time: 33.10900
[CW] eval: return: 816.26990, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 819.86100, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 30.88962, qf2_loss: 30.70095, policy_loss: -569.45179, policy_entropy: -1.01298, alpha: 0.46483, time: 33.14620
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 830.67823, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 29.77869, qf2_loss: 29.73778, policy_loss: -571.84563, policy_entropy: -1.00990, alpha: 0.46620, time: 33.15960
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 829.60865, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 29.51820, qf2_loss: 29.43853, policy_loss: -574.55634, policy_entropy: -1.00038, alpha: 0.46814, time: 33.10896
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 829.34201, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 29.28587, qf2_loss: 29.20584, policy_loss: -573.00750, policy_entropy: -1.00460, alpha: 0.46833, time: 33.21610
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 820.40849, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 34.37947, qf2_loss: 34.48265, policy_loss: -572.90863, policy_entropy: -1.01575, alpha: 0.46995, time: 32.69320
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 821.76865, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 34.25280, qf2_loss: 34.75380, policy_loss: -572.71838, policy_entropy: -1.00063, alpha: 0.47116, time: 32.86269
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 826.54073, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 32.27333, qf2_loss: 32.27602, policy_loss: -578.38871, policy_entropy: -0.97331, alpha: 0.46820, time: 32.88731
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 807.32757, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 35.17221, qf2_loss: 35.16194, policy_loss: -576.62576, policy_entropy: -0.97898, alpha: 0.46633, time: 32.78466
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 832.48678, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 35.64355, qf2_loss: 35.25250, policy_loss: -575.27029, policy_entropy: -0.99250, alpha: 0.46270, time: 33.02513
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 831.56226, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 35.33206, qf2_loss: 34.93627, policy_loss: -578.41449, policy_entropy: -0.97677, alpha: 0.46091, time: 33.15904
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 825.98574, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 30.42028, qf2_loss: 30.23678, policy_loss: -577.49741, policy_entropy: -1.01488, alpha: 0.46054, time: 33.42536
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 837.84882, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 34.14339, qf2_loss: 34.11692, policy_loss: -579.84649, policy_entropy: -0.98921, alpha: 0.46006, time: 33.12559
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 827.35071, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 30.23663, qf2_loss: 30.34051, policy_loss: -580.20814, policy_entropy: -1.00388, alpha: 0.45910, time: 32.75355
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 839.40277, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 32.85430, qf2_loss: 32.70629, policy_loss: -585.10254, policy_entropy: -0.96742, alpha: 0.45825, time: 32.77115
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 829.01344, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 31.90450, qf2_loss: 32.00636, policy_loss: -580.77666, policy_entropy: -1.00622, alpha: 0.45449, time: 32.70880
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 828.46008, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 35.04034, qf2_loss: 34.95798, policy_loss: -581.66305, policy_entropy: -1.00974, alpha: 0.45642, time: 32.82835
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 825.62424, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 31.75482, qf2_loss: 31.72164, policy_loss: -583.55698, policy_entropy: -0.99301, alpha: 0.45673, time: 32.97048
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 753.84521, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 29.74756, qf2_loss: 29.67131, policy_loss: -584.03852, policy_entropy: -1.00008, alpha: 0.45644, time: 32.81881
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 824.44216, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 31.79865, qf2_loss: 31.85385, policy_loss: -584.26310, policy_entropy: -0.99705, alpha: 0.45552, time: 33.41253
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 829.92152, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 40.57313, qf2_loss: 40.10626, policy_loss: -583.69189, policy_entropy: -0.98835, alpha: 0.45532, time: 32.73171
[CW] eval: return: 796.59451, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 823.42104, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 35.44974, qf2_loss: 36.15532, policy_loss: -584.27815, policy_entropy: -0.99432, alpha: 0.45363, time: 32.69359
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 805.03910, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 37.04163, qf2_loss: 37.21500, policy_loss: -585.21866, policy_entropy: -1.00512, alpha: 0.45279, time: 32.89641
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 756.29387, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 31.16349, qf2_loss: 31.49261, policy_loss: -584.86226, policy_entropy: -0.99878, alpha: 0.45333, time: 33.00048
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 830.61612, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 27.91784, qf2_loss: 28.01365, policy_loss: -584.54511, policy_entropy: -1.01313, alpha: 0.45407, time: 32.98944
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 830.00390, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 32.39966, qf2_loss: 31.77746, policy_loss: -588.75295, policy_entropy: -0.99740, alpha: 0.45490, time: 33.14081
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 752.65555, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 31.66860, qf2_loss: 31.59282, policy_loss: -587.22407, policy_entropy: -1.00195, alpha: 0.45530, time: 33.20337
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 832.55638, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 31.17646, qf2_loss: 31.04500, policy_loss: -590.57654, policy_entropy: -0.98567, alpha: 0.45462, time: 33.41262
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 758.06843, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 32.31222, qf2_loss: 32.13095, policy_loss: -586.79059, policy_entropy: -1.00395, alpha: 0.45378, time: 33.12498
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 750.55360, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 28.37771, qf2_loss: 28.48762, policy_loss: -590.76942, policy_entropy: -0.98352, alpha: 0.45275, time: 32.77149
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 755.82873, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 36.03879, qf2_loss: 36.00854, policy_loss: -590.19813, policy_entropy: -0.98594, alpha: 0.45141, time: 33.01283
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 840.79953, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 37.91914, qf2_loss: 37.48258, policy_loss: -589.94134, policy_entropy: -0.98882, alpha: 0.44771, time: 33.05033
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 831.07422, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 36.05408, qf2_loss: 36.45118, policy_loss: -593.10264, policy_entropy: -0.97612, alpha: 0.44613, time: 32.89361
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 661.15924, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 35.64138, qf2_loss: 35.06130, policy_loss: -594.54349, policy_entropy: -0.98616, alpha: 0.44290, time: 32.79316
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 836.02784, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 31.93005, qf2_loss: 32.15595, policy_loss: -594.20775, policy_entropy: -1.00206, alpha: 0.44186, time: 36.48659
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 834.37029, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 28.75039, qf2_loss: 29.19078, policy_loss: -592.41312, policy_entropy: -1.02197, alpha: 0.44295, time: 32.98914
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 842.46948, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 31.53330, qf2_loss: 31.56066, policy_loss: -593.84305, policy_entropy: -1.00376, alpha: 0.44591, time: 33.07464
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 825.75333, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 28.78792, qf2_loss: 28.64755, policy_loss: -592.11908, policy_entropy: -0.99348, alpha: 0.44599, time: 33.03337
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 829.57799, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 30.68275, qf2_loss: 30.55424, policy_loss: -592.40622, policy_entropy: -1.01203, alpha: 0.44546, time: 32.78809
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 839.20283, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 32.19439, qf2_loss: 32.46658, policy_loss: -590.32591, policy_entropy: -1.01294, alpha: 0.44779, time: 32.66448
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 840.45152, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 30.91595, qf2_loss: 30.56377, policy_loss: -592.61744, policy_entropy: -1.00433, alpha: 0.44916, time: 32.86823
[CW] eval: return: 838.22585, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 839.38792, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 40.59772, qf2_loss: 40.59256, policy_loss: -596.66869, policy_entropy: -0.99211, alpha: 0.44878, time: 32.69737
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 831.62545, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 41.28405, qf2_loss: 41.28242, policy_loss: -598.81327, policy_entropy: -0.96516, alpha: 0.44552, time: 32.99808
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 541.37698, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 41.62806, qf2_loss: 41.31700, policy_loss: -598.58742, policy_entropy: -0.97177, alpha: 0.44103, time: 32.76806
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 735.98925, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 29.65133, qf2_loss: 29.41795, policy_loss: -600.30584, policy_entropy: -0.97392, alpha: 0.43632, time: 32.79083
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 842.09202, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 31.46262, qf2_loss: 31.24399, policy_loss: -597.06388, policy_entropy: -1.00257, alpha: 0.43473, time: 32.79152
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 840.07262, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 28.12358, qf2_loss: 27.44379, policy_loss: -599.45145, policy_entropy: -0.99997, alpha: 0.43518, time: 32.86670
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 825.41254, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 27.24713, qf2_loss: 27.60001, policy_loss: -598.40476, policy_entropy: -1.00435, alpha: 0.43564, time: 33.01176
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 836.58344, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 28.16065, qf2_loss: 28.31080, policy_loss: -602.20041, policy_entropy: -1.00239, alpha: 0.43529, time: 32.72958
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 830.48916, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 32.27422, qf2_loss: 32.79511, policy_loss: -599.72073, policy_entropy: -1.00763, alpha: 0.43772, time: 33.05764
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 821.37071, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 32.37182, qf2_loss: 32.33848, policy_loss: -605.00333, policy_entropy: -0.97730, alpha: 0.43627, time: 32.92216
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 837.51573, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 30.74496, qf2_loss: 30.45287, policy_loss: -602.55519, policy_entropy: -0.97962, alpha: 0.43279, time: 32.77282
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 839.64299, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 32.91898, qf2_loss: 32.82605, policy_loss: -597.75884, policy_entropy: -1.01844, alpha: 0.43281, time: 32.87534
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 835.07022, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 32.45942, qf2_loss: 32.98700, policy_loss: -602.84604, policy_entropy: -0.98151, alpha: 0.43290, time: 32.72261
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 828.43878, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 33.49686, qf2_loss: 32.90952, policy_loss: -605.37859, policy_entropy: -0.99394, alpha: 0.43128, time: 32.86038
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 831.16577, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 25.11791, qf2_loss: 25.01863, policy_loss: -604.40380, policy_entropy: -0.99540, alpha: 0.43024, time: 32.96934
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 742.11624, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 28.84758, qf2_loss: 29.04183, policy_loss: -602.61867, policy_entropy: -1.00277, alpha: 0.42999, time: 33.01929
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 741.23305, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 28.61573, qf2_loss: 28.80201, policy_loss: -605.52491, policy_entropy: -0.97914, alpha: 0.42908, time: 32.82848
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 835.42466, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 35.93352, qf2_loss: 35.70267, policy_loss: -605.96855, policy_entropy: -0.97240, alpha: 0.42538, time: 32.81906
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 599.16233, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 33.90159, qf2_loss: 33.94985, policy_loss: -604.13046, policy_entropy: -1.02422, alpha: 0.42373, time: 32.89032
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 740.33385, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 36.05581, qf2_loss: 36.11394, policy_loss: -606.54352, policy_entropy: -1.00723, alpha: 0.42737, time: 32.88601
[CW] eval: return: 753.25075, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 829.69051, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 31.57158, qf2_loss: 31.78197, policy_loss: -608.40355, policy_entropy: -0.98333, alpha: 0.42688, time: 32.97549
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 834.06735, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 35.69437, qf2_loss: 35.06752, policy_loss: -606.95774, policy_entropy: -1.00941, alpha: 0.42637, time: 33.06821
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 835.73329, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 30.83496, qf2_loss: 30.90121, policy_loss: -609.22582, policy_entropy: -1.00560, alpha: 0.42696, time: 33.08077
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 682.36998, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 29.30386, qf2_loss: 29.25593, policy_loss: -605.96660, policy_entropy: -1.00714, alpha: 0.42802, time: 33.16989
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 832.35802, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 28.78213, qf2_loss: 28.89457, policy_loss: -608.56497, policy_entropy: -1.01554, alpha: 0.42929, time: 32.64198
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 835.54686, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 30.26757, qf2_loss: 31.16316, policy_loss: -607.67529, policy_entropy: -1.00926, alpha: 0.43061, time: 32.81388
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 744.49715, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 30.70663, qf2_loss: 30.13385, policy_loss: -608.10618, policy_entropy: -1.01228, alpha: 0.43326, time: 33.05493
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 664.73629, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 31.53349, qf2_loss: 31.73096, policy_loss: -609.70076, policy_entropy: -0.99393, alpha: 0.43279, time: 32.98212
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 821.92692, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 38.01071, qf2_loss: 37.33154, policy_loss: -610.48962, policy_entropy: -0.98950, alpha: 0.43330, time: 33.11556
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 830.32493, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 29.79923, qf2_loss: 29.79080, policy_loss: -611.34499, policy_entropy: -0.99206, alpha: 0.43125, time: 32.95845
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 837.76328, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 29.08330, qf2_loss: 28.70715, policy_loss: -611.33469, policy_entropy: -1.01162, alpha: 0.43058, time: 32.78113
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 838.26766, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 27.91712, qf2_loss: 27.90644, policy_loss: -610.29433, policy_entropy: -1.01874, alpha: 0.43289, time: 33.21457
[CW] ---------------------------
[CW] ---- Iteration:   673 ----
[CW] collect: return: 825.59203, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 28.41527, qf2_loss: 28.49754, policy_loss: -614.89731, policy_entropy: -0.98240, alpha: 0.43443, time: 32.78767
[CW] ---------------------------
[CW] ---- Iteration:   674 ----
[CW] collect: return: 834.73872, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 26.69220, qf2_loss: 26.48648, policy_loss: -611.62380, policy_entropy: -1.01020, alpha: 0.43253, time: 32.73777
[CW] ---------------------------
[CW] ---- Iteration:   675 ----
[CW] collect: return: 831.25403, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 33.32609, qf2_loss: 33.64263, policy_loss: -610.76996, policy_entropy: -1.00839, alpha: 0.43403, time: 33.42441
[CW] ---------------------------
[CW] ---- Iteration:   676 ----
[CW] collect: return: 836.65729, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 26.99201, qf2_loss: 27.29907, policy_loss: -613.30800, policy_entropy: -0.99101, alpha: 0.43486, time: 33.61632
[CW] ---------------------------
[CW] ---- Iteration:   677 ----
[CW] collect: return: 834.11245, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 28.46150, qf2_loss: 28.43380, policy_loss: -614.56382, policy_entropy: -0.99035, alpha: 0.43232, time: 32.98320
[CW] ---------------------------
[CW] ---- Iteration:   678 ----
[CW] collect: return: 833.22694, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 31.53419, qf2_loss: 31.09617, policy_loss: -614.98582, policy_entropy: -0.98647, alpha: 0.43145, time: 33.60794
[CW] ---------------------------
[CW] ---- Iteration:   679 ----
[CW] collect: return: 757.51799, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 32.48938, qf2_loss: 32.04367, policy_loss: -613.37298, policy_entropy: -1.02052, alpha: 0.43105, time: 32.97682
[CW] ---------------------------
[CW] ---- Iteration:   680 ----
[CW] collect: return: 840.13090, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 34.40367, qf2_loss: 34.15287, policy_loss: -613.35545, policy_entropy: -0.99719, alpha: 0.43359, time: 32.85588
[CW] eval: return: 822.15584, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   681 ----
[CW] collect: return: 830.06251, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 29.22108, qf2_loss: 29.11300, policy_loss: -614.95324, policy_entropy: -0.97815, alpha: 0.43130, time: 32.82334
[CW] ---------------------------
[CW] ---- Iteration:   682 ----
[CW] collect: return: 835.06674, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 27.17172, qf2_loss: 27.26932, policy_loss: -616.12564, policy_entropy: -0.98358, alpha: 0.42784, time: 32.93373
[CW] ---------------------------
[CW] ---- Iteration:   683 ----
[CW] collect: return: 834.67669, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 30.66415, qf2_loss: 30.85364, policy_loss: -616.09444, policy_entropy: -0.97442, alpha: 0.42573, time: 33.02498
[CW] ---------------------------
[CW] ---- Iteration:   684 ----
[CW] collect: return: 828.07856, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 31.15510, qf2_loss: 30.89997, policy_loss: -614.39566, policy_entropy: -0.99720, alpha: 0.42351, time: 33.16480
[CW] ---------------------------
[CW] ---- Iteration:   685 ----
[CW] collect: return: 832.80377, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 29.86818, qf2_loss: 29.78794, policy_loss: -618.11206, policy_entropy: -0.98348, alpha: 0.42150, time: 32.99993
[CW] ---------------------------
[CW] ---- Iteration:   686 ----
[CW] collect: return: 832.54993, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 29.27990, qf2_loss: 29.28920, policy_loss: -617.41886, policy_entropy: -1.00091, alpha: 0.42057, time: 32.98535
[CW] ---------------------------
[CW] ---- Iteration:   687 ----
[CW] collect: return: 841.22363, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 30.24491, qf2_loss: 30.34688, policy_loss: -616.32059, policy_entropy: -1.00499, alpha: 0.42046, time: 32.98154
[CW] ---------------------------
[CW] ---- Iteration:   688 ----
[CW] collect: return: 826.71144, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 30.96367, qf2_loss: 31.03499, policy_loss: -615.16062, policy_entropy: -0.98829, alpha: 0.42074, time: 32.75503
[CW] ---------------------------
[CW] ---- Iteration:   689 ----
[CW] collect: return: 751.97774, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 33.18275, qf2_loss: 33.15764, policy_loss: -618.80300, policy_entropy: -0.98046, alpha: 0.41874, time: 32.98006
[CW] ---------------------------
[CW] ---- Iteration:   690 ----
[CW] collect: return: 830.69243, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 40.73723, qf2_loss: 40.56699, policy_loss: -620.21362, policy_entropy: -0.97589, alpha: 0.41582, time: 33.23400
[CW] ---------------------------
[CW] ---- Iteration:   691 ----
[CW] collect: return: 828.91750, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 28.81698, qf2_loss: 29.07977, policy_loss: -620.02011, policy_entropy: -0.97780, alpha: 0.41186, time: 32.85779
[CW] ---------------------------
[CW] ---- Iteration:   692 ----
[CW] collect: return: 836.79698, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 27.88431, qf2_loss: 27.51566, policy_loss: -617.74123, policy_entropy: -1.00598, alpha: 0.41124, time: 32.80039
[CW] ---------------------------
[CW] ---- Iteration:   693 ----
[CW] collect: return: 842.80148, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 29.92227, qf2_loss: 29.97831, policy_loss: -620.98972, policy_entropy: -0.99192, alpha: 0.41156, time: 32.83487
[CW] ---------------------------
[CW] ---- Iteration:   694 ----
[CW] collect: return: 835.79549, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 29.48064, qf2_loss: 29.08833, policy_loss: -622.02220, policy_entropy: -0.97231, alpha: 0.40907, time: 32.84148
[CW] ---------------------------
[CW] ---- Iteration:   695 ----
[CW] collect: return: 836.05972, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 25.81977, qf2_loss: 25.77409, policy_loss: -622.94749, policy_entropy: -0.98034, alpha: 0.40570, time: 32.89520
[CW] ---------------------------
[CW] ---- Iteration:   696 ----
[CW] collect: return: 837.41824, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 29.23731, qf2_loss: 29.03011, policy_loss: -620.02835, policy_entropy: -1.01651, alpha: 0.40571, time: 33.10132
[CW] ---------------------------
[CW] ---- Iteration:   697 ----
[CW] collect: return: 833.30485, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 28.90188, qf2_loss: 28.66196, policy_loss: -622.06583, policy_entropy: -0.99497, alpha: 0.40631, time: 32.65049
[CW] ---------------------------
[CW] ---- Iteration:   698 ----
[CW] collect: return: 829.07714, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 29.90546, qf2_loss: 29.87927, policy_loss: -622.55837, policy_entropy: -0.99946, alpha: 0.40559, time: 32.80672
[CW] ---------------------------
[CW] ---- Iteration:   699 ----
[CW] collect: return: 820.56645, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 36.28284, qf2_loss: 36.82371, policy_loss: -621.53992, policy_entropy: -1.00258, alpha: 0.40576, time: 32.93994
[CW] ---------------------------
[CW] ---- Iteration:   700 ----
[CW] collect: return: 829.89469, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 40.17565, qf2_loss: 39.77291, policy_loss: -622.03526, policy_entropy: -0.99798, alpha: 0.40660, time: 32.97437
[CW] eval: return: 827.71434, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   701 ----
[CW] collect: return: 819.96428, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 30.49604, qf2_loss: 30.44387, policy_loss: -624.02317, policy_entropy: -0.97386, alpha: 0.40447, time: 32.93063
[CW] ---------------------------
[CW] ---- Iteration:   702 ----
[CW] collect: return: 825.79039, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 28.87500, qf2_loss: 28.87019, policy_loss: -627.32703, policy_entropy: -0.96112, alpha: 0.40042, time: 32.73556
[CW] ---------------------------
[CW] ---- Iteration:   703 ----
[CW] collect: return: 840.01093, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 36.10100, qf2_loss: 36.35970, policy_loss: -623.72376, policy_entropy: -1.00626, alpha: 0.39832, time: 33.04401
[CW] ---------------------------
[CW] ---- Iteration:   704 ----
[CW] collect: return: 829.60251, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 30.96582, qf2_loss: 30.98754, policy_loss: -621.89205, policy_entropy: -1.00335, alpha: 0.39825, time: 32.70534
[CW] ---------------------------
[CW] ---- Iteration:   705 ----
[CW] collect: return: 826.53418, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 30.26974, qf2_loss: 30.15560, policy_loss: -622.95232, policy_entropy: -1.00863, alpha: 0.39832, time: 33.13807
[CW] ---------------------------
[CW] ---- Iteration:   706 ----
[CW] collect: return: 838.30853, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 28.52157, qf2_loss: 28.40430, policy_loss: -625.30369, policy_entropy: -0.99669, alpha: 0.39971, time: 32.95765
[CW] ---------------------------
[CW] ---- Iteration:   707 ----
[CW] collect: return: 835.15571, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 29.47993, qf2_loss: 29.39513, policy_loss: -627.45832, policy_entropy: -1.00646, alpha: 0.39881, time: 32.91517
[CW] ---------------------------
[CW] ---- Iteration:   708 ----
[CW] collect: return: 845.41263, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 25.61411, qf2_loss: 25.53646, policy_loss: -628.30744, policy_entropy: -0.99021, alpha: 0.39934, time: 32.96204
[CW] ---------------------------
[CW] ---- Iteration:   709 ----
[CW] collect: return: 839.40916, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 26.91675, qf2_loss: 26.77331, policy_loss: -628.90327, policy_entropy: -0.98847, alpha: 0.39860, time: 32.69076
[CW] ---------------------------
[CW] ---- Iteration:   710 ----
[CW] collect: return: 837.79752, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 33.47719, qf2_loss: 33.57891, policy_loss: -626.37311, policy_entropy: -0.98092, alpha: 0.39727, time: 32.99757
[CW] ---------------------------
[CW] ---- Iteration:   711 ----
[CW] collect: return: 820.89225, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 27.32124, qf2_loss: 27.19836, policy_loss: -628.80487, policy_entropy: -0.99556, alpha: 0.39431, time: 32.87198
[CW] ---------------------------
[CW] ---- Iteration:   712 ----
[CW] collect: return: 846.44183, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 29.68650, qf2_loss: 29.68214, policy_loss: -628.87303, policy_entropy: -0.99535, alpha: 0.39468, time: 33.21529
[CW] ---------------------------
[CW] ---- Iteration:   713 ----
[CW] collect: return: 838.49114, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 34.61987, qf2_loss: 34.05617, policy_loss: -630.74019, policy_entropy: -0.97239, alpha: 0.39322, time: 33.05952
[CW] ---------------------------
[CW] ---- Iteration:   714 ----
[CW] collect: return: 828.38075, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 30.57550, qf2_loss: 31.38284, policy_loss: -629.14160, policy_entropy: -0.99068, alpha: 0.39016, time: 32.77172
[CW] ---------------------------
[CW] ---- Iteration:   715 ----
[CW] collect: return: 827.18970, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 26.10291, qf2_loss: 25.85207, policy_loss: -630.06861, policy_entropy: -0.99222, alpha: 0.38857, time: 33.09345
[CW] ---------------------------
[CW] ---- Iteration:   716 ----
[CW] collect: return: 838.81476, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 29.31332, qf2_loss: 28.97541, policy_loss: -627.62966, policy_entropy: -1.00938, alpha: 0.38863, time: 33.12723
[CW] ---------------------------
[CW] ---- Iteration:   717 ----
[CW] collect: return: 828.82314, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 26.77837, qf2_loss: 26.56996, policy_loss: -629.92636, policy_entropy: -1.01121, alpha: 0.38989, time: 32.91742
[CW] ---------------------------
[CW] ---- Iteration:   718 ----
[CW] collect: return: 837.25700, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 26.65792, qf2_loss: 26.69374, policy_loss: -630.87762, policy_entropy: -0.99137, alpha: 0.39050, time: 33.00271
[CW] ---------------------------
[CW] ---- Iteration:   719 ----
[CW] collect: return: 837.84010, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 29.20208, qf2_loss: 29.30170, policy_loss: -630.92697, policy_entropy: -0.98963, alpha: 0.38984, time: 33.11451
[CW] ---------------------------
[CW] ---- Iteration:   720 ----
[CW] collect: return: 835.39169, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 39.41912, qf2_loss: 39.43592, policy_loss: -633.47711, policy_entropy: -0.99233, alpha: 0.38803, time: 32.81019
[CW] eval: return: 837.69604, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   721 ----
[CW] collect: return: 837.59167, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 37.84851, qf2_loss: 37.77361, policy_loss: -633.57838, policy_entropy: -0.95951, alpha: 0.38528, time: 32.91430
[CW] ---------------------------
[CW] ---- Iteration:   722 ----
[CW] collect: return: 829.71058, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 28.72327, qf2_loss: 28.23325, policy_loss: -631.81603, policy_entropy: -0.99138, alpha: 0.38244, time: 32.96740
[CW] ---------------------------
[CW] ---- Iteration:   723 ----
[CW] collect: return: 837.27139, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 27.01877, qf2_loss: 27.16406, policy_loss: -632.29248, policy_entropy: -1.00070, alpha: 0.38181, time: 32.83023
[CW] ---------------------------
[CW] ---- Iteration:   724 ----
[CW] collect: return: 837.18403, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 24.71204, qf2_loss: 25.00018, policy_loss: -634.76048, policy_entropy: -0.99177, alpha: 0.38080, time: 32.91650
[CW] ---------------------------
[CW] ---- Iteration:   725 ----
[CW] collect: return: 829.68230, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 28.43384, qf2_loss: 28.53369, policy_loss: -633.36744, policy_entropy: -1.00282, alpha: 0.38113, time: 33.30580
[CW] ---------------------------
[CW] ---- Iteration:   726 ----
[CW] collect: return: 835.83513, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 27.47487, qf2_loss: 27.57527, policy_loss: -634.52427, policy_entropy: -1.01692, alpha: 0.38168, time: 32.63832
[CW] ---------------------------
[CW] ---- Iteration:   727 ----
[CW] collect: return: 835.79219, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 31.75236, qf2_loss: 31.53529, policy_loss: -633.35964, policy_entropy: -1.01167, alpha: 0.38363, time: 32.73423
[CW] ---------------------------
[CW] ---- Iteration:   728 ----
[CW] collect: return: 843.43163, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 26.92105, qf2_loss: 26.82576, policy_loss: -635.63974, policy_entropy: -0.99856, alpha: 0.38440, time: 32.88928
[CW] ---------------------------
[CW] ---- Iteration:   729 ----
[CW] collect: return: 833.10277, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 25.17138, qf2_loss: 25.27123, policy_loss: -637.06909, policy_entropy: -0.99139, alpha: 0.38376, time: 33.30742
[CW] ---------------------------
[CW] ---- Iteration:   730 ----
[CW] collect: return: 846.02222, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 24.77944, qf2_loss: 24.72510, policy_loss: -638.47602, policy_entropy: -1.00145, alpha: 0.38365, time: 32.75975
[CW] ---------------------------
[CW] ---- Iteration:   731 ----
[CW] collect: return: 828.24431, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 32.88184, qf2_loss: 32.98261, policy_loss: -633.83546, policy_entropy: -1.01340, alpha: 0.38442, time: 33.00794
[CW] ---------------------------
[CW] ---- Iteration:   732 ----
[CW] collect: return: 748.24225, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 32.94897, qf2_loss: 33.04365, policy_loss: -636.56219, policy_entropy: -0.98974, alpha: 0.38525, time: 32.95244
[CW] ---------------------------
[CW] ---- Iteration:   733 ----
[CW] collect: return: 819.35418, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 31.99952, qf2_loss: 32.09883, policy_loss: -635.05446, policy_entropy: -1.00105, alpha: 0.38465, time: 33.60606
[CW] ---------------------------
[CW] ---- Iteration:   734 ----
[CW] collect: return: 825.84608, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 26.22350, qf2_loss: 25.84817, policy_loss: -638.44943, policy_entropy: -0.99225, alpha: 0.38381, time: 33.17585
[CW] ---------------------------
[CW] ---- Iteration:   735 ----
[CW] collect: return: 840.91055, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 34.46054, qf2_loss: 33.90163, policy_loss: -635.73110, policy_entropy: -0.99564, alpha: 0.38307, time: 32.79521
[CW] ---------------------------
[CW] ---- Iteration:   736 ----
[CW] collect: return: 825.37857, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 27.54278, qf2_loss: 27.35193, policy_loss: -637.24366, policy_entropy: -0.98683, alpha: 0.38198, time: 32.75177
[CW] ---------------------------
[CW] ---- Iteration:   737 ----
[CW] collect: return: 834.65598, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 26.39562, qf2_loss: 25.86509, policy_loss: -639.91935, policy_entropy: -0.99651, alpha: 0.38094, time: 32.66185
[CW] ---------------------------
[CW] ---- Iteration:   738 ----
[CW] collect: return: 840.80674, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 23.00704, qf2_loss: 23.03806, policy_loss: -641.50939, policy_entropy: -0.98378, alpha: 0.38045, time: 32.78132
[CW] ---------------------------
[CW] ---- Iteration:   739 ----
[CW] collect: return: 834.98937, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 26.42513, qf2_loss: 26.49066, policy_loss: -639.33434, policy_entropy: -0.99437, alpha: 0.37881, time: 33.20599
[CW] ---------------------------
[CW] ---- Iteration:   740 ----
[CW] collect: return: 845.42726, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 24.02641, qf2_loss: 23.65061, policy_loss: -640.11209, policy_entropy: -1.00127, alpha: 0.37831, time: 32.63004
[CW] eval: return: 834.20546, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   741 ----
[CW] collect: return: 832.37075, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 23.84068, qf2_loss: 24.06770, policy_loss: -639.76486, policy_entropy: -0.99573, alpha: 0.37782, time: 32.79037
[CW] ---------------------------
[CW] ---- Iteration:   742 ----
[CW] collect: return: 831.44256, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 26.95083, qf2_loss: 27.26650, policy_loss: -637.54360, policy_entropy: -1.00519, alpha: 0.37802, time: 32.84533
[CW] ---------------------------
[CW] ---- Iteration:   743 ----
[CW] collect: return: 828.14373, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 29.60497, qf2_loss: 29.03021, policy_loss: -641.20326, policy_entropy: -0.99703, alpha: 0.37834, time: 32.89848
[CW] ---------------------------
[CW] ---- Iteration:   744 ----
[CW] collect: return: 824.43939, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 30.57178, qf2_loss: 30.61081, policy_loss: -640.59350, policy_entropy: -0.98155, alpha: 0.37663, time: 33.00785
[CW] ---------------------------
[CW] ---- Iteration:   745 ----
[CW] collect: return: 816.65439, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 29.31621, qf2_loss: 29.30822, policy_loss: -642.02144, policy_entropy: -0.99101, alpha: 0.37563, time: 32.58147
[CW] ---------------------------
[CW] ---- Iteration:   746 ----
[CW] collect: return: 827.35063, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 31.31170, qf2_loss: 31.46258, policy_loss: -638.40775, policy_entropy: -1.01022, alpha: 0.37512, time: 32.99568
[CW] ---------------------------
[CW] ---- Iteration:   747 ----
[CW] collect: return: 843.47684, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 34.88468, qf2_loss: 34.29351, policy_loss: -638.88091, policy_entropy: -0.99577, alpha: 0.37659, time: 33.22941
[CW] ---------------------------
[CW] ---- Iteration:   748 ----
[CW] collect: return: 837.55510, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 28.42523, qf2_loss: 28.71477, policy_loss: -639.59657, policy_entropy: -1.00546, alpha: 0.37529, time: 32.75309
[CW] ---------------------------
[CW] ---- Iteration:   749 ----
[CW] collect: return: 841.93502, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 30.42950, qf2_loss: 30.17136, policy_loss: -643.40437, policy_entropy: -0.97677, alpha: 0.37484, time: 33.02337
[CW] ---------------------------
[CW] ---- Iteration:   750 ----
[CW] collect: return: 842.45241, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 41.69841, qf2_loss: 41.14793, policy_loss: -643.02884, policy_entropy: -0.99128, alpha: 0.37357, time: 32.83656
[CW] ---------------------------
[CW] ---- Iteration:   751 ----
[CW] collect: return: 837.16020, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 31.82847, qf2_loss: 31.93942, policy_loss: -640.29478, policy_entropy: -1.00010, alpha: 0.37210, time: 33.33723
[CW] ---------------------------
[CW] ---- Iteration:   752 ----
[CW] collect: return: 824.49777, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 23.91579, qf2_loss: 23.74504, policy_loss: -640.38825, policy_entropy: -1.00170, alpha: 0.37261, time: 32.87182
[CW] ---------------------------
[CW] ---- Iteration:   753 ----
[CW] collect: return: 752.14421, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 25.09606, qf2_loss: 24.88641, policy_loss: -643.11707, policy_entropy: -1.00274, alpha: 0.37276, time: 32.99923
[CW] ---------------------------
[CW] ---- Iteration:   754 ----
[CW] collect: return: 835.19327, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 27.78159, qf2_loss: 28.12893, policy_loss: -643.55225, policy_entropy: -1.00088, alpha: 0.37268, time: 32.95012
[CW] ---------------------------
[CW] ---- Iteration:   755 ----
[CW] collect: return: 841.41250, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 23.87113, qf2_loss: 23.63753, policy_loss: -645.07128, policy_entropy: -1.00239, alpha: 0.37255, time: 32.86579
[CW] ---------------------------
[CW] ---- Iteration:   756 ----
[CW] collect: return: 839.05883, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 25.77574, qf2_loss: 25.46723, policy_loss: -643.64798, policy_entropy: -1.02007, alpha: 0.37389, time: 32.88297
[CW] ---------------------------
[CW] ---- Iteration:   757 ----
[CW] collect: return: 844.74010, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 27.96019, qf2_loss: 27.96131, policy_loss: -645.56722, policy_entropy: -0.98276, alpha: 0.37530, time: 32.72329
[CW] ---------------------------
[CW] ---- Iteration:   758 ----
[CW] collect: return: 843.23350, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 28.08569, qf2_loss: 28.42969, policy_loss: -646.11655, policy_entropy: -1.00500, alpha: 0.37363, time: 32.84097
[CW] ---------------------------
[CW] ---- Iteration:   759 ----
[CW] collect: return: 839.93464, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 31.10868, qf2_loss: 30.80849, policy_loss: -647.48072, policy_entropy: -0.98879, alpha: 0.37377, time: 33.01721
[CW] ---------------------------
[CW] ---- Iteration:   760 ----
[CW] collect: return: 767.65653, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 31.70017, qf2_loss: 31.33719, policy_loss: -643.44271, policy_entropy: -1.00994, alpha: 0.37371, time: 32.86328
[CW] eval: return: 757.13845, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   761 ----
[CW] collect: return: 733.40624, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 24.44451, qf2_loss: 24.52143, policy_loss: -646.80070, policy_entropy: -1.00440, alpha: 0.37398, time: 32.94243
[CW] ---------------------------
[CW] ---- Iteration:   762 ----
[CW] collect: return: 822.94064, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 24.39279, qf2_loss: 24.23934, policy_loss: -645.24739, policy_entropy: -1.02373, alpha: 0.37471, time: 32.64415
[CW] ---------------------------
[CW] ---- Iteration:   763 ----
[CW] collect: return: 837.16332, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 24.47725, qf2_loss: 24.59814, policy_loss: -649.26265, policy_entropy: -0.98421, alpha: 0.37695, time: 33.15157
[CW] ---------------------------
[CW] ---- Iteration:   764 ----
[CW] collect: return: 848.04796, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 23.33714, qf2_loss: 23.29954, policy_loss: -646.98470, policy_entropy: -1.00273, alpha: 0.37518, time: 32.74715
[CW] ---------------------------
[CW] ---- Iteration:   765 ----
[CW] collect: return: 825.78820, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 26.45802, qf2_loss: 25.94035, policy_loss: -646.79775, policy_entropy: -1.00669, alpha: 0.37608, time: 32.61588
[CW] ---------------------------
[CW] ---- Iteration:   766 ----
[CW] collect: return: 843.21718, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 27.27648, qf2_loss: 27.94990, policy_loss: -646.44994, policy_entropy: -0.98656, alpha: 0.37622, time: 33.01931
[CW] ---------------------------
[CW] ---- Iteration:   767 ----
[CW] collect: return: 836.11821, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 30.51136, qf2_loss: 29.94215, policy_loss: -649.73961, policy_entropy: -0.99807, alpha: 0.37521, time: 33.25085
[CW] ---------------------------
[CW] ---- Iteration:   768 ----
[CW] collect: return: 838.97336, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 26.24318, qf2_loss: 25.97174, policy_loss: -649.51985, policy_entropy: -0.98042, alpha: 0.37376, time: 33.02396
[CW] ---------------------------
[CW] ---- Iteration:   769 ----
[CW] collect: return: 840.23783, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 24.59603, qf2_loss: 24.56635, policy_loss: -650.65198, policy_entropy: -0.98767, alpha: 0.37163, time: 32.81177
[CW] ---------------------------
[CW] ---- Iteration:   770 ----
[CW] collect: return: 836.17477, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 23.91918, qf2_loss: 23.84823, policy_loss: -650.39785, policy_entropy: -1.00970, alpha: 0.37146, time: 32.80705
[CW] ---------------------------
[CW] ---- Iteration:   771 ----
[CW] collect: return: 836.25724, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 25.78673, qf2_loss: 25.71239, policy_loss: -649.79674, policy_entropy: -1.00849, alpha: 0.37301, time: 33.10172
[CW] ---------------------------
[CW] ---- Iteration:   772 ----
[CW] collect: return: 838.25814, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 22.03735, qf2_loss: 22.14479, policy_loss: -653.29216, policy_entropy: -0.98656, alpha: 0.37227, time: 32.74850
[CW] ---------------------------
[CW] ---- Iteration:   773 ----
[CW] collect: return: 835.92182, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 25.16969, qf2_loss: 25.05921, policy_loss: -650.87171, policy_entropy: -0.99924, alpha: 0.37110, time: 33.04929
[CW] ---------------------------
[CW] ---- Iteration:   774 ----
[CW] collect: return: 834.55964, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 26.70323, qf2_loss: 26.59924, policy_loss: -649.92021, policy_entropy: -1.00746, alpha: 0.37240, time: 32.72732
[CW] ---------------------------
[CW] ---- Iteration:   775 ----
[CW] collect: return: 845.45657, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 25.91200, qf2_loss: 26.30282, policy_loss: -648.12661, policy_entropy: -1.02475, alpha: 0.37366, time: 32.78308
[CW] ---------------------------
[CW] ---- Iteration:   776 ----
[CW] collect: return: 838.76823, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 25.84212, qf2_loss: 25.94718, policy_loss: -649.55667, policy_entropy: -0.99317, alpha: 0.37483, time: 33.10597
[CW] ---------------------------
[CW] ---- Iteration:   777 ----
[CW] collect: return: 839.52742, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 25.09334, qf2_loss: 24.91246, policy_loss: -649.54172, policy_entropy: -1.00235, alpha: 0.37460, time: 32.71552
[CW] ---------------------------
[CW] ---- Iteration:   778 ----
[CW] collect: return: 835.42608, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 26.99557, qf2_loss: 26.51907, policy_loss: -649.32841, policy_entropy: -0.99928, alpha: 0.37470, time: 32.96726
[CW] ---------------------------
[CW] ---- Iteration:   779 ----
[CW] collect: return: 844.15197, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 25.37542, qf2_loss: 25.45503, policy_loss: -654.69815, policy_entropy: -0.97116, alpha: 0.37345, time: 32.91756
[CW] ---------------------------
[CW] ---- Iteration:   780 ----
[CW] collect: return: 839.51928, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 24.70923, qf2_loss: 24.84954, policy_loss: -653.40974, policy_entropy: -0.98868, alpha: 0.37079, time: 32.66889
[CW] eval: return: 828.72859, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   781 ----
[CW] collect: return: 841.80291, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 27.81901, qf2_loss: 27.47523, policy_loss: -654.24727, policy_entropy: -0.97702, alpha: 0.36916, time: 32.98634
[CW] ---------------------------
[CW] ---- Iteration:   782 ----
[CW] collect: return: 839.75722, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 26.85094, qf2_loss: 26.88852, policy_loss: -651.25812, policy_entropy: -0.99818, alpha: 0.36706, time: 32.93039
[CW] ---------------------------
[CW] ---- Iteration:   783 ----
[CW] collect: return: 847.06846, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 26.61837, qf2_loss: 26.97591, policy_loss: -651.41327, policy_entropy: -0.99444, alpha: 0.36601, time: 32.81364
[CW] ---------------------------
[CW] ---- Iteration:   784 ----
[CW] collect: return: 839.66646, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 24.79300, qf2_loss: 24.58820, policy_loss: -649.14083, policy_entropy: -1.00886, alpha: 0.36651, time: 32.75291
[CW] ---------------------------
[CW] ---- Iteration:   785 ----
[CW] collect: return: 832.37219, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 28.17412, qf2_loss: 27.82703, policy_loss: -654.97271, policy_entropy: -0.97555, alpha: 0.36620, time: 33.14552
[CW] ---------------------------
[CW] ---- Iteration:   786 ----
[CW] collect: return: 837.99806, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 25.48636, qf2_loss: 25.26138, policy_loss: -656.02594, policy_entropy: -0.97291, alpha: 0.36355, time: 33.03152
[CW] ---------------------------
[CW] ---- Iteration:   787 ----
[CW] collect: return: 827.43294, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 28.39072, qf2_loss: 28.14459, policy_loss: -652.89741, policy_entropy: -1.00675, alpha: 0.36230, time: 33.04896
[CW] ---------------------------
[CW] ---- Iteration:   788 ----
[CW] collect: return: 812.49738, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 27.51347, qf2_loss: 27.62187, policy_loss: -655.41879, policy_entropy: -0.99495, alpha: 0.36177, time: 33.11759
[CW] ---------------------------
[CW] ---- Iteration:   789 ----
[CW] collect: return: 834.73056, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 30.31176, qf2_loss: 29.88866, policy_loss: -656.25716, policy_entropy: -0.99015, alpha: 0.36175, time: 33.03454
[CW] ---------------------------
[CW] ---- Iteration:   790 ----
[CW] collect: return: 831.16695, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 29.48892, qf2_loss: 29.97484, policy_loss: -652.51580, policy_entropy: -1.00654, alpha: 0.36109, time: 33.58583
[CW] ---------------------------
[CW] ---- Iteration:   791 ----
[CW] collect: return: 835.76446, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 24.84137, qf2_loss: 24.57555, policy_loss: -657.07001, policy_entropy: -0.97417, alpha: 0.36071, time: 32.77567
[CW] ---------------------------
[CW] ---- Iteration:   792 ----
[CW] collect: return: 839.23209, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 26.20090, qf2_loss: 26.09211, policy_loss: -654.39605, policy_entropy: -0.99461, alpha: 0.35935, time: 33.15447
[CW] ---------------------------
[CW] ---- Iteration:   793 ----
[CW] collect: return: 834.54457, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 25.82529, qf2_loss: 25.40359, policy_loss: -658.22118, policy_entropy: -0.96501, alpha: 0.35664, time: 32.79657
[CW] ---------------------------
[CW] ---- Iteration:   794 ----
[CW] collect: return: 839.01526, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 22.98072, qf2_loss: 23.30655, policy_loss: -656.72494, policy_entropy: -0.99260, alpha: 0.35416, time: 32.89868
[CW] ---------------------------
[CW] ---- Iteration:   795 ----
[CW] collect: return: 819.14097, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 24.84546, qf2_loss: 24.88760, policy_loss: -657.16559, policy_entropy: -0.99962, alpha: 0.35382, time: 32.97936
[CW] ---------------------------
[CW] ---- Iteration:   796 ----
[CW] collect: return: 823.64704, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 30.45329, qf2_loss: 30.69719, policy_loss: -658.85574, policy_entropy: -0.97983, alpha: 0.35212, time: 32.80752
[CW] ---------------------------
[CW] ---- Iteration:   797 ----
[CW] collect: return: 832.89393, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 26.06088, qf2_loss: 26.13790, policy_loss: -657.09440, policy_entropy: -1.00409, alpha: 0.35146, time: 32.85469
[CW] ---------------------------
[CW] ---- Iteration:   798 ----
[CW] collect: return: 815.33293, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 25.97773, qf2_loss: 25.44995, policy_loss: -657.30561, policy_entropy: -1.00193, alpha: 0.35178, time: 33.08885
[CW] ---------------------------
[CW] ---- Iteration:   799 ----
[CW] collect: return: 833.20235, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 24.55544, qf2_loss: 24.40753, policy_loss: -660.21587, policy_entropy: -0.98209, alpha: 0.35113, time: 33.21890
[CW] ---------------------------
[CW] ---- Iteration:   800 ----
[CW] collect: return: 744.43515, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 25.90547, qf2_loss: 25.66068, policy_loss: -657.29138, policy_entropy: -0.98303, alpha: 0.34998, time: 32.70518
[CW] eval: return: 812.07170, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   801 ----
[CW] collect: return: 829.31902, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 24.75594, qf2_loss: 24.89899, policy_loss: -659.00937, policy_entropy: -0.99799, alpha: 0.34861, time: 32.95524
[CW] ---------------------------
[CW] ---- Iteration:   802 ----
[CW] collect: return: 836.77947, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 23.97711, qf2_loss: 24.07898, policy_loss: -661.97567, policy_entropy: -0.97189, alpha: 0.34739, time: 32.93823
[CW] ---------------------------
[CW] ---- Iteration:   803 ----
[CW] collect: return: 832.20556, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 24.19459, qf2_loss: 23.69373, policy_loss: -662.19974, policy_entropy: -0.98534, alpha: 0.34472, time: 32.77529
[CW] ---------------------------
[CW] ---- Iteration:   804 ----
[CW] collect: return: 844.58136, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 25.04952, qf2_loss: 25.41623, policy_loss: -658.55942, policy_entropy: -0.99661, alpha: 0.34409, time: 32.83307
[CW] ---------------------------
[CW] ---- Iteration:   805 ----
[CW] collect: return: 832.86920, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 25.97957, qf2_loss: 25.59413, policy_loss: -660.39732, policy_entropy: -1.00100, alpha: 0.34384, time: 32.87292
[CW] ---------------------------
[CW] ---- Iteration:   806 ----
[CW] collect: return: 849.54541, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 30.60517, qf2_loss: 29.93320, policy_loss: -661.67562, policy_entropy: -0.98930, alpha: 0.34406, time: 33.05089
[CW] ---------------------------
[CW] ---- Iteration:   807 ----
[CW] collect: return: 750.77892, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 59.99814, qf2_loss: 59.69991, policy_loss: -657.64096, policy_entropy: -1.01839, alpha: 0.34334, time: 32.68640
[CW] ---------------------------
[CW] ---- Iteration:   808 ----
[CW] collect: return: 829.42959, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 93.08104, qf2_loss: 92.40917, policy_loss: -660.61815, policy_entropy: -0.98467, alpha: 0.34427, time: 33.21826
[CW] ---------------------------
[CW] ---- Iteration:   809 ----
[CW] collect: return: 834.46716, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 32.75402, qf2_loss: 33.12833, policy_loss: -661.01742, policy_entropy: -0.95967, alpha: 0.34138, time: 32.77269
[CW] ---------------------------
[CW] ---- Iteration:   810 ----
[CW] collect: return: 840.39486, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 25.95957, qf2_loss: 26.03971, policy_loss: -662.27850, policy_entropy: -1.00368, alpha: 0.33897, time: 32.64274
[CW] ---------------------------
[CW] ---- Iteration:   811 ----
[CW] collect: return: 828.83453, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 23.03938, qf2_loss: 22.99069, policy_loss: -663.32883, policy_entropy: -0.99011, alpha: 0.33890, time: 32.64128
[CW] ---------------------------
[CW] ---- Iteration:   812 ----
[CW] collect: return: 838.02231, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 23.29462, qf2_loss: 23.38428, policy_loss: -660.46739, policy_entropy: -1.01640, alpha: 0.33879, time: 32.98327
[CW] ---------------------------
[CW] ---- Iteration:   813 ----
[CW] collect: return: 838.93118, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 21.43786, qf2_loss: 21.21208, policy_loss: -662.37388, policy_entropy: -1.01206, alpha: 0.34035, time: 32.69324
[CW] ---------------------------
[CW] ---- Iteration:   814 ----
[CW] collect: return: 691.73304, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 25.25169, qf2_loss: 25.35551, policy_loss: -661.38093, policy_entropy: -1.01739, alpha: 0.34188, time: 32.63976
[CW] ---------------------------
[CW] ---- Iteration:   815 ----
[CW] collect: return: 838.87804, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 22.52020, qf2_loss: 22.70630, policy_loss: -662.86301, policy_entropy: -1.01675, alpha: 0.34374, time: 33.14030
[CW] ---------------------------
[CW] ---- Iteration:   816 ----
[CW] collect: return: 842.49440, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 22.17983, qf2_loss: 22.38348, policy_loss: -662.65175, policy_entropy: -1.01074, alpha: 0.34507, time: 32.80684
[CW] ---------------------------
[CW] ---- Iteration:   817 ----
[CW] collect: return: 834.27843, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 23.28505, qf2_loss: 23.03734, policy_loss: -663.97378, policy_entropy: -0.99624, alpha: 0.34579, time: 32.75681
[CW] ---------------------------
[CW] ---- Iteration:   818 ----
[CW] collect: return: 842.91763, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 25.69532, qf2_loss: 25.61392, policy_loss: -662.96935, policy_entropy: -1.01353, alpha: 0.34659, time: 32.75867
[CW] ---------------------------
[CW] ---- Iteration:   819 ----
[CW] collect: return: 832.39228, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 25.57915, qf2_loss: 25.69120, policy_loss: -663.63951, policy_entropy: -1.00096, alpha: 0.34644, time: 32.78768
[CW] ---------------------------
[CW] ---- Iteration:   820 ----
[CW] collect: return: 742.74082, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 30.41692, qf2_loss: 30.59742, policy_loss: -663.76624, policy_entropy: -0.98551, alpha: 0.34710, time: 33.00573
[CW] eval: return: 832.22368, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   821 ----
[CW] collect: return: 751.32179, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 25.66990, qf2_loss: 25.44108, policy_loss: -662.43677, policy_entropy: -1.00199, alpha: 0.34562, time: 32.58089
[CW] ---------------------------
[CW] ---- Iteration:   822 ----
[CW] collect: return: 842.62449, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 22.53810, qf2_loss: 22.60482, policy_loss: -663.26933, policy_entropy: -1.00229, alpha: 0.34542, time: 32.92378
[CW] ---------------------------
[CW] ---- Iteration:   823 ----
[CW] collect: return: 847.28733, steps: 1000.00000, total_steps: 829000.00000
[CW] train: qf1_loss: 23.66723, qf2_loss: 23.64773, policy_loss: -664.20739, policy_entropy: -0.98858, alpha: 0.34536, time: 32.70252
[CW] ---------------------------
[CW] ---- Iteration:   824 ----
[CW] collect: return: 833.87007, steps: 1000.00000, total_steps: 830000.00000
[CW] train: qf1_loss: 22.18812, qf2_loss: 21.91493, policy_loss: -666.95635, policy_entropy: -0.99595, alpha: 0.34433, time: 33.00532
[CW] ---------------------------
[CW] ---- Iteration:   825 ----
[CW] collect: return: 834.53735, steps: 1000.00000, total_steps: 831000.00000
[CW] train: qf1_loss: 21.15829, qf2_loss: 21.17104, policy_loss: -669.11925, policy_entropy: -0.98438, alpha: 0.34323, time: 32.85036
[CW] ---------------------------
[CW] ---- Iteration:   826 ----
[CW] collect: return: 817.43706, steps: 1000.00000, total_steps: 832000.00000
[CW] train: qf1_loss: 24.90498, qf2_loss: 24.76023, policy_loss: -664.41982, policy_entropy: -1.02267, alpha: 0.34385, time: 33.21583
[CW] ---------------------------
[CW] ---- Iteration:   827 ----
[CW] collect: return: 835.47270, steps: 1000.00000, total_steps: 833000.00000
[CW] train: qf1_loss: 23.61091, qf2_loss: 23.72451, policy_loss: -667.35401, policy_entropy: -0.99372, alpha: 0.34450, time: 32.79754
[CW] ---------------------------
[CW] ---- Iteration:   828 ----
[CW] collect: return: 838.57219, steps: 1000.00000, total_steps: 834000.00000
[CW] train: qf1_loss: 25.30584, qf2_loss: 24.97250, policy_loss: -667.22810, policy_entropy: -1.00156, alpha: 0.34416, time: 32.84022
[CW] ---------------------------
[CW] ---- Iteration:   829 ----
[CW] collect: return: 822.02880, steps: 1000.00000, total_steps: 835000.00000
[CW] train: qf1_loss: 22.69034, qf2_loss: 22.58784, policy_loss: -665.31703, policy_entropy: -1.01367, alpha: 0.34485, time: 32.74570
[CW] ---------------------------
[CW] ---- Iteration:   830 ----
[CW] collect: return: 833.68090, steps: 1000.00000, total_steps: 836000.00000
[CW] train: qf1_loss: 29.12540, qf2_loss: 28.90552, policy_loss: -665.20401, policy_entropy: -0.99435, alpha: 0.34575, time: 34.54170
[CW] ---------------------------
[CW] ---- Iteration:   831 ----
[CW] collect: return: 828.15256, steps: 1000.00000, total_steps: 837000.00000
[CW] train: qf1_loss: 44.73592, qf2_loss: 44.58940, policy_loss: -665.11069, policy_entropy: -0.99118, alpha: 0.34561, time: 33.04538
[CW] ---------------------------
[CW] ---- Iteration:   832 ----
[CW] collect: return: 822.59612, steps: 1000.00000, total_steps: 838000.00000
[CW] train: qf1_loss: 29.03895, qf2_loss: 29.13191, policy_loss: -670.23623, policy_entropy: -0.97264, alpha: 0.34295, time: 32.75528
[CW] ---------------------------
