{"collect/return": 822.5961154079996, "collect/steps": 1000.0, "collect/total_steps": 838000.0, "train/qf1_loss": 29.038953580856322, "train/qf2_loss": 29.131905479431154, "train/policy_loss": -670.23623046875, "train/policy_entropy": -0.9726355999708176, "train/alpha": 0.3429458236694336, "train/time": 32.75527596473694, "eval/return": 832.2236770869553, "eval/steps": 1000.0, "_timestamp": 1678385787.2133052, "_runtime": 28684.8779733181, "_step": 832}