[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 17.97173, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.73135, qf2_loss: 1.74811, policy_loss: -7.83290, policy_entropy: 4.09711, alpha: 0.98504, time: 50.37082
[CW] eval: return: 14.77247, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 19.28851, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.09267, qf2_loss: 0.09238, policy_loss: -8.51010, policy_entropy: 4.10092, alpha: 0.95626, time: 50.74918
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 16.85441, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.08581, qf2_loss: 0.08570, policy_loss: -9.20256, policy_entropy: 4.10110, alpha: 0.92871, time: 50.51524
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 10.78886, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.07816, qf2_loss: 0.07805, policy_loss: -10.14147, policy_entropy: 4.10180, alpha: 0.90231, time: 50.60783
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 15.70370, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.07053, qf2_loss: 0.07050, policy_loss: -11.20783, policy_entropy: 4.10071, alpha: 0.87698, time: 50.59212
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 9.27567, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06552, qf2_loss: 0.06568, policy_loss: -12.33151, policy_entropy: 4.10061, alpha: 0.85267, time: 50.51297
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 8.13447, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.06772, qf2_loss: 0.06831, policy_loss: -13.49846, policy_entropy: 4.10019, alpha: 0.82930, time: 50.68208
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 11.51960, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.08452, qf2_loss: 0.08557, policy_loss: -14.69834, policy_entropy: 4.10160, alpha: 0.80683, time: 50.79875
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 14.34584, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.07706, qf2_loss: 0.07761, policy_loss: -15.91232, policy_entropy: 4.10168, alpha: 0.78519, time: 50.70693
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 25.45391, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.10058, qf2_loss: 0.10163, policy_loss: -17.12820, policy_entropy: 4.10075, alpha: 0.76435, time: 50.85031
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 13.47577, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.06543, qf2_loss: 0.06573, policy_loss: -18.32605, policy_entropy: 4.10034, alpha: 0.74426, time: 50.63222
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 15.81573, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.09070, qf2_loss: 0.09154, policy_loss: -19.49781, policy_entropy: 4.10022, alpha: 0.72488, time: 50.62940
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 11.25501, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.09440, qf2_loss: 0.09527, policy_loss: -20.64830, policy_entropy: 4.10091, alpha: 0.70617, time: 50.72843
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 18.98893, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.09568, qf2_loss: 0.09655, policy_loss: -21.77648, policy_entropy: 4.10038, alpha: 0.68809, time: 50.54464
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 13.64309, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.09813, qf2_loss: 0.09900, policy_loss: -22.86856, policy_entropy: 4.09934, alpha: 0.67062, time: 50.52541
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 7.28106, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.09606, qf2_loss: 0.09689, policy_loss: -23.93513, policy_entropy: 4.09992, alpha: 0.65372, time: 50.79912
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 14.77219, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.11038, qf2_loss: 0.11147, policy_loss: -24.96667, policy_entropy: 4.09880, alpha: 0.63737, time: 50.63052
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 12.20443, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.10216, qf2_loss: 0.10311, policy_loss: -25.97607, policy_entropy: 4.09706, alpha: 0.62154, time: 50.73748
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 19.36201, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.10155, qf2_loss: 0.10236, policy_loss: -26.96965, policy_entropy: 4.09636, alpha: 0.60620, time: 50.80118
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 15.14680, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.13009, qf2_loss: 0.13156, policy_loss: -27.91770, policy_entropy: 4.09787, alpha: 0.59134, time: 50.73927
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 14.70461, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.11593, qf2_loss: 0.11705, policy_loss: -28.85374, policy_entropy: 4.09548, alpha: 0.57692, time: 50.84169
[CW] eval: return: 11.58116, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 23.11610, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.10589, qf2_loss: 0.10676, policy_loss: -29.76931, policy_entropy: 4.09649, alpha: 0.56294, time: 50.76689
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 15.70818, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.13876, qf2_loss: 0.13960, policy_loss: -30.64239, policy_entropy: 4.09432, alpha: 0.54936, time: 50.87121
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 25.56305, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.12830, qf2_loss: 0.12942, policy_loss: -31.50365, policy_entropy: 4.09212, alpha: 0.53619, time: 50.90698
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 12.78981, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.12755, qf2_loss: 0.12884, policy_loss: -32.33423, policy_entropy: 4.09218, alpha: 0.52339, time: 50.82262
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 10.35830, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.12238, qf2_loss: 0.12335, policy_loss: -33.12619, policy_entropy: 4.09077, alpha: 0.51095, time: 50.86725
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 15.14559, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.18914, qf2_loss: 0.19153, policy_loss: -33.89645, policy_entropy: 4.09024, alpha: 0.49886, time: 50.84593
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 15.00815, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.11443, qf2_loss: 0.11521, policy_loss: -34.65436, policy_entropy: 4.08899, alpha: 0.48711, time: 50.77400
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 17.11718, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.12342, qf2_loss: 0.12444, policy_loss: -35.39839, policy_entropy: 4.08624, alpha: 0.47568, time: 51.01171
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 14.94713, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.14167, qf2_loss: 0.14305, policy_loss: -36.10745, policy_entropy: 4.08475, alpha: 0.46456, time: 50.87748
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 11.17720, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.15501, qf2_loss: 0.15669, policy_loss: -36.78647, policy_entropy: 4.08277, alpha: 0.45373, time: 50.95570
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 16.36938, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.14828, qf2_loss: 0.14961, policy_loss: -37.44933, policy_entropy: 4.08263, alpha: 0.44320, time: 51.05783
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 22.30802, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.14621, qf2_loss: 0.14762, policy_loss: -38.08493, policy_entropy: 4.08037, alpha: 0.43294, time: 50.96310
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 21.34735, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.17013, qf2_loss: 0.17194, policy_loss: -38.73872, policy_entropy: 4.07602, alpha: 0.42295, time: 51.30116
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 18.78266, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.14360, qf2_loss: 0.14449, policy_loss: -39.33294, policy_entropy: 4.07693, alpha: 0.41321, time: 51.08473
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 19.90126, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.15162, qf2_loss: 0.15246, policy_loss: -39.92321, policy_entropy: 4.07728, alpha: 0.40373, time: 51.19084
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 24.73640, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.17047, qf2_loss: 0.17151, policy_loss: -40.47568, policy_entropy: 4.07463, alpha: 0.39448, time: 50.97044
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 5.48623, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.14937, qf2_loss: 0.14979, policy_loss: -41.01143, policy_entropy: 4.07584, alpha: 0.38547, time: 51.09747
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 20.44589, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.14121, qf2_loss: 0.14120, policy_loss: -41.53880, policy_entropy: 4.07904, alpha: 0.37668, time: 51.08372
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 17.06473, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.20638, qf2_loss: 0.20818, policy_loss: -42.03786, policy_entropy: 4.07790, alpha: 0.36810, time: 51.46364
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 20.34192, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.12093, qf2_loss: 0.12049, policy_loss: -42.52954, policy_entropy: 4.07656, alpha: 0.35974, time: 51.43800
[CW] eval: return: 17.84113, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 13.73385, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.15034, qf2_loss: 0.15048, policy_loss: -42.99678, policy_entropy: 4.07572, alpha: 0.35158, time: 51.03732
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 16.75523, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.16981, qf2_loss: 0.17054, policy_loss: -43.46418, policy_entropy: 4.07461, alpha: 0.34362, time: 51.01607
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 40.17275, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.18098, qf2_loss: 0.18204, policy_loss: -43.91678, policy_entropy: 4.07175, alpha: 0.33585, time: 50.93273
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 27.90578, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.15594, qf2_loss: 0.15635, policy_loss: -44.34149, policy_entropy: 4.06841, alpha: 0.32828, time: 50.56984
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 19.02006, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.15682, qf2_loss: 0.15713, policy_loss: -44.75864, policy_entropy: 4.06640, alpha: 0.32088, time: 50.66776
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 16.31911, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.17643, qf2_loss: 0.17717, policy_loss: -45.14695, policy_entropy: 4.06536, alpha: 0.31366, time: 51.69077
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 15.57849, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.14651, qf2_loss: 0.14691, policy_loss: -45.52920, policy_entropy: 4.06301, alpha: 0.30661, time: 50.64757
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 24.24334, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.17214, qf2_loss: 0.17292, policy_loss: -45.90517, policy_entropy: 4.06060, alpha: 0.29973, time: 50.81615
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 20.36179, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.17849, qf2_loss: 0.17934, policy_loss: -46.24992, policy_entropy: 4.05717, alpha: 0.29302, time: 50.56057
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 14.59414, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.18330, qf2_loss: 0.18425, policy_loss: -46.59616, policy_entropy: 4.05379, alpha: 0.28646, time: 50.79019
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 18.15138, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.16686, qf2_loss: 0.16718, policy_loss: -46.91825, policy_entropy: 4.04885, alpha: 0.28005, time: 50.69361
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 12.01801, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.17731, qf2_loss: 0.17813, policy_loss: -47.22167, policy_entropy: 4.04801, alpha: 0.27380, time: 50.61971
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 18.72325, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.18506, qf2_loss: 0.18640, policy_loss: -47.50289, policy_entropy: 4.04455, alpha: 0.26768, time: 55.48203
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 17.27804, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.16152, qf2_loss: 0.16235, policy_loss: -47.79865, policy_entropy: 4.03986, alpha: 0.26172, time: 50.75906
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 36.71271, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.22431, qf2_loss: 0.22546, policy_loss: -48.08282, policy_entropy: 4.03704, alpha: 0.25589, time: 50.61023
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 18.88287, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.18599, qf2_loss: 0.18708, policy_loss: -48.36289, policy_entropy: 4.02775, alpha: 0.25019, time: 50.63643
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 11.72990, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.18601, qf2_loss: 0.18756, policy_loss: -48.63307, policy_entropy: 4.01623, alpha: 0.24463, time: 50.57768
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 32.86383, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.17889, qf2_loss: 0.18061, policy_loss: -48.86431, policy_entropy: 4.00797, alpha: 0.23920, time: 50.50807
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 16.06633, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.23671, qf2_loss: 0.23932, policy_loss: -49.09298, policy_entropy: 4.00329, alpha: 0.23390, time: 50.63163
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 18.73041, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.20041, qf2_loss: 0.20268, policy_loss: -49.32435, policy_entropy: 3.99592, alpha: 0.22872, time: 50.60920
[CW] eval: return: 27.72563, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 17.99633, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.19901, qf2_loss: 0.20126, policy_loss: -49.53374, policy_entropy: 3.98316, alpha: 0.22365, time: 50.74417
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 29.09954, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.23972, qf2_loss: 0.24209, policy_loss: -49.74997, policy_entropy: 3.97476, alpha: 0.21871, time: 50.48207
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 15.24086, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.17106, qf2_loss: 0.17397, policy_loss: -49.90706, policy_entropy: 3.97127, alpha: 0.21387, time: 50.59807
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 38.04523, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.25547, qf2_loss: 0.26043, policy_loss: -50.12599, policy_entropy: 3.95357, alpha: 0.20915, time: 50.44715
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 17.40195, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.19689, qf2_loss: 0.20123, policy_loss: -50.31277, policy_entropy: 3.92991, alpha: 0.20454, time: 50.36926
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 22.01889, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.23019, qf2_loss: 0.23568, policy_loss: -50.47636, policy_entropy: 3.91513, alpha: 0.20004, time: 50.64957
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 22.07497, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.21390, qf2_loss: 0.22142, policy_loss: -50.60702, policy_entropy: 3.91408, alpha: 0.19564, time: 50.51049
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 21.31569, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.21809, qf2_loss: 0.22516, policy_loss: -50.76167, policy_entropy: 3.88498, alpha: 0.19134, time: 50.61843
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 25.86895, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.24307, qf2_loss: 0.25229, policy_loss: -50.88554, policy_entropy: 3.85786, alpha: 0.18714, time: 50.69891
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 55.68744, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.23631, qf2_loss: 0.24528, policy_loss: -51.06513, policy_entropy: 3.79578, alpha: 0.18305, time: 50.51312
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 58.72580, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.26457, qf2_loss: 0.27291, policy_loss: -51.21999, policy_entropy: 3.72062, alpha: 0.17908, time: 50.45616
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 32.39184, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.33679, qf2_loss: 0.34343, policy_loss: -51.31996, policy_entropy: 3.63684, alpha: 0.17521, time: 50.70152
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 42.96552, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.32924, qf2_loss: 0.33629, policy_loss: -51.51170, policy_entropy: 3.50109, alpha: 0.17146, time: 50.64376
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 74.53382, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.30212, qf2_loss: 0.30957, policy_loss: -51.65370, policy_entropy: 3.33913, alpha: 0.16783, time: 50.65864
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 74.86871, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.40190, qf2_loss: 0.41143, policy_loss: -51.82953, policy_entropy: 3.07498, alpha: 0.16434, time: 50.52999
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 75.59420, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.30677, qf2_loss: 0.31198, policy_loss: -52.03795, policy_entropy: 2.76891, alpha: 0.16102, time: 50.44102
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 81.51053, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.33167, qf2_loss: 0.33624, policy_loss: -52.27062, policy_entropy: 2.40348, alpha: 0.15785, time: 50.53681
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 110.15104, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.36437, qf2_loss: 0.36985, policy_loss: -52.54931, policy_entropy: 2.11603, alpha: 0.15484, time: 50.46130
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 92.78963, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.39149, qf2_loss: 0.39657, policy_loss: -52.86497, policy_entropy: 1.74104, alpha: 0.15196, time: 52.60260
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 79.87472, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.40056, qf2_loss: 0.40664, policy_loss: -53.21550, policy_entropy: 1.28115, alpha: 0.14925, time: 50.56731
[CW] eval: return: 81.73576, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 121.00265, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.35813, qf2_loss: 0.36348, policy_loss: -53.62222, policy_entropy: 0.99228, alpha: 0.14670, time: 50.50401
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 73.89196, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.47374, qf2_loss: 0.47649, policy_loss: -54.01909, policy_entropy: 0.74892, alpha: 0.14423, time: 50.53036
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 53.97528, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.48363, qf2_loss: 0.49016, policy_loss: -54.44149, policy_entropy: 0.72895, alpha: 0.14183, time: 50.67458
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 121.97525, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.38752, qf2_loss: 0.39281, policy_loss: -54.81347, policy_entropy: 0.73029, alpha: 0.13941, time: 50.50034
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 68.85545, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.55321, qf2_loss: 0.56105, policy_loss: -55.13044, policy_entropy: 0.92297, alpha: 0.13698, time: 50.50162
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 116.99758, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.40593, qf2_loss: 0.41032, policy_loss: -55.50042, policy_entropy: 0.86153, alpha: 0.13451, time: 50.76382
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 89.14877, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.48377, qf2_loss: 0.48810, policy_loss: -55.80414, policy_entropy: 0.73272, alpha: 0.13210, time: 50.51865
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 37.70253, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.49725, qf2_loss: 0.50253, policy_loss: -56.04448, policy_entropy: 0.76482, alpha: 0.12971, time: 50.47402
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 94.02014, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.47387, qf2_loss: 0.47770, policy_loss: -56.33940, policy_entropy: 0.75315, alpha: 0.12734, time: 50.75962
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 59.32656, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.42964, qf2_loss: 0.43478, policy_loss: -56.62590, policy_entropy: 0.58082, alpha: 0.12500, time: 50.41481
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 37.70987, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.59911, qf2_loss: 0.60110, policy_loss: -56.89404, policy_entropy: 0.52424, alpha: 0.12274, time: 50.55231
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 41.77165, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.47084, qf2_loss: 0.47314, policy_loss: -57.12822, policy_entropy: 0.61207, alpha: 0.12047, time: 50.64373
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 82.68095, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.41659, qf2_loss: 0.41759, policy_loss: -57.34383, policy_entropy: 0.65122, alpha: 0.11819, time: 50.61720
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 48.80813, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.43064, qf2_loss: 0.43403, policy_loss: -57.55681, policy_entropy: 0.77376, alpha: 0.11592, time: 50.32767
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 47.31158, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.38960, qf2_loss: 0.39088, policy_loss: -57.72874, policy_entropy: 0.77131, alpha: 0.11364, time: 50.09029
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 58.82485, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.41071, qf2_loss: 0.41397, policy_loss: -57.90237, policy_entropy: 0.82972, alpha: 0.11139, time: 50.24133
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 117.63673, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.41418, qf2_loss: 0.41638, policy_loss: -58.04172, policy_entropy: 0.96299, alpha: 0.10913, time: 50.25470
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 117.45773, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.36958, qf2_loss: 0.37195, policy_loss: -58.21950, policy_entropy: 1.04105, alpha: 0.10687, time: 50.17037
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 58.61856, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.38889, qf2_loss: 0.39233, policy_loss: -58.28038, policy_entropy: 1.14574, alpha: 0.10463, time: 50.28729
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 115.98238, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.38213, qf2_loss: 0.38404, policy_loss: -58.36431, policy_entropy: 1.22696, alpha: 0.10239, time: 50.20039
[CW] eval: return: 96.87287, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 86.39324, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.32998, qf2_loss: 0.33072, policy_loss: -58.42256, policy_entropy: 1.32132, alpha: 0.10016, time: 50.36890
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 84.55290, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.39265, qf2_loss: 0.39675, policy_loss: -58.49163, policy_entropy: 1.41489, alpha: 0.09796, time: 50.57327
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 85.09294, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.32007, qf2_loss: 0.31986, policy_loss: -58.56038, policy_entropy: 1.44308, alpha: 0.09579, time: 50.34869
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 80.25004, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.31435, qf2_loss: 0.31416, policy_loss: -58.60974, policy_entropy: 1.50948, alpha: 0.09365, time: 50.58301
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 76.21845, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.31544, qf2_loss: 0.31654, policy_loss: -58.62620, policy_entropy: 1.51866, alpha: 0.09155, time: 50.47508
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 60.81254, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.36035, qf2_loss: 0.36139, policy_loss: -58.69874, policy_entropy: 1.37478, alpha: 0.08952, time: 50.32271
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 94.71655, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.43947, qf2_loss: 0.44010, policy_loss: -58.69809, policy_entropy: 1.46749, alpha: 0.08753, time: 50.42132
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 100.62994, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.30868, qf2_loss: 0.30835, policy_loss: -58.79571, policy_entropy: 1.19935, alpha: 0.08562, time: 50.43026
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 100.23617, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.31484, qf2_loss: 0.31534, policy_loss: -58.84440, policy_entropy: 1.01112, alpha: 0.08380, time: 50.41764
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 77.43184, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.37881, qf2_loss: 0.37888, policy_loss: -58.96411, policy_entropy: 0.89799, alpha: 0.08205, time: 50.93025
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 124.27555, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.33025, qf2_loss: 0.33073, policy_loss: -59.03521, policy_entropy: 0.71975, alpha: 0.08037, time: 50.61491
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 45.56198, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.41747, qf2_loss: 0.41840, policy_loss: -59.07850, policy_entropy: 0.65259, alpha: 0.07873, time: 50.78023
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 54.25044, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.40245, qf2_loss: 0.40228, policy_loss: -59.09930, policy_entropy: 0.51695, alpha: 0.07715, time: 50.76844
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 46.67771, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.38344, qf2_loss: 0.38711, policy_loss: -59.19236, policy_entropy: 0.34319, alpha: 0.07562, time: 50.49353
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 48.20204, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.34025, qf2_loss: 0.34213, policy_loss: -59.24012, policy_entropy: 0.20437, alpha: 0.07414, time: 50.54604
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 127.30475, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.37738, qf2_loss: 0.38009, policy_loss: -59.42064, policy_entropy: -0.01148, alpha: 0.07272, time: 50.27214
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 26.99220, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.35022, qf2_loss: 0.35129, policy_loss: -59.45813, policy_entropy: -0.18581, alpha: 0.07136, time: 50.55289
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 59.08291, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.49367, qf2_loss: 0.49678, policy_loss: -59.48997, policy_entropy: -0.19467, alpha: 0.07004, time: 50.30816
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 29.94671, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.39189, qf2_loss: 0.39581, policy_loss: -59.58246, policy_entropy: -0.33108, alpha: 0.06873, time: 50.41569
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 32.48945, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.34036, qf2_loss: 0.34487, policy_loss: -59.67536, policy_entropy: -0.53843, alpha: 0.06747, time: 52.38399
[CW] eval: return: 55.93981, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 72.85606, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.44243, qf2_loss: 0.44481, policy_loss: -59.78402, policy_entropy: -0.59995, alpha: 0.06625, time: 50.69427
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 72.88071, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.36827, qf2_loss: 0.37389, policy_loss: -59.89754, policy_entropy: -0.66499, alpha: 0.06505, time: 50.28434
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 55.61561, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.41358, qf2_loss: 0.41732, policy_loss: -60.10541, policy_entropy: -0.81529, alpha: 0.06389, time: 50.24314
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 57.64816, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.38069, qf2_loss: 0.38617, policy_loss: -60.18124, policy_entropy: -0.91289, alpha: 0.06274, time: 50.19055
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 84.64214, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.46989, qf2_loss: 0.47301, policy_loss: -60.22882, policy_entropy: -0.87090, alpha: 0.06162, time: 50.45473
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 77.89632, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.40360, qf2_loss: 0.40908, policy_loss: -60.38697, policy_entropy: -1.05009, alpha: 0.06052, time: 50.42648
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 90.94344, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.41100, qf2_loss: 0.41412, policy_loss: -60.47756, policy_entropy: -1.08097, alpha: 0.05944, time: 50.26607
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 29.19140, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.38093, qf2_loss: 0.38503, policy_loss: -60.61133, policy_entropy: -1.10566, alpha: 0.05836, time: 50.28940
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 37.90864, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.48380, qf2_loss: 0.48876, policy_loss: -60.71706, policy_entropy: -1.18397, alpha: 0.05731, time: 50.25437
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 39.44754, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.39009, qf2_loss: 0.39391, policy_loss: -60.97347, policy_entropy: -1.20253, alpha: 0.05628, time: 50.35630
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 53.26458, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.42010, qf2_loss: 0.42185, policy_loss: -60.84473, policy_entropy: -1.03445, alpha: 0.05523, time: 50.36262
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 50.20814, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.42191, qf2_loss: 0.42534, policy_loss: -60.98783, policy_entropy: -1.05583, alpha: 0.05418, time: 50.18128
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 77.64037, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.39130, qf2_loss: 0.39395, policy_loss: -61.05130, policy_entropy: -1.09879, alpha: 0.05314, time: 50.15559
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 139.45941, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.51544, qf2_loss: 0.51645, policy_loss: -61.20835, policy_entropy: -1.16785, alpha: 0.05213, time: 50.16493
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 81.32279, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.39806, qf2_loss: 0.39876, policy_loss: -61.30828, policy_entropy: -1.17714, alpha: 0.05113, time: 50.55508
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 117.81714, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.50173, qf2_loss: 0.50328, policy_loss: -61.35525, policy_entropy: -1.11424, alpha: 0.05014, time: 50.53359
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 191.89781, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.44172, qf2_loss: 0.44560, policy_loss: -61.46769, policy_entropy: -1.11020, alpha: 0.04915, time: 50.35018
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 115.95162, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.44103, qf2_loss: 0.44696, policy_loss: -61.79966, policy_entropy: -1.21402, alpha: 0.04818, time: 50.39983
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 83.69017, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.44125, qf2_loss: 0.44347, policy_loss: -61.61586, policy_entropy: -1.12499, alpha: 0.04723, time: 50.29840
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 107.55280, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.39776, qf2_loss: 0.40141, policy_loss: -61.77495, policy_entropy: -1.22083, alpha: 0.04628, time: 50.35038
[CW] eval: return: 111.46175, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 162.27819, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.73324, qf2_loss: 0.73430, policy_loss: -61.90859, policy_entropy: -1.14381, alpha: 0.04536, time: 50.33724
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 98.78749, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.37102, qf2_loss: 0.37397, policy_loss: -62.01055, policy_entropy: -1.18084, alpha: 0.04444, time: 50.81481
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 52.88972, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.38420, qf2_loss: 0.38938, policy_loss: -62.01768, policy_entropy: -1.11955, alpha: 0.04353, time: 50.81461
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 79.47951, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.45330, qf2_loss: 0.45510, policy_loss: -62.04709, policy_entropy: -1.16294, alpha: 0.04264, time: 50.89434
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 135.26645, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.44718, qf2_loss: 0.45142, policy_loss: -62.16059, policy_entropy: -1.14610, alpha: 0.04175, time: 50.66796
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 39.48507, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.42062, qf2_loss: 0.42750, policy_loss: -62.00785, policy_entropy: -1.09487, alpha: 0.04089, time: 50.60676
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 92.89724, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.38354, qf2_loss: 0.38822, policy_loss: -61.92997, policy_entropy: -1.07001, alpha: 0.04003, time: 50.60807
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 164.78774, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.40833, qf2_loss: 0.41200, policy_loss: -62.19088, policy_entropy: -1.22651, alpha: 0.03919, time: 50.79329
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 105.93569, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.47021, qf2_loss: 0.47320, policy_loss: -62.20771, policy_entropy: -1.30823, alpha: 0.03838, time: 50.36626
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 200.21543, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.50723, qf2_loss: 0.51269, policy_loss: -62.27550, policy_entropy: -1.39381, alpha: 0.03761, time: 50.37400
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 164.91870, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.45842, qf2_loss: 0.45799, policy_loss: -62.29042, policy_entropy: -1.34610, alpha: 0.03684, time: 50.28123
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 84.97529, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.59771, qf2_loss: 0.60547, policy_loss: -62.37307, policy_entropy: -1.46686, alpha: 0.03609, time: 50.74334
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 94.08676, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.44969, qf2_loss: 0.45473, policy_loss: -62.39752, policy_entropy: -1.53539, alpha: 0.03537, time: 51.20141
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 124.00358, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.40023, qf2_loss: 0.40314, policy_loss: -62.25977, policy_entropy: -1.58354, alpha: 0.03467, time: 52.06599
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 164.95746, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.41103, qf2_loss: 0.41685, policy_loss: -62.36950, policy_entropy: -1.76507, alpha: 0.03399, time: 50.51295
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 123.52950, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.63978, qf2_loss: 0.64527, policy_loss: -62.40374, policy_entropy: -1.84679, alpha: 0.03334, time: 50.62898
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 132.30640, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.42552, qf2_loss: 0.43003, policy_loss: -62.62775, policy_entropy: -2.00462, alpha: 0.03271, time: 50.58257
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 117.76177, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.44686, qf2_loss: 0.45469, policy_loss: -62.64364, policy_entropy: -2.02610, alpha: 0.03211, time: 50.35348
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 149.33298, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.70747, qf2_loss: 0.71794, policy_loss: -62.66730, policy_entropy: -2.13912, alpha: 0.03151, time: 50.49828
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 46.68381, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.43614, qf2_loss: 0.44245, policy_loss: -62.55755, policy_entropy: -2.17243, alpha: 0.03093, time: 50.13497
[CW] eval: return: 151.31083, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 30.70008, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.48819, qf2_loss: 0.49781, policy_loss: -62.62748, policy_entropy: -2.39788, alpha: 0.03037, time: 50.44157
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 169.71136, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.48382, qf2_loss: 0.49160, policy_loss: -62.65088, policy_entropy: -2.39024, alpha: 0.02984, time: 50.06042
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 230.24845, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.50453, qf2_loss: 0.51032, policy_loss: -62.75253, policy_entropy: -2.49943, alpha: 0.02931, time: 53.09017
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 30.84339, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.73242, qf2_loss: 0.74328, policy_loss: -62.81038, policy_entropy: -2.70898, alpha: 0.02881, time: 50.30392
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 64.46345, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.46523, qf2_loss: 0.47306, policy_loss: -62.89151, policy_entropy: -2.82359, alpha: 0.02832, time: 50.84655
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 186.45832, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.50602, qf2_loss: 0.51329, policy_loss: -62.59886, policy_entropy: -2.88166, alpha: 0.02786, time: 50.72730
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 147.31914, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.51986, qf2_loss: 0.53205, policy_loss: -63.01822, policy_entropy: -3.10138, alpha: 0.02741, time: 50.84512
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 97.04845, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.74624, qf2_loss: 0.75391, policy_loss: -63.03991, policy_entropy: -3.16454, alpha: 0.02698, time: 50.92400
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 136.10171, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.57392, qf2_loss: 0.58201, policy_loss: -63.09614, policy_entropy: -3.13354, alpha: 0.02655, time: 51.30671
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 62.73752, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.58523, qf2_loss: 0.59498, policy_loss: -62.83882, policy_entropy: -3.27940, alpha: 0.02613, time: 51.03955
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 126.40375, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.56138, qf2_loss: 0.57216, policy_loss: -63.12343, policy_entropy: -3.47990, alpha: 0.02574, time: 50.96104
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 55.24962, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.52782, qf2_loss: 0.53860, policy_loss: -63.46422, policy_entropy: -3.51507, alpha: 0.02535, time: 51.24349
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 76.13193, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.54238, qf2_loss: 0.55657, policy_loss: -63.36890, policy_entropy: -3.53724, alpha: 0.02498, time: 51.12032
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 100.80352, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.57613, qf2_loss: 0.58961, policy_loss: -63.34058, policy_entropy: -3.54100, alpha: 0.02460, time: 51.32612
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 154.09798, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.59894, qf2_loss: 0.61198, policy_loss: -63.12870, policy_entropy: -3.66435, alpha: 0.02423, time: 51.13049
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 87.92211, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.57018, qf2_loss: 0.58094, policy_loss: -63.52563, policy_entropy: -3.92200, alpha: 0.02387, time: 51.11369
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 217.75736, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.69008, qf2_loss: 0.70619, policy_loss: -63.51063, policy_entropy: -3.85765, alpha: 0.02354, time: 50.81942
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 74.67131, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.70273, qf2_loss: 0.71629, policy_loss: -63.85636, policy_entropy: -4.10070, alpha: 0.02322, time: 50.91359
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 44.28278, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.61990, qf2_loss: 0.63788, policy_loss: -63.69042, policy_entropy: -4.11488, alpha: 0.02291, time: 51.05545
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 145.33399, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.55895, qf2_loss: 0.57295, policy_loss: -63.50655, policy_entropy: -4.15140, alpha: 0.02260, time: 51.66252
[CW] eval: return: 117.79387, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 219.31755, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.65281, qf2_loss: 0.66547, policy_loss: -63.72953, policy_entropy: -4.22953, alpha: 0.02230, time: 51.27434
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 54.04937, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.67110, qf2_loss: 0.69077, policy_loss: -63.79104, policy_entropy: -4.07626, alpha: 0.02200, time: 51.08778
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 98.42229, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.56497, qf2_loss: 0.57360, policy_loss: -63.76025, policy_entropy: -4.19912, alpha: 0.02168, time: 51.09174
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 221.20196, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.60333, qf2_loss: 0.61846, policy_loss: -63.92738, policy_entropy: -4.15904, alpha: 0.02137, time: 51.10919
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 172.46578, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.75814, qf2_loss: 0.77716, policy_loss: -63.89379, policy_entropy: -4.04036, alpha: 0.02104, time: 51.04962
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 65.07958, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.66546, qf2_loss: 0.68436, policy_loss: -64.40281, policy_entropy: -4.21885, alpha: 0.02071, time: 51.23911
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 160.36345, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.62509, qf2_loss: 0.63526, policy_loss: -64.07383, policy_entropy: -4.41047, alpha: 0.02042, time: 51.19200
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 129.61889, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.83824, qf2_loss: 0.85104, policy_loss: -64.15902, policy_entropy: -4.47495, alpha: 0.02014, time: 50.90144
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 274.09158, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.68852, qf2_loss: 0.70627, policy_loss: -64.52105, policy_entropy: -4.60199, alpha: 0.01988, time: 50.93765
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 211.73455, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.69439, qf2_loss: 0.70933, policy_loss: -64.74658, policy_entropy: -4.94015, alpha: 0.01965, time: 51.08443
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 163.83790, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.66902, qf2_loss: 0.68741, policy_loss: -64.50186, policy_entropy: -4.99642, alpha: 0.01946, time: 51.13111
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 199.35040, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.69783, qf2_loss: 0.71046, policy_loss: -65.07777, policy_entropy: -5.34832, alpha: 0.01929, time: 51.58290
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 212.61751, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.67008, qf2_loss: 0.68445, policy_loss: -65.16049, policy_entropy: -5.54444, alpha: 0.01919, time: 53.12427
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 119.26856, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 1.12456, qf2_loss: 1.14855, policy_loss: -64.70777, policy_entropy: -5.21832, alpha: 0.01906, time: 51.36181
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 206.34096, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.91114, qf2_loss: 0.93671, policy_loss: -65.17773, policy_entropy: -5.24623, alpha: 0.01892, time: 51.35806
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 174.62314, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.70863, qf2_loss: 0.72212, policy_loss: -65.59067, policy_entropy: -5.42198, alpha: 0.01875, time: 51.26995
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 241.33213, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.62852, qf2_loss: 0.64459, policy_loss: -65.93322, policy_entropy: -5.53305, alpha: 0.01863, time: 51.20478
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 45.84701, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.66242, qf2_loss: 0.67748, policy_loss: -65.98107, policy_entropy: -5.41831, alpha: 0.01853, time: 51.23636
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 279.57105, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.74339, qf2_loss: 0.76116, policy_loss: -65.81427, policy_entropy: -5.08405, alpha: 0.01835, time: 51.25978
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 164.64431, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.80750, qf2_loss: 0.82422, policy_loss: -65.57846, policy_entropy: -4.84492, alpha: 0.01809, time: 51.28093
[CW] eval: return: 187.30782, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 86.22525, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.75487, qf2_loss: 0.77405, policy_loss: -66.04344, policy_entropy: -5.17129, alpha: 0.01785, time: 50.99241
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 245.25083, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.96770, qf2_loss: 0.99222, policy_loss: -66.13774, policy_entropy: -5.16007, alpha: 0.01764, time: 51.02835
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 213.90757, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.78353, qf2_loss: 0.80140, policy_loss: -66.66663, policy_entropy: -5.38224, alpha: 0.01745, time: 50.97822
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 223.64589, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.81840, qf2_loss: 0.83462, policy_loss: -66.36741, policy_entropy: -5.34416, alpha: 0.01729, time: 51.27177
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 198.47930, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.80614, qf2_loss: 0.82042, policy_loss: -66.57783, policy_entropy: -5.34322, alpha: 0.01712, time: 50.91586
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 93.21991, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.84353, qf2_loss: 0.86066, policy_loss: -66.39859, policy_entropy: -5.24192, alpha: 0.01692, time: 50.95702
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 193.53330, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 1.10660, qf2_loss: 1.11353, policy_loss: -67.30714, policy_entropy: -5.55647, alpha: 0.01676, time: 50.97673
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 272.52900, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.81517, qf2_loss: 0.83369, policy_loss: -66.97682, policy_entropy: -5.28259, alpha: 0.01659, time: 50.96742
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 53.93984, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.72368, qf2_loss: 0.73846, policy_loss: -67.38392, policy_entropy: -5.60199, alpha: 0.01643, time: 51.05602
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 246.04709, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.76064, qf2_loss: 0.76879, policy_loss: -67.37390, policy_entropy: -5.67179, alpha: 0.01632, time: 51.13459
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 209.35423, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 1.01185, qf2_loss: 1.03181, policy_loss: -67.71567, policy_entropy: -5.70385, alpha: 0.01622, time: 51.31860
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 88.99965, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.73616, qf2_loss: 0.74999, policy_loss: -67.28250, policy_entropy: -5.74050, alpha: 0.01613, time: 50.96270
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 149.81319, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.89896, qf2_loss: 0.90726, policy_loss: -67.39199, policy_entropy: -5.59904, alpha: 0.01605, time: 50.96202
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 277.37462, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 1.07653, qf2_loss: 1.09015, policy_loss: -67.94278, policy_entropy: -5.54331, alpha: 0.01589, time: 51.61570
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 277.83069, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.78413, qf2_loss: 0.79629, policy_loss: -67.73440, policy_entropy: -5.65152, alpha: 0.01575, time: 50.82746
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 321.94489, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.83511, qf2_loss: 0.84883, policy_loss: -68.42181, policy_entropy: -5.75543, alpha: 0.01563, time: 50.96430
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 350.36866, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.83319, qf2_loss: 0.84295, policy_loss: -68.37823, policy_entropy: -5.88060, alpha: 0.01558, time: 51.33350
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 49.82449, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.90856, qf2_loss: 0.91937, policy_loss: -68.23325, policy_entropy: -5.87782, alpha: 0.01553, time: 51.68983
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 370.74926, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 1.54589, qf2_loss: 1.56471, policy_loss: -68.35166, policy_entropy: -6.02354, alpha: 0.01550, time: 50.83435
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 289.61170, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 1.08403, qf2_loss: 1.10066, policy_loss: -68.98945, policy_entropy: -6.17988, alpha: 0.01556, time: 50.99266
[CW] eval: return: 229.53562, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 403.52847, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.87515, qf2_loss: 0.89316, policy_loss: -69.06785, policy_entropy: -5.93728, alpha: 0.01558, time: 50.85467
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 345.59826, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.81932, qf2_loss: 0.83232, policy_loss: -68.54065, policy_entropy: -5.99135, alpha: 0.01556, time: 50.71007
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 306.64277, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.78695, qf2_loss: 0.79919, policy_loss: -69.42984, policy_entropy: -6.23361, alpha: 0.01559, time: 51.01522
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 195.41416, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 1.33322, qf2_loss: 1.35513, policy_loss: -69.89984, policy_entropy: -6.26765, alpha: 0.01573, time: 51.07170
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 356.08022, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.94589, qf2_loss: 0.94901, policy_loss: -70.03487, policy_entropy: -6.23944, alpha: 0.01586, time: 51.23428
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 333.63873, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 1.09951, qf2_loss: 1.11267, policy_loss: -69.65647, policy_entropy: -6.26036, alpha: 0.01598, time: 51.26014
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 273.68742, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.87561, qf2_loss: 0.87919, policy_loss: -70.51463, policy_entropy: -6.31066, alpha: 0.01615, time: 51.30122
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 355.39981, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 0.96518, qf2_loss: 0.97914, policy_loss: -70.97177, policy_entropy: -6.19146, alpha: 0.01630, time: 51.32573
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 241.70674, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.97132, qf2_loss: 0.97015, policy_loss: -71.00387, policy_entropy: -6.32925, alpha: 0.01647, time: 51.17757
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 391.84112, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 1.12395, qf2_loss: 1.13910, policy_loss: -70.53867, policy_entropy: -6.20076, alpha: 0.01664, time: 51.14781
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 440.89539, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 1.34986, qf2_loss: 1.37356, policy_loss: -70.82512, policy_entropy: -6.30053, alpha: 0.01679, time: 51.09574
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 351.01641, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 1.02095, qf2_loss: 1.03206, policy_loss: -71.31832, policy_entropy: -6.24419, alpha: 0.01702, time: 51.12306
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 268.65134, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.86721, qf2_loss: 0.87837, policy_loss: -71.22868, policy_entropy: -6.06164, alpha: 0.01714, time: 51.19695
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 439.16368, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.99331, qf2_loss: 1.00094, policy_loss: -71.27889, policy_entropy: -6.02450, alpha: 0.01717, time: 51.19181
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 452.60672, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 1.40113, qf2_loss: 1.42044, policy_loss: -71.37005, policy_entropy: -5.94961, alpha: 0.01716, time: 51.17721
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 410.16022, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 1.46945, qf2_loss: 1.48574, policy_loss: -71.88660, policy_entropy: -5.91434, alpha: 0.01712, time: 51.69168
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 371.89055, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 1.36826, qf2_loss: 1.38268, policy_loss: -73.10616, policy_entropy: -6.32582, alpha: 0.01716, time: 51.44961
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 76.74153, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 1.44690, qf2_loss: 1.46050, policy_loss: -72.70469, policy_entropy: -6.14537, alpha: 0.01739, time: 51.18131
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 121.91857, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 1.01901, qf2_loss: 1.03146, policy_loss: -71.97252, policy_entropy: -6.09447, alpha: 0.01748, time: 50.97756
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 381.03716, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.94684, qf2_loss: 0.95437, policy_loss: -72.48114, policy_entropy: -6.00641, alpha: 0.01757, time: 50.96171
[CW] eval: return: 376.68549, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 261.32073, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 1.17659, qf2_loss: 1.19564, policy_loss: -72.78718, policy_entropy: -6.10595, alpha: 0.01758, time: 51.16048
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 461.35999, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 1.25496, qf2_loss: 1.27769, policy_loss: -72.96800, policy_entropy: -6.21352, alpha: 0.01776, time: 51.01404
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 395.76830, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 1.20348, qf2_loss: 1.21108, policy_loss: -73.36618, policy_entropy: -6.38092, alpha: 0.01799, time: 51.17107
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 269.95056, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 1.09701, qf2_loss: 1.10916, policy_loss: -72.96700, policy_entropy: -6.34787, alpha: 0.01834, time: 51.03696
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 402.42150, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 1.25139, qf2_loss: 1.26897, policy_loss: -73.30178, policy_entropy: -6.35516, alpha: 0.01869, time: 50.83264
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 382.85558, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 1.18750, qf2_loss: 1.19899, policy_loss: -74.50126, policy_entropy: -6.53462, alpha: 0.01913, time: 51.13281
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 220.43478, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 1.06695, qf2_loss: 1.08348, policy_loss: -73.64250, policy_entropy: -6.15753, alpha: 0.01949, time: 50.80819
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 403.79072, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 1.23626, qf2_loss: 1.24752, policy_loss: -74.29645, policy_entropy: -6.11318, alpha: 0.01961, time: 50.79281
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 441.88427, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 1.64004, qf2_loss: 1.66321, policy_loss: -74.75722, policy_entropy: -6.22401, alpha: 0.01978, time: 50.82754
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 352.06698, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 2.98829, qf2_loss: 3.02607, policy_loss: -74.93960, policy_entropy: -6.20022, alpha: 0.02008, time: 50.81939
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 363.36236, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 2.07537, qf2_loss: 2.07879, policy_loss: -74.90177, policy_entropy: -6.31138, alpha: 0.02030, time: 50.80740
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 399.33399, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 1.58914, qf2_loss: 1.61085, policy_loss: -75.11877, policy_entropy: -6.33037, alpha: 0.02066, time: 50.74903
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 418.21580, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 1.15920, qf2_loss: 1.17860, policy_loss: -76.08383, policy_entropy: -6.45518, alpha: 0.02108, time: 50.85851
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 395.43755, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 1.06701, qf2_loss: 1.07549, policy_loss: -75.51166, policy_entropy: -6.20714, alpha: 0.02149, time: 50.87608
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 394.75690, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 1.99269, qf2_loss: 2.00429, policy_loss: -76.04244, policy_entropy: -6.21569, alpha: 0.02179, time: 50.76660
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 407.99972, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 1.86255, qf2_loss: 1.87924, policy_loss: -76.43690, policy_entropy: -6.10911, alpha: 0.02196, time: 51.23026
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 271.11350, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 1.38246, qf2_loss: 1.39000, policy_loss: -76.77374, policy_entropy: -6.22160, alpha: 0.02214, time: 50.67898
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 428.78556, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 1.28782, qf2_loss: 1.30051, policy_loss: -76.22185, policy_entropy: -6.16245, alpha: 0.02240, time: 50.75827
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 113.81891, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 1.22113, qf2_loss: 1.22945, policy_loss: -77.27556, policy_entropy: -6.21457, alpha: 0.02265, time: 50.68422
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 466.01491, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 1.40620, qf2_loss: 1.41906, policy_loss: -76.41617, policy_entropy: -6.04313, alpha: 0.02283, time: 50.74047
[CW] eval: return: 443.70181, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 441.18061, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 1.66627, qf2_loss: 1.67747, policy_loss: -77.38409, policy_entropy: -6.15624, alpha: 0.02297, time: 50.69124
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 390.08381, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 1.70469, qf2_loss: 1.69643, policy_loss: -77.87915, policy_entropy: -6.07572, alpha: 0.02308, time: 50.67010
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 467.16921, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 1.51738, qf2_loss: 1.53685, policy_loss: -77.62566, policy_entropy: -6.14727, alpha: 0.02320, time: 52.15493
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 440.98400, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 1.96631, qf2_loss: 1.99448, policy_loss: -78.35908, policy_entropy: -6.17012, alpha: 0.02345, time: 50.62122
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 439.69580, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 2.36440, qf2_loss: 2.36547, policy_loss: -78.62986, policy_entropy: -6.25233, alpha: 0.02371, time: 50.59841
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 498.53433, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 4.24610, qf2_loss: 4.25911, policy_loss: -78.70197, policy_entropy: -6.18400, alpha: 0.02396, time: 50.88844
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 448.27323, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 1.69060, qf2_loss: 1.70826, policy_loss: -78.81074, policy_entropy: -6.08442, alpha: 0.02414, time: 50.85681
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 412.11996, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 1.39343, qf2_loss: 1.39772, policy_loss: -79.13711, policy_entropy: -6.12311, alpha: 0.02431, time: 50.96318
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 376.56580, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 1.37296, qf2_loss: 1.38213, policy_loss: -80.04907, policy_entropy: -6.15850, alpha: 0.02445, time: 50.90994
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 271.85699, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 1.34036, qf2_loss: 1.35322, policy_loss: -79.78658, policy_entropy: -6.02739, alpha: 0.02464, time: 51.00482
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 457.28310, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 2.91885, qf2_loss: 2.93674, policy_loss: -80.72913, policy_entropy: -6.30213, alpha: 0.02474, time: 50.95572
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 391.54925, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 2.15209, qf2_loss: 2.17201, policy_loss: -80.81653, policy_entropy: -6.30838, alpha: 0.02528, time: 50.93332
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 446.38340, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 1.53986, qf2_loss: 1.54665, policy_loss: -80.66571, policy_entropy: -6.06276, alpha: 0.02553, time: 50.96801
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 60.81552, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 3.39282, qf2_loss: 3.42270, policy_loss: -81.22900, policy_entropy: -6.49545, alpha: 0.02573, time: 50.96065
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 376.60328, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 2.59174, qf2_loss: 2.59353, policy_loss: -80.62185, policy_entropy: -6.93640, alpha: 0.02684, time: 50.72443
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 209.62482, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 1.67242, qf2_loss: 1.67130, policy_loss: -80.58777, policy_entropy: -6.15418, alpha: 0.02758, time: 50.78436
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 447.52376, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 1.55648, qf2_loss: 1.57140, policy_loss: -81.67227, policy_entropy: -5.97439, alpha: 0.02770, time: 50.80021
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 439.46104, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 1.65471, qf2_loss: 1.66781, policy_loss: -82.16831, policy_entropy: -5.75134, alpha: 0.02755, time: 50.68882
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 456.52512, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 1.55856, qf2_loss: 1.56601, policy_loss: -83.15399, policy_entropy: -5.96670, alpha: 0.02724, time: 50.79178
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 493.04573, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 1.83299, qf2_loss: 1.83401, policy_loss: -82.71193, policy_entropy: -5.98833, alpha: 0.02722, time: 51.07876
[CW] eval: return: 417.90848, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 483.68987, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 1.88630, qf2_loss: 1.89837, policy_loss: -82.77006, policy_entropy: -6.05081, alpha: 0.02729, time: 51.09409
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 436.09184, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 2.02595, qf2_loss: 2.06581, policy_loss: -82.96328, policy_entropy: -6.03135, alpha: 0.02734, time: 50.73000
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 471.49383, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 1.82407, qf2_loss: 1.82538, policy_loss: -84.06582, policy_entropy: -6.14798, alpha: 0.02750, time: 50.84926
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 442.22665, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 1.81694, qf2_loss: 1.83727, policy_loss: -84.04906, policy_entropy: -6.16908, alpha: 0.02763, time: 51.31400
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 529.36655, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 2.04325, qf2_loss: 2.06559, policy_loss: -84.62735, policy_entropy: -6.17657, alpha: 0.02792, time: 50.70496
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 497.20514, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 1.87834, qf2_loss: 1.88599, policy_loss: -84.63297, policy_entropy: -6.30698, alpha: 0.02824, time: 50.78679
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 481.11287, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 2.38403, qf2_loss: 2.40972, policy_loss: -85.31085, policy_entropy: -6.30229, alpha: 0.02886, time: 51.19212
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 474.69214, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 2.20762, qf2_loss: 2.20509, policy_loss: -86.07504, policy_entropy: -6.03586, alpha: 0.02913, time: 50.91329
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 415.15075, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 2.89140, qf2_loss: 2.92366, policy_loss: -85.53788, policy_entropy: -5.92254, alpha: 0.02906, time: 50.74546
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 91.48143, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 4.52880, qf2_loss: 4.56994, policy_loss: -85.37366, policy_entropy: -6.03461, alpha: 0.02901, time: 50.85626
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 488.63511, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 2.00546, qf2_loss: 2.02002, policy_loss: -86.59380, policy_entropy: -6.10025, alpha: 0.02909, time: 50.79989
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 441.89462, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 1.90790, qf2_loss: 1.94117, policy_loss: -85.94761, policy_entropy: -5.93116, alpha: 0.02920, time: 50.97866
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 277.60748, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 1.78008, qf2_loss: 1.80324, policy_loss: -87.61541, policy_entropy: -5.99658, alpha: 0.02911, time: 51.15354
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 486.86508, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 1.94560, qf2_loss: 1.96710, policy_loss: -87.22777, policy_entropy: -5.99965, alpha: 0.02911, time: 51.13910
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 522.93932, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 1.93514, qf2_loss: 1.96246, policy_loss: -87.57197, policy_entropy: -6.02896, alpha: 0.02909, time: 51.01439
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 540.87616, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 2.23749, qf2_loss: 2.25024, policy_loss: -87.85607, policy_entropy: -5.94471, alpha: 0.02918, time: 51.08115
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 488.87782, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 2.50327, qf2_loss: 2.52540, policy_loss: -88.73290, policy_entropy: -6.08513, alpha: 0.02915, time: 51.02091
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 513.99983, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 2.04954, qf2_loss: 2.09295, policy_loss: -87.92334, policy_entropy: -6.00310, alpha: 0.02918, time: 51.35562
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 185.94362, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 2.05239, qf2_loss: 2.07441, policy_loss: -88.68822, policy_entropy: -6.13297, alpha: 0.02928, time: 51.09173
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 490.25607, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 4.40722, qf2_loss: 4.43033, policy_loss: -88.44556, policy_entropy: -6.11059, alpha: 0.02939, time: 51.20315
[CW] eval: return: 459.71519, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 513.51113, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 2.70325, qf2_loss: 2.74828, policy_loss: -88.47541, policy_entropy: -6.29566, alpha: 0.02971, time: 50.99064
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 240.41996, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 2.65937, qf2_loss: 2.68308, policy_loss: -89.49854, policy_entropy: -6.31430, alpha: 0.03024, time: 50.83198
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 469.40819, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 2.54780, qf2_loss: 2.56036, policy_loss: -89.05021, policy_entropy: -6.14973, alpha: 0.03063, time: 50.96656
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 498.27893, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 1.94415, qf2_loss: 1.96489, policy_loss: -90.28167, policy_entropy: -6.09974, alpha: 0.03084, time: 51.39147
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 229.13116, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 1.98056, qf2_loss: 2.02005, policy_loss: -89.90274, policy_entropy: -5.94451, alpha: 0.03092, time: 50.91221
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 543.51284, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 2.37866, qf2_loss: 2.42496, policy_loss: -91.28117, policy_entropy: -6.04803, alpha: 0.03097, time: 50.82707
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 538.48388, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 2.58705, qf2_loss: 2.62145, policy_loss: -90.58641, policy_entropy: -5.76632, alpha: 0.03081, time: 51.07380
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 127.45644, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 2.58944, qf2_loss: 2.62499, policy_loss: -91.78967, policy_entropy: -5.79313, alpha: 0.03044, time: 51.16854
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 432.47243, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 3.43442, qf2_loss: 3.50618, policy_loss: -91.25650, policy_entropy: -5.68226, alpha: 0.03009, time: 50.81196
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 573.20825, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 2.74715, qf2_loss: 2.76983, policy_loss: -91.01088, policy_entropy: -5.86481, alpha: 0.02965, time: 51.14724
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 524.60000, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 2.08560, qf2_loss: 2.11820, policy_loss: -91.68380, policy_entropy: -5.95930, alpha: 0.02956, time: 51.97475
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 385.34653, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 1.90455, qf2_loss: 1.95478, policy_loss: -91.44992, policy_entropy: -5.84634, alpha: 0.02941, time: 50.83724
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 176.26464, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 1.92670, qf2_loss: 1.95597, policy_loss: -92.69013, policy_entropy: -6.05560, alpha: 0.02930, time: 51.07587
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 169.35254, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 3.02825, qf2_loss: 3.05700, policy_loss: -92.81523, policy_entropy: -5.98792, alpha: 0.02929, time: 50.80321
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 389.49899, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 4.32224, qf2_loss: 4.39189, policy_loss: -92.82956, policy_entropy: -5.82025, alpha: 0.02916, time: 50.87644
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 422.20554, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 2.26559, qf2_loss: 2.31277, policy_loss: -94.20798, policy_entropy: -6.20026, alpha: 0.02921, time: 50.80326
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 418.84848, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 2.09190, qf2_loss: 2.13154, policy_loss: -94.36298, policy_entropy: -6.09305, alpha: 0.02946, time: 50.64227
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 52.75125, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 2.33478, qf2_loss: 2.37985, policy_loss: -93.83835, policy_entropy: -6.06664, alpha: 0.02958, time: 50.75388
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 465.52146, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 2.79455, qf2_loss: 2.82898, policy_loss: -94.03632, policy_entropy: -5.82904, alpha: 0.02951, time: 50.78574
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 350.01497, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 2.38479, qf2_loss: 2.45172, policy_loss: -94.34675, policy_entropy: -5.88367, alpha: 0.02929, time: 50.77063
[CW] eval: return: 487.99995, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 517.07047, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 3.58507, qf2_loss: 3.63840, policy_loss: -95.46821, policy_entropy: -6.03509, alpha: 0.02922, time: 50.74436
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 395.63672, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 2.94483, qf2_loss: 2.96961, policy_loss: -95.43490, policy_entropy: -5.94914, alpha: 0.02920, time: 50.90941
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 530.50882, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 2.66290, qf2_loss: 2.70019, policy_loss: -95.47276, policy_entropy: -5.95714, alpha: 0.02917, time: 50.80241
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 535.14594, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 2.49320, qf2_loss: 2.51335, policy_loss: -96.02299, policy_entropy: -6.01379, alpha: 0.02911, time: 51.01270
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 276.40404, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 3.13753, qf2_loss: 3.17551, policy_loss: -96.04659, policy_entropy: -5.96943, alpha: 0.02913, time: 50.60176
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 535.13388, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 3.69886, qf2_loss: 3.72971, policy_loss: -96.22023, policy_entropy: -5.98284, alpha: 0.02909, time: 50.77198
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 544.65209, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 2.58079, qf2_loss: 2.62161, policy_loss: -97.60317, policy_entropy: -6.12785, alpha: 0.02907, time: 50.98553
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 572.10973, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 2.64921, qf2_loss: 2.69378, policy_loss: -96.99553, policy_entropy: -6.06244, alpha: 0.02929, time: 50.78956
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 500.52430, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 2.44279, qf2_loss: 2.48365, policy_loss: -98.13869, policy_entropy: -6.14198, alpha: 0.02942, time: 51.03258
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 282.60215, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 2.67363, qf2_loss: 2.72695, policy_loss: -97.55969, policy_entropy: -6.05876, alpha: 0.02955, time: 50.91391
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 234.74098, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 2.52429, qf2_loss: 2.54660, policy_loss: -98.64444, policy_entropy: -6.16601, alpha: 0.02969, time: 50.93750
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 521.32056, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 2.47637, qf2_loss: 2.50766, policy_loss: -99.26900, policy_entropy: -6.11374, alpha: 0.02991, time: 50.89191
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 500.77394, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 2.61453, qf2_loss: 2.65495, policy_loss: -99.00675, policy_entropy: -6.11956, alpha: 0.03010, time: 51.02464
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 278.56384, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 2.66259, qf2_loss: 2.68975, policy_loss: -98.01941, policy_entropy: -5.96885, alpha: 0.03013, time: 50.60546
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 546.20101, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 3.07230, qf2_loss: 3.10202, policy_loss: -97.79636, policy_entropy: -5.92192, alpha: 0.03010, time: 50.86504
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 110.85857, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 3.76915, qf2_loss: 3.79543, policy_loss: -98.94414, policy_entropy: -6.04423, alpha: 0.03009, time: 51.33876
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 556.35892, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 4.76724, qf2_loss: 4.81270, policy_loss: -98.51830, policy_entropy: -5.95601, alpha: 0.03008, time: 51.95701
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 545.65728, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 3.75072, qf2_loss: 3.83673, policy_loss: -99.95073, policy_entropy: -6.06685, alpha: 0.03010, time: 50.84650
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 519.10225, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 2.93873, qf2_loss: 2.97469, policy_loss: -99.09241, policy_entropy: -6.14833, alpha: 0.03016, time: 50.86377
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 553.02997, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 2.71247, qf2_loss: 2.73887, policy_loss: -99.54115, policy_entropy: -6.25786, alpha: 0.03046, time: 50.78310
[CW] eval: return: 493.35866, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 534.93875, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 2.67112, qf2_loss: 2.73180, policy_loss: -100.45692, policy_entropy: -6.30591, alpha: 0.03090, time: 51.01443
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 514.06000, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 2.80203, qf2_loss: 2.80683, policy_loss: -100.95526, policy_entropy: -6.07645, alpha: 0.03134, time: 50.93573
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 546.72152, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 3.00130, qf2_loss: 3.05704, policy_loss: -101.76174, policy_entropy: -6.16816, alpha: 0.03146, time: 50.81437
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 336.17281, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 2.90770, qf2_loss: 2.97295, policy_loss: -101.54895, policy_entropy: -6.00662, alpha: 0.03162, time: 50.89274
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 527.07529, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 3.69226, qf2_loss: 3.75823, policy_loss: -102.93635, policy_entropy: -6.08090, alpha: 0.03164, time: 50.80267
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 262.07688, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 7.04057, qf2_loss: 7.09994, policy_loss: -102.20472, policy_entropy: -6.09946, alpha: 0.03174, time: 50.76735
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 229.83983, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 4.41649, qf2_loss: 4.42273, policy_loss: -100.85731, policy_entropy: -5.95948, alpha: 0.03175, time: 50.91884
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 496.31357, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 3.22410, qf2_loss: 3.28192, policy_loss: -101.92764, policy_entropy: -6.27185, alpha: 0.03201, time: 50.52422
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 231.93123, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 2.85142, qf2_loss: 2.90493, policy_loss: -102.28602, policy_entropy: -6.14715, alpha: 0.03234, time: 50.54620
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 540.42229, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 2.45913, qf2_loss: 2.50072, policy_loss: -102.48300, policy_entropy: -5.93051, alpha: 0.03249, time: 50.63848
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 552.19279, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 2.65231, qf2_loss: 2.70016, policy_loss: -102.96059, policy_entropy: -5.89981, alpha: 0.03240, time: 50.39598
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 529.07098, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 2.77945, qf2_loss: 2.83176, policy_loss: -104.34760, policy_entropy: -5.98021, alpha: 0.03223, time: 50.95810
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 521.05035, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 3.23053, qf2_loss: 3.25402, policy_loss: -104.28122, policy_entropy: -5.95688, alpha: 0.03215, time: 51.09887
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 557.42112, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 2.71496, qf2_loss: 2.76926, policy_loss: -104.34233, policy_entropy: -6.05051, alpha: 0.03216, time: 51.13814
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 584.13752, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 3.25216, qf2_loss: 3.27280, policy_loss: -104.40559, policy_entropy: -5.91806, alpha: 0.03218, time: 51.38793
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 264.61598, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 3.19420, qf2_loss: 3.22592, policy_loss: -104.98221, policy_entropy: -6.06149, alpha: 0.03211, time: 51.22953
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 343.84521, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 3.09984, qf2_loss: 3.15663, policy_loss: -104.52021, policy_entropy: -6.04054, alpha: 0.03219, time: 51.09261
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 537.16773, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 3.50234, qf2_loss: 3.58093, policy_loss: -105.63587, policy_entropy: -6.22472, alpha: 0.03240, time: 50.99087
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 119.71912, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 3.37205, qf2_loss: 3.38328, policy_loss: -106.39336, policy_entropy: -6.20891, alpha: 0.03277, time: 50.77042
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 542.18126, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 3.79322, qf2_loss: 3.85536, policy_loss: -106.49857, policy_entropy: -6.16890, alpha: 0.03314, time: 51.50749
[CW] eval: return: 379.20564, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 528.57680, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 8.39937, qf2_loss: 8.46711, policy_loss: -105.82736, policy_entropy: -5.93627, alpha: 0.03330, time: 51.23699
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 559.60835, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 5.05866, qf2_loss: 5.16584, policy_loss: -106.94200, policy_entropy: -6.13799, alpha: 0.03330, time: 50.91707
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 137.07395, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 3.75961, qf2_loss: 3.80462, policy_loss: -105.51451, policy_entropy: -6.04044, alpha: 0.03350, time: 51.11127
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 548.61549, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 3.29070, qf2_loss: 3.34305, policy_loss: -106.25803, policy_entropy: -5.95093, alpha: 0.03350, time: 50.81785
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 578.21215, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 2.75480, qf2_loss: 2.80393, policy_loss: -106.86350, policy_entropy: -5.99970, alpha: 0.03348, time: 50.81055
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 384.31977, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 3.40493, qf2_loss: 3.44839, policy_loss: -107.79249, policy_entropy: -6.07067, alpha: 0.03351, time: 50.72638
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 521.97349, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 4.46948, qf2_loss: 4.51881, policy_loss: -106.05882, policy_entropy: -5.97590, alpha: 0.03351, time: 50.78547
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 456.12463, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 9.85846, qf2_loss: 9.90743, policy_loss: -108.48805, policy_entropy: -6.34143, alpha: 0.03371, time: 50.64148
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 575.31560, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 4.49742, qf2_loss: 4.57224, policy_loss: -106.94411, policy_entropy: -6.16908, alpha: 0.03426, time: 50.70081
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 74.12589, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 4.17226, qf2_loss: 4.25700, policy_loss: -107.28100, policy_entropy: -6.03096, alpha: 0.03445, time: 50.56994
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 93.11625, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 5.01720, qf2_loss: 5.11714, policy_loss: -108.39706, policy_entropy: -5.94521, alpha: 0.03448, time: 50.85044
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 589.09274, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 4.38202, qf2_loss: 4.44051, policy_loss: -108.87700, policy_entropy: -5.96861, alpha: 0.03430, time: 51.64106
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 615.06546, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 3.65814, qf2_loss: 3.67983, policy_loss: -108.92371, policy_entropy: -5.94732, alpha: 0.03428, time: 51.88692
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 75.57644, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 3.89113, qf2_loss: 3.95958, policy_loss: -109.34954, policy_entropy: -5.95366, alpha: 0.03417, time: 51.05800
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 596.65130, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 4.09617, qf2_loss: 4.08918, policy_loss: -108.16696, policy_entropy: -5.84773, alpha: 0.03401, time: 51.13871
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 594.87822, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 3.69482, qf2_loss: 3.76384, policy_loss: -109.28260, policy_entropy: -5.88884, alpha: 0.03383, time: 51.00794
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 623.49000, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 3.53419, qf2_loss: 3.57511, policy_loss: -111.03721, policy_entropy: -6.33119, alpha: 0.03394, time: 50.91862
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 568.03175, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 4.34679, qf2_loss: 4.40051, policy_loss: -110.03400, policy_entropy: -6.17542, alpha: 0.03442, time: 50.90443
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 561.06126, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 5.93217, qf2_loss: 6.09607, policy_loss: -110.80128, policy_entropy: -6.20629, alpha: 0.03477, time: 50.90853
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 585.17853, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 8.54521, qf2_loss: 8.56581, policy_loss: -111.46753, policy_entropy: -6.22755, alpha: 0.03512, time: 50.86260
[CW] eval: return: 478.76594, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 557.52718, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 8.24679, qf2_loss: 8.37330, policy_loss: -111.60150, policy_entropy: -6.07437, alpha: 0.03542, time: 51.01675
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 516.18126, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 6.09790, qf2_loss: 6.08569, policy_loss: -111.59993, policy_entropy: -6.10553, alpha: 0.03554, time: 50.99562
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 534.81555, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 4.83568, qf2_loss: 4.94221, policy_loss: -112.54115, policy_entropy: -6.23500, alpha: 0.03590, time: 50.65266
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 533.32455, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 3.59421, qf2_loss: 3.61968, policy_loss: -112.80414, policy_entropy: -6.12951, alpha: 0.03623, time: 50.88824
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 557.53497, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 3.52021, qf2_loss: 3.58092, policy_loss: -111.41860, policy_entropy: -6.02399, alpha: 0.03634, time: 50.87459
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 553.48463, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 3.48970, qf2_loss: 3.53190, policy_loss: -113.84408, policy_entropy: -6.13238, alpha: 0.03654, time: 50.58889
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 518.90983, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 3.73773, qf2_loss: 3.79406, policy_loss: -113.09090, policy_entropy: -6.05675, alpha: 0.03671, time: 50.73564
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 548.79024, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 4.14170, qf2_loss: 4.22627, policy_loss: -113.20876, policy_entropy: -6.00216, alpha: 0.03678, time: 50.56973
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 531.26861, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 11.00716, qf2_loss: 11.03377, policy_loss: -114.68119, policy_entropy: -6.34584, alpha: 0.03712, time: 50.52819
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 564.82420, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 7.08348, qf2_loss: 7.12455, policy_loss: -113.88223, policy_entropy: -6.26567, alpha: 0.03767, time: 50.69562
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 560.09087, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 4.31505, qf2_loss: 4.35213, policy_loss: -114.12656, policy_entropy: -6.25565, alpha: 0.03818, time: 50.64231
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 552.70299, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 3.76910, qf2_loss: 3.81350, policy_loss: -114.63433, policy_entropy: -6.13797, alpha: 0.03858, time: 50.94484
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 575.92291, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 3.85594, qf2_loss: 3.89528, policy_loss: -114.46326, policy_entropy: -6.16329, alpha: 0.03894, time: 50.52043
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 582.71349, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 3.67390, qf2_loss: 3.73883, policy_loss: -114.88468, policy_entropy: -6.01981, alpha: 0.03915, time: 52.79814
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 455.11872, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 3.60234, qf2_loss: 3.64358, policy_loss: -116.37937, policy_entropy: -6.26163, alpha: 0.03943, time: 50.65464
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 566.68589, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 5.58833, qf2_loss: 5.65968, policy_loss: -116.10200, policy_entropy: -6.03736, alpha: 0.03980, time: 50.79709
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 507.83908, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 5.12495, qf2_loss: 5.13608, policy_loss: -115.23171, policy_entropy: -6.13416, alpha: 0.04001, time: 50.87918
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 536.49543, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 7.61236, qf2_loss: 7.66427, policy_loss: -117.74702, policy_entropy: -6.31527, alpha: 0.04039, time: 50.60811
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 600.26476, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 4.34045, qf2_loss: 4.38786, policy_loss: -115.82540, policy_entropy: -5.97285, alpha: 0.04085, time: 50.81888
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 594.86839, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 4.21623, qf2_loss: 4.27450, policy_loss: -116.72600, policy_entropy: -6.11106, alpha: 0.04085, time: 50.81902
[CW] eval: return: 517.27952, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 197.11661, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 5.29992, qf2_loss: 5.29256, policy_loss: -116.94064, policy_entropy: -6.07848, alpha: 0.04121, time: 50.85743
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 585.13037, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 6.63928, qf2_loss: 6.70933, policy_loss: -117.15624, policy_entropy: -6.16526, alpha: 0.04137, time: 50.71918
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 177.63408, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 5.26702, qf2_loss: 5.36577, policy_loss: -117.74723, policy_entropy: -6.13388, alpha: 0.04164, time: 50.82035
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 166.65187, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 3.78368, qf2_loss: 3.87382, policy_loss: -117.07441, policy_entropy: -6.04954, alpha: 0.04195, time: 50.64095
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 552.58466, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 5.65356, qf2_loss: 5.69620, policy_loss: -117.68017, policy_entropy: -5.92813, alpha: 0.04200, time: 50.63314
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 565.71609, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 5.51975, qf2_loss: 5.55196, policy_loss: -116.54255, policy_entropy: -5.76898, alpha: 0.04162, time: 50.77349
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 158.31227, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 6.02873, qf2_loss: 6.03194, policy_loss: -119.35377, policy_entropy: -5.91994, alpha: 0.04124, time: 50.45655
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 119.12847, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 3.92755, qf2_loss: 3.97707, policy_loss: -119.26897, policy_entropy: -6.03120, alpha: 0.04121, time: 50.46151
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 309.94453, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 4.42481, qf2_loss: 4.50908, policy_loss: -119.17440, policy_entropy: -6.00138, alpha: 0.04125, time: 50.65756
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 548.59184, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 4.31546, qf2_loss: 4.36113, policy_loss: -118.35179, policy_entropy: -6.01924, alpha: 0.04136, time: 50.37367
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 557.56701, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 4.14275, qf2_loss: 4.19235, policy_loss: -120.22955, policy_entropy: -6.05396, alpha: 0.04127, time: 50.32128
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 548.29310, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 4.88734, qf2_loss: 4.89303, policy_loss: -119.31033, policy_entropy: -5.96538, alpha: 0.04138, time: 50.51739
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 612.86430, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 5.96963, qf2_loss: 6.06996, policy_loss: -119.60045, policy_entropy: -5.95027, alpha: 0.04130, time: 50.63892
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 536.18226, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 10.22333, qf2_loss: 10.18555, policy_loss: -118.86291, policy_entropy: -6.11102, alpha: 0.04127, time: 50.67884
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 187.87462, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 6.80613, qf2_loss: 6.87114, policy_loss: -119.71734, policy_entropy: -6.06717, alpha: 0.04152, time: 51.89384
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 544.15814, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 6.92093, qf2_loss: 6.96668, policy_loss: -120.14619, policy_entropy: -6.10981, alpha: 0.04169, time: 50.61065
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 603.56694, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 7.59804, qf2_loss: 7.71863, policy_loss: -120.58337, policy_entropy: -6.20195, alpha: 0.04202, time: 50.81612
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 152.73225, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 5.02791, qf2_loss: 5.08981, policy_loss: -120.87801, policy_entropy: -6.06364, alpha: 0.04235, time: 50.49724
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 582.39954, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 5.04455, qf2_loss: 5.11048, policy_loss: -121.39552, policy_entropy: -6.07107, alpha: 0.04257, time: 50.57461
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 600.97149, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 4.87857, qf2_loss: 4.92291, policy_loss: -120.19364, policy_entropy: -5.79736, alpha: 0.04237, time: 51.18403
[CW] eval: return: 518.90050, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 566.46622, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 4.60177, qf2_loss: 4.63964, policy_loss: -120.82963, policy_entropy: -5.82545, alpha: 0.04186, time: 50.82523
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 444.22248, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 4.36192, qf2_loss: 4.44619, policy_loss: -121.64458, policy_entropy: -6.03578, alpha: 0.04180, time: 50.68199
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 601.77464, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 4.60940, qf2_loss: 4.65778, policy_loss: -121.21273, policy_entropy: -6.04633, alpha: 0.04178, time: 50.49036
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 305.97353, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 5.53639, qf2_loss: 5.66089, policy_loss: -122.88653, policy_entropy: -6.06836, alpha: 0.04200, time: 50.66217
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 614.29011, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 5.80939, qf2_loss: 5.80859, policy_loss: -122.82280, policy_entropy: -6.04312, alpha: 0.04209, time: 50.93023
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 465.54897, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 5.40181, qf2_loss: 5.48673, policy_loss: -123.77564, policy_entropy: -6.09351, alpha: 0.04222, time: 50.90281
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 315.93050, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 6.17945, qf2_loss: 6.24056, policy_loss: -123.91435, policy_entropy: -6.10181, alpha: 0.04250, time: 51.15285
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 316.60774, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 5.24813, qf2_loss: 5.28004, policy_loss: -121.98264, policy_entropy: -5.97080, alpha: 0.04254, time: 51.43104
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 286.27982, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 5.45566, qf2_loss: 5.56735, policy_loss: -123.37333, policy_entropy: -5.90044, alpha: 0.04246, time: 50.85189
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 106.46222, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 5.49072, qf2_loss: 5.57913, policy_loss: -123.82010, policy_entropy: -5.94674, alpha: 0.04223, time: 51.22903
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 578.31953, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 5.17487, qf2_loss: 5.28535, policy_loss: -124.00441, policy_entropy: -5.90565, alpha: 0.04208, time: 50.81514
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 455.25214, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 5.73062, qf2_loss: 5.84725, policy_loss: -124.06527, policy_entropy: -5.85983, alpha: 0.04189, time: 50.96144
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 571.26256, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 5.82064, qf2_loss: 5.88460, policy_loss: -123.85942, policy_entropy: -5.85621, alpha: 0.04160, time: 51.29622
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 571.02008, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 12.50970, qf2_loss: 12.57571, policy_loss: -126.26778, policy_entropy: -6.11932, alpha: 0.04147, time: 50.97315
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 601.61877, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 9.10841, qf2_loss: 9.19753, policy_loss: -123.54283, policy_entropy: -5.82903, alpha: 0.04163, time: 50.92269
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 542.54655, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 7.43811, qf2_loss: 7.50073, policy_loss: -124.72461, policy_entropy: -5.96376, alpha: 0.04135, time: 51.12718
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 603.82900, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 4.68966, qf2_loss: 4.77281, policy_loss: -126.52222, policy_entropy: -6.20377, alpha: 0.04143, time: 51.15783
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 559.97227, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 4.90939, qf2_loss: 5.00971, policy_loss: -126.04726, policy_entropy: -6.09606, alpha: 0.04173, time: 51.04318
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 569.70184, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 6.82748, qf2_loss: 6.88490, policy_loss: -126.22558, policy_entropy: -5.98139, alpha: 0.04185, time: 50.82972
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 558.92693, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 14.26630, qf2_loss: 14.65089, policy_loss: -125.38834, policy_entropy: -5.86876, alpha: 0.04157, time: 50.79757
[CW] eval: return: 556.96093, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 574.33439, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 9.42554, qf2_loss: 9.43581, policy_loss: -126.44128, policy_entropy: -6.04071, alpha: 0.04167, time: 50.84885
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 576.28192, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 6.81354, qf2_loss: 6.90695, policy_loss: -126.04820, policy_entropy: -5.99346, alpha: 0.04160, time: 50.92904
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 615.75249, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 5.33727, qf2_loss: 5.37814, policy_loss: -126.57345, policy_entropy: -6.11633, alpha: 0.04164, time: 51.10569
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 565.58986, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 5.42864, qf2_loss: 5.53418, policy_loss: -128.09793, policy_entropy: -6.16167, alpha: 0.04194, time: 50.92119
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 593.46752, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 5.35591, qf2_loss: 5.47672, policy_loss: -128.31077, policy_entropy: -6.20520, alpha: 0.04236, time: 50.92131
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 542.48735, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 5.59331, qf2_loss: 5.63572, policy_loss: -127.73529, policy_entropy: -6.19498, alpha: 0.04281, time: 50.65238
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 294.57148, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 5.49083, qf2_loss: 5.55952, policy_loss: -127.78065, policy_entropy: -6.06419, alpha: 0.04311, time: 50.60682
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 303.33261, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 5.30690, qf2_loss: 5.41016, policy_loss: -129.96038, policy_entropy: -6.25113, alpha: 0.04341, time: 50.68563
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 618.49452, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 5.74913, qf2_loss: 5.87533, policy_loss: -129.77745, policy_entropy: -6.17771, alpha: 0.04381, time: 50.83088
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 581.13253, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 5.18832, qf2_loss: 5.25881, policy_loss: -129.08241, policy_entropy: -6.05111, alpha: 0.04410, time: 51.18805
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 609.88892, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 6.31150, qf2_loss: 6.30386, policy_loss: -130.99416, policy_entropy: -6.05338, alpha: 0.04417, time: 50.66884
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 524.64968, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 6.25924, qf2_loss: 6.34245, policy_loss: -130.41792, policy_entropy: -5.99035, alpha: 0.04429, time: 51.30225
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 555.37017, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 10.53282, qf2_loss: 10.56606, policy_loss: -130.09459, policy_entropy: -5.99081, alpha: 0.04417, time: 50.54591
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 542.21848, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 11.74388, qf2_loss: 11.80181, policy_loss: -132.22279, policy_entropy: -6.00125, alpha: 0.04418, time: 50.97614
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 605.74590, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 8.34667, qf2_loss: 8.46516, policy_loss: -132.41781, policy_entropy: -6.00739, alpha: 0.04422, time: 50.83362
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 535.51723, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 6.10726, qf2_loss: 6.17047, policy_loss: -130.35576, policy_entropy: -5.90432, alpha: 0.04421, time: 50.71779
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 552.61010, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 5.36270, qf2_loss: 5.42413, policy_loss: -130.81107, policy_entropy: -5.87546, alpha: 0.04393, time: 50.89397
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 571.26608, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 5.88579, qf2_loss: 5.94306, policy_loss: -131.45296, policy_entropy: -6.02571, alpha: 0.04375, time: 50.71317
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 600.00800, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 5.31566, qf2_loss: 5.39418, policy_loss: -131.10481, policy_entropy: -5.94448, alpha: 0.04369, time: 50.66712
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 605.43599, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 5.77942, qf2_loss: 5.95191, policy_loss: -132.25849, policy_entropy: -6.01029, alpha: 0.04374, time: 50.75413
[CW] eval: return: 573.61980, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 613.56922, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 5.43298, qf2_loss: 5.44197, policy_loss: -133.21587, policy_entropy: -5.96935, alpha: 0.04360, time: 50.95151
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 589.30529, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 5.09828, qf2_loss: 5.24277, policy_loss: -132.98971, policy_entropy: -5.97596, alpha: 0.04364, time: 50.83180
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 638.88471, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 5.66721, qf2_loss: 5.77070, policy_loss: -132.07785, policy_entropy: -5.93269, alpha: 0.04358, time: 50.75836
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 586.42744, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 6.03268, qf2_loss: 6.12790, policy_loss: -132.36006, policy_entropy: -6.03097, alpha: 0.04345, time: 50.73684
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 551.72400, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 6.31000, qf2_loss: 6.37512, policy_loss: -134.04085, policy_entropy: -6.09270, alpha: 0.04354, time: 50.72380
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 523.59930, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 7.83672, qf2_loss: 7.97786, policy_loss: -134.77898, policy_entropy: -6.12980, alpha: 0.04384, time: 50.71697
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 592.10689, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 5.85753, qf2_loss: 5.96897, policy_loss: -133.70390, policy_entropy: -6.04216, alpha: 0.04406, time: 51.15210
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 602.02469, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 7.21739, qf2_loss: 7.33673, policy_loss: -133.55354, policy_entropy: -5.97103, alpha: 0.04400, time: 50.61778
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 572.76929, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 9.86163, qf2_loss: 9.99487, policy_loss: -134.04287, policy_entropy: -6.13398, alpha: 0.04410, time: 50.72160
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 527.79560, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 11.61815, qf2_loss: 11.50494, policy_loss: -133.53814, policy_entropy: -5.96357, alpha: 0.04428, time: 50.70645
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 575.18685, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 7.78709, qf2_loss: 7.93974, policy_loss: -135.13490, policy_entropy: -5.93540, alpha: 0.04411, time: 50.88593
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 540.14656, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 7.18960, qf2_loss: 7.25152, policy_loss: -135.66485, policy_entropy: -5.91511, alpha: 0.04403, time: 50.71309
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 374.20227, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 5.83026, qf2_loss: 5.87712, policy_loss: -133.89621, policy_entropy: -5.95640, alpha: 0.04386, time: 51.14005
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 560.58204, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 7.79346, qf2_loss: 7.91205, policy_loss: -136.53239, policy_entropy: -6.07131, alpha: 0.04383, time: 50.80535
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 50.20892, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 7.09854, qf2_loss: 7.25839, policy_loss: -134.10412, policy_entropy: -5.99831, alpha: 0.04401, time: 51.03458
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 491.56828, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 6.45196, qf2_loss: 6.51507, policy_loss: -136.44888, policy_entropy: -6.13582, alpha: 0.04404, time: 50.98633
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 544.80156, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 8.67356, qf2_loss: 8.81830, policy_loss: -136.86235, policy_entropy: -6.08429, alpha: 0.04428, time: 50.62485
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 547.46275, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 8.63911, qf2_loss: 8.76408, policy_loss: -136.08001, policy_entropy: -6.05375, alpha: 0.04444, time: 50.86658
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 574.13008, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 6.26264, qf2_loss: 6.32232, policy_loss: -136.17694, policy_entropy: -6.01990, alpha: 0.04454, time: 50.70571
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 177.24057, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 5.91168, qf2_loss: 5.96306, policy_loss: -135.85641, policy_entropy: -5.94838, alpha: 0.04451, time: 50.90202
[CW] eval: return: 477.14984, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 622.12926, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 5.60356, qf2_loss: 5.70949, policy_loss: -138.21743, policy_entropy: -6.16469, alpha: 0.04455, time: 51.31181
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 324.51179, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 20.32969, qf2_loss: 20.40924, policy_loss: -136.72356, policy_entropy: -5.96308, alpha: 0.04481, time: 51.39303
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 544.67635, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 9.18550, qf2_loss: 9.31580, policy_loss: -136.94727, policy_entropy: -6.10831, alpha: 0.04491, time: 50.83564
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 507.94517, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 6.33331, qf2_loss: 6.42144, policy_loss: -138.37220, policy_entropy: -6.18283, alpha: 0.04522, time: 50.82631
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 622.91402, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 5.55775, qf2_loss: 5.58965, policy_loss: -138.78092, policy_entropy: -6.07625, alpha: 0.04541, time: 50.72564
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 47.31792, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 7.49035, qf2_loss: 7.60662, policy_loss: -137.81363, policy_entropy: -6.05941, alpha: 0.04556, time: 50.99614
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 489.10735, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 11.46539, qf2_loss: 11.72320, policy_loss: -136.77193, policy_entropy: -6.02929, alpha: 0.04561, time: 53.03669
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 537.87084, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 6.62808, qf2_loss: 6.72968, policy_loss: -138.24674, policy_entropy: -5.96489, alpha: 0.04573, time: 51.02788
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 562.26991, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 7.56108, qf2_loss: 7.72776, policy_loss: -139.13307, policy_entropy: -6.06007, alpha: 0.04575, time: 50.95209
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 597.06561, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 6.03409, qf2_loss: 6.15325, policy_loss: -137.31961, policy_entropy: -5.99420, alpha: 0.04576, time: 50.76442
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 583.12678, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 5.70124, qf2_loss: 5.69872, policy_loss: -140.10261, policy_entropy: -6.19522, alpha: 0.04600, time: 50.73773
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 311.00750, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 5.76857, qf2_loss: 5.87458, policy_loss: -137.85592, policy_entropy: -6.06934, alpha: 0.04618, time: 50.81550
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 219.94249, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 5.54956, qf2_loss: 5.64602, policy_loss: -138.73349, policy_entropy: -6.08075, alpha: 0.04653, time: 51.14486
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 345.99932, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 5.59952, qf2_loss: 5.69539, policy_loss: -138.77544, policy_entropy: -5.94658, alpha: 0.04652, time: 52.65200
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 44.43063, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 5.94533, qf2_loss: 6.03652, policy_loss: -138.85860, policy_entropy: -5.97553, alpha: 0.04644, time: 50.99053
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 48.41807, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 14.51382, qf2_loss: 14.72037, policy_loss: -139.44579, policy_entropy: -5.85616, alpha: 0.04639, time: 50.96237
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 64.20163, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 15.01280, qf2_loss: 15.02906, policy_loss: -139.80779, policy_entropy: -5.90118, alpha: 0.04588, time: 51.01984
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 24.46627, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 7.77944, qf2_loss: 7.83421, policy_loss: -137.57191, policy_entropy: -5.85391, alpha: 0.04572, time: 50.96478
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 507.36752, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 6.44552, qf2_loss: 6.52245, policy_loss: -139.36004, policy_entropy: -5.90744, alpha: 0.04544, time: 50.88335
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 70.51834, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 6.39051, qf2_loss: 6.46217, policy_loss: -141.09038, policy_entropy: -6.03910, alpha: 0.04538, time: 51.22494
[CW] eval: return: 383.22469, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 63.71372, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 7.48064, qf2_loss: 7.51674, policy_loss: -139.62501, policy_entropy: -5.99734, alpha: 0.04532, time: 50.81590
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 65.74694, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 7.38465, qf2_loss: 7.45337, policy_loss: -139.17960, policy_entropy: -5.94057, alpha: 0.04537, time: 50.94904
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 382.65958, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 6.63674, qf2_loss: 6.66416, policy_loss: -139.75963, policy_entropy: -6.02402, alpha: 0.04529, time: 51.04581
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 561.65478, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 7.23886, qf2_loss: 7.29196, policy_loss: -139.44711, policy_entropy: -6.01297, alpha: 0.04530, time: 50.69835
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 253.53974, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 8.23030, qf2_loss: 8.40696, policy_loss: -140.19979, policy_entropy: -6.01746, alpha: 0.04536, time: 50.68976
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 67.11443, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 8.75143, qf2_loss: 8.70725, policy_loss: -140.05879, policy_entropy: -6.04547, alpha: 0.04539, time: 50.87413
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 439.90111, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 7.50976, qf2_loss: 7.57440, policy_loss: -140.59168, policy_entropy: -6.21238, alpha: 0.04566, time: 50.64804
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 31.52668, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 7.10179, qf2_loss: 7.22802, policy_loss: -139.66877, policy_entropy: -6.04380, alpha: 0.04603, time: 50.47521
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 552.61482, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 7.34732, qf2_loss: 7.44694, policy_loss: -140.15365, policy_entropy: -6.11517, alpha: 0.04613, time: 51.19290
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 533.18474, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 6.94787, qf2_loss: 6.96025, policy_loss: -142.29187, policy_entropy: -6.34456, alpha: 0.04665, time: 50.45645
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 576.09881, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 7.08109, qf2_loss: 7.17101, policy_loss: -139.86899, policy_entropy: -6.04816, alpha: 0.04713, time: 50.63058
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 490.35302, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 7.75441, qf2_loss: 7.87542, policy_loss: -140.95804, policy_entropy: -6.11418, alpha: 0.04728, time: 50.71740
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 579.98940, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 8.68452, qf2_loss: 8.77762, policy_loss: -142.69107, policy_entropy: -6.21404, alpha: 0.04768, time: 50.65550
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 529.60504, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 12.32987, qf2_loss: 12.45181, policy_loss: -142.23309, policy_entropy: -6.09085, alpha: 0.04801, time: 50.73521
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 318.61897, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 13.78553, qf2_loss: 13.93248, policy_loss: -143.98315, policy_entropy: -6.19899, alpha: 0.04846, time: 50.71756
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 547.26191, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 14.24761, qf2_loss: 14.37555, policy_loss: -142.83199, policy_entropy: -6.12696, alpha: 0.04871, time: 50.86453
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 530.21728, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 8.14177, qf2_loss: 8.20979, policy_loss: -144.15774, policy_entropy: -6.14023, alpha: 0.04912, time: 51.87790
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 528.79032, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 6.10669, qf2_loss: 6.16296, policy_loss: -144.83528, policy_entropy: -6.07847, alpha: 0.04930, time: 51.32184
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 603.95656, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 6.98317, qf2_loss: 7.04842, policy_loss: -143.86069, policy_entropy: -5.97807, alpha: 0.04935, time: 51.02984
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 48.75948, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 8.16891, qf2_loss: 8.22791, policy_loss: -143.75113, policy_entropy: -5.96581, alpha: 0.04934, time: 50.91365
[CW] eval: return: 535.30321, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 544.44278, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 13.01821, qf2_loss: 13.33325, policy_loss: -144.92692, policy_entropy: -5.91577, alpha: 0.04923, time: 51.21062
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 551.82503, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 8.29295, qf2_loss: 8.40111, policy_loss: -144.80111, policy_entropy: -5.88751, alpha: 0.04903, time: 51.02819
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 532.32067, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 7.61535, qf2_loss: 7.76925, policy_loss: -144.87267, policy_entropy: -5.93997, alpha: 0.04877, time: 50.74147
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 561.85704, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 6.05768, qf2_loss: 6.15897, policy_loss: -143.96665, policy_entropy: -5.92623, alpha: 0.04863, time: 50.64462
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 594.73554, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 6.92448, qf2_loss: 7.00361, policy_loss: -145.05868, policy_entropy: -5.88539, alpha: 0.04846, time: 50.85675
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 588.07133, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 17.61031, qf2_loss: 17.59564, policy_loss: -144.61079, policy_entropy: -5.85750, alpha: 0.04813, time: 50.68553
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 275.86539, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 10.47913, qf2_loss: 10.64103, policy_loss: -144.69043, policy_entropy: -5.83276, alpha: 0.04767, time: 51.21304
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 581.59783, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 7.18291, qf2_loss: 7.24773, policy_loss: -147.40103, policy_entropy: -6.00314, alpha: 0.04756, time: 50.84675
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 424.41600, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 6.37016, qf2_loss: 6.39976, policy_loss: -147.70162, policy_entropy: -5.94296, alpha: 0.04745, time: 51.14950
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 171.39318, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 6.62671, qf2_loss: 6.66564, policy_loss: -144.78537, policy_entropy: -5.67603, alpha: 0.04706, time: 51.27668
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 577.51073, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 6.16527, qf2_loss: 6.22253, policy_loss: -146.49646, policy_entropy: -5.79571, alpha: 0.04642, time: 51.01920
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 572.18487, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 6.09846, qf2_loss: 6.15985, policy_loss: -146.99899, policy_entropy: -5.90068, alpha: 0.04613, time: 50.96322
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 614.38962, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 6.69836, qf2_loss: 6.80737, policy_loss: -146.65981, policy_entropy: -5.80760, alpha: 0.04579, time: 51.00921
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 566.62821, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 6.95141, qf2_loss: 6.94226, policy_loss: -148.92716, policy_entropy: -5.97699, alpha: 0.04547, time: 50.84891
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 618.87171, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 6.51964, qf2_loss: 6.53137, policy_loss: -148.68310, policy_entropy: -5.89750, alpha: 0.04541, time: 50.59444
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 387.87654, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 7.13584, qf2_loss: 7.27088, policy_loss: -146.85180, policy_entropy: -5.62898, alpha: 0.04493, time: 50.75054
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 561.05311, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 7.21361, qf2_loss: 7.33619, policy_loss: -148.24599, policy_entropy: -5.82651, alpha: 0.04423, time: 51.16090
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 588.22988, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 9.74981, qf2_loss: 9.88483, policy_loss: -148.00039, policy_entropy: -5.84008, alpha: 0.04396, time: 50.75512
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 610.00011, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 13.23649, qf2_loss: 13.22922, policy_loss: -146.65569, policy_entropy: -5.80570, alpha: 0.04355, time: 50.75267
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 272.73343, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 10.74618, qf2_loss: 10.80897, policy_loss: -147.45315, policy_entropy: -5.80372, alpha: 0.04316, time: 51.07990
[CW] eval: return: 532.55153, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 551.33105, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 8.21130, qf2_loss: 8.25243, policy_loss: -147.85233, policy_entropy: -5.96255, alpha: 0.04295, time: 51.08819
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 611.13198, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 7.56202, qf2_loss: 7.68591, policy_loss: -146.54941, policy_entropy: -5.94286, alpha: 0.04295, time: 50.96424
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 152.59695, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 7.41529, qf2_loss: 7.45202, policy_loss: -146.05982, policy_entropy: -5.84708, alpha: 0.04271, time: 51.03506
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 611.51749, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 9.32065, qf2_loss: 9.38405, policy_loss: -147.43237, policy_entropy: -5.93571, alpha: 0.04246, time: 51.09530
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 586.35010, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 8.62205, qf2_loss: 8.74784, policy_loss: -150.04411, policy_entropy: -6.25724, alpha: 0.04253, time: 50.76613
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 589.34607, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 9.08371, qf2_loss: 9.20485, policy_loss: -150.72253, policy_entropy: -6.18828, alpha: 0.04306, time: 50.71520
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 550.24902, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 8.69970, qf2_loss: 8.79025, policy_loss: -148.57741, policy_entropy: -6.15123, alpha: 0.04335, time: 50.91447
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 630.59006, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 6.89099, qf2_loss: 6.99184, policy_loss: -150.58622, policy_entropy: -6.25986, alpha: 0.04377, time: 50.56052
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 514.81944, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 7.26415, qf2_loss: 7.36549, policy_loss: -148.83198, policy_entropy: -6.08731, alpha: 0.04412, time: 50.82641
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 563.74205, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 8.10445, qf2_loss: 8.12814, policy_loss: -149.95982, policy_entropy: -6.12595, alpha: 0.04434, time: 50.86617
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 290.92607, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 9.12150, qf2_loss: 9.19942, policy_loss: -149.02564, policy_entropy: -6.12273, alpha: 0.04471, time: 50.81971
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 584.01921, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 8.90222, qf2_loss: 9.12969, policy_loss: -149.59728, policy_entropy: -5.93226, alpha: 0.04472, time: 51.44410
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 494.56686, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 15.28874, qf2_loss: 15.38157, policy_loss: -149.84882, policy_entropy: -6.25623, alpha: 0.04487, time: 51.01347
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 297.10912, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 9.60132, qf2_loss: 9.61427, policy_loss: -149.22088, policy_entropy: -6.23225, alpha: 0.04540, time: 50.61601
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 59.62711, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 8.67253, qf2_loss: 8.74102, policy_loss: -151.33733, policy_entropy: -6.18132, alpha: 0.04594, time: 50.80710
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 596.21779, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 8.09744, qf2_loss: 8.14907, policy_loss: -148.85730, policy_entropy: -5.92678, alpha: 0.04606, time: 50.91998
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 571.22933, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 7.58355, qf2_loss: 7.72020, policy_loss: -150.94719, policy_entropy: -6.14179, alpha: 0.04609, time: 51.11576
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 541.12217, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 8.18508, qf2_loss: 8.24692, policy_loss: -152.08457, policy_entropy: -6.13744, alpha: 0.04642, time: 50.78667
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 517.40004, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 9.26568, qf2_loss: 9.47226, policy_loss: -151.96594, policy_entropy: -6.15068, alpha: 0.04684, time: 51.00146
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 291.43005, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 10.46548, qf2_loss: 10.40426, policy_loss: -150.40904, policy_entropy: -5.91681, alpha: 0.04692, time: 50.94548
[CW] eval: return: 536.21618, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 579.01896, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 9.44414, qf2_loss: 9.61073, policy_loss: -151.76307, policy_entropy: -6.00896, alpha: 0.04675, time: 50.85019
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 319.41889, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 9.67193, qf2_loss: 9.82481, policy_loss: -150.40766, policy_entropy: -5.92656, alpha: 0.04667, time: 51.10826
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 251.04608, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 8.10639, qf2_loss: 8.28979, policy_loss: -152.20491, policy_entropy: -6.03165, alpha: 0.04659, time: 51.32513
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 427.82094, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 10.09066, qf2_loss: 10.19922, policy_loss: -153.59721, policy_entropy: -6.10682, alpha: 0.04675, time: 51.48271
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 243.71780, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 10.95742, qf2_loss: 11.01050, policy_loss: -152.48517, policy_entropy: -5.98013, alpha: 0.04691, time: 51.36153
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 123.49865, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 9.44048, qf2_loss: 9.56013, policy_loss: -154.09999, policy_entropy: -6.18574, alpha: 0.04709, time: 51.09792
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 601.91478, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 8.92804, qf2_loss: 9.02566, policy_loss: -153.88769, policy_entropy: -5.97437, alpha: 0.04728, time: 51.04285
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 96.09895, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 8.38988, qf2_loss: 8.42393, policy_loss: -154.33371, policy_entropy: -6.15567, alpha: 0.04737, time: 51.00341
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 395.03170, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 8.64864, qf2_loss: 8.75242, policy_loss: -156.13852, policy_entropy: -6.19350, alpha: 0.04778, time: 50.67137
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 99.34907, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 21.41682, qf2_loss: 21.61421, policy_loss: -153.21142, policy_entropy: -5.93883, alpha: 0.04799, time: 51.11917
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 500.44340, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 27.89358, qf2_loss: 28.00774, policy_loss: -154.56372, policy_entropy: -6.24036, alpha: 0.04805, time: 51.11736
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 417.77697, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 13.82506, qf2_loss: 14.06910, policy_loss: -153.81279, policy_entropy: -6.39212, alpha: 0.04895, time: 50.77800
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 405.18234, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 8.68083, qf2_loss: 8.91317, policy_loss: -153.85174, policy_entropy: -6.14816, alpha: 0.04968, time: 53.61773
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 564.82682, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 8.36785, qf2_loss: 8.50166, policy_loss: -155.86670, policy_entropy: -6.22101, alpha: 0.05011, time: 50.91899
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 558.36402, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 6.84878, qf2_loss: 6.90947, policy_loss: -154.53799, policy_entropy: -5.90025, alpha: 0.05030, time: 50.63824
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 112.06469, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 7.57069, qf2_loss: 7.68761, policy_loss: -155.54882, policy_entropy: -5.91698, alpha: 0.05007, time: 50.85386
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 129.91022, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 7.26644, qf2_loss: 7.40318, policy_loss: -155.36196, policy_entropy: -5.89495, alpha: 0.04975, time: 50.77167
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 188.28222, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 8.45547, qf2_loss: 8.58763, policy_loss: -154.71729, policy_entropy: -5.70904, alpha: 0.04932, time: 50.68905
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 515.66680, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 20.71027, qf2_loss: 20.82549, policy_loss: -153.90994, policy_entropy: -5.72036, alpha: 0.04861, time: 51.07244
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 532.52664, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 16.58631, qf2_loss: 16.58202, policy_loss: -158.19277, policy_entropy: -5.95505, alpha: 0.04825, time: 50.65299
[CW] eval: return: 452.59152, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 242.00472, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 12.87136, qf2_loss: 13.02700, policy_loss: -157.21688, policy_entropy: -5.88721, alpha: 0.04801, time: 50.87796
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 310.27598, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 9.89726, qf2_loss: 9.90837, policy_loss: -156.09051, policy_entropy: -5.88037, alpha: 0.04772, time: 51.12108
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 60.76191, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 9.55001, qf2_loss: 9.66170, policy_loss: -154.87626, policy_entropy: -5.75989, alpha: 0.04739, time: 51.21118
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 553.99565, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 9.04032, qf2_loss: 8.88574, policy_loss: -155.78475, policy_entropy: -5.82593, alpha: 0.04686, time: 50.99735
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 56.06247, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 10.48875, qf2_loss: 10.49862, policy_loss: -158.62095, policy_entropy: -5.87658, alpha: 0.04647, time: 50.98483
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 166.39031, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 7.92101, qf2_loss: 8.03217, policy_loss: -155.98567, policy_entropy: -5.69713, alpha: 0.04606, time: 50.99919
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 238.09169, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 7.71985, qf2_loss: 7.85172, policy_loss: -155.98046, policy_entropy: -5.63248, alpha: 0.04543, time: 50.66121
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 462.48995, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 8.62583, qf2_loss: 8.69616, policy_loss: -157.36473, policy_entropy: -5.74198, alpha: 0.04472, time: 50.95257
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 558.37981, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 8.41299, qf2_loss: 8.51679, policy_loss: -156.35965, policy_entropy: -5.73113, alpha: 0.04416, time: 51.07805
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 471.50299, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 9.15163, qf2_loss: 9.31310, policy_loss: -156.98462, policy_entropy: -5.69552, alpha: 0.04364, time: 50.87946
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 49.77788, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 7.42225, qf2_loss: 7.47175, policy_loss: -158.68461, policy_entropy: -6.00519, alpha: 0.04325, time: 51.08515
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 23.20530, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 9.97022, qf2_loss: 10.04115, policy_loss: -158.62593, policy_entropy: -5.94639, alpha: 0.04324, time: 50.61060
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 9.40862, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 8.35567, qf2_loss: 8.39441, policy_loss: -158.10486, policy_entropy: -6.02244, alpha: 0.04323, time: 51.47235
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 7.52448, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 7.69544, qf2_loss: 7.78702, policy_loss: -157.37948, policy_entropy: -6.06296, alpha: 0.04342, time: 50.59394
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 578.18282, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 9.25332, qf2_loss: 9.38902, policy_loss: -158.11854, policy_entropy: -6.12308, alpha: 0.04360, time: 50.77305
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 368.97671, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 8.72761, qf2_loss: 8.87741, policy_loss: -156.36146, policy_entropy: -6.02632, alpha: 0.04372, time: 51.06340
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 580.85408, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 8.70955, qf2_loss: 8.87914, policy_loss: -157.93652, policy_entropy: -6.28152, alpha: 0.04393, time: 51.14299
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 566.79285, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 7.03809, qf2_loss: 7.08055, policy_loss: -156.45495, policy_entropy: -6.08322, alpha: 0.04429, time: 51.16968
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 566.70728, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 8.39120, qf2_loss: 8.41253, policy_loss: -158.04560, policy_entropy: -6.28986, alpha: 0.04470, time: 51.12084
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 467.78379, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 10.03910, qf2_loss: 10.15596, policy_loss: -157.90731, policy_entropy: -6.20302, alpha: 0.04521, time: 51.01823
[CW] eval: return: 421.02355, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 573.60074, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 24.34425, qf2_loss: 24.50336, policy_loss: -158.08124, policy_entropy: -6.14666, alpha: 0.04559, time: 50.98369
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 6.51036, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 25.42894, qf2_loss: 25.57504, policy_loss: -156.65411, policy_entropy: -6.33736, alpha: 0.04606, time: 51.00980
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 4.00578, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 15.64690, qf2_loss: 15.63884, policy_loss: -158.04112, policy_entropy: -6.36008, alpha: 0.04696, time: 50.79181
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 3.11692, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 8.70577, qf2_loss: 8.73677, policy_loss: -159.06614, policy_entropy: -5.85525, alpha: 0.04723, time: 51.13289
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 2.87998, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 7.53070, qf2_loss: 7.58469, policy_loss: -157.19298, policy_entropy: -5.78379, alpha: 0.04678, time: 50.98151
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 10.02306, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 12.39356, qf2_loss: 12.60730, policy_loss: -158.03018, policy_entropy: -5.89818, alpha: 0.04632, time: 50.96061
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 4.41663, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 12.89798, qf2_loss: 13.02395, policy_loss: -157.32007, policy_entropy: -5.71607, alpha: 0.04590, time: 50.79561
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 6.52072, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 10.24961, qf2_loss: 10.43154, policy_loss: -158.80362, policy_entropy: -6.10371, alpha: 0.04574, time: 50.93563
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 6.04888, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 7.69327, qf2_loss: 7.86124, policy_loss: -157.30369, policy_entropy: -5.96393, alpha: 0.04581, time: 50.88721
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 5.52874, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 7.49404, qf2_loss: 7.65685, policy_loss: -157.54516, policy_entropy: -6.08789, alpha: 0.04586, time: 50.74288
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 6.55056, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 7.34234, qf2_loss: 7.43935, policy_loss: -158.74007, policy_entropy: -6.11695, alpha: 0.04614, time: 50.96717
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 7.89992, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 8.54079, qf2_loss: 8.57223, policy_loss: -157.18509, policy_entropy: -6.03067, alpha: 0.04631, time: 50.56710
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 8.19742, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 9.48645, qf2_loss: 9.60536, policy_loss: -158.88869, policy_entropy: -6.02203, alpha: 0.04627, time: 50.65509
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 8.26415, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 7.82342, qf2_loss: 7.92396, policy_loss: -159.77534, policy_entropy: -6.10574, alpha: 0.04652, time: 50.80548
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 9.46857, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 9.35427, qf2_loss: 9.45599, policy_loss: -160.01875, policy_entropy: -6.04453, alpha: 0.04658, time: 50.64170
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 10.53192, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 8.03028, qf2_loss: 8.17353, policy_loss: -159.62890, policy_entropy: -5.98694, alpha: 0.04678, time: 50.77448
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 10.24825, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 9.16787, qf2_loss: 9.42676, policy_loss: -158.54683, policy_entropy: -5.63085, alpha: 0.04635, time: 50.68167
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 8.48480, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 8.69461, qf2_loss: 8.77338, policy_loss: -158.72376, policy_entropy: -5.76831, alpha: 0.04561, time: 51.08648
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 7.34097, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 10.15578, qf2_loss: 10.21974, policy_loss: -158.60121, policy_entropy: -5.81937, alpha: 0.04517, time: 51.39224
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 575.05117, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 9.93696, qf2_loss: 10.16021, policy_loss: -160.71129, policy_entropy: -6.17190, alpha: 0.04511, time: 51.39037
[CW] eval: return: 415.73990, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 330.31026, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 15.13709, qf2_loss: 15.17024, policy_loss: -159.86194, policy_entropy: -6.35839, alpha: 0.04566, time: 51.35447
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 407.04909, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 11.52312, qf2_loss: 11.76962, policy_loss: -158.81298, policy_entropy: -5.87379, alpha: 0.04605, time: 51.52238
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 602.52390, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 13.55329, qf2_loss: 13.60909, policy_loss: -158.70532, policy_entropy: -5.80357, alpha: 0.04572, time: 51.28036
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 610.18131, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 14.42023, qf2_loss: 14.33657, policy_loss: -160.81070, policy_entropy: -6.21678, alpha: 0.04551, time: 51.28760
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 478.52632, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 9.98007, qf2_loss: 10.04036, policy_loss: -159.32674, policy_entropy: -6.13607, alpha: 0.04596, time: 51.25351
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 56.24178, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 11.03879, qf2_loss: 11.27755, policy_loss: -161.93935, policy_entropy: -6.39751, alpha: 0.04642, time: 51.06449
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 563.85979, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 10.11535, qf2_loss: 10.28604, policy_loss: -158.35065, policy_entropy: -6.36687, alpha: 0.04723, time: 51.15856
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 569.96788, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 10.20357, qf2_loss: 10.12988, policy_loss: -160.00688, policy_entropy: -6.48177, alpha: 0.04834, time: 51.13617
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 595.63769, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 13.91330, qf2_loss: 14.37920, policy_loss: -161.65387, policy_entropy: -6.52602, alpha: 0.04933, time: 51.03057
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 512.25131, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 16.42921, qf2_loss: 16.81051, policy_loss: -160.16065, policy_entropy: -6.10139, alpha: 0.05017, time: 51.27652
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 14.26175, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 17.79370, qf2_loss: 18.04701, policy_loss: -160.56852, policy_entropy: -6.15616, alpha: 0.05033, time: 51.36369
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 584.34196, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 20.27396, qf2_loss: 20.43952, policy_loss: -160.24307, policy_entropy: -6.36788, alpha: 0.05093, time: 51.93249
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 172.95384, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 13.05921, qf2_loss: 13.03473, policy_loss: -159.31247, policy_entropy: -6.55953, alpha: 0.05195, time: 51.60461
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 590.09827, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 9.67616, qf2_loss: 9.64990, policy_loss: -161.09588, policy_entropy: -6.57570, alpha: 0.05334, time: 50.73297
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 642.40538, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 8.61882, qf2_loss: 8.76560, policy_loss: -161.46630, policy_entropy: -6.49346, alpha: 0.05470, time: 50.66688
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 625.71190, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 8.00129, qf2_loss: 7.96578, policy_loss: -162.77478, policy_entropy: -6.43095, alpha: 0.05587, time: 50.57038
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 139.57379, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 8.98292, qf2_loss: 9.11750, policy_loss: -160.74538, policy_entropy: -6.35076, alpha: 0.05680, time: 51.23722
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 572.91584, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 9.55453, qf2_loss: 9.50807, policy_loss: -160.74819, policy_entropy: -6.29214, alpha: 0.05781, time: 51.31919
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 619.17223, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 10.07841, qf2_loss: 10.08404, policy_loss: -160.89936, policy_entropy: -6.39319, alpha: 0.05873, time: 51.26787
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 541.49693, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 11.06641, qf2_loss: 11.03594, policy_loss: -161.13127, policy_entropy: -6.29364, alpha: 0.05980, time: 51.47011
[CW] eval: return: 574.71771, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 591.16533, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 10.70231, qf2_loss: 10.72653, policy_loss: -161.69402, policy_entropy: -5.97646, alpha: 0.06022, time: 51.55369
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 571.70931, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 10.70034, qf2_loss: 10.64065, policy_loss: -161.56002, policy_entropy: -6.35631, alpha: 0.06061, time: 51.67043
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 550.63246, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 10.94506, qf2_loss: 11.03616, policy_loss: -161.46384, policy_entropy: -6.15071, alpha: 0.06143, time: 51.46940
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 542.06445, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 12.73386, qf2_loss: 12.70984, policy_loss: -163.71636, policy_entropy: -6.31833, alpha: 0.06204, time: 51.50579
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 547.21836, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 17.42422, qf2_loss: 17.69790, policy_loss: -162.06671, policy_entropy: -6.24959, alpha: 0.06298, time: 51.96831
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 496.47919, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 24.33252, qf2_loss: 24.46902, policy_loss: -161.69765, policy_entropy: -5.99305, alpha: 0.06368, time: 51.62687
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 426.41164, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 13.80174, qf2_loss: 13.85932, policy_loss: -163.22030, policy_entropy: -6.06744, alpha: 0.06356, time: 51.41120
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 440.61314, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 14.52318, qf2_loss: 14.55361, policy_loss: -164.79482, policy_entropy: -6.10000, alpha: 0.06393, time: 51.68311
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 502.00497, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 11.53001, qf2_loss: 11.66097, policy_loss: -164.51235, policy_entropy: -5.67247, alpha: 0.06371, time: 51.94289
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 395.87327, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 10.79534, qf2_loss: 10.83272, policy_loss: -165.08956, policy_entropy: -5.57465, alpha: 0.06247, time: 51.58797
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 524.03151, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 14.51540, qf2_loss: 14.77050, policy_loss: -165.76085, policy_entropy: -5.50119, alpha: 0.06085, time: 51.42569
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 467.60892, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 16.46945, qf2_loss: 16.24984, policy_loss: -164.36506, policy_entropy: -5.45159, alpha: 0.05945, time: 51.66310
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 504.08977, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 17.60387, qf2_loss: 17.82615, policy_loss: -163.85772, policy_entropy: -5.43312, alpha: 0.05787, time: 52.13642
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 462.21948, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 12.00195, qf2_loss: 12.18409, policy_loss: -165.88491, policy_entropy: -5.52974, alpha: 0.05656, time: 51.58435
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 108.01544, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 10.67751, qf2_loss: 10.91848, policy_loss: -166.73794, policy_entropy: -5.61219, alpha: 0.05546, time: 51.62606
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 391.43030, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 15.09249, qf2_loss: 15.17459, policy_loss: -164.48171, policy_entropy: -5.58021, alpha: 0.05454, time: 51.80513
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 486.65326, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 13.36739, qf2_loss: 13.47238, policy_loss: -164.13875, policy_entropy: -5.56982, alpha: 0.05348, time: 51.92561
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 498.62701, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 11.01720, qf2_loss: 11.20841, policy_loss: -166.58023, policy_entropy: -5.84799, alpha: 0.05284, time: 51.91902
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 454.12502, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 11.75445, qf2_loss: 11.83518, policy_loss: -166.67305, policy_entropy: -5.75200, alpha: 0.05242, time: 51.57471
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 434.33935, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 11.90554, qf2_loss: 12.04142, policy_loss: -164.38652, policy_entropy: -5.77850, alpha: 0.05178, time: 51.52317
[CW] eval: return: 5.02225, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 5.29163, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 10.03159, qf2_loss: 10.25194, policy_loss: -166.41010, policy_entropy: -5.84302, alpha: 0.05146, time: 51.60057
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 548.26084, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 10.34930, qf2_loss: 10.47682, policy_loss: -166.03041, policy_entropy: -5.71908, alpha: 0.05089, time: 51.61718
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 8.07865, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 9.53932, qf2_loss: 9.60958, policy_loss: -165.38830, policy_entropy: -5.70772, alpha: 0.05023, time: 51.85958
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 319.71673, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 24.86570, qf2_loss: 24.80266, policy_loss: -166.34996, policy_entropy: -5.46349, alpha: 0.04937, time: 51.40161
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 457.76957, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 17.92618, qf2_loss: 17.93389, policy_loss: -167.48935, policy_entropy: -5.99033, alpha: 0.04877, time: 51.71852
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 545.46971, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 78.29124, qf2_loss: 78.34975, policy_loss: -165.97152, policy_entropy: -5.95462, alpha: 0.04844, time: 51.78730
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 404.40405, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 63.35608, qf2_loss: 64.23807, policy_loss: -165.57994, policy_entropy: -6.64415, alpha: 0.04914, time: 51.37255
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 74.95366, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 36.07635, qf2_loss: 36.33307, policy_loss: -166.77733, policy_entropy: -6.70403, alpha: 0.05070, time: 52.16711
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 36.85124, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 17.99059, qf2_loss: 18.28520, policy_loss: -168.02406, policy_entropy: -6.94197, alpha: 0.05256, time: 51.63868
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 328.32629, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 14.34538, qf2_loss: 14.28342, policy_loss: -166.30678, policy_entropy: -6.61849, alpha: 0.05432, time: 51.58549
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 113.72983, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 16.14379, qf2_loss: 16.17606, policy_loss: -165.20385, policy_entropy: -6.60105, alpha: 0.05588, time: 52.19250
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 32.79875, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 13.86191, qf2_loss: 13.85686, policy_loss: -166.35742, policy_entropy: -6.38812, alpha: 0.05728, time: 51.97920
[CW] ---------------------------
