{"collect/return": 32.79874526918866, "collect/steps": 1000.0, "collect/total_steps": 678000.0, "train/qf1_loss": 13.861907000541686, "train/qf2_loss": 13.856862444877624, "train/policy_loss": -166.35741775512696, "train/policy_entropy": -6.388117265701294, "train/alpha": 0.05727632388472557, "train/time": 51.97919845581055, "eval/return": 5.022248988170782, "eval/steps": 1000.0, "_timestamp": 1678731573.3024707, "_runtime": 35939.230947732925, "_step": 672}