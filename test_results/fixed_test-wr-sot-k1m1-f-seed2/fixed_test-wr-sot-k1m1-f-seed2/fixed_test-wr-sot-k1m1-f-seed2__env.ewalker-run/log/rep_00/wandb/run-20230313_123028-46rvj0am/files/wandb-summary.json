{"collect/return": 232.05678150057793, "collect/steps": 1000.0, "collect/total_steps": 1006000.0, "train/qf1_loss": 0.07467922817915679, "train/qf2_loss": 0.07459525074809789, "train/policy_loss": -34.728492736816406, "train/policy_entropy": -5.748695993423462, "train/alpha": 0.00643177651334554, "train/time": 34.84762930870056, "eval/return": 223.8285451739561, "eval/steps": 1000.0, "_timestamp": 1678745206.96094, "_runtime": 38178.11514997482, "_step": 1000, "_wandb": {"runtime": 38178}}