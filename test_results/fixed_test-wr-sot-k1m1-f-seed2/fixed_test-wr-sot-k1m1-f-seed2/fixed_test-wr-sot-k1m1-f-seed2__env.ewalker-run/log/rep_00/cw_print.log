[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 28.40192, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.68013, qf2_loss: 1.68091, policy_loss: -7.82279, policy_entropy: 4.09864, alpha: 0.98504, time: 33.12124
[CW] eval: return: 24.96082, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 26.65467, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.08255, qf2_loss: 0.08266, policy_loss: -8.52543, policy_entropy: 4.10044, alpha: 0.95626, time: 33.04169
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 28.25444, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.07536, qf2_loss: 0.07546, policy_loss: -9.23231, policy_entropy: 4.10120, alpha: 0.92871, time: 32.95794
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 27.23139, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.06833, qf2_loss: 0.06830, policy_loss: -10.15140, policy_entropy: 4.10026, alpha: 0.90231, time: 32.91028
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 24.30570, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06457, qf2_loss: 0.06443, policy_loss: -11.17856, policy_entropy: 4.10150, alpha: 0.87699, time: 33.00572
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 23.24263, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06096, qf2_loss: 0.06039, policy_loss: -12.27751, policy_entropy: 4.10085, alpha: 0.85267, time: 33.14426
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 25.21727, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.06505, qf2_loss: 0.06381, policy_loss: -13.43297, policy_entropy: 4.10196, alpha: 0.82931, time: 33.11975
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 23.22499, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.07597, qf2_loss: 0.07525, policy_loss: -14.62288, policy_entropy: 4.10114, alpha: 0.80683, time: 33.23399
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 27.08636, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.07688, qf2_loss: 0.07723, policy_loss: -15.83887, policy_entropy: 4.10181, alpha: 0.78520, time: 33.44140
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 25.67315, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.07947, qf2_loss: 0.08015, policy_loss: -17.03947, policy_entropy: 4.10146, alpha: 0.76436, time: 33.13468
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 25.55942, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.07356, qf2_loss: 0.07432, policy_loss: -18.23010, policy_entropy: 4.09998, alpha: 0.74427, time: 33.00139
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 20.14179, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.08098, qf2_loss: 0.08201, policy_loss: -19.40039, policy_entropy: 4.10135, alpha: 0.72489, time: 32.99100
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 26.58527, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.08238, qf2_loss: 0.08368, policy_loss: -20.53886, policy_entropy: 4.10192, alpha: 0.70617, time: 33.03419
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 27.68163, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.07748, qf2_loss: 0.07868, policy_loss: -21.65540, policy_entropy: 4.10025, alpha: 0.68810, time: 32.99869
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 24.10224, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.08243, qf2_loss: 0.08385, policy_loss: -22.74286, policy_entropy: 4.10129, alpha: 0.67062, time: 33.23335
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 28.40967, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.06995, qf2_loss: 0.07110, policy_loss: -23.79998, policy_entropy: 4.10091, alpha: 0.65372, time: 33.34511
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 26.20181, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.08018, qf2_loss: 0.08179, policy_loss: -24.82933, policy_entropy: 4.10100, alpha: 0.63737, time: 33.37729
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 28.94060, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.08469, qf2_loss: 0.08658, policy_loss: -25.83509, policy_entropy: 4.10131, alpha: 0.62153, time: 33.20373
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 24.29675, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.07562, qf2_loss: 0.07722, policy_loss: -26.81127, policy_entropy: 4.10103, alpha: 0.60619, time: 33.07550
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 26.92126, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.06843, qf2_loss: 0.06988, policy_loss: -27.76202, policy_entropy: 4.10157, alpha: 0.59132, time: 33.14798
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 25.46812, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.06543, qf2_loss: 0.06677, policy_loss: -28.68406, policy_entropy: 4.10020, alpha: 0.57690, time: 32.99744
[CW] eval: return: 25.00682, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 25.83447, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.08803, qf2_loss: 0.09024, policy_loss: -29.57326, policy_entropy: 4.10179, alpha: 0.56291, time: 33.10126
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 25.21772, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.08917, qf2_loss: 0.09142, policy_loss: -30.44581, policy_entropy: 4.10262, alpha: 0.54934, time: 33.13512
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 30.20825, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.05741, qf2_loss: 0.05853, policy_loss: -31.28819, policy_entropy: 4.10164, alpha: 0.53615, time: 33.07529
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 25.33007, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.07933, qf2_loss: 0.08133, policy_loss: -32.10836, policy_entropy: 4.10125, alpha: 0.52335, time: 33.05649
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 24.23099, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.05575, qf2_loss: 0.05693, policy_loss: -32.90325, policy_entropy: 4.10094, alpha: 0.51090, time: 33.08439
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 27.30650, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.08316, qf2_loss: 0.08545, policy_loss: -33.67345, policy_entropy: 4.09979, alpha: 0.49881, time: 33.53747
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 24.11570, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.07306, qf2_loss: 0.07503, policy_loss: -34.41762, policy_entropy: 4.10105, alpha: 0.48705, time: 33.32915
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 35.79438, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.08145, qf2_loss: 0.08358, policy_loss: -35.14647, policy_entropy: 4.10126, alpha: 0.47561, time: 33.22649
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 21.10496, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.06786, qf2_loss: 0.06963, policy_loss: -35.84001, policy_entropy: 4.10056, alpha: 0.46448, time: 33.55409
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 23.90881, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.08205, qf2_loss: 0.08437, policy_loss: -36.52060, policy_entropy: 4.10099, alpha: 0.45365, time: 33.44885
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 24.22569, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.05870, qf2_loss: 0.06019, policy_loss: -37.17756, policy_entropy: 4.10113, alpha: 0.44311, time: 33.17512
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 20.29557, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.07108, qf2_loss: 0.07304, policy_loss: -37.81226, policy_entropy: 4.10239, alpha: 0.43284, time: 33.67442
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 30.22318, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.06116, qf2_loss: 0.06282, policy_loss: -38.42587, policy_entropy: 4.10135, alpha: 0.42284, time: 33.57888
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 25.08469, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.08383, qf2_loss: 0.08640, policy_loss: -39.01777, policy_entropy: 4.10184, alpha: 0.41309, time: 33.49249
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 27.27782, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.05156, qf2_loss: 0.05295, policy_loss: -39.59621, policy_entropy: 4.10187, alpha: 0.40360, time: 33.25669
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 22.12144, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.07328, qf2_loss: 0.07538, policy_loss: -40.14993, policy_entropy: 4.10164, alpha: 0.39435, time: 33.31859
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 23.17828, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.07489, qf2_loss: 0.07729, policy_loss: -40.68585, policy_entropy: 4.10132, alpha: 0.38532, time: 33.67246
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 22.84391, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.05386, qf2_loss: 0.05534, policy_loss: -41.20398, policy_entropy: 4.10123, alpha: 0.37653, time: 33.37411
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 21.14070, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.07169, qf2_loss: 0.07396, policy_loss: -41.70155, policy_entropy: 4.10159, alpha: 0.36795, time: 33.48916
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 22.98928, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.06927, qf2_loss: 0.07120, policy_loss: -42.18062, policy_entropy: 4.10093, alpha: 0.35958, time: 33.57421
[CW] eval: return: 26.39554, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 24.65599, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.04998, qf2_loss: 0.05148, policy_loss: -42.64947, policy_entropy: 4.10134, alpha: 0.35142, time: 33.07981
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 27.39374, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.08030, qf2_loss: 0.08314, policy_loss: -43.09956, policy_entropy: 4.10242, alpha: 0.34346, time: 33.50763
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 26.43934, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.05677, qf2_loss: 0.05859, policy_loss: -43.52987, policy_entropy: 4.10204, alpha: 0.33569, time: 33.47726
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 26.26627, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.05988, qf2_loss: 0.06183, policy_loss: -43.94410, policy_entropy: 4.10160, alpha: 0.32811, time: 33.27759
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 24.72802, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.07020, qf2_loss: 0.07266, policy_loss: -44.35048, policy_entropy: 4.10189, alpha: 0.32071, time: 33.11122
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 31.61036, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.05880, qf2_loss: 0.06070, policy_loss: -44.73302, policy_entropy: 4.10249, alpha: 0.31348, time: 33.63794
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 25.62740, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.07477, qf2_loss: 0.07751, policy_loss: -45.09712, policy_entropy: 4.10156, alpha: 0.30643, time: 33.76692
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 26.26566, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.04835, qf2_loss: 0.04989, policy_loss: -45.45951, policy_entropy: 4.10151, alpha: 0.29954, time: 33.15553
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 24.18829, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.06480, qf2_loss: 0.06704, policy_loss: -45.79925, policy_entropy: 4.10098, alpha: 0.29281, time: 33.29115
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 28.42492, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.07252, qf2_loss: 0.07526, policy_loss: -46.12849, policy_entropy: 4.10154, alpha: 0.28625, time: 33.31634
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 25.88826, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.04309, qf2_loss: 0.04446, policy_loss: -46.44357, policy_entropy: 4.10254, alpha: 0.27983, time: 33.64200
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 21.56697, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.05424, qf2_loss: 0.05607, policy_loss: -46.74041, policy_entropy: 4.10245, alpha: 0.27357, time: 33.80106
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 24.86612, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.08324, qf2_loss: 0.08643, policy_loss: -47.02879, policy_entropy: 4.10124, alpha: 0.26745, time: 33.86707
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 26.58568, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.05285, qf2_loss: 0.05479, policy_loss: -47.30411, policy_entropy: 4.10130, alpha: 0.26147, time: 33.63602
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 25.70998, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.05246, qf2_loss: 0.05433, policy_loss: -47.57170, policy_entropy: 4.10161, alpha: 0.25563, time: 33.60352
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 24.63683, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.06545, qf2_loss: 0.06792, policy_loss: -47.82759, policy_entropy: 4.10182, alpha: 0.24993, time: 33.64733
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 25.40889, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.04299, qf2_loss: 0.04451, policy_loss: -48.06261, policy_entropy: 4.10117, alpha: 0.24435, time: 33.87525
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 23.33945, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.06615, qf2_loss: 0.06880, policy_loss: -48.28964, policy_entropy: 4.10129, alpha: 0.23891, time: 33.55098
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 24.86454, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.05980, qf2_loss: 0.06207, policy_loss: -48.50984, policy_entropy: 4.10159, alpha: 0.23358, time: 33.36812
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 29.39913, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.05605, qf2_loss: 0.05818, policy_loss: -48.71756, policy_entropy: 4.10170, alpha: 0.22838, time: 33.02627
[CW] eval: return: 24.45710, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 30.18742, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.04517, qf2_loss: 0.04677, policy_loss: -48.91785, policy_entropy: 4.10148, alpha: 0.22330, time: 33.37440
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 24.11841, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.06231, qf2_loss: 0.06488, policy_loss: -49.09795, policy_entropy: 4.10147, alpha: 0.21833, time: 33.22434
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 27.98827, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.08173, qf2_loss: 0.08503, policy_loss: -49.27679, policy_entropy: 4.10048, alpha: 0.21348, time: 33.46109
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 22.50673, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.03015, qf2_loss: 0.03116, policy_loss: -49.44597, policy_entropy: 4.10175, alpha: 0.20873, time: 33.45345
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 24.75301, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.05268, qf2_loss: 0.05484, policy_loss: -49.60222, policy_entropy: 4.10285, alpha: 0.20409, time: 33.60873
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 22.81131, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.05424, qf2_loss: 0.05633, policy_loss: -49.75161, policy_entropy: 4.10102, alpha: 0.19956, time: 33.51126
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 27.23572, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.04616, qf2_loss: 0.04797, policy_loss: -49.88979, policy_entropy: 4.10115, alpha: 0.19513, time: 33.48525
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 23.98332, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.05766, qf2_loss: 0.05989, policy_loss: -50.02183, policy_entropy: 4.10076, alpha: 0.19080, time: 33.66503
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 28.24310, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.06525, qf2_loss: 0.06788, policy_loss: -50.14127, policy_entropy: 4.10113, alpha: 0.18656, time: 33.53680
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 24.59852, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.03945, qf2_loss: 0.04100, policy_loss: -50.25656, policy_entropy: 4.10210, alpha: 0.18242, time: 33.48988
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 21.89695, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.05785, qf2_loss: 0.06016, policy_loss: -50.35925, policy_entropy: 4.10211, alpha: 0.17838, time: 33.62963
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 24.86698, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.03983, qf2_loss: 0.04145, policy_loss: -50.45177, policy_entropy: 4.10100, alpha: 0.17442, time: 33.52820
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 23.44840, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.05358, qf2_loss: 0.05569, policy_loss: -50.54848, policy_entropy: 4.10079, alpha: 0.17055, time: 33.68111
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 23.08850, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.04716, qf2_loss: 0.04911, policy_loss: -50.62874, policy_entropy: 4.10064, alpha: 0.16677, time: 33.48514
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 27.68071, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.05511, qf2_loss: 0.05733, policy_loss: -50.70648, policy_entropy: 4.10231, alpha: 0.16308, time: 33.89846
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 25.13977, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.05243, qf2_loss: 0.05457, policy_loss: -50.76808, policy_entropy: 4.10152, alpha: 0.15946, time: 33.82573
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 23.01906, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.05297, qf2_loss: 0.05485, policy_loss: -50.83152, policy_entropy: 4.10139, alpha: 0.15593, time: 33.90975
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 24.15005, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.03621, qf2_loss: 0.03769, policy_loss: -50.88036, policy_entropy: 4.10175, alpha: 0.15248, time: 33.31833
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 25.63307, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.05960, qf2_loss: 0.06210, policy_loss: -50.93014, policy_entropy: 4.10249, alpha: 0.14910, time: 33.42867
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 25.09513, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.03565, qf2_loss: 0.03698, policy_loss: -50.97167, policy_entropy: 4.10288, alpha: 0.14580, time: 33.33261
[CW] eval: return: 25.55666, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 24.86282, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.05080, qf2_loss: 0.05290, policy_loss: -51.00500, policy_entropy: 4.10056, alpha: 0.14257, time: 33.31647
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 25.31895, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.04128, qf2_loss: 0.04293, policy_loss: -51.03553, policy_entropy: 4.10151, alpha: 0.13941, time: 33.70256
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 24.31622, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.04596, qf2_loss: 0.04778, policy_loss: -51.05078, policy_entropy: 4.10174, alpha: 0.13633, time: 33.71629
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 24.91809, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.05903, qf2_loss: 0.06144, policy_loss: -51.06940, policy_entropy: 4.10090, alpha: 0.13331, time: 33.61807
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 24.22727, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.03634, qf2_loss: 0.03775, policy_loss: -51.08110, policy_entropy: 4.10116, alpha: 0.13036, time: 33.67857
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 23.41854, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.05093, qf2_loss: 0.05299, policy_loss: -51.08439, policy_entropy: 4.10134, alpha: 0.12747, time: 33.93515
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 21.80923, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.04039, qf2_loss: 0.04192, policy_loss: -51.08476, policy_entropy: 4.10231, alpha: 0.12465, time: 33.92141
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 23.92168, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.03921, qf2_loss: 0.04082, policy_loss: -51.07902, policy_entropy: 4.10150, alpha: 0.12189, time: 33.62502
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 29.04086, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.04805, qf2_loss: 0.05006, policy_loss: -51.06607, policy_entropy: 4.10188, alpha: 0.11919, time: 33.92404
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 21.58411, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.03353, qf2_loss: 0.03488, policy_loss: -51.05757, policy_entropy: 4.10170, alpha: 0.11655, time: 33.78799
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 27.24604, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.05413, qf2_loss: 0.05627, policy_loss: -51.03259, policy_entropy: 4.10072, alpha: 0.11398, time: 33.91959
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 28.34140, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.03362, qf2_loss: 0.03490, policy_loss: -51.00905, policy_entropy: 4.10101, alpha: 0.11145, time: 33.89727
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 22.41009, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.04890, qf2_loss: 0.05100, policy_loss: -50.97430, policy_entropy: 4.10029, alpha: 0.10899, time: 33.85303
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 28.86859, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.03407, qf2_loss: 0.03544, policy_loss: -50.94370, policy_entropy: 4.10132, alpha: 0.10658, time: 33.83921
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 25.84940, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.04076, qf2_loss: 0.04228, policy_loss: -50.90816, policy_entropy: 4.10190, alpha: 0.10422, time: 33.63378
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 23.08905, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.03740, qf2_loss: 0.03885, policy_loss: -50.86092, policy_entropy: 4.10163, alpha: 0.10192, time: 33.68830
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 25.61976, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.06312, qf2_loss: 0.06583, policy_loss: -50.81952, policy_entropy: 4.10064, alpha: 0.09966, time: 33.57995
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 29.82092, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.02456, qf2_loss: 0.02551, policy_loss: -50.76627, policy_entropy: 4.09988, alpha: 0.09746, time: 33.71540
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 26.24053, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.05145, qf2_loss: 0.05360, policy_loss: -50.71328, policy_entropy: 4.10097, alpha: 0.09530, time: 33.49943
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 24.54628, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.04112, qf2_loss: 0.04289, policy_loss: -50.65429, policy_entropy: 4.10232, alpha: 0.09320, time: 33.84922
[CW] eval: return: 24.69902, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 29.28655, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.03469, qf2_loss: 0.03602, policy_loss: -50.59489, policy_entropy: 4.10082, alpha: 0.09113, time: 33.57349
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 24.73671, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.03624, qf2_loss: 0.03779, policy_loss: -50.52945, policy_entropy: 4.10120, alpha: 0.08912, time: 41.74870
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 24.18913, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.04274, qf2_loss: 0.04447, policy_loss: -50.46111, policy_entropy: 4.10068, alpha: 0.08715, time: 33.77969
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 20.61672, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.03013, qf2_loss: 0.03141, policy_loss: -50.38806, policy_entropy: 4.10075, alpha: 0.08522, time: 33.68900
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 25.78221, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.04358, qf2_loss: 0.04535, policy_loss: -50.30942, policy_entropy: 4.10002, alpha: 0.08334, time: 33.30155
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 23.10491, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.02011, qf2_loss: 0.02084, policy_loss: -50.23616, policy_entropy: 4.10105, alpha: 0.08149, time: 33.58848
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 26.43305, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.07859, qf2_loss: 0.08214, policy_loss: -50.15213, policy_entropy: 4.10000, alpha: 0.07969, time: 33.91553
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 20.44105, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.01259, qf2_loss: 0.01296, policy_loss: -50.06713, policy_entropy: 4.09913, alpha: 0.07793, time: 33.77461
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 23.97399, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.03609, qf2_loss: 0.03752, policy_loss: -49.97848, policy_entropy: 4.09946, alpha: 0.07621, time: 33.83473
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 23.66144, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.03010, qf2_loss: 0.03141, policy_loss: -49.89246, policy_entropy: 4.09967, alpha: 0.07452, time: 34.74415
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 24.13554, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.05759, qf2_loss: 0.06024, policy_loss: -49.79627, policy_entropy: 4.09993, alpha: 0.07287, time: 33.71954
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 21.12008, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.01428, qf2_loss: 0.01473, policy_loss: -49.70165, policy_entropy: 4.09938, alpha: 0.07126, time: 33.43718
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 25.38287, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.04022, qf2_loss: 0.04201, policy_loss: -49.59806, policy_entropy: 4.09962, alpha: 0.06968, time: 33.83853
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 26.87717, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.06000, qf2_loss: 0.06301, policy_loss: -49.50870, policy_entropy: 4.09963, alpha: 0.06814, time: 33.99690
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 23.34747, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.01674, qf2_loss: 0.01644, policy_loss: -49.39583, policy_entropy: 4.09913, alpha: 0.06664, time: 34.15269
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 22.97541, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.02507, qf2_loss: 0.02659, policy_loss: -49.29380, policy_entropy: 4.09816, alpha: 0.06516, time: 33.64777
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 26.12383, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.04120, qf2_loss: 0.04300, policy_loss: -49.18743, policy_entropy: 4.09943, alpha: 0.06372, time: 33.65126
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 24.47314, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.03891, qf2_loss: 0.04049, policy_loss: -49.07754, policy_entropy: 4.09926, alpha: 0.06231, time: 33.84474
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 23.86300, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.03156, qf2_loss: 0.03296, policy_loss: -48.97346, policy_entropy: 4.09789, alpha: 0.06094, time: 34.01823
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 25.18759, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.03312, qf2_loss: 0.03455, policy_loss: -48.85848, policy_entropy: 4.09964, alpha: 0.05959, time: 33.48894
[CW] eval: return: 25.25910, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 26.70954, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.02887, qf2_loss: 0.03012, policy_loss: -48.74227, policy_entropy: 4.10001, alpha: 0.05827, time: 33.85014
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 24.84226, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.02475, qf2_loss: 0.02578, policy_loss: -48.62481, policy_entropy: 4.09892, alpha: 0.05698, time: 33.82760
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 22.95893, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.03711, qf2_loss: 0.03875, policy_loss: -48.50977, policy_entropy: 4.09964, alpha: 0.05572, time: 33.57677
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 25.62428, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.04459, qf2_loss: 0.04652, policy_loss: -48.38836, policy_entropy: 4.09944, alpha: 0.05449, time: 33.78801
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 23.21260, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.01910, qf2_loss: 0.01987, policy_loss: -48.26497, policy_entropy: 4.09877, alpha: 0.05328, time: 33.98654
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 23.22175, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.04250, qf2_loss: 0.04445, policy_loss: -48.14129, policy_entropy: 4.09750, alpha: 0.05211, time: 33.58091
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 23.50897, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.03776, qf2_loss: 0.03959, policy_loss: -48.01662, policy_entropy: 4.09803, alpha: 0.05095, time: 33.62949
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 33.92737, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.02694, qf2_loss: 0.02821, policy_loss: -47.88893, policy_entropy: 4.09686, alpha: 0.04983, time: 33.78221
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 25.57923, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.02376, qf2_loss: 0.02466, policy_loss: -47.76148, policy_entropy: 4.09747, alpha: 0.04873, time: 33.97182
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 22.83407, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.03204, qf2_loss: 0.03336, policy_loss: -47.63484, policy_entropy: 4.09668, alpha: 0.04765, time: 33.63545
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 24.64432, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.03212, qf2_loss: 0.03371, policy_loss: -47.49843, policy_entropy: 4.10028, alpha: 0.04659, time: 33.75659
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 23.83928, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.02877, qf2_loss: 0.02994, policy_loss: -47.36971, policy_entropy: 4.09756, alpha: 0.04556, time: 33.52673
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 22.69226, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.03107, qf2_loss: 0.03241, policy_loss: -47.23991, policy_entropy: 4.09607, alpha: 0.04456, time: 33.92469
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 38.96740, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.02842, qf2_loss: 0.02966, policy_loss: -47.10921, policy_entropy: 4.09424, alpha: 0.04357, time: 33.54148
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 22.44771, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.02635, qf2_loss: 0.02750, policy_loss: -46.96705, policy_entropy: 4.09562, alpha: 0.04261, time: 33.78683
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 26.18850, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.03452, qf2_loss: 0.03612, policy_loss: -46.83106, policy_entropy: 4.09533, alpha: 0.04167, time: 33.63610
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 23.78930, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.03114, qf2_loss: 0.03243, policy_loss: -46.68878, policy_entropy: 4.09529, alpha: 0.04075, time: 33.65561
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 27.27141, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.03271, qf2_loss: 0.03414, policy_loss: -46.55767, policy_entropy: 4.09509, alpha: 0.03985, time: 33.17217
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 28.31106, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.02591, qf2_loss: 0.02681, policy_loss: -46.41983, policy_entropy: 4.09397, alpha: 0.03897, time: 33.35197
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 23.41290, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.02757, qf2_loss: 0.02885, policy_loss: -46.27802, policy_entropy: 4.09069, alpha: 0.03810, time: 34.00867
[CW] eval: return: 26.77371, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 28.71054, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.04256, qf2_loss: 0.04448, policy_loss: -46.13456, policy_entropy: 4.09365, alpha: 0.03726, time: 33.47482
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 30.12672, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.03319, qf2_loss: 0.03457, policy_loss: -45.99688, policy_entropy: 4.09590, alpha: 0.03644, time: 33.55861
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 24.87525, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.02265, qf2_loss: 0.02371, policy_loss: -45.85046, policy_entropy: 4.09358, alpha: 0.03563, time: 33.75641
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 25.38667, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.02017, qf2_loss: 0.02103, policy_loss: -45.70172, policy_entropy: 4.09471, alpha: 0.03485, time: 33.39133
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 22.13717, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.03902, qf2_loss: 0.04060, policy_loss: -45.56028, policy_entropy: 4.09269, alpha: 0.03408, time: 33.78906
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 23.47893, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.02155, qf2_loss: 0.02253, policy_loss: -45.41681, policy_entropy: 4.09412, alpha: 0.03332, time: 33.67292
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 27.41894, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.03295, qf2_loss: 0.03455, policy_loss: -45.26805, policy_entropy: 4.09206, alpha: 0.03259, time: 33.60198
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 25.76734, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.03193, qf2_loss: 0.03373, policy_loss: -45.12243, policy_entropy: 4.09546, alpha: 0.03186, time: 33.97470
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 20.97612, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.02498, qf2_loss: 0.02592, policy_loss: -44.97402, policy_entropy: 4.08954, alpha: 0.03116, time: 33.66254
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 26.14251, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.01937, qf2_loss: 0.02005, policy_loss: -44.82788, policy_entropy: 4.09299, alpha: 0.03047, time: 33.73309
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 19.72474, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.04072, qf2_loss: 0.04262, policy_loss: -44.67664, policy_entropy: 4.08932, alpha: 0.02980, time: 33.64676
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 26.20264, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.01075, qf2_loss: 0.01099, policy_loss: -44.53206, policy_entropy: 4.08952, alpha: 0.02914, time: 33.71875
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 26.69801, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.02817, qf2_loss: 0.02965, policy_loss: -44.37920, policy_entropy: 4.08787, alpha: 0.02850, time: 34.35487
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 23.43328, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.03193, qf2_loss: 0.03325, policy_loss: -44.23928, policy_entropy: 4.08953, alpha: 0.02787, time: 33.60383
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 23.48776, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.03839, qf2_loss: 0.04007, policy_loss: -44.08196, policy_entropy: 4.08781, alpha: 0.02725, time: 32.85475
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 26.31348, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.02018, qf2_loss: 0.02111, policy_loss: -43.93602, policy_entropy: 4.08504, alpha: 0.02665, time: 33.20671
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 23.83834, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.02775, qf2_loss: 0.02917, policy_loss: -43.78219, policy_entropy: 4.08794, alpha: 0.02606, time: 33.73633
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 26.95160, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.01837, qf2_loss: 0.01908, policy_loss: -43.63579, policy_entropy: 4.08782, alpha: 0.02548, time: 33.69314
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 25.76941, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.02222, qf2_loss: 0.02302, policy_loss: -43.47876, policy_entropy: 4.08841, alpha: 0.02492, time: 33.70428
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 25.68130, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.02468, qf2_loss: 0.02577, policy_loss: -43.32545, policy_entropy: 4.08603, alpha: 0.02437, time: 33.58033
[CW] eval: return: 24.97431, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 24.78387, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.02417, qf2_loss: 0.02522, policy_loss: -43.17682, policy_entropy: 4.09004, alpha: 0.02383, time: 33.43991
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 22.21567, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.02226, qf2_loss: 0.02327, policy_loss: -43.02150, policy_entropy: 4.08810, alpha: 0.02330, time: 33.35842
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 23.77169, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.04015, qf2_loss: 0.04193, policy_loss: -42.85796, policy_entropy: 4.08427, alpha: 0.02279, time: 33.42612
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 23.39055, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.01225, qf2_loss: 0.01282, policy_loss: -42.71316, policy_entropy: 4.08724, alpha: 0.02229, time: 33.52615
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 35.99500, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.02459, qf2_loss: 0.02588, policy_loss: -42.56379, policy_entropy: 4.08340, alpha: 0.02179, time: 33.45032
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 23.48087, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.02390, qf2_loss: 0.02508, policy_loss: -42.40713, policy_entropy: 4.08639, alpha: 0.02131, time: 33.27438
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 30.37281, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.02748, qf2_loss: 0.02879, policy_loss: -42.25769, policy_entropy: 4.08591, alpha: 0.02084, time: 33.57406
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 25.89792, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.03053, qf2_loss: 0.03195, policy_loss: -42.10758, policy_entropy: 4.08177, alpha: 0.02038, time: 33.84347
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 24.05732, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.01483, qf2_loss: 0.01522, policy_loss: -41.94763, policy_entropy: 4.07854, alpha: 0.01993, time: 33.48136
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 33.82638, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.02404, qf2_loss: 0.02521, policy_loss: -41.80405, policy_entropy: 4.08089, alpha: 0.01949, time: 33.54767
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 25.44230, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.02670, qf2_loss: 0.02783, policy_loss: -41.65101, policy_entropy: 4.08382, alpha: 0.01906, time: 33.69454
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 24.97681, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.01748, qf2_loss: 0.01831, policy_loss: -41.49160, policy_entropy: 4.07574, alpha: 0.01864, time: 33.68846
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 30.56635, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.02634, qf2_loss: 0.02755, policy_loss: -41.34142, policy_entropy: 4.07557, alpha: 0.01823, time: 33.56026
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 27.94184, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.01890, qf2_loss: 0.01938, policy_loss: -41.18368, policy_entropy: 4.07675, alpha: 0.01782, time: 33.69042
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 26.07106, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.02407, qf2_loss: 0.02527, policy_loss: -41.03849, policy_entropy: 4.06221, alpha: 0.01743, time: 33.76937
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 24.69803, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.02371, qf2_loss: 0.02462, policy_loss: -40.88587, policy_entropy: 4.06393, alpha: 0.01705, time: 34.10390
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 25.92982, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.02675, qf2_loss: 0.02798, policy_loss: -40.73280, policy_entropy: 4.06283, alpha: 0.01667, time: 34.13485
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 25.40338, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.02023, qf2_loss: 0.02114, policy_loss: -40.57898, policy_entropy: 4.06618, alpha: 0.01630, time: 33.47099
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 23.27026, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.01752, qf2_loss: 0.01820, policy_loss: -40.42693, policy_entropy: 4.06387, alpha: 0.01594, time: 33.42608
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 22.83071, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.02902, qf2_loss: 0.03017, policy_loss: -40.26664, policy_entropy: 4.04832, alpha: 0.01559, time: 33.44742
[CW] eval: return: 25.62608, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 26.52763, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.01900, qf2_loss: 0.01953, policy_loss: -40.11902, policy_entropy: 4.06163, alpha: 0.01525, time: 33.54554
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 27.69820, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.02637, qf2_loss: 0.02766, policy_loss: -39.96747, policy_entropy: 4.05282, alpha: 0.01491, time: 33.65445
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 27.91068, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.01912, qf2_loss: 0.02007, policy_loss: -39.81390, policy_entropy: 4.05510, alpha: 0.01458, time: 33.60275
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 26.65465, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.03113, qf2_loss: 0.03273, policy_loss: -39.66492, policy_entropy: 4.05900, alpha: 0.01426, time: 33.31098
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 23.00877, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.02095, qf2_loss: 0.02135, policy_loss: -39.50967, policy_entropy: 4.03580, alpha: 0.01394, time: 33.64608
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 25.61864, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.02443, qf2_loss: 0.02589, policy_loss: -39.35431, policy_entropy: 4.03880, alpha: 0.01364, time: 33.38337
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 24.01014, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.01514, qf2_loss: 0.01531, policy_loss: -39.20583, policy_entropy: 4.03002, alpha: 0.01334, time: 33.67066
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 30.75786, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.02529, qf2_loss: 0.02611, policy_loss: -39.05913, policy_entropy: 4.04481, alpha: 0.01304, time: 33.62597
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 27.79885, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.01720, qf2_loss: 0.01782, policy_loss: -38.91064, policy_entropy: 4.03807, alpha: 0.01275, time: 33.60071
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 22.46481, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.02801, qf2_loss: 0.02992, policy_loss: -38.75392, policy_entropy: 4.04206, alpha: 0.01247, time: 33.68976
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 28.62326, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.01774, qf2_loss: 0.01798, policy_loss: -38.60735, policy_entropy: 4.04028, alpha: 0.01220, time: 33.71916
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 25.68244, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.01916, qf2_loss: 0.01977, policy_loss: -38.45941, policy_entropy: 3.98990, alpha: 0.01193, time: 33.33622
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 26.01676, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.02057, qf2_loss: 0.02140, policy_loss: -38.31375, policy_entropy: 4.00034, alpha: 0.01167, time: 34.90291
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 26.00626, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.01548, qf2_loss: 0.01601, policy_loss: -38.16068, policy_entropy: 3.98585, alpha: 0.01141, time: 33.87726
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 20.90573, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.03365, qf2_loss: 0.03541, policy_loss: -38.01321, policy_entropy: 3.98062, alpha: 0.01116, time: 33.41807
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 24.82480, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.02018, qf2_loss: 0.02078, policy_loss: -37.86058, policy_entropy: 3.98581, alpha: 0.01091, time: 33.79866
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 29.12728, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.01661, qf2_loss: 0.01721, policy_loss: -37.71365, policy_entropy: 3.96589, alpha: 0.01067, time: 33.44143
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 26.66317, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.01659, qf2_loss: 0.01706, policy_loss: -37.56463, policy_entropy: 3.88110, alpha: 0.01044, time: 33.65105
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 24.90155, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.02106, qf2_loss: 0.02184, policy_loss: -37.42156, policy_entropy: 3.88635, alpha: 0.01021, time: 33.39044
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 25.02320, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.03706, qf2_loss: 0.03943, policy_loss: -37.26392, policy_entropy: 3.80354, alpha: 0.00999, time: 33.70674
[CW] eval: return: 23.13457, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 19.66546, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.01607, qf2_loss: 0.01588, policy_loss: -37.11564, policy_entropy: 3.69486, alpha: 0.00977, time: 33.58279
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 26.65724, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.02056, qf2_loss: 0.02147, policy_loss: -36.97810, policy_entropy: 3.59560, alpha: 0.00956, time: 33.56440
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 15.70766, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.01944, qf2_loss: 0.02037, policy_loss: -36.82576, policy_entropy: 3.45493, alpha: 0.00936, time: 33.37611
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 26.73698, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.01721, qf2_loss: 0.01756, policy_loss: -36.68936, policy_entropy: 3.48866, alpha: 0.00916, time: 33.48826
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 16.36045, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.01975, qf2_loss: 0.02082, policy_loss: -36.53398, policy_entropy: 3.52033, alpha: 0.00897, time: 38.91441
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 26.17456, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.03083, qf2_loss: 0.03181, policy_loss: -36.39935, policy_entropy: 3.32047, alpha: 0.00878, time: 33.54797
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 25.68830, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.02155, qf2_loss: 0.02247, policy_loss: -36.24993, policy_entropy: 3.42740, alpha: 0.00859, time: 33.42480
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 26.15172, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.01503, qf2_loss: 0.01575, policy_loss: -36.10758, policy_entropy: 3.26658, alpha: 0.00841, time: 33.64572
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 19.82560, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.03249, qf2_loss: 0.03418, policy_loss: -35.97310, policy_entropy: 3.01167, alpha: 0.00824, time: 33.82545
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 17.42755, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.01831, qf2_loss: 0.01859, policy_loss: -35.82302, policy_entropy: 3.43532, alpha: 0.00806, time: 33.90014
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 33.81868, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.01531, qf2_loss: 0.01556, policy_loss: -35.67116, policy_entropy: 3.20183, alpha: 0.00789, time: 33.99715
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 15.13635, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.02937, qf2_loss: 0.02972, policy_loss: -35.52558, policy_entropy: 3.09141, alpha: 0.00773, time: 33.95377
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 25.78026, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.01876, qf2_loss: 0.02115, policy_loss: -35.38447, policy_entropy: 2.89792, alpha: 0.00757, time: 33.81487
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 13.94840, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 0.01670, qf2_loss: 0.01778, policy_loss: -35.24247, policy_entropy: 3.36657, alpha: 0.00741, time: 33.72552
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 26.19919, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.02028, qf2_loss: 0.02175, policy_loss: -35.09136, policy_entropy: 3.16324, alpha: 0.00725, time: 33.97560
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 17.88475, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.01389, qf2_loss: 0.01479, policy_loss: -34.94228, policy_entropy: 3.02624, alpha: 0.00709, time: 33.66737
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 26.17233, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.02517, qf2_loss: 0.02755, policy_loss: -34.80936, policy_entropy: 3.37988, alpha: 0.00694, time: 33.73061
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 29.17887, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.02276, qf2_loss: 0.02321, policy_loss: -34.68027, policy_entropy: 3.26239, alpha: 0.00679, time: 33.87237
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 29.17919, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.02196, qf2_loss: 0.02380, policy_loss: -34.55347, policy_entropy: 3.04134, alpha: 0.00664, time: 33.69441
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 30.00195, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.03157, qf2_loss: 0.03278, policy_loss: -34.39669, policy_entropy: 2.83565, alpha: 0.00651, time: 33.84505
[CW] eval: return: 33.05540, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 38.29989, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.02596, qf2_loss: 0.02749, policy_loss: -34.28222, policy_entropy: 2.80484, alpha: 0.00637, time: 33.67778
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 35.42950, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.01926, qf2_loss: 0.02013, policy_loss: -34.14543, policy_entropy: 2.69142, alpha: 0.00624, time: 33.57008
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 30.01664, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.02432, qf2_loss: 0.02453, policy_loss: -34.01668, policy_entropy: 2.61468, alpha: 0.00611, time: 33.75112
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 42.40962, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.02947, qf2_loss: 0.03107, policy_loss: -33.88555, policy_entropy: 2.39933, alpha: 0.00598, time: 33.43228
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 32.09961, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.03184, qf2_loss: 0.03287, policy_loss: -33.76631, policy_entropy: 2.09401, alpha: 0.00586, time: 33.59975
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 33.10154, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.02722, qf2_loss: 0.02724, policy_loss: -33.63287, policy_entropy: 1.92016, alpha: 0.00575, time: 33.59205
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 42.92983, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.03185, qf2_loss: 0.03302, policy_loss: -33.52557, policy_entropy: 1.54354, alpha: 0.00564, time: 33.78062
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 45.01232, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 0.03263, qf2_loss: 0.03415, policy_loss: -33.40257, policy_entropy: 1.14745, alpha: 0.00554, time: 33.47047
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 37.36463, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.04789, qf2_loss: 0.04897, policy_loss: -33.28502, policy_entropy: 0.68168, alpha: 0.00544, time: 33.87452
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 40.44831, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.02765, qf2_loss: 0.02804, policy_loss: -33.18710, policy_entropy: -0.12063, alpha: 0.00535, time: 34.13901
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 39.38891, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 0.05162, qf2_loss: 0.05466, policy_loss: -33.07397, policy_entropy: -0.73755, alpha: 0.00527, time: 33.41314
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 47.42897, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.04167, qf2_loss: 0.04208, policy_loss: -32.98050, policy_entropy: -1.14557, alpha: 0.00520, time: 33.55537
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 43.58386, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.04636, qf2_loss: 0.04799, policy_loss: -32.88759, policy_entropy: -1.90047, alpha: 0.00513, time: 33.63758
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 40.48102, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.05754, qf2_loss: 0.05956, policy_loss: -32.80409, policy_entropy: -2.55368, alpha: 0.00508, time: 33.96147
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 42.53730, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.04446, qf2_loss: 0.04546, policy_loss: -32.70332, policy_entropy: -4.61675, alpha: 0.00504, time: 33.63808
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 22.01790, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.03942, qf2_loss: 0.03972, policy_loss: -32.62266, policy_entropy: -6.76459, alpha: 0.00503, time: 33.66010
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 60.36677, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 0.04957, qf2_loss: 0.05075, policy_loss: -32.53390, policy_entropy: -5.49392, alpha: 0.00504, time: 33.52353
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 60.57518, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 0.04237, qf2_loss: 0.04250, policy_loss: -32.45778, policy_entropy: -4.26543, alpha: 0.00502, time: 33.34558
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 25.11146, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 0.06101, qf2_loss: 0.06211, policy_loss: -32.39318, policy_entropy: -4.65974, alpha: 0.00499, time: 34.34921
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 32.91218, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.04713, qf2_loss: 0.04715, policy_loss: -32.35399, policy_entropy: -4.68882, alpha: 0.00497, time: 33.55342
[CW] eval: return: 32.14440, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 27.37639, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 0.04336, qf2_loss: 0.04054, policy_loss: -32.31156, policy_entropy: -4.71403, alpha: 0.00494, time: 33.95235
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 27.59828, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 0.04758, qf2_loss: 0.04698, policy_loss: -32.27015, policy_entropy: -5.35294, alpha: 0.00492, time: 33.97683
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 57.60622, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 0.06219, qf2_loss: 0.06343, policy_loss: -32.19951, policy_entropy: -4.66201, alpha: 0.00490, time: 33.57756
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 38.78284, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 0.05672, qf2_loss: 0.05490, policy_loss: -32.16360, policy_entropy: -5.14572, alpha: 0.00488, time: 33.90863
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 9.99885, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 0.04500, qf2_loss: 0.04355, policy_loss: -32.12018, policy_entropy: -4.37297, alpha: 0.00485, time: 33.58033
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 39.46865, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 0.05466, qf2_loss: 0.05488, policy_loss: -32.08652, policy_entropy: -5.44988, alpha: 0.00482, time: 33.62387
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 22.63682, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 0.05824, qf2_loss: 0.05861, policy_loss: -32.05954, policy_entropy: -5.96153, alpha: 0.00482, time: 33.42668
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 25.03570, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 0.06804, qf2_loss: 0.07027, policy_loss: -31.99477, policy_entropy: -4.76248, alpha: 0.00481, time: 33.55491
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 43.59247, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 0.04961, qf2_loss: 0.04884, policy_loss: -31.86919, policy_entropy: -3.09457, alpha: 0.00475, time: 33.75980
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 45.17835, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 0.05063, qf2_loss: 0.05007, policy_loss: -31.80965, policy_entropy: -4.40721, alpha: 0.00468, time: 33.64179
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 44.38784, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 0.06002, qf2_loss: 0.06224, policy_loss: -31.72333, policy_entropy: -3.64653, alpha: 0.00464, time: 33.62625
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 46.49704, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 0.05492, qf2_loss: 0.05479, policy_loss: -31.60538, policy_entropy: -2.00306, alpha: 0.00455, time: 33.28693
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 20.52364, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 0.06807, qf2_loss: 0.07076, policy_loss: -31.51212, policy_entropy: -2.46991, alpha: 0.00445, time: 33.81723
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 25.21254, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 0.06932, qf2_loss: 0.07120, policy_loss: -31.39545, policy_entropy: -4.00714, alpha: 0.00437, time: 33.54044
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 34.27518, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 0.04531, qf2_loss: 0.04578, policy_loss: -31.30637, policy_entropy: -4.92342, alpha: 0.00433, time: 34.38022
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 26.09604, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 0.04446, qf2_loss: 0.04459, policy_loss: -31.19530, policy_entropy: -4.91607, alpha: 0.00431, time: 33.53721
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 33.84150, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 0.04066, qf2_loss: 0.04086, policy_loss: -31.10931, policy_entropy: -4.90346, alpha: 0.00427, time: 33.54781
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 17.64213, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 0.03979, qf2_loss: 0.03984, policy_loss: -31.04621, policy_entropy: -5.11537, alpha: 0.00424, time: 33.46329
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 21.24921, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 0.04055, qf2_loss: 0.04071, policy_loss: -30.93089, policy_entropy: -4.54651, alpha: 0.00421, time: 33.59437
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 53.34373, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 0.04375, qf2_loss: 0.04449, policy_loss: -30.86441, policy_entropy: -3.36020, alpha: 0.00416, time: 33.46289
[CW] eval: return: 42.69251, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 78.78877, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 0.04872, qf2_loss: 0.04961, policy_loss: -30.78913, policy_entropy: -4.54471, alpha: 0.00408, time: 33.54741
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 17.27352, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 0.04762, qf2_loss: 0.04856, policy_loss: -30.69523, policy_entropy: -3.43956, alpha: 0.00403, time: 33.67220
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 46.85937, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 0.05486, qf2_loss: 0.05649, policy_loss: -30.61235, policy_entropy: -3.19588, alpha: 0.00395, time: 33.44365
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 93.81595, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 0.04199, qf2_loss: 0.04197, policy_loss: -30.51798, policy_entropy: -2.44628, alpha: 0.00385, time: 33.55870
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 44.88457, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 0.05559, qf2_loss: 0.05717, policy_loss: -30.49716, policy_entropy: -3.70099, alpha: 0.00376, time: 33.51715
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 74.15596, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 0.05515, qf2_loss: 0.05582, policy_loss: -30.43237, policy_entropy: -3.29679, alpha: 0.00369, time: 33.45876
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 44.75172, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 0.05655, qf2_loss: 0.05816, policy_loss: -30.33591, policy_entropy: -3.05109, alpha: 0.00361, time: 33.48111
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 45.94272, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 0.05603, qf2_loss: 0.05694, policy_loss: -30.27155, policy_entropy: -3.61486, alpha: 0.00354, time: 33.78955
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 52.12122, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 0.04816, qf2_loss: 0.04838, policy_loss: -30.25089, policy_entropy: -3.70446, alpha: 0.00348, time: 33.53802
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 50.57212, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 0.04513, qf2_loss: 0.04477, policy_loss: -30.12006, policy_entropy: -3.38173, alpha: 0.00341, time: 33.51050
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 79.12227, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 0.04577, qf2_loss: 0.04664, policy_loss: -30.03126, policy_entropy: -4.13862, alpha: 0.00335, time: 33.79485
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 44.77665, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 0.05126, qf2_loss: 0.05212, policy_loss: -29.97237, policy_entropy: -4.38197, alpha: 0.00330, time: 33.73209
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 58.50668, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 0.04758, qf2_loss: 0.04753, policy_loss: -29.89505, policy_entropy: -3.99800, alpha: 0.00326, time: 33.67430
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 44.97280, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 0.03847, qf2_loss: 0.03875, policy_loss: -29.90070, policy_entropy: -3.96012, alpha: 0.00320, time: 33.35071
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 44.81065, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 0.04193, qf2_loss: 0.04348, policy_loss: -29.81534, policy_entropy: -3.69770, alpha: 0.00315, time: 33.43052
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 30.69783, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 0.04793, qf2_loss: 0.04985, policy_loss: -29.68225, policy_entropy: -2.98114, alpha: 0.00308, time: 33.04670
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 24.91674, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 0.03883, qf2_loss: 0.03935, policy_loss: -29.64022, policy_entropy: -3.00832, alpha: 0.00300, time: 35.55945
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 24.81555, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 0.04385, qf2_loss: 0.04468, policy_loss: -29.58386, policy_entropy: -2.45870, alpha: 0.00292, time: 33.79840
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 43.59442, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 0.05813, qf2_loss: 0.05933, policy_loss: -29.49424, policy_entropy: -2.24227, alpha: 0.00284, time: 33.60720
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 26.28507, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 0.04816, qf2_loss: 0.04978, policy_loss: -29.42945, policy_entropy: -2.55694, alpha: 0.00276, time: 33.51258
[CW] eval: return: 37.15668, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 25.24000, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 0.02836, qf2_loss: 0.02864, policy_loss: -29.29539, policy_entropy: -3.58833, alpha: 0.00269, time: 33.43329
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 44.31729, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 0.02958, qf2_loss: 0.02983, policy_loss: -29.23316, policy_entropy: -3.83483, alpha: 0.00264, time: 33.12973
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 54.55073, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 0.03024, qf2_loss: 0.03037, policy_loss: -29.15506, policy_entropy: -2.44377, alpha: 0.00259, time: 33.42588
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 49.15409, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 0.03108, qf2_loss: 0.03217, policy_loss: -29.02100, policy_entropy: -3.71899, alpha: 0.00253, time: 33.30232
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 23.62849, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 0.04355, qf2_loss: 0.04526, policy_loss: -28.97521, policy_entropy: -3.10847, alpha: 0.00248, time: 33.56184
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 43.89322, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 0.03023, qf2_loss: 0.03056, policy_loss: -28.90616, policy_entropy: -3.12293, alpha: 0.00242, time: 32.95703
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 85.06777, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 0.02887, qf2_loss: 0.02924, policy_loss: -28.85942, policy_entropy: -3.46717, alpha: 0.00237, time: 33.56720
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 42.75478, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 0.03360, qf2_loss: 0.03358, policy_loss: -28.74279, policy_entropy: -3.48993, alpha: 0.00232, time: 33.77951
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 44.30418, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 0.03195, qf2_loss: 0.03238, policy_loss: -28.69684, policy_entropy: -4.50030, alpha: 0.00229, time: 33.65084
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 45.19612, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 0.02612, qf2_loss: 0.02574, policy_loss: -28.59892, policy_entropy: -4.94655, alpha: 0.00226, time: 33.09698
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 45.02278, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 0.02938, qf2_loss: 0.02936, policy_loss: -28.48090, policy_entropy: -3.00006, alpha: 0.00223, time: 33.55836
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 66.20184, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 0.02985, qf2_loss: 0.02951, policy_loss: -28.39912, policy_entropy: -3.08302, alpha: 0.00217, time: 33.42066
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 58.44529, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 0.03373, qf2_loss: 0.03494, policy_loss: -28.36242, policy_entropy: -3.13250, alpha: 0.00212, time: 33.62177
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 50.27747, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 0.03874, qf2_loss: 0.04018, policy_loss: -28.28729, policy_entropy: -3.49075, alpha: 0.00208, time: 33.12751
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 56.08056, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 0.03404, qf2_loss: 0.03496, policy_loss: -28.18077, policy_entropy: -3.26241, alpha: 0.00203, time: 33.92476
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 44.35176, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 0.03085, qf2_loss: 0.03132, policy_loss: -28.10045, policy_entropy: -3.15364, alpha: 0.00199, time: 33.60697
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 43.76624, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 0.03159, qf2_loss: 0.03102, policy_loss: -28.05440, policy_entropy: -4.13683, alpha: 0.00195, time: 33.70075
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 49.28890, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 0.03400, qf2_loss: 0.03544, policy_loss: -27.97192, policy_entropy: -4.24208, alpha: 0.00192, time: 33.48116
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 81.03462, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 0.02711, qf2_loss: 0.02842, policy_loss: -27.91569, policy_entropy: -4.17868, alpha: 0.00189, time: 33.42411
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 43.21713, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 0.02620, qf2_loss: 0.02666, policy_loss: -27.82185, policy_entropy: -5.32284, alpha: 0.00187, time: 33.43958
[CW] eval: return: 53.94698, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 44.30386, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 0.02314, qf2_loss: 0.02319, policy_loss: -27.70001, policy_entropy: -5.32856, alpha: 0.00186, time: 33.19850
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 44.41602, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 0.03374, qf2_loss: 0.03515, policy_loss: -27.67357, policy_entropy: -4.01659, alpha: 0.00184, time: 33.89955
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 47.52068, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 0.03239, qf2_loss: 0.03297, policy_loss: -27.57610, policy_entropy: -4.14686, alpha: 0.00181, time: 33.54309
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 42.61699, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 0.03145, qf2_loss: 0.03258, policy_loss: -27.53966, policy_entropy: -4.24097, alpha: 0.00177, time: 33.95651
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 68.56851, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 0.03436, qf2_loss: 0.03506, policy_loss: -27.44604, policy_entropy: -4.29625, alpha: 0.00174, time: 33.32918
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 67.38112, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 0.03725, qf2_loss: 0.03832, policy_loss: -27.37017, policy_entropy: -5.01094, alpha: 0.00172, time: 33.33447
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 43.84487, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 0.02735, qf2_loss: 0.02829, policy_loss: -27.24113, policy_entropy: -4.56689, alpha: 0.00170, time: 35.84237
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 43.82163, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 0.02739, qf2_loss: 0.02750, policy_loss: -27.19351, policy_entropy: -4.27898, alpha: 0.00167, time: 33.28521
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 43.75006, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 0.02711, qf2_loss: 0.02717, policy_loss: -27.16596, policy_entropy: -5.86742, alpha: 0.00165, time: 33.51534
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 43.91398, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 0.02667, qf2_loss: 0.02716, policy_loss: -27.10514, policy_entropy: -6.31864, alpha: 0.00166, time: 33.58554
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 28.93177, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 0.03149, qf2_loss: 0.03233, policy_loss: -27.07007, policy_entropy: -7.28226, alpha: 0.00168, time: 33.67682
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 100.10884, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 0.03135, qf2_loss: 0.03265, policy_loss: -26.95809, policy_entropy: -5.85833, alpha: 0.00168, time: 33.40460
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 42.28813, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 0.04245, qf2_loss: 0.04338, policy_loss: -26.89124, policy_entropy: -5.94100, alpha: 0.00169, time: 33.30547
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 82.09178, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 0.03306, qf2_loss: 0.03408, policy_loss: -26.84919, policy_entropy: -3.90173, alpha: 0.00166, time: 33.45342
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 44.31845, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 0.02911, qf2_loss: 0.02929, policy_loss: -26.70567, policy_entropy: -6.03827, alpha: 0.00163, time: 33.62942
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 57.37185, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 0.03134, qf2_loss: 0.03270, policy_loss: -26.57642, policy_entropy: -6.39387, alpha: 0.00164, time: 33.34109
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 42.12733, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 0.03325, qf2_loss: 0.03379, policy_loss: -26.58047, policy_entropy: -7.00533, alpha: 0.00165, time: 33.45858
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 50.26313, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 0.03188, qf2_loss: 0.03220, policy_loss: -26.49123, policy_entropy: -8.07235, alpha: 0.00169, time: 33.34814
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 42.80104, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 0.02967, qf2_loss: 0.02994, policy_loss: -26.42361, policy_entropy: -7.11420, alpha: 0.00173, time: 33.17361
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 64.17267, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 0.02846, qf2_loss: 0.02821, policy_loss: -26.36005, policy_entropy: -6.04297, alpha: 0.00175, time: 33.48774
[CW] eval: return: 54.98524, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 53.50404, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 0.03516, qf2_loss: 0.03664, policy_loss: -26.19367, policy_entropy: -6.26932, alpha: 0.00174, time: 33.37064
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 45.11222, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 0.03440, qf2_loss: 0.03546, policy_loss: -26.18991, policy_entropy: -7.36466, alpha: 0.00177, time: 33.42119
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 42.59914, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 0.02759, qf2_loss: 0.02834, policy_loss: -26.10043, policy_entropy: -7.64652, alpha: 0.00182, time: 33.52780
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 54.50879, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 0.04059, qf2_loss: 0.04180, policy_loss: -26.06344, policy_entropy: -6.77103, alpha: 0.00186, time: 33.28531
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 43.13763, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 0.03186, qf2_loss: 0.03272, policy_loss: -25.96824, policy_entropy: -6.04507, alpha: 0.00187, time: 33.13362
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 45.40649, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 0.03418, qf2_loss: 0.03472, policy_loss: -25.87755, policy_entropy: -5.73990, alpha: 0.00187, time: 33.23135
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 46.15659, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 0.03809, qf2_loss: 0.03810, policy_loss: -25.78056, policy_entropy: -6.57709, alpha: 0.00186, time: 33.32562
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 103.58507, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 0.03807, qf2_loss: 0.03940, policy_loss: -25.72724, policy_entropy: -5.43308, alpha: 0.00188, time: 33.16477
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 51.81837, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 0.03638, qf2_loss: 0.03700, policy_loss: -25.68387, policy_entropy: -6.98489, alpha: 0.00188, time: 33.50473
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 91.98927, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 0.04423, qf2_loss: 0.04525, policy_loss: -25.62575, policy_entropy: -6.50370, alpha: 0.00191, time: 33.26889
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 22.13964, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 0.04143, qf2_loss: 0.04052, policy_loss: -25.48402, policy_entropy: -6.82562, alpha: 0.00194, time: 33.84306
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 103.51720, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 0.04268, qf2_loss: 0.04405, policy_loss: -25.48384, policy_entropy: -4.86401, alpha: 0.00193, time: 33.62648
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 55.13284, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 0.03988, qf2_loss: 0.04191, policy_loss: -25.36009, policy_entropy: -6.36779, alpha: 0.00191, time: 33.46830
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 49.60038, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 0.03599, qf2_loss: 0.03622, policy_loss: -25.33392, policy_entropy: -7.13931, alpha: 0.00194, time: 33.89390
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 122.22187, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 0.04260, qf2_loss: 0.04321, policy_loss: -25.29528, policy_entropy: -7.27097, alpha: 0.00199, time: 33.72578
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 66.14913, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 0.04615, qf2_loss: 0.04649, policy_loss: -25.32311, policy_entropy: -8.72041, alpha: 0.00207, time: 33.31118
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 85.17972, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 0.09545, qf2_loss: 0.10005, policy_loss: -25.15893, policy_entropy: -10.09221, alpha: 0.00221, time: 33.09137
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 22.45955, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 0.04888, qf2_loss: 0.04816, policy_loss: -25.08380, policy_entropy: -9.90632, alpha: 0.00237, time: 33.55084
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 120.19337, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 0.04983, qf2_loss: 0.05030, policy_loss: -25.05497, policy_entropy: -7.87250, alpha: 0.00247, time: 33.75454
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 103.77139, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 0.04900, qf2_loss: 0.04982, policy_loss: -25.14249, policy_entropy: -8.21921, alpha: 0.00254, time: 34.00631
[CW] eval: return: 92.36328, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 94.12302, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 0.06102, qf2_loss: 0.06223, policy_loss: -25.07987, policy_entropy: -8.02327, alpha: 0.00262, time: 33.62035
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 104.88879, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 0.04737, qf2_loss: 0.04829, policy_loss: -24.97239, policy_entropy: -8.36733, alpha: 0.00271, time: 33.40812
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 97.05912, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 0.05136, qf2_loss: 0.05200, policy_loss: -24.94212, policy_entropy: -8.49936, alpha: 0.00281, time: 33.37532
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 106.67693, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 0.06818, qf2_loss: 0.06985, policy_loss: -24.98520, policy_entropy: -8.49961, alpha: 0.00291, time: 33.29761
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 126.71967, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 0.06430, qf2_loss: 0.06480, policy_loss: -24.97367, policy_entropy: -7.67907, alpha: 0.00301, time: 33.55153
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 74.21086, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 0.08815, qf2_loss: 0.09272, policy_loss: -24.96185, policy_entropy: -8.24673, alpha: 0.00310, time: 33.41177
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 109.74074, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 0.07654, qf2_loss: 0.07825, policy_loss: -24.98493, policy_entropy: -8.33188, alpha: 0.00321, time: 33.45891
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 109.45321, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 0.06112, qf2_loss: 0.06204, policy_loss: -24.99602, policy_entropy: -8.30022, alpha: 0.00332, time: 33.40455
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 102.73654, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 0.07118, qf2_loss: 0.07199, policy_loss: -25.02516, policy_entropy: -8.33540, alpha: 0.00345, time: 33.90589
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 89.56687, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 0.08072, qf2_loss: 0.08145, policy_loss: -24.94780, policy_entropy: -7.92413, alpha: 0.00356, time: 33.56730
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 108.28477, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 0.07399, qf2_loss: 0.07427, policy_loss: -25.00327, policy_entropy: -8.14602, alpha: 0.00368, time: 34.06525
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 107.37385, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 0.07221, qf2_loss: 0.07221, policy_loss: -24.90682, policy_entropy: -7.70534, alpha: 0.00380, time: 33.30187
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 78.12581, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 0.07918, qf2_loss: 0.08007, policy_loss: -24.95141, policy_entropy: -8.00433, alpha: 0.00391, time: 33.50324
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 89.43909, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 0.07420, qf2_loss: 0.07525, policy_loss: -24.90538, policy_entropy: -8.21724, alpha: 0.00405, time: 33.42538
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 109.47033, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 0.09266, qf2_loss: 0.09470, policy_loss: -24.93141, policy_entropy: -7.91414, alpha: 0.00420, time: 33.53743
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 124.41337, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 0.08102, qf2_loss: 0.08162, policy_loss: -24.91647, policy_entropy: -7.46116, alpha: 0.00432, time: 33.28020
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 104.83495, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 0.07596, qf2_loss: 0.07713, policy_loss: -24.93264, policy_entropy: -7.80987, alpha: 0.00444, time: 33.48644
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 107.54875, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 0.08076, qf2_loss: 0.08053, policy_loss: -24.94736, policy_entropy: -7.62388, alpha: 0.00458, time: 33.78838
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 108.92882, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 0.09292, qf2_loss: 0.09384, policy_loss: -24.88555, policy_entropy: -7.69114, alpha: 0.00472, time: 33.44369
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 85.52588, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 0.07559, qf2_loss: 0.07666, policy_loss: -24.88433, policy_entropy: -7.01499, alpha: 0.00484, time: 33.40310
[CW] eval: return: 102.02017, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 143.99488, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 0.08864, qf2_loss: 0.09192, policy_loss: -24.89685, policy_entropy: -6.59825, alpha: 0.00492, time: 33.37188
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 114.42586, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 0.06984, qf2_loss: 0.07020, policy_loss: -24.91433, policy_entropy: -6.93552, alpha: 0.00498, time: 33.47351
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 61.39268, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 0.08414, qf2_loss: 0.08427, policy_loss: -24.83617, policy_entropy: -7.26766, alpha: 0.00510, time: 33.36479
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 131.10520, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 0.08196, qf2_loss: 0.08456, policy_loss: -24.88645, policy_entropy: -6.67385, alpha: 0.00521, time: 33.34875
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 43.10109, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 0.07687, qf2_loss: 0.07672, policy_loss: -24.79815, policy_entropy: -6.76583, alpha: 0.00529, time: 33.61428
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 149.17004, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 0.07305, qf2_loss: 0.07418, policy_loss: -24.73544, policy_entropy: -6.20305, alpha: 0.00536, time: 33.44600
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 131.57992, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 0.08534, qf2_loss: 0.08664, policy_loss: -24.77460, policy_entropy: -5.86608, alpha: 0.00536, time: 33.54283
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 122.51343, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 0.06685, qf2_loss: 0.06749, policy_loss: -24.73863, policy_entropy: -6.02858, alpha: 0.00533, time: 33.25611
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 102.99401, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 0.06781, qf2_loss: 0.06923, policy_loss: -24.68792, policy_entropy: -6.31285, alpha: 0.00538, time: 33.41139
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 114.25928, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 0.07191, qf2_loss: 0.07202, policy_loss: -24.63454, policy_entropy: -5.86680, alpha: 0.00538, time: 33.52705
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 135.09238, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 0.06837, qf2_loss: 0.06848, policy_loss: -24.65193, policy_entropy: -6.25323, alpha: 0.00539, time: 33.60163
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 130.90720, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 0.06699, qf2_loss: 0.06760, policy_loss: -24.64006, policy_entropy: -5.53595, alpha: 0.00539, time: 33.43071
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 116.37532, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 0.07526, qf2_loss: 0.07603, policy_loss: -24.55127, policy_entropy: -5.21843, alpha: 0.00529, time: 37.53346
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 140.50187, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 0.06999, qf2_loss: 0.07028, policy_loss: -24.55313, policy_entropy: -4.76670, alpha: 0.00517, time: 33.43977
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 133.95084, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 0.07689, qf2_loss: 0.07953, policy_loss: -24.52517, policy_entropy: -5.94975, alpha: 0.00504, time: 33.67770
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 119.45487, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 0.07631, qf2_loss: 0.07621, policy_loss: -24.55843, policy_entropy: -6.90535, alpha: 0.00510, time: 33.39693
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 135.00724, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 0.06881, qf2_loss: 0.06804, policy_loss: -24.47169, policy_entropy: -6.17754, alpha: 0.00520, time: 33.18290
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 94.87478, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 0.08265, qf2_loss: 0.08525, policy_loss: -24.46101, policy_entropy: -6.11037, alpha: 0.00523, time: 33.69615
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 113.10588, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 0.09308, qf2_loss: 0.09388, policy_loss: -24.49309, policy_entropy: -6.75126, alpha: 0.00526, time: 33.45937
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 30.69687, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 0.09386, qf2_loss: 0.09542, policy_loss: -24.43332, policy_entropy: -7.30291, alpha: 0.00543, time: 33.57761
[CW] eval: return: 72.78773, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 133.91947, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 0.08548, qf2_loss: 0.08753, policy_loss: -24.41909, policy_entropy: -6.89963, alpha: 0.00559, time: 33.32084
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 9.98476, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 0.07879, qf2_loss: 0.07877, policy_loss: -24.39797, policy_entropy: -6.13775, alpha: 0.00569, time: 33.54068
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 126.25744, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 0.08463, qf2_loss: 0.08588, policy_loss: -24.40483, policy_entropy: -5.81906, alpha: 0.00569, time: 33.61628
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 109.99755, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 0.08914, qf2_loss: 0.09111, policy_loss: -24.40526, policy_entropy: -5.56327, alpha: 0.00564, time: 33.38882
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 132.88604, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 0.06740, qf2_loss: 0.06773, policy_loss: -24.35358, policy_entropy: -5.60573, alpha: 0.00556, time: 33.64679
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 123.74130, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 0.07809, qf2_loss: 0.07761, policy_loss: -24.38458, policy_entropy: -5.59547, alpha: 0.00550, time: 33.59770
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 124.68649, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 0.07031, qf2_loss: 0.07029, policy_loss: -24.30610, policy_entropy: -5.81840, alpha: 0.00545, time: 33.42006
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 123.70191, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 0.06758, qf2_loss: 0.06801, policy_loss: -24.31422, policy_entropy: -5.39857, alpha: 0.00539, time: 33.42208
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 36.64860, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 0.07307, qf2_loss: 0.07455, policy_loss: -24.31699, policy_entropy: -5.45450, alpha: 0.00529, time: 33.44182
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 135.71136, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 0.07916, qf2_loss: 0.08115, policy_loss: -24.26985, policy_entropy: -5.35677, alpha: 0.00518, time: 33.78537
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 157.30928, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 0.07284, qf2_loss: 0.07254, policy_loss: -24.26083, policy_entropy: -5.65607, alpha: 0.00509, time: 33.77616
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 156.68705, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 0.07119, qf2_loss: 0.07171, policy_loss: -24.28984, policy_entropy: -5.76807, alpha: 0.00504, time: 33.29435
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 120.64497, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 0.08789, qf2_loss: 0.08895, policy_loss: -24.26060, policy_entropy: -5.50558, alpha: 0.00499, time: 33.67637
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 156.95057, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 0.08016, qf2_loss: 0.08201, policy_loss: -24.26286, policy_entropy: -5.90762, alpha: 0.00494, time: 33.26454
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 132.90342, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 0.06652, qf2_loss: 0.06629, policy_loss: -24.23262, policy_entropy: -6.22227, alpha: 0.00494, time: 33.83709
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 166.94243, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 0.06530, qf2_loss: 0.06532, policy_loss: -24.32595, policy_entropy: -5.97928, alpha: 0.00496, time: 33.42138
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 157.30720, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 0.06030, qf2_loss: 0.06136, policy_loss: -24.29484, policy_entropy: -5.87911, alpha: 0.00495, time: 33.37715
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 152.86332, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 0.06339, qf2_loss: 0.06376, policy_loss: -24.26961, policy_entropy: -5.88163, alpha: 0.00493, time: 33.58777
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 132.03363, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 0.08155, qf2_loss: 0.08042, policy_loss: -24.26276, policy_entropy: -5.78239, alpha: 0.00491, time: 33.84888
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 133.39160, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 0.06498, qf2_loss: 0.06702, policy_loss: -24.26295, policy_entropy: -5.86003, alpha: 0.00488, time: 33.88043
[CW] eval: return: 113.40145, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 128.56779, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 0.06808, qf2_loss: 0.06872, policy_loss: -24.18640, policy_entropy: -5.56448, alpha: 0.00481, time: 33.36091
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 168.34371, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 0.06228, qf2_loss: 0.06204, policy_loss: -24.20218, policy_entropy: -5.84083, alpha: 0.00476, time: 33.32302
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 136.56568, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 0.05874, qf2_loss: 0.05895, policy_loss: -24.22839, policy_entropy: -5.80094, alpha: 0.00472, time: 33.50118
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 142.17963, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 0.08277, qf2_loss: 0.08466, policy_loss: -24.17172, policy_entropy: -6.23207, alpha: 0.00471, time: 33.18563
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 168.51285, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 0.06333, qf2_loss: 0.06342, policy_loss: -24.29860, policy_entropy: -6.15079, alpha: 0.00474, time: 33.15116
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 84.14562, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 0.06100, qf2_loss: 0.06104, policy_loss: -24.21075, policy_entropy: -6.15229, alpha: 0.00479, time: 33.34779
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 156.94642, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 0.05696, qf2_loss: 0.05846, policy_loss: -24.16937, policy_entropy: -6.08591, alpha: 0.00483, time: 33.46229
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 159.61398, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 0.06003, qf2_loss: 0.06124, policy_loss: -24.28070, policy_entropy: -5.87087, alpha: 0.00481, time: 33.63999
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 115.69437, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 0.06282, qf2_loss: 0.06305, policy_loss: -24.16623, policy_entropy: -6.06167, alpha: 0.00480, time: 33.68742
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 161.32095, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 0.05891, qf2_loss: 0.06063, policy_loss: -24.22432, policy_entropy: -5.75817, alpha: 0.00480, time: 33.47943
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 144.22449, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 0.05889, qf2_loss: 0.06087, policy_loss: -24.17024, policy_entropy: -5.56625, alpha: 0.00471, time: 34.18655
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 150.14582, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 0.06251, qf2_loss: 0.06170, policy_loss: -24.14366, policy_entropy: -5.61188, alpha: 0.00464, time: 33.76391
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 142.29992, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 0.06315, qf2_loss: 0.06406, policy_loss: -24.17249, policy_entropy: -6.49870, alpha: 0.00462, time: 33.42850
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 150.88284, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 0.08247, qf2_loss: 0.08454, policy_loss: -24.19763, policy_entropy: -6.27057, alpha: 0.00471, time: 33.68372
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 136.24897, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 0.07403, qf2_loss: 0.07593, policy_loss: -24.18659, policy_entropy: -6.09060, alpha: 0.00476, time: 33.90492
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 146.13976, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 0.05622, qf2_loss: 0.05618, policy_loss: -24.18635, policy_entropy: -6.58280, alpha: 0.00482, time: 33.47966
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 139.52424, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 0.06474, qf2_loss: 0.06634, policy_loss: -24.19848, policy_entropy: -6.33887, alpha: 0.00494, time: 33.62876
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 135.09681, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 0.06107, qf2_loss: 0.06120, policy_loss: -24.14730, policy_entropy: -5.91111, alpha: 0.00497, time: 33.76908
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 172.33542, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 0.09033, qf2_loss: 0.09168, policy_loss: -24.16993, policy_entropy: -6.03485, alpha: 0.00493, time: 33.89790
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 141.41000, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 0.06333, qf2_loss: 0.06521, policy_loss: -24.14293, policy_entropy: -6.08283, alpha: 0.00496, time: 33.77823
[CW] eval: return: 151.20972, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 139.64387, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 0.06096, qf2_loss: 0.06179, policy_loss: -24.13782, policy_entropy: -5.69481, alpha: 0.00492, time: 33.67200
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 169.46894, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 0.06684, qf2_loss: 0.06761, policy_loss: -24.26937, policy_entropy: -6.29621, alpha: 0.00491, time: 33.52520
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 170.38229, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 0.06099, qf2_loss: 0.06143, policy_loss: -24.23206, policy_entropy: -6.18269, alpha: 0.00498, time: 33.66983
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 147.17296, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 0.05924, qf2_loss: 0.05828, policy_loss: -24.20150, policy_entropy: -6.03463, alpha: 0.00501, time: 33.66928
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 165.75304, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 0.05773, qf2_loss: 0.05824, policy_loss: -24.10702, policy_entropy: -5.97559, alpha: 0.00500, time: 33.35013
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 153.57215, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 0.06017, qf2_loss: 0.06064, policy_loss: -24.20838, policy_entropy: -5.85911, alpha: 0.00501, time: 33.33044
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 172.20615, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 0.06340, qf2_loss: 0.06438, policy_loss: -24.29097, policy_entropy: -5.89234, alpha: 0.00496, time: 33.42577
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 168.88223, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 0.06418, qf2_loss: 0.06493, policy_loss: -24.21497, policy_entropy: -5.92544, alpha: 0.00494, time: 33.52748
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 112.96326, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 0.06798, qf2_loss: 0.06817, policy_loss: -24.22887, policy_entropy: -5.86604, alpha: 0.00491, time: 33.62288
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 142.87774, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 0.07679, qf2_loss: 0.07722, policy_loss: -24.23095, policy_entropy: -6.37208, alpha: 0.00494, time: 33.67475
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 134.40884, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 0.08114, qf2_loss: 0.08395, policy_loss: -24.18997, policy_entropy: -6.48768, alpha: 0.00503, time: 33.33858
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 165.27399, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 0.07269, qf2_loss: 0.07213, policy_loss: -24.19780, policy_entropy: -6.29739, alpha: 0.00515, time: 33.98674
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 123.95484, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 0.06324, qf2_loss: 0.06378, policy_loss: -24.25812, policy_entropy: -5.99798, alpha: 0.00518, time: 33.24766
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 130.78452, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 0.06770, qf2_loss: 0.06992, policy_loss: -24.25059, policy_entropy: -5.89121, alpha: 0.00517, time: 33.73521
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 130.69115, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 0.06456, qf2_loss: 0.06500, policy_loss: -24.25457, policy_entropy: -6.33238, alpha: 0.00518, time: 33.61935
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 151.09156, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 0.06583, qf2_loss: 0.06550, policy_loss: -24.26365, policy_entropy: -6.06240, alpha: 0.00525, time: 34.08112
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 169.73686, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 0.07375, qf2_loss: 0.07479, policy_loss: -24.29133, policy_entropy: -6.07869, alpha: 0.00528, time: 33.41167
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 164.97465, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 0.07179, qf2_loss: 0.07376, policy_loss: -24.28933, policy_entropy: -5.70074, alpha: 0.00524, time: 33.88658
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 161.46269, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 0.07800, qf2_loss: 0.07907, policy_loss: -24.35614, policy_entropy: -5.79505, alpha: 0.00515, time: 33.86761
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 173.34662, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 0.07087, qf2_loss: 0.07077, policy_loss: -24.33148, policy_entropy: -5.95484, alpha: 0.00514, time: 33.79313
[CW] eval: return: 136.60074, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 150.72955, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 0.07282, qf2_loss: 0.07362, policy_loss: -24.35689, policy_entropy: -5.53932, alpha: 0.00508, time: 33.49100
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 167.84380, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 0.06595, qf2_loss: 0.06696, policy_loss: -24.26122, policy_entropy: -6.05101, alpha: 0.00499, time: 33.50805
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 143.07033, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 0.06170, qf2_loss: 0.06209, policy_loss: -24.34250, policy_entropy: -5.77803, alpha: 0.00501, time: 33.59246
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 152.28373, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 0.06840, qf2_loss: 0.06985, policy_loss: -24.35902, policy_entropy: -6.03748, alpha: 0.00496, time: 33.46727
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 157.13592, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 0.06761, qf2_loss: 0.06786, policy_loss: -24.37373, policy_entropy: -6.17302, alpha: 0.00500, time: 33.43718
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 124.55938, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 0.08411, qf2_loss: 0.08467, policy_loss: -24.40589, policy_entropy: -5.57433, alpha: 0.00499, time: 33.42427
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 87.57498, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 0.06304, qf2_loss: 0.06390, policy_loss: -24.34769, policy_entropy: -5.71460, alpha: 0.00488, time: 33.45283
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 139.40971, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 0.07099, qf2_loss: 0.07177, policy_loss: -24.37967, policy_entropy: -5.82021, alpha: 0.00485, time: 33.61708
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 130.32140, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 0.07051, qf2_loss: 0.07207, policy_loss: -24.33366, policy_entropy: -5.83978, alpha: 0.00481, time: 33.92637
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 153.58005, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 0.06709, qf2_loss: 0.06831, policy_loss: -24.34734, policy_entropy: -5.75725, alpha: 0.00474, time: 33.87031
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 137.29436, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 0.06523, qf2_loss: 0.06581, policy_loss: -24.37491, policy_entropy: -5.67588, alpha: 0.00469, time: 33.55777
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 162.72548, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 0.07122, qf2_loss: 0.07259, policy_loss: -24.39231, policy_entropy: -5.85808, alpha: 0.00465, time: 33.53200
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 170.73040, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 0.07453, qf2_loss: 0.07508, policy_loss: -24.35280, policy_entropy: -5.80856, alpha: 0.00459, time: 33.65332
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 156.35403, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 0.06566, qf2_loss: 0.06676, policy_loss: -24.39664, policy_entropy: -5.71923, alpha: 0.00456, time: 33.93380
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 131.39557, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 0.06920, qf2_loss: 0.06931, policy_loss: -24.41252, policy_entropy: -5.75711, alpha: 0.00450, time: 33.43888
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 149.12385, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 0.07223, qf2_loss: 0.07285, policy_loss: -24.38278, policy_entropy: -5.87363, alpha: 0.00445, time: 34.20672
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 125.51576, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 0.06766, qf2_loss: 0.06864, policy_loss: -24.47478, policy_entropy: -6.17728, alpha: 0.00446, time: 33.48738
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 130.68189, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 0.06268, qf2_loss: 0.06342, policy_loss: -24.39024, policy_entropy: -6.03443, alpha: 0.00450, time: 34.49121
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 122.77205, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 0.07209, qf2_loss: 0.07249, policy_loss: -24.45977, policy_entropy: -5.83915, alpha: 0.00446, time: 33.43292
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 169.96898, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 0.08741, qf2_loss: 0.09086, policy_loss: -24.45761, policy_entropy: -6.14036, alpha: 0.00447, time: 33.56741
[CW] eval: return: 152.63568, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 137.90577, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 0.06432, qf2_loss: 0.06386, policy_loss: -24.46938, policy_entropy: -5.69562, alpha: 0.00446, time: 33.74728
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 163.97948, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 0.06916, qf2_loss: 0.07032, policy_loss: -24.40427, policy_entropy: -6.00917, alpha: 0.00442, time: 33.39894
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 147.14294, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 0.06678, qf2_loss: 0.06634, policy_loss: -24.42061, policy_entropy: -6.11486, alpha: 0.00443, time: 33.92861
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 158.70527, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 0.06824, qf2_loss: 0.06934, policy_loss: -24.44397, policy_entropy: -5.83002, alpha: 0.00442, time: 33.61682
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 148.93566, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 0.06366, qf2_loss: 0.06297, policy_loss: -24.42675, policy_entropy: -6.00006, alpha: 0.00440, time: 33.88229
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 151.11119, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 0.07053, qf2_loss: 0.07239, policy_loss: -24.47537, policy_entropy: -6.44525, alpha: 0.00445, time: 33.72153
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 160.01176, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 0.07318, qf2_loss: 0.07609, policy_loss: -24.51410, policy_entropy: -6.65087, alpha: 0.00457, time: 33.71425
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 167.10135, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 0.08045, qf2_loss: 0.08163, policy_loss: -24.56630, policy_entropy: -5.86164, alpha: 0.00464, time: 33.18091
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 153.58629, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 0.06276, qf2_loss: 0.06260, policy_loss: -24.44989, policy_entropy: -5.93622, alpha: 0.00462, time: 33.59119
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 162.53205, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 0.06146, qf2_loss: 0.06202, policy_loss: -24.58330, policy_entropy: -5.98325, alpha: 0.00461, time: 32.99373
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 151.17566, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 0.09113, qf2_loss: 0.09219, policy_loss: -24.56023, policy_entropy: -5.87653, alpha: 0.00458, time: 33.42453
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 167.93629, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 0.06385, qf2_loss: 0.06469, policy_loss: -24.53825, policy_entropy: -6.01557, alpha: 0.00458, time: 33.74796
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 149.22614, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 0.07259, qf2_loss: 0.07463, policy_loss: -24.46892, policy_entropy: -5.82243, alpha: 0.00458, time: 33.99458
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 160.15272, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 0.08475, qf2_loss: 0.08654, policy_loss: -24.56155, policy_entropy: -5.96864, alpha: 0.00452, time: 34.31325
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 167.56955, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 0.06561, qf2_loss: 0.06643, policy_loss: -24.59974, policy_entropy: -5.90082, alpha: 0.00451, time: 33.56661
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 163.59034, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 0.07931, qf2_loss: 0.08219, policy_loss: -24.57555, policy_entropy: -5.96692, alpha: 0.00449, time: 35.61280
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 153.84429, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 0.06847, qf2_loss: 0.06871, policy_loss: -24.54450, policy_entropy: -5.75281, alpha: 0.00449, time: 33.53940
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 169.47403, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 0.06611, qf2_loss: 0.06643, policy_loss: -24.64160, policy_entropy: -6.14035, alpha: 0.00444, time: 33.51648
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 160.22792, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 0.06284, qf2_loss: 0.06252, policy_loss: -24.71185, policy_entropy: -6.28827, alpha: 0.00451, time: 34.01104
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 170.18654, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 0.06507, qf2_loss: 0.06472, policy_loss: -24.65034, policy_entropy: -5.89468, alpha: 0.00454, time: 33.71040
[CW] eval: return: 144.26347, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 136.89736, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 0.07148, qf2_loss: 0.07254, policy_loss: -24.74527, policy_entropy: -6.14497, alpha: 0.00453, time: 33.68982
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 157.06500, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 0.07568, qf2_loss: 0.07674, policy_loss: -24.65119, policy_entropy: -6.21692, alpha: 0.00458, time: 33.50667
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 162.92507, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 0.08172, qf2_loss: 0.08459, policy_loss: -24.69055, policy_entropy: -6.27373, alpha: 0.00463, time: 33.55684
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 176.18202, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 0.07564, qf2_loss: 0.07672, policy_loss: -24.71993, policy_entropy: -6.31202, alpha: 0.00473, time: 34.01283
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 143.40290, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 0.06978, qf2_loss: 0.07014, policy_loss: -24.69280, policy_entropy: -5.97452, alpha: 0.00477, time: 33.75005
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 164.19234, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 0.07348, qf2_loss: 0.07262, policy_loss: -24.67398, policy_entropy: -6.01505, alpha: 0.00477, time: 33.88458
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 168.95425, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 0.06499, qf2_loss: 0.06695, policy_loss: -24.75845, policy_entropy: -6.17483, alpha: 0.00478, time: 33.55296
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 138.74498, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 0.07492, qf2_loss: 0.07586, policy_loss: -24.73324, policy_entropy: -6.06911, alpha: 0.00482, time: 33.51617
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 136.02267, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 0.07032, qf2_loss: 0.06981, policy_loss: -24.71405, policy_entropy: -5.80883, alpha: 0.00482, time: 33.69945
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 159.87443, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 0.06990, qf2_loss: 0.07148, policy_loss: -24.78923, policy_entropy: -5.86160, alpha: 0.00477, time: 33.85482
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 143.01593, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 0.06938, qf2_loss: 0.07051, policy_loss: -24.81566, policy_entropy: -6.10862, alpha: 0.00474, time: 33.98163
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 151.50661, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 0.07697, qf2_loss: 0.07754, policy_loss: -24.78796, policy_entropy: -6.19056, alpha: 0.00480, time: 33.36986
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 152.29421, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 0.06624, qf2_loss: 0.06610, policy_loss: -24.79085, policy_entropy: -6.07051, alpha: 0.00484, time: 33.69209
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 164.23756, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 0.07182, qf2_loss: 0.07200, policy_loss: -24.83540, policy_entropy: -6.15816, alpha: 0.00489, time: 33.77321
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 125.39233, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 0.07945, qf2_loss: 0.08045, policy_loss: -24.84438, policy_entropy: -5.69909, alpha: 0.00487, time: 33.45906
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 165.69140, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 0.08529, qf2_loss: 0.08573, policy_loss: -24.84082, policy_entropy: -5.87781, alpha: 0.00480, time: 33.84791
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 161.20681, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 0.06587, qf2_loss: 0.06657, policy_loss: -24.83606, policy_entropy: -6.40827, alpha: 0.00482, time: 33.96311
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 167.53201, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 0.07673, qf2_loss: 0.07735, policy_loss: -24.77721, policy_entropy: -6.18456, alpha: 0.00492, time: 33.73925
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 165.91118, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 0.08342, qf2_loss: 0.08476, policy_loss: -24.83515, policy_entropy: -6.25203, alpha: 0.00498, time: 33.73787
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 169.25826, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 0.07697, qf2_loss: 0.07924, policy_loss: -24.83713, policy_entropy: -5.95011, alpha: 0.00503, time: 33.45646
[CW] eval: return: 152.25277, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 165.67149, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 0.07509, qf2_loss: 0.07551, policy_loss: -24.85869, policy_entropy: -6.06403, alpha: 0.00503, time: 33.62171
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 171.36547, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 0.07785, qf2_loss: 0.07726, policy_loss: -24.97378, policy_entropy: -6.02587, alpha: 0.00505, time: 33.67258
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 151.33628, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 0.07297, qf2_loss: 0.07539, policy_loss: -24.96672, policy_entropy: -5.96846, alpha: 0.00504, time: 33.47656
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 108.78401, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 0.06689, qf2_loss: 0.06694, policy_loss: -24.86377, policy_entropy: -5.71154, alpha: 0.00500, time: 35.04014
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 172.41558, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 0.06784, qf2_loss: 0.06820, policy_loss: -25.04726, policy_entropy: -5.87150, alpha: 0.00493, time: 33.49471
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 143.51580, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 0.07632, qf2_loss: 0.07789, policy_loss: -24.98090, policy_entropy: -5.99517, alpha: 0.00489, time: 33.82081
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 168.06383, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 0.08519, qf2_loss: 0.08553, policy_loss: -24.93893, policy_entropy: -6.22779, alpha: 0.00494, time: 33.57125
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 140.70826, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 0.07020, qf2_loss: 0.07049, policy_loss: -24.95525, policy_entropy: -5.89232, alpha: 0.00496, time: 33.75903
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 144.76079, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 0.06477, qf2_loss: 0.06547, policy_loss: -24.97512, policy_entropy: -5.93399, alpha: 0.00491, time: 33.50139
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 167.80494, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 0.06845, qf2_loss: 0.06875, policy_loss: -25.02634, policy_entropy: -6.01440, alpha: 0.00492, time: 33.71268
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 161.56957, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 0.07260, qf2_loss: 0.07371, policy_loss: -25.04869, policy_entropy: -5.97836, alpha: 0.00492, time: 33.85715
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 160.46777, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 0.08288, qf2_loss: 0.08280, policy_loss: -25.01646, policy_entropy: -6.09498, alpha: 0.00493, time: 33.72771
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 137.02313, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 0.08271, qf2_loss: 0.08379, policy_loss: -25.08362, policy_entropy: -6.11004, alpha: 0.00495, time: 33.32304
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 151.93616, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 0.07547, qf2_loss: 0.07636, policy_loss: -25.12000, policy_entropy: -6.16412, alpha: 0.00500, time: 33.77108
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 123.59197, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 0.08375, qf2_loss: 0.08416, policy_loss: -24.99385, policy_entropy: -6.04064, alpha: 0.00505, time: 34.01540
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 141.59657, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 0.08188, qf2_loss: 0.08539, policy_loss: -25.13561, policy_entropy: -5.96778, alpha: 0.00505, time: 34.02085
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 145.46699, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 0.07647, qf2_loss: 0.07602, policy_loss: -25.18386, policy_entropy: -5.82799, alpha: 0.00499, time: 34.10290
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 153.22360, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 0.07592, qf2_loss: 0.07612, policy_loss: -25.15813, policy_entropy: -5.93409, alpha: 0.00498, time: 33.94040
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 141.71626, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 0.07944, qf2_loss: 0.07974, policy_loss: -25.16976, policy_entropy: -6.06848, alpha: 0.00499, time: 33.68354
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 136.97554, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 0.07535, qf2_loss: 0.07575, policy_loss: -25.22185, policy_entropy: -5.96296, alpha: 0.00496, time: 33.45966
[CW] eval: return: 150.01276, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 166.13673, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 0.08029, qf2_loss: 0.08174, policy_loss: -25.22970, policy_entropy: -6.17365, alpha: 0.00498, time: 33.64032
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 162.48550, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 0.07567, qf2_loss: 0.07539, policy_loss: -25.23978, policy_entropy: -5.92417, alpha: 0.00501, time: 33.59425
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 151.97749, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 0.07614, qf2_loss: 0.07636, policy_loss: -25.17716, policy_entropy: -5.78305, alpha: 0.00497, time: 33.49997
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 166.67020, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 0.07382, qf2_loss: 0.07458, policy_loss: -25.22831, policy_entropy: -5.95790, alpha: 0.00491, time: 33.34976
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 150.06663, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 0.07807, qf2_loss: 0.07946, policy_loss: -25.24763, policy_entropy: -6.09048, alpha: 0.00492, time: 33.64322
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 167.50387, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 0.07297, qf2_loss: 0.07387, policy_loss: -25.26980, policy_entropy: -6.12279, alpha: 0.00497, time: 33.60763
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 172.79848, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 0.07416, qf2_loss: 0.07470, policy_loss: -25.29164, policy_entropy: -6.11191, alpha: 0.00501, time: 33.69955
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 175.59164, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 0.08263, qf2_loss: 0.08338, policy_loss: -25.29298, policy_entropy: -5.84154, alpha: 0.00501, time: 33.21122
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 169.57490, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 0.07755, qf2_loss: 0.07864, policy_loss: -25.35556, policy_entropy: -5.92266, alpha: 0.00496, time: 34.13460
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 170.84789, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 0.07195, qf2_loss: 0.07310, policy_loss: -25.36842, policy_entropy: -6.14813, alpha: 0.00499, time: 33.49724
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 151.09650, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 0.09697, qf2_loss: 0.09738, policy_loss: -25.35114, policy_entropy: -6.07612, alpha: 0.00499, time: 33.64548
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 165.81646, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 0.08436, qf2_loss: 0.08535, policy_loss: -25.37418, policy_entropy: -6.08879, alpha: 0.00504, time: 33.49502
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 167.59051, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 0.06994, qf2_loss: 0.07056, policy_loss: -25.39169, policy_entropy: -5.85425, alpha: 0.00503, time: 33.79810
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 152.65497, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 0.07232, qf2_loss: 0.07254, policy_loss: -25.27772, policy_entropy: -6.10237, alpha: 0.00502, time: 33.66415
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 156.55728, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 0.07873, qf2_loss: 0.07970, policy_loss: -25.39286, policy_entropy: -5.99229, alpha: 0.00506, time: 33.55419
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 170.43539, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 0.09696, qf2_loss: 0.09888, policy_loss: -25.43728, policy_entropy: -5.83737, alpha: 0.00502, time: 33.38410
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 173.30992, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 0.07470, qf2_loss: 0.07497, policy_loss: -25.37244, policy_entropy: -6.06384, alpha: 0.00500, time: 33.55434
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 152.97301, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 0.07298, qf2_loss: 0.07413, policy_loss: -25.42398, policy_entropy: -5.91671, alpha: 0.00500, time: 33.58088
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 156.97214, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 0.07588, qf2_loss: 0.07746, policy_loss: -25.45794, policy_entropy: -5.79738, alpha: 0.00495, time: 33.95992
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 171.73710, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 0.07901, qf2_loss: 0.07800, policy_loss: -25.49035, policy_entropy: -5.89174, alpha: 0.00489, time: 33.72371
[CW] eval: return: 168.52026, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 165.30145, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 0.08998, qf2_loss: 0.09297, policy_loss: -25.58219, policy_entropy: -5.88484, alpha: 0.00484, time: 33.67359
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 180.24570, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 0.07568, qf2_loss: 0.07657, policy_loss: -25.57115, policy_entropy: -6.12612, alpha: 0.00483, time: 33.61485
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 178.66887, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 0.08077, qf2_loss: 0.08052, policy_loss: -25.56611, policy_entropy: -5.99250, alpha: 0.00487, time: 33.46480
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 167.11283, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 0.07260, qf2_loss: 0.07317, policy_loss: -25.50125, policy_entropy: -5.98987, alpha: 0.00488, time: 33.60212
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 176.87883, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 0.07508, qf2_loss: 0.07563, policy_loss: -25.47635, policy_entropy: -6.04059, alpha: 0.00486, time: 33.68713
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 181.14726, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 0.08111, qf2_loss: 0.08259, policy_loss: -25.52046, policy_entropy: -5.91209, alpha: 0.00484, time: 33.55973
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 167.54187, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 0.07711, qf2_loss: 0.07781, policy_loss: -25.54065, policy_entropy: -5.92456, alpha: 0.00482, time: 33.54772
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 162.61126, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 0.07870, qf2_loss: 0.07904, policy_loss: -25.56396, policy_entropy: -5.78036, alpha: 0.00480, time: 33.53061
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 170.83002, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 0.08089, qf2_loss: 0.08097, policy_loss: -25.62708, policy_entropy: -5.88046, alpha: 0.00472, time: 33.50966
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 173.33196, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 0.08483, qf2_loss: 0.08611, policy_loss: -25.81396, policy_entropy: -6.15592, alpha: 0.00473, time: 33.63086
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 174.18965, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 0.09960, qf2_loss: 0.10190, policy_loss: -25.72416, policy_entropy: -6.08580, alpha: 0.00476, time: 33.42116
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 173.74274, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 0.07725, qf2_loss: 0.07671, policy_loss: -25.73963, policy_entropy: -6.14891, alpha: 0.00482, time: 33.81844
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 180.99874, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 0.09447, qf2_loss: 0.09567, policy_loss: -25.72196, policy_entropy: -6.16966, alpha: 0.00487, time: 33.68772
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 177.06825, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 0.07493, qf2_loss: 0.07638, policy_loss: -25.75757, policy_entropy: -6.03707, alpha: 0.00491, time: 33.63284
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 176.85442, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 0.08593, qf2_loss: 0.08713, policy_loss: -25.78444, policy_entropy: -6.13490, alpha: 0.00492, time: 34.22330
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 179.31178, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 0.08825, qf2_loss: 0.08965, policy_loss: -25.86860, policy_entropy: -5.85959, alpha: 0.00494, time: 33.70837
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 166.71984, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 0.07400, qf2_loss: 0.07355, policy_loss: -25.80737, policy_entropy: -5.78488, alpha: 0.00488, time: 33.77064
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 168.14439, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 0.07755, qf2_loss: 0.07953, policy_loss: -25.78473, policy_entropy: -5.68004, alpha: 0.00478, time: 33.59208
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 171.88076, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 0.08496, qf2_loss: 0.08549, policy_loss: -25.84953, policy_entropy: -6.10208, alpha: 0.00472, time: 33.76236
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 182.56050, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 0.08804, qf2_loss: 0.08793, policy_loss: -25.87454, policy_entropy: -5.91745, alpha: 0.00475, time: 35.25161
[CW] eval: return: 165.45353, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 161.98413, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 0.08004, qf2_loss: 0.08017, policy_loss: -25.84033, policy_entropy: -5.92589, alpha: 0.00474, time: 33.74603
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 176.69630, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 0.08100, qf2_loss: 0.08157, policy_loss: -25.79537, policy_entropy: -5.97908, alpha: 0.00469, time: 33.60938
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 133.56589, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 0.08189, qf2_loss: 0.08325, policy_loss: -25.90969, policy_entropy: -6.32245, alpha: 0.00473, time: 33.96634
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 165.51156, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 0.08909, qf2_loss: 0.08999, policy_loss: -25.91352, policy_entropy: -6.01351, alpha: 0.00480, time: 33.88215
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 186.28959, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 0.07792, qf2_loss: 0.07823, policy_loss: -26.08791, policy_entropy: -6.16642, alpha: 0.00481, time: 33.98054
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 179.03660, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 0.08709, qf2_loss: 0.08803, policy_loss: -25.94334, policy_entropy: -6.16843, alpha: 0.00490, time: 33.72500
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 175.33255, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 0.08640, qf2_loss: 0.08784, policy_loss: -26.05202, policy_entropy: -6.04929, alpha: 0.00494, time: 33.87192
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 175.57903, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 0.09859, qf2_loss: 0.10016, policy_loss: -26.05598, policy_entropy: -5.91197, alpha: 0.00493, time: 33.73212
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 171.81273, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 0.08449, qf2_loss: 0.08425, policy_loss: -26.00537, policy_entropy: -5.90137, alpha: 0.00489, time: 33.71128
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 165.68958, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 0.09098, qf2_loss: 0.09369, policy_loss: -26.21359, policy_entropy: -6.11091, alpha: 0.00490, time: 33.47839
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 171.39854, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 0.07738, qf2_loss: 0.07781, policy_loss: -26.04811, policy_entropy: -5.97123, alpha: 0.00489, time: 34.41278
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 170.51210, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 0.08199, qf2_loss: 0.08235, policy_loss: -26.12421, policy_entropy: -6.10571, alpha: 0.00492, time: 33.74703
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 169.92294, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 0.08431, qf2_loss: 0.08487, policy_loss: -26.16302, policy_entropy: -5.94708, alpha: 0.00494, time: 33.63932
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 168.19623, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 0.07977, qf2_loss: 0.08019, policy_loss: -26.16621, policy_entropy: -5.59761, alpha: 0.00488, time: 33.95143
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 176.90646, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 0.08747, qf2_loss: 0.08884, policy_loss: -26.24328, policy_entropy: -6.08012, alpha: 0.00478, time: 33.68706
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 154.36861, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 0.09208, qf2_loss: 0.09326, policy_loss: -26.19444, policy_entropy: -6.08154, alpha: 0.00482, time: 34.01415
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 174.90912, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 0.09022, qf2_loss: 0.08994, policy_loss: -26.30083, policy_entropy: -6.04340, alpha: 0.00482, time: 33.60432
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 171.08134, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 0.08546, qf2_loss: 0.08758, policy_loss: -26.25016, policy_entropy: -5.89436, alpha: 0.00483, time: 33.93470
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 168.77903, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 0.09484, qf2_loss: 0.09560, policy_loss: -26.27609, policy_entropy: -5.74259, alpha: 0.00477, time: 33.75167
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 178.98209, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 0.08152, qf2_loss: 0.08058, policy_loss: -26.38168, policy_entropy: -6.28474, alpha: 0.00476, time: 33.55106
[CW] eval: return: 170.23925, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 172.59024, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 0.07879, qf2_loss: 0.08032, policy_loss: -26.44877, policy_entropy: -6.05797, alpha: 0.00484, time: 33.59654
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 177.69371, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 0.07733, qf2_loss: 0.07817, policy_loss: -26.45381, policy_entropy: -6.11671, alpha: 0.00485, time: 33.25180
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 172.23426, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 0.08138, qf2_loss: 0.08250, policy_loss: -26.37477, policy_entropy: -5.87317, alpha: 0.00487, time: 33.26799
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 179.90610, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 0.07995, qf2_loss: 0.08039, policy_loss: -26.40827, policy_entropy: -5.86625, alpha: 0.00483, time: 33.99278
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 174.39238, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 0.09010, qf2_loss: 0.09216, policy_loss: -26.55523, policy_entropy: -5.72142, alpha: 0.00475, time: 33.78869
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 175.25141, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 0.08402, qf2_loss: 0.08337, policy_loss: -26.52323, policy_entropy: -5.90924, alpha: 0.00469, time: 33.74253
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 176.00373, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 0.07841, qf2_loss: 0.08006, policy_loss: -26.42307, policy_entropy: -5.92466, alpha: 0.00466, time: 33.93062
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 168.08250, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 0.08995, qf2_loss: 0.09013, policy_loss: -26.56744, policy_entropy: -6.17535, alpha: 0.00467, time: 33.68627
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 174.68343, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 0.07818, qf2_loss: 0.07904, policy_loss: -26.58490, policy_entropy: -6.04992, alpha: 0.00469, time: 33.78727
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 165.11256, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 0.10547, qf2_loss: 0.10567, policy_loss: -26.70977, policy_entropy: -5.79473, alpha: 0.00469, time: 33.96779
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 172.89084, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 0.07928, qf2_loss: 0.08073, policy_loss: -26.65445, policy_entropy: -5.88452, alpha: 0.00462, time: 33.91811
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 172.08411, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 0.08293, qf2_loss: 0.08382, policy_loss: -26.57529, policy_entropy: -5.92445, alpha: 0.00459, time: 33.41103
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 179.09463, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 0.08549, qf2_loss: 0.08624, policy_loss: -26.64007, policy_entropy: -6.04628, alpha: 0.00459, time: 33.44339
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 158.59786, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 0.07770, qf2_loss: 0.07928, policy_loss: -26.62411, policy_entropy: -6.05457, alpha: 0.00461, time: 33.97348
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 173.34793, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 0.08160, qf2_loss: 0.08178, policy_loss: -26.74990, policy_entropy: -6.14113, alpha: 0.00462, time: 33.96572
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 176.42839, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 0.07824, qf2_loss: 0.07933, policy_loss: -26.77867, policy_entropy: -5.99974, alpha: 0.00467, time: 33.96868
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 180.35535, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 0.07962, qf2_loss: 0.08067, policy_loss: -26.74460, policy_entropy: -5.97398, alpha: 0.00465, time: 33.53534
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 133.64877, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 0.08951, qf2_loss: 0.09057, policy_loss: -26.77443, policy_entropy: -6.03994, alpha: 0.00466, time: 33.65726
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 177.82158, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 0.10217, qf2_loss: 0.10305, policy_loss: -26.79582, policy_entropy: -5.91436, alpha: 0.00465, time: 34.14041
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 172.15357, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 0.09787, qf2_loss: 0.09939, policy_loss: -26.84001, policy_entropy: -6.57002, alpha: 0.00473, time: 33.88015
[CW] eval: return: 177.12917, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 179.70101, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 0.09092, qf2_loss: 0.09137, policy_loss: -26.75125, policy_entropy: -5.96844, alpha: 0.00484, time: 33.53772
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 176.41616, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 0.07982, qf2_loss: 0.08028, policy_loss: -26.94073, policy_entropy: -5.89522, alpha: 0.00481, time: 33.40559
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 186.22824, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 0.08689, qf2_loss: 0.08669, policy_loss: -26.87844, policy_entropy: -5.99542, alpha: 0.00479, time: 33.38810
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 174.59846, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 0.08765, qf2_loss: 0.09052, policy_loss: -26.86793, policy_entropy: -5.84850, alpha: 0.00476, time: 33.54248
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 170.11871, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 0.08820, qf2_loss: 0.08748, policy_loss: -26.96069, policy_entropy: -6.02101, alpha: 0.00474, time: 34.05136
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 177.20146, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 0.08126, qf2_loss: 0.08235, policy_loss: -26.99794, policy_entropy: -6.09782, alpha: 0.00474, time: 33.82435
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 170.54446, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 0.07963, qf2_loss: 0.08049, policy_loss: -27.08301, policy_entropy: -6.27919, alpha: 0.00479, time: 33.89067
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 175.75192, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 0.08764, qf2_loss: 0.08752, policy_loss: -26.98562, policy_entropy: -6.16176, alpha: 0.00490, time: 33.68865
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 175.39069, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 0.08153, qf2_loss: 0.08266, policy_loss: -26.96622, policy_entropy: -6.01372, alpha: 0.00494, time: 33.45744
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 165.93931, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 0.08399, qf2_loss: 0.08462, policy_loss: -27.04697, policy_entropy: -5.84699, alpha: 0.00492, time: 33.88920
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 165.32432, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 0.09075, qf2_loss: 0.09235, policy_loss: -27.13835, policy_entropy: -6.04029, alpha: 0.00489, time: 34.05481
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 181.41012, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 0.08572, qf2_loss: 0.08633, policy_loss: -27.09440, policy_entropy: -5.84861, alpha: 0.00487, time: 33.32074
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 172.03755, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 0.08668, qf2_loss: 0.08817, policy_loss: -27.05490, policy_entropy: -6.30246, alpha: 0.00490, time: 33.64505
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 175.74440, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 0.08873, qf2_loss: 0.08951, policy_loss: -27.12835, policy_entropy: -5.94741, alpha: 0.00495, time: 33.78475
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 176.91249, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 0.08075, qf2_loss: 0.08234, policy_loss: -27.13873, policy_entropy: -6.07006, alpha: 0.00496, time: 33.89783
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 171.30960, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 0.08794, qf2_loss: 0.08780, policy_loss: -27.21171, policy_entropy: -5.99997, alpha: 0.00496, time: 33.96319
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 181.29027, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 0.08189, qf2_loss: 0.08297, policy_loss: -27.18874, policy_entropy: -5.68913, alpha: 0.00492, time: 34.09479
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 173.31329, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 0.08016, qf2_loss: 0.07975, policy_loss: -27.21209, policy_entropy: -6.04470, alpha: 0.00484, time: 33.67326
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 173.38154, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 0.09223, qf2_loss: 0.09355, policy_loss: -27.25142, policy_entropy: -6.06728, alpha: 0.00488, time: 33.74360
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 171.04717, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 0.08383, qf2_loss: 0.08576, policy_loss: -27.19671, policy_entropy: -6.10093, alpha: 0.00489, time: 34.05801
[CW] eval: return: 172.14745, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 173.19263, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 0.08099, qf2_loss: 0.08068, policy_loss: -27.38013, policy_entropy: -5.96894, alpha: 0.00490, time: 33.45413
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 184.69662, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 0.08532, qf2_loss: 0.08558, policy_loss: -27.31604, policy_entropy: -5.83543, alpha: 0.00489, time: 33.59642
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 165.06377, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 0.11585, qf2_loss: 0.11916, policy_loss: -27.30545, policy_entropy: -6.28400, alpha: 0.00490, time: 34.11270
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 180.59598, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 0.08388, qf2_loss: 0.08444, policy_loss: -27.34254, policy_entropy: -5.86122, alpha: 0.00493, time: 33.74860
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 182.64130, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 0.09538, qf2_loss: 0.09663, policy_loss: -27.50322, policy_entropy: -6.00160, alpha: 0.00490, time: 33.92308
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 179.69476, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 0.08987, qf2_loss: 0.09020, policy_loss: -27.42132, policy_entropy: -6.23603, alpha: 0.00494, time: 33.55171
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 174.97125, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 0.08485, qf2_loss: 0.08729, policy_loss: -27.51180, policy_entropy: -6.30877, alpha: 0.00502, time: 33.74401
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 179.64944, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 0.08313, qf2_loss: 0.08422, policy_loss: -27.54705, policy_entropy: -6.00603, alpha: 0.00510, time: 33.75944
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 173.67027, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 0.08213, qf2_loss: 0.08343, policy_loss: -27.67175, policy_entropy: -5.90076, alpha: 0.00508, time: 33.45341
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 181.86569, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 0.09025, qf2_loss: 0.09014, policy_loss: -27.52807, policy_entropy: -5.95217, alpha: 0.00505, time: 33.97448
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 169.18075, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 0.09357, qf2_loss: 0.09406, policy_loss: -27.59308, policy_entropy: -5.98574, alpha: 0.00505, time: 33.92208
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 181.82226, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 0.08694, qf2_loss: 0.08843, policy_loss: -27.61474, policy_entropy: -6.01205, alpha: 0.00505, time: 33.90659
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 156.13037, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 0.08659, qf2_loss: 0.08778, policy_loss: -27.67573, policy_entropy: -5.97049, alpha: 0.00503, time: 34.00876
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 171.51892, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 0.08556, qf2_loss: 0.08570, policy_loss: -27.64601, policy_entropy: -6.09358, alpha: 0.00505, time: 35.39375
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 81.50888, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 0.08827, qf2_loss: 0.08889, policy_loss: -27.75814, policy_entropy: -5.99409, alpha: 0.00506, time: 33.66966
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 185.19868, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 0.08837, qf2_loss: 0.09017, policy_loss: -27.80821, policy_entropy: -6.18326, alpha: 0.00508, time: 34.15580
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 171.78215, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 0.09757, qf2_loss: 0.09773, policy_loss: -27.74240, policy_entropy: -6.04585, alpha: 0.00514, time: 34.01401
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 187.82440, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 0.08293, qf2_loss: 0.08401, policy_loss: -27.76229, policy_entropy: -5.67764, alpha: 0.00511, time: 34.24498
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 178.87221, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 0.09181, qf2_loss: 0.09276, policy_loss: -27.83140, policy_entropy: -6.17830, alpha: 0.00505, time: 33.75408
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 179.89757, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 0.08234, qf2_loss: 0.08375, policy_loss: -27.94505, policy_entropy: -6.16251, alpha: 0.00513, time: 33.83564
[CW] eval: return: 178.94333, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 186.81714, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 0.09823, qf2_loss: 0.09823, policy_loss: -27.88435, policy_entropy: -6.10546, alpha: 0.00517, time: 33.82117
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 168.39552, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 0.08482, qf2_loss: 0.08648, policy_loss: -27.94480, policy_entropy: -6.26204, alpha: 0.00524, time: 33.61854
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 183.66078, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 0.10068, qf2_loss: 0.10153, policy_loss: -27.99557, policy_entropy: -5.97657, alpha: 0.00530, time: 33.64245
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 182.42131, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 0.10309, qf2_loss: 0.10265, policy_loss: -28.03058, policy_entropy: -5.81976, alpha: 0.00525, time: 33.86657
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 181.79759, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 0.09199, qf2_loss: 0.09291, policy_loss: -27.98577, policy_entropy: -5.93052, alpha: 0.00522, time: 34.04852
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 175.96077, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 0.08513, qf2_loss: 0.08587, policy_loss: -28.04165, policy_entropy: -5.88830, alpha: 0.00519, time: 34.05646
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 178.10352, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 0.08625, qf2_loss: 0.08785, policy_loss: -28.07947, policy_entropy: -6.04010, alpha: 0.00515, time: 33.48086
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 184.40977, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 0.08801, qf2_loss: 0.08937, policy_loss: -28.04212, policy_entropy: -5.93816, alpha: 0.00516, time: 33.73770
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 165.13458, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 0.08364, qf2_loss: 0.08449, policy_loss: -28.12789, policy_entropy: -6.00288, alpha: 0.00515, time: 33.42688
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 182.81490, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 0.09649, qf2_loss: 0.09934, policy_loss: -28.19748, policy_entropy: -5.79244, alpha: 0.00509, time: 35.70470
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 182.78036, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 0.08202, qf2_loss: 0.08305, policy_loss: -28.17272, policy_entropy: -5.83717, alpha: 0.00505, time: 34.15586
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 179.55638, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 0.08991, qf2_loss: 0.09141, policy_loss: -28.22808, policy_entropy: -6.03723, alpha: 0.00502, time: 34.19363
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 165.05544, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 0.10257, qf2_loss: 0.10313, policy_loss: -28.25951, policy_entropy: -5.81667, alpha: 0.00500, time: 33.79529
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 174.56032, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 0.09169, qf2_loss: 0.09255, policy_loss: -28.34392, policy_entropy: -5.94093, alpha: 0.00495, time: 34.17113
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 184.73062, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 0.07877, qf2_loss: 0.07963, policy_loss: -28.34556, policy_entropy: -6.06481, alpha: 0.00494, time: 33.92574
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 172.59330, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 0.08484, qf2_loss: 0.08638, policy_loss: -28.35316, policy_entropy: -6.10725, alpha: 0.00498, time: 34.03516
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 185.82899, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 0.07771, qf2_loss: 0.07949, policy_loss: -28.40396, policy_entropy: -6.12851, alpha: 0.00503, time: 33.70491
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 172.02379, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 0.08992, qf2_loss: 0.09062, policy_loss: -28.35654, policy_entropy: -6.06211, alpha: 0.00504, time: 33.90721
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 176.36412, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 0.09327, qf2_loss: 0.09290, policy_loss: -28.43435, policy_entropy: -6.07342, alpha: 0.00507, time: 33.59401
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 183.51194, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 0.08070, qf2_loss: 0.08159, policy_loss: -28.44604, policy_entropy: -6.01140, alpha: 0.00509, time: 34.01140
[CW] eval: return: 183.08613, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 175.90677, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 0.08803, qf2_loss: 0.08879, policy_loss: -28.50026, policy_entropy: -5.89293, alpha: 0.00508, time: 33.67231
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 182.77772, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 0.08740, qf2_loss: 0.08775, policy_loss: -28.49621, policy_entropy: -5.87946, alpha: 0.00501, time: 33.89059
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 168.07437, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 0.08417, qf2_loss: 0.08507, policy_loss: -28.59983, policy_entropy: -6.28280, alpha: 0.00505, time: 33.79009
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 186.97826, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 0.08026, qf2_loss: 0.07982, policy_loss: -28.58103, policy_entropy: -5.88121, alpha: 0.00511, time: 33.75587
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 173.53926, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 0.08562, qf2_loss: 0.08768, policy_loss: -28.61876, policy_entropy: -5.76956, alpha: 0.00504, time: 33.88722
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 183.54636, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 0.08944, qf2_loss: 0.09040, policy_loss: -28.72374, policy_entropy: -5.89041, alpha: 0.00497, time: 33.77073
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 176.65891, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 0.07849, qf2_loss: 0.07890, policy_loss: -28.60014, policy_entropy: -5.98607, alpha: 0.00495, time: 33.77353
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 172.91996, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 0.07074, qf2_loss: 0.07270, policy_loss: -28.63742, policy_entropy: -6.09149, alpha: 0.00498, time: 33.84132
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 178.88045, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 0.08869, qf2_loss: 0.08893, policy_loss: -28.62191, policy_entropy: -5.91229, alpha: 0.00496, time: 33.73696
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 180.13236, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 0.07819, qf2_loss: 0.07901, policy_loss: -28.68646, policy_entropy: -5.92402, alpha: 0.00493, time: 33.78238
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 181.01194, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 0.07743, qf2_loss: 0.07885, policy_loss: -28.71625, policy_entropy: -5.79620, alpha: 0.00489, time: 33.56086
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 183.40932, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 0.07505, qf2_loss: 0.07577, policy_loss: -28.82875, policy_entropy: -5.98367, alpha: 0.00484, time: 33.86047
[CW] ---------------------------
[CW] ---- Iteration:   673 ----
[CW] collect: return: 179.58380, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 0.08421, qf2_loss: 0.08539, policy_loss: -28.85845, policy_entropy: -6.51623, alpha: 0.00492, time: 34.26301
[CW] ---------------------------
[CW] ---- Iteration:   674 ----
[CW] collect: return: 183.91701, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 0.08253, qf2_loss: 0.08500, policy_loss: -28.77442, policy_entropy: -6.24142, alpha: 0.00505, time: 34.16586
[CW] ---------------------------
[CW] ---- Iteration:   675 ----
[CW] collect: return: 176.60162, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 0.08671, qf2_loss: 0.08589, policy_loss: -28.88331, policy_entropy: -6.12661, alpha: 0.00513, time: 33.67969
[CW] ---------------------------
[CW] ---- Iteration:   676 ----
[CW] collect: return: 172.04324, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 0.08559, qf2_loss: 0.08790, policy_loss: -28.90095, policy_entropy: -5.94849, alpha: 0.00512, time: 34.01220
[CW] ---------------------------
[CW] ---- Iteration:   677 ----
[CW] collect: return: 186.60212, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 0.09455, qf2_loss: 0.09515, policy_loss: -28.85822, policy_entropy: -5.93038, alpha: 0.00511, time: 33.51755
[CW] ---------------------------
[CW] ---- Iteration:   678 ----
[CW] collect: return: 184.00451, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 0.07570, qf2_loss: 0.07689, policy_loss: -28.93058, policy_entropy: -6.07437, alpha: 0.00511, time: 33.84168
[CW] ---------------------------
[CW] ---- Iteration:   679 ----
[CW] collect: return: 179.03248, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 0.08028, qf2_loss: 0.08126, policy_loss: -28.99136, policy_entropy: -6.07455, alpha: 0.00514, time: 33.94284
[CW] ---------------------------
[CW] ---- Iteration:   680 ----
[CW] collect: return: 181.07875, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 0.07049, qf2_loss: 0.07198, policy_loss: -29.03152, policy_entropy: -5.86711, alpha: 0.00512, time: 33.78961
[CW] eval: return: 182.17833, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   681 ----
[CW] collect: return: 182.38678, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 0.08082, qf2_loss: 0.08016, policy_loss: -29.06050, policy_entropy: -6.15317, alpha: 0.00513, time: 33.70632
[CW] ---------------------------
[CW] ---- Iteration:   682 ----
[CW] collect: return: 188.83352, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 0.07854, qf2_loss: 0.08036, policy_loss: -29.08700, policy_entropy: -5.98458, alpha: 0.00516, time: 33.91223
[CW] ---------------------------
[CW] ---- Iteration:   683 ----
[CW] collect: return: 182.96882, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 0.08359, qf2_loss: 0.08553, policy_loss: -28.97258, policy_entropy: -6.06321, alpha: 0.00515, time: 33.80374
[CW] ---------------------------
[CW] ---- Iteration:   684 ----
[CW] collect: return: 185.12935, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 0.07608, qf2_loss: 0.07632, policy_loss: -29.05962, policy_entropy: -6.12584, alpha: 0.00518, time: 33.32336
[CW] ---------------------------
[CW] ---- Iteration:   685 ----
[CW] collect: return: 185.28593, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 0.08041, qf2_loss: 0.08177, policy_loss: -29.13222, policy_entropy: -5.89584, alpha: 0.00522, time: 33.83681
[CW] ---------------------------
[CW] ---- Iteration:   686 ----
[CW] collect: return: 186.28181, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 0.07898, qf2_loss: 0.07823, policy_loss: -29.12960, policy_entropy: -5.70228, alpha: 0.00512, time: 33.96316
[CW] ---------------------------
[CW] ---- Iteration:   687 ----
[CW] collect: return: 188.92996, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 0.07762, qf2_loss: 0.07858, policy_loss: -29.04040, policy_entropy: -5.94501, alpha: 0.00507, time: 34.00180
[CW] ---------------------------
[CW] ---- Iteration:   688 ----
[CW] collect: return: 186.55894, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 0.08284, qf2_loss: 0.08479, policy_loss: -29.08353, policy_entropy: -6.00162, alpha: 0.00505, time: 33.87309
[CW] ---------------------------
[CW] ---- Iteration:   689 ----
[CW] collect: return: 172.65921, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 0.07585, qf2_loss: 0.07752, policy_loss: -29.13976, policy_entropy: -6.05965, alpha: 0.00506, time: 33.42511
[CW] ---------------------------
[CW] ---- Iteration:   690 ----
[CW] collect: return: 181.55212, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 0.07523, qf2_loss: 0.07581, policy_loss: -29.15028, policy_entropy: -5.92060, alpha: 0.00508, time: 33.47799
[CW] ---------------------------
[CW] ---- Iteration:   691 ----
[CW] collect: return: 174.72009, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 0.07721, qf2_loss: 0.07668, policy_loss: -29.22447, policy_entropy: -6.15960, alpha: 0.00508, time: 33.84369
[CW] ---------------------------
[CW] ---- Iteration:   692 ----
[CW] collect: return: 193.07462, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 0.07583, qf2_loss: 0.07712, policy_loss: -29.28406, policy_entropy: -6.09981, alpha: 0.00514, time: 33.91877
[CW] ---------------------------
[CW] ---- Iteration:   693 ----
[CW] collect: return: 186.52383, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 0.07349, qf2_loss: 0.07463, policy_loss: -29.22991, policy_entropy: -5.96463, alpha: 0.00514, time: 33.95485
[CW] ---------------------------
[CW] ---- Iteration:   694 ----
[CW] collect: return: 167.69336, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 0.07638, qf2_loss: 0.07642, policy_loss: -29.32476, policy_entropy: -5.93065, alpha: 0.00514, time: 33.93484
[CW] ---------------------------
[CW] ---- Iteration:   695 ----
[CW] collect: return: 178.83839, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 0.07278, qf2_loss: 0.07367, policy_loss: -29.20857, policy_entropy: -5.97279, alpha: 0.00509, time: 33.99167
[CW] ---------------------------
[CW] ---- Iteration:   696 ----
[CW] collect: return: 188.74341, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 0.09085, qf2_loss: 0.09404, policy_loss: -29.31816, policy_entropy: -5.80892, alpha: 0.00508, time: 34.26244
[CW] ---------------------------
[CW] ---- Iteration:   697 ----
[CW] collect: return: 189.46713, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 0.07702, qf2_loss: 0.07683, policy_loss: -29.43081, policy_entropy: -6.01061, alpha: 0.00502, time: 33.85436
[CW] ---------------------------
[CW] ---- Iteration:   698 ----
[CW] collect: return: 174.55565, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 0.07550, qf2_loss: 0.07482, policy_loss: -29.45114, policy_entropy: -6.22651, alpha: 0.00508, time: 34.01033
[CW] ---------------------------
[CW] ---- Iteration:   699 ----
[CW] collect: return: 162.51623, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 0.07699, qf2_loss: 0.07977, policy_loss: -29.38456, policy_entropy: -5.95703, alpha: 0.00513, time: 33.86873
[CW] ---------------------------
[CW] ---- Iteration:   700 ----
[CW] collect: return: 184.43959, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 0.07553, qf2_loss: 0.07648, policy_loss: -29.38887, policy_entropy: -5.91544, alpha: 0.00509, time: 33.94335
[CW] eval: return: 179.71868, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   701 ----
[CW] collect: return: 181.86447, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 0.07635, qf2_loss: 0.07841, policy_loss: -29.37757, policy_entropy: -6.13440, alpha: 0.00508, time: 33.61985
[CW] ---------------------------
[CW] ---- Iteration:   702 ----
[CW] collect: return: 177.88380, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 0.08032, qf2_loss: 0.07923, policy_loss: -29.45716, policy_entropy: -6.20401, alpha: 0.00516, time: 33.60311
[CW] ---------------------------
[CW] ---- Iteration:   703 ----
[CW] collect: return: 183.65262, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 0.08023, qf2_loss: 0.08066, policy_loss: -29.46554, policy_entropy: -6.08404, alpha: 0.00521, time: 33.92815
[CW] ---------------------------
[CW] ---- Iteration:   704 ----
[CW] collect: return: 179.42432, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 0.08155, qf2_loss: 0.08217, policy_loss: -29.44853, policy_entropy: -5.89716, alpha: 0.00520, time: 33.90539
[CW] ---------------------------
[CW] ---- Iteration:   705 ----
[CW] collect: return: 184.73081, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 0.08003, qf2_loss: 0.08090, policy_loss: -29.59761, policy_entropy: -6.04503, alpha: 0.00519, time: 33.98319
[CW] ---------------------------
[CW] ---- Iteration:   706 ----
[CW] collect: return: 185.39081, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 0.08514, qf2_loss: 0.08597, policy_loss: -29.58829, policy_entropy: -5.89807, alpha: 0.00518, time: 33.69877
[CW] ---------------------------
[CW] ---- Iteration:   707 ----
[CW] collect: return: 186.77650, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 0.09235, qf2_loss: 0.09325, policy_loss: -29.51538, policy_entropy: -6.17819, alpha: 0.00520, time: 33.94301
[CW] ---------------------------
[CW] ---- Iteration:   708 ----
[CW] collect: return: 186.16509, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 0.07963, qf2_loss: 0.08154, policy_loss: -29.62595, policy_entropy: -6.09561, alpha: 0.00525, time: 33.84880
[CW] ---------------------------
[CW] ---- Iteration:   709 ----
[CW] collect: return: 188.16008, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 0.07601, qf2_loss: 0.07516, policy_loss: -29.67305, policy_entropy: -6.08455, alpha: 0.00529, time: 33.64764
[CW] ---------------------------
[CW] ---- Iteration:   710 ----
[CW] collect: return: 173.76759, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 0.07899, qf2_loss: 0.08092, policy_loss: -29.65902, policy_entropy: -5.99025, alpha: 0.00528, time: 33.48806
[CW] ---------------------------
[CW] ---- Iteration:   711 ----
[CW] collect: return: 170.23581, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 0.07788, qf2_loss: 0.07930, policy_loss: -29.61952, policy_entropy: -5.98326, alpha: 0.00529, time: 33.84033
[CW] ---------------------------
[CW] ---- Iteration:   712 ----
[CW] collect: return: 153.78589, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 0.08813, qf2_loss: 0.08859, policy_loss: -29.68396, policy_entropy: -5.90697, alpha: 0.00527, time: 33.66998
[CW] ---------------------------
[CW] ---- Iteration:   713 ----
[CW] collect: return: 181.30977, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 0.08227, qf2_loss: 0.08421, policy_loss: -29.71346, policy_entropy: -6.00558, alpha: 0.00525, time: 34.31180
[CW] ---------------------------
[CW] ---- Iteration:   714 ----
[CW] collect: return: 181.47826, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 0.07762, qf2_loss: 0.07815, policy_loss: -29.73651, policy_entropy: -5.99996, alpha: 0.00525, time: 33.71041
[CW] ---------------------------
[CW] ---- Iteration:   715 ----
[CW] collect: return: 180.92988, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 0.08049, qf2_loss: 0.08105, policy_loss: -29.79963, policy_entropy: -6.01143, alpha: 0.00526, time: 33.54208
[CW] ---------------------------
[CW] ---- Iteration:   716 ----
[CW] collect: return: 171.92449, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 0.07452, qf2_loss: 0.07478, policy_loss: -29.75136, policy_entropy: -5.79012, alpha: 0.00521, time: 34.00583
[CW] ---------------------------
[CW] ---- Iteration:   717 ----
[CW] collect: return: 181.83395, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 0.08049, qf2_loss: 0.08375, policy_loss: -29.77920, policy_entropy: -5.90177, alpha: 0.00517, time: 34.19962
[CW] ---------------------------
[CW] ---- Iteration:   718 ----
[CW] collect: return: 184.91495, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 0.07717, qf2_loss: 0.07696, policy_loss: -29.79181, policy_entropy: -6.29790, alpha: 0.00520, time: 34.32028
[CW] ---------------------------
[CW] ---- Iteration:   719 ----
[CW] collect: return: 179.86074, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 0.07648, qf2_loss: 0.07752, policy_loss: -29.82248, policy_entropy: -6.01513, alpha: 0.00528, time: 33.68008
[CW] ---------------------------
[CW] ---- Iteration:   720 ----
[CW] collect: return: 182.12994, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 0.07398, qf2_loss: 0.07430, policy_loss: -29.85090, policy_entropy: -5.79099, alpha: 0.00523, time: 33.43321
[CW] eval: return: 179.13209, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   721 ----
[CW] collect: return: 172.98403, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 0.07298, qf2_loss: 0.07433, policy_loss: -29.85228, policy_entropy: -5.81945, alpha: 0.00516, time: 33.90036
[CW] ---------------------------
[CW] ---- Iteration:   722 ----
[CW] collect: return: 189.08167, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 0.07297, qf2_loss: 0.07351, policy_loss: -29.85166, policy_entropy: -5.71587, alpha: 0.00508, time: 33.94784
[CW] ---------------------------
[CW] ---- Iteration:   723 ----
[CW] collect: return: 183.52885, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 0.07672, qf2_loss: 0.07742, policy_loss: -29.89803, policy_entropy: -5.92258, alpha: 0.00500, time: 33.80240
[CW] ---------------------------
[CW] ---- Iteration:   724 ----
[CW] collect: return: 161.03909, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 0.09114, qf2_loss: 0.09172, policy_loss: -29.92740, policy_entropy: -6.04676, alpha: 0.00499, time: 33.89665
[CW] ---------------------------
[CW] ---- Iteration:   725 ----
[CW] collect: return: 181.39151, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 0.06955, qf2_loss: 0.07072, policy_loss: -29.92992, policy_entropy: -6.15969, alpha: 0.00502, time: 36.32076
[CW] ---------------------------
[CW] ---- Iteration:   726 ----
[CW] collect: return: 173.44355, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 0.08126, qf2_loss: 0.08350, policy_loss: -29.92988, policy_entropy: -6.18826, alpha: 0.00508, time: 34.03574
[CW] ---------------------------
[CW] ---- Iteration:   727 ----
[CW] collect: return: 189.73223, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 0.07033, qf2_loss: 0.07090, policy_loss: -30.04695, policy_entropy: -6.34733, alpha: 0.00520, time: 33.74274
[CW] ---------------------------
[CW] ---- Iteration:   728 ----
[CW] collect: return: 185.06547, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 0.08011, qf2_loss: 0.08000, policy_loss: -29.97690, policy_entropy: -6.07431, alpha: 0.00528, time: 33.91762
[CW] ---------------------------
[CW] ---- Iteration:   729 ----
[CW] collect: return: 129.61037, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 0.07313, qf2_loss: 0.07371, policy_loss: -30.04326, policy_entropy: -5.85094, alpha: 0.00526, time: 33.63176
[CW] ---------------------------
[CW] ---- Iteration:   730 ----
[CW] collect: return: 183.91186, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 0.08322, qf2_loss: 0.08504, policy_loss: -30.10250, policy_entropy: -5.72184, alpha: 0.00518, time: 33.51393
[CW] ---------------------------
[CW] ---- Iteration:   731 ----
[CW] collect: return: 174.81007, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 0.08798, qf2_loss: 0.08804, policy_loss: -30.08984, policy_entropy: -5.73869, alpha: 0.00512, time: 34.21003
[CW] ---------------------------
[CW] ---- Iteration:   732 ----
[CW] collect: return: 179.08314, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 0.06733, qf2_loss: 0.06845, policy_loss: -30.11008, policy_entropy: -5.61467, alpha: 0.00496, time: 34.07775
[CW] ---------------------------
[CW] ---- Iteration:   733 ----
[CW] collect: return: 180.70569, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 0.07348, qf2_loss: 0.07484, policy_loss: -30.24423, policy_entropy: -6.14949, alpha: 0.00492, time: 34.29440
[CW] ---------------------------
[CW] ---- Iteration:   734 ----
[CW] collect: return: 180.58341, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 0.07590, qf2_loss: 0.07634, policy_loss: -30.17975, policy_entropy: -6.14284, alpha: 0.00500, time: 34.10810
[CW] ---------------------------
[CW] ---- Iteration:   735 ----
[CW] collect: return: 184.17399, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 0.07513, qf2_loss: 0.07527, policy_loss: -30.19213, policy_entropy: -6.09387, alpha: 0.00502, time: 33.84136
[CW] ---------------------------
[CW] ---- Iteration:   736 ----
[CW] collect: return: 186.20158, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 0.06654, qf2_loss: 0.06754, policy_loss: -30.26465, policy_entropy: -6.00917, alpha: 0.00503, time: 33.78825
[CW] ---------------------------
[CW] ---- Iteration:   737 ----
[CW] collect: return: 176.92004, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 0.06975, qf2_loss: 0.07088, policy_loss: -30.25854, policy_entropy: -6.14123, alpha: 0.00506, time: 35.60652
[CW] ---------------------------
[CW] ---- Iteration:   738 ----
[CW] collect: return: 188.40436, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 0.06792, qf2_loss: 0.06864, policy_loss: -30.22762, policy_entropy: -6.01617, alpha: 0.00511, time: 34.15071
[CW] ---------------------------
[CW] ---- Iteration:   739 ----
[CW] collect: return: 172.49587, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 0.07716, qf2_loss: 0.07831, policy_loss: -30.22911, policy_entropy: -5.81861, alpha: 0.00506, time: 33.89686
[CW] ---------------------------
[CW] ---- Iteration:   740 ----
[CW] collect: return: 182.56226, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 0.07303, qf2_loss: 0.07319, policy_loss: -30.24563, policy_entropy: -5.94818, alpha: 0.00502, time: 34.03426
[CW] eval: return: 183.83360, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   741 ----
[CW] collect: return: 195.13991, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 0.07504, qf2_loss: 0.07732, policy_loss: -30.24649, policy_entropy: -5.77624, alpha: 0.00497, time: 33.62387
[CW] ---------------------------
[CW] ---- Iteration:   742 ----
[CW] collect: return: 157.67478, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 0.07308, qf2_loss: 0.07322, policy_loss: -30.33642, policy_entropy: -5.96888, alpha: 0.00494, time: 33.44187
[CW] ---------------------------
[CW] ---- Iteration:   743 ----
[CW] collect: return: 173.98711, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 0.07615, qf2_loss: 0.07744, policy_loss: -30.37727, policy_entropy: -6.01819, alpha: 0.00492, time: 33.82277
[CW] ---------------------------
[CW] ---- Iteration:   744 ----
[CW] collect: return: 183.43408, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 0.08048, qf2_loss: 0.08223, policy_loss: -30.42969, policy_entropy: -5.92706, alpha: 0.00491, time: 33.64958
[CW] ---------------------------
[CW] ---- Iteration:   745 ----
[CW] collect: return: 190.19165, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 0.07877, qf2_loss: 0.07956, policy_loss: -30.35852, policy_entropy: -5.94843, alpha: 0.00490, time: 34.15885
[CW] ---------------------------
[CW] ---- Iteration:   746 ----
[CW] collect: return: 173.36585, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 0.06591, qf2_loss: 0.06780, policy_loss: -30.39382, policy_entropy: -5.98293, alpha: 0.00488, time: 34.14926
[CW] ---------------------------
[CW] ---- Iteration:   747 ----
[CW] collect: return: 180.16897, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 0.07901, qf2_loss: 0.07899, policy_loss: -30.42016, policy_entropy: -6.04076, alpha: 0.00489, time: 33.99217
[CW] ---------------------------
[CW] ---- Iteration:   748 ----
[CW] collect: return: 157.61492, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 0.06807, qf2_loss: 0.06905, policy_loss: -30.41486, policy_entropy: -5.86198, alpha: 0.00487, time: 33.60758
[CW] ---------------------------
[CW] ---- Iteration:   749 ----
[CW] collect: return: 179.56005, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 0.07064, qf2_loss: 0.07119, policy_loss: -30.53011, policy_entropy: -5.93149, alpha: 0.00484, time: 33.49401
[CW] ---------------------------
[CW] ---- Iteration:   750 ----
[CW] collect: return: 191.10626, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 0.06765, qf2_loss: 0.06787, policy_loss: -30.52646, policy_entropy: -6.12221, alpha: 0.00484, time: 33.45257
[CW] ---------------------------
[CW] ---- Iteration:   751 ----
[CW] collect: return: 182.44499, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 0.06860, qf2_loss: 0.06919, policy_loss: -30.49027, policy_entropy: -5.91271, alpha: 0.00485, time: 33.92395
[CW] ---------------------------
[CW] ---- Iteration:   752 ----
[CW] collect: return: 184.69761, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 0.07265, qf2_loss: 0.07387, policy_loss: -30.51252, policy_entropy: -5.98702, alpha: 0.00484, time: 34.42513
[CW] ---------------------------
[CW] ---- Iteration:   753 ----
[CW] collect: return: 174.64806, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 0.07041, qf2_loss: 0.07096, policy_loss: -30.54780, policy_entropy: -6.04472, alpha: 0.00483, time: 34.08756
[CW] ---------------------------
[CW] ---- Iteration:   754 ----
[CW] collect: return: 175.14059, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 0.07390, qf2_loss: 0.07427, policy_loss: -30.63732, policy_entropy: -6.42954, alpha: 0.00490, time: 34.02701
[CW] ---------------------------
[CW] ---- Iteration:   755 ----
[CW] collect: return: 188.67798, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 0.07851, qf2_loss: 0.08156, policy_loss: -30.60237, policy_entropy: -5.86419, alpha: 0.00499, time: 34.10538
[CW] ---------------------------
[CW] ---- Iteration:   756 ----
[CW] collect: return: 189.04638, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 0.07150, qf2_loss: 0.07013, policy_loss: -30.65200, policy_entropy: -6.15516, alpha: 0.00496, time: 33.76645
[CW] ---------------------------
[CW] ---- Iteration:   757 ----
[CW] collect: return: 192.45966, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 0.06941, qf2_loss: 0.06970, policy_loss: -30.72760, policy_entropy: -6.01281, alpha: 0.00502, time: 34.02393
[CW] ---------------------------
[CW] ---- Iteration:   758 ----
[CW] collect: return: 168.85772, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 0.07046, qf2_loss: 0.07131, policy_loss: -30.65230, policy_entropy: -5.84139, alpha: 0.00500, time: 33.79978
[CW] ---------------------------
[CW] ---- Iteration:   759 ----
[CW] collect: return: 183.43925, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 0.08975, qf2_loss: 0.09145, policy_loss: -30.62576, policy_entropy: -5.54225, alpha: 0.00491, time: 33.52518
[CW] ---------------------------
[CW] ---- Iteration:   760 ----
[CW] collect: return: 199.69407, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 0.07457, qf2_loss: 0.07609, policy_loss: -30.70902, policy_entropy: -5.55783, alpha: 0.00473, time: 33.91066
[CW] eval: return: 183.91480, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   761 ----
[CW] collect: return: 190.96495, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 0.07118, qf2_loss: 0.07333, policy_loss: -30.70248, policy_entropy: -5.86678, alpha: 0.00464, time: 33.88481
[CW] ---------------------------
[CW] ---- Iteration:   762 ----
[CW] collect: return: 188.78022, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 0.06916, qf2_loss: 0.06907, policy_loss: -30.73854, policy_entropy: -5.79427, alpha: 0.00460, time: 33.64687
[CW] ---------------------------
[CW] ---- Iteration:   763 ----
[CW] collect: return: 188.67291, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 0.06917, qf2_loss: 0.06953, policy_loss: -30.74696, policy_entropy: -6.01827, alpha: 0.00455, time: 34.02153
[CW] ---------------------------
[CW] ---- Iteration:   764 ----
[CW] collect: return: 184.13967, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 0.06594, qf2_loss: 0.06678, policy_loss: -30.76357, policy_entropy: -6.07090, alpha: 0.00456, time: 34.00341
[CW] ---------------------------
[CW] ---- Iteration:   765 ----
[CW] collect: return: 191.85310, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 0.07227, qf2_loss: 0.07395, policy_loss: -30.80181, policy_entropy: -6.12276, alpha: 0.00460, time: 34.24900
[CW] ---------------------------
[CW] ---- Iteration:   766 ----
[CW] collect: return: 194.26351, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 0.06652, qf2_loss: 0.06826, policy_loss: -30.76479, policy_entropy: -5.76165, alpha: 0.00459, time: 34.64922
[CW] ---------------------------
[CW] ---- Iteration:   767 ----
[CW] collect: return: 171.76520, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 0.07069, qf2_loss: 0.07152, policy_loss: -30.77762, policy_entropy: -5.95733, alpha: 0.00454, time: 34.18509
[CW] ---------------------------
[CW] ---- Iteration:   768 ----
[CW] collect: return: 186.89874, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 0.08685, qf2_loss: 0.08794, policy_loss: -30.85097, policy_entropy: -5.97107, alpha: 0.00454, time: 34.08241
[CW] ---------------------------
[CW] ---- Iteration:   769 ----
[CW] collect: return: 185.28252, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 0.07225, qf2_loss: 0.07228, policy_loss: -30.85503, policy_entropy: -5.88370, alpha: 0.00451, time: 34.07411
[CW] ---------------------------
[CW] ---- Iteration:   770 ----
[CW] collect: return: 174.19983, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 0.06775, qf2_loss: 0.06751, policy_loss: -30.80603, policy_entropy: -6.18066, alpha: 0.00451, time: 33.77321
[CW] ---------------------------
[CW] ---- Iteration:   771 ----
[CW] collect: return: 187.36417, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 0.07700, qf2_loss: 0.07929, policy_loss: -31.01082, policy_entropy: -6.08375, alpha: 0.00456, time: 33.97821
[CW] ---------------------------
[CW] ---- Iteration:   772 ----
[CW] collect: return: 189.11917, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 0.07012, qf2_loss: 0.07112, policy_loss: -30.87253, policy_entropy: -5.85656, alpha: 0.00456, time: 33.94269
[CW] ---------------------------
[CW] ---- Iteration:   773 ----
[CW] collect: return: 173.66020, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 0.08313, qf2_loss: 0.08332, policy_loss: -30.99838, policy_entropy: -5.97462, alpha: 0.00452, time: 34.27167
[CW] ---------------------------
[CW] ---- Iteration:   774 ----
[CW] collect: return: 161.11140, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 0.08502, qf2_loss: 0.08756, policy_loss: -30.93049, policy_entropy: -5.63801, alpha: 0.00447, time: 33.83751
[CW] ---------------------------
[CW] ---- Iteration:   775 ----
[CW] collect: return: 183.79684, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 0.07021, qf2_loss: 0.07067, policy_loss: -31.01727, policy_entropy: -5.88350, alpha: 0.00439, time: 33.94938
[CW] ---------------------------
[CW] ---- Iteration:   776 ----
[CW] collect: return: 169.62966, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 0.06612, qf2_loss: 0.06654, policy_loss: -30.99279, policy_entropy: -6.16077, alpha: 0.00438, time: 33.88116
[CW] ---------------------------
[CW] ---- Iteration:   777 ----
[CW] collect: return: 179.52194, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 0.08009, qf2_loss: 0.08055, policy_loss: -31.01117, policy_entropy: -6.11308, alpha: 0.00445, time: 34.29699
[CW] ---------------------------
[CW] ---- Iteration:   778 ----
[CW] collect: return: 183.58006, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 0.07248, qf2_loss: 0.07291, policy_loss: -31.00713, policy_entropy: -5.80102, alpha: 0.00443, time: 33.83127
[CW] ---------------------------
[CW] ---- Iteration:   779 ----
[CW] collect: return: 188.85226, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 0.07923, qf2_loss: 0.07837, policy_loss: -31.01590, policy_entropy: -6.13770, alpha: 0.00441, time: 33.76784
[CW] ---------------------------
[CW] ---- Iteration:   780 ----
[CW] collect: return: 184.03173, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 0.08137, qf2_loss: 0.08135, policy_loss: -31.01693, policy_entropy: -5.90465, alpha: 0.00444, time: 34.40299
[CW] eval: return: 182.40280, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   781 ----
[CW] collect: return: 182.87723, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 0.07736, qf2_loss: 0.07872, policy_loss: -31.04955, policy_entropy: -6.05409, alpha: 0.00440, time: 33.78536
[CW] ---------------------------
[CW] ---- Iteration:   782 ----
[CW] collect: return: 194.28062, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 0.07047, qf2_loss: 0.07175, policy_loss: -31.07505, policy_entropy: -6.02780, alpha: 0.00443, time: 33.72564
[CW] ---------------------------
[CW] ---- Iteration:   783 ----
[CW] collect: return: 197.51317, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 0.07031, qf2_loss: 0.07158, policy_loss: -31.14547, policy_entropy: -6.08610, alpha: 0.00445, time: 33.80712
[CW] ---------------------------
[CW] ---- Iteration:   784 ----
[CW] collect: return: 166.93623, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 0.06657, qf2_loss: 0.06768, policy_loss: -31.17038, policy_entropy: -6.03207, alpha: 0.00446, time: 33.76085
[CW] ---------------------------
[CW] ---- Iteration:   785 ----
[CW] collect: return: 196.40334, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 0.08457, qf2_loss: 0.08456, policy_loss: -31.11387, policy_entropy: -6.36257, alpha: 0.00451, time: 34.09530
[CW] ---------------------------
[CW] ---- Iteration:   786 ----
[CW] collect: return: 183.26453, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 0.07929, qf2_loss: 0.08215, policy_loss: -31.08491, policy_entropy: -6.22740, alpha: 0.00463, time: 33.97693
[CW] ---------------------------
[CW] ---- Iteration:   787 ----
[CW] collect: return: 194.09794, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 0.06463, qf2_loss: 0.06579, policy_loss: -31.21735, policy_entropy: -6.03601, alpha: 0.00467, time: 34.02801
[CW] ---------------------------
[CW] ---- Iteration:   788 ----
[CW] collect: return: 176.45568, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 0.06782, qf2_loss: 0.06769, policy_loss: -31.15711, policy_entropy: -6.03349, alpha: 0.00467, time: 33.91054
[CW] ---------------------------
[CW] ---- Iteration:   789 ----
[CW] collect: return: 195.64934, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 0.07594, qf2_loss: 0.07686, policy_loss: -31.14816, policy_entropy: -6.11468, alpha: 0.00470, time: 34.14116
[CW] ---------------------------
[CW] ---- Iteration:   790 ----
[CW] collect: return: 188.50172, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 0.07490, qf2_loss: 0.07526, policy_loss: -31.16098, policy_entropy: -5.76704, alpha: 0.00469, time: 33.50933
[CW] ---------------------------
[CW] ---- Iteration:   791 ----
[CW] collect: return: 176.57357, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 0.06394, qf2_loss: 0.06419, policy_loss: -31.32596, policy_entropy: -5.99725, alpha: 0.00465, time: 34.31365
[CW] ---------------------------
[CW] ---- Iteration:   792 ----
[CW] collect: return: 168.89427, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 0.07052, qf2_loss: 0.07107, policy_loss: -31.24206, policy_entropy: -6.01216, alpha: 0.00465, time: 33.89297
[CW] ---------------------------
[CW] ---- Iteration:   793 ----
[CW] collect: return: 171.71922, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 0.07587, qf2_loss: 0.07728, policy_loss: -31.27728, policy_entropy: -5.91538, alpha: 0.00465, time: 34.13700
[CW] ---------------------------
[CW] ---- Iteration:   794 ----
[CW] collect: return: 188.59243, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 0.06807, qf2_loss: 0.06985, policy_loss: -31.21384, policy_entropy: -5.77119, alpha: 0.00458, time: 34.21127
[CW] ---------------------------
[CW] ---- Iteration:   795 ----
[CW] collect: return: 193.53272, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 0.07410, qf2_loss: 0.07368, policy_loss: -31.29383, policy_entropy: -5.89361, alpha: 0.00454, time: 34.16221
[CW] ---------------------------
[CW] ---- Iteration:   796 ----
[CW] collect: return: 192.45193, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 0.07475, qf2_loss: 0.07676, policy_loss: -31.29557, policy_entropy: -5.68917, alpha: 0.00446, time: 33.83144
[CW] ---------------------------
[CW] ---- Iteration:   797 ----
[CW] collect: return: 200.47653, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 0.07020, qf2_loss: 0.07079, policy_loss: -31.36275, policy_entropy: -5.85681, alpha: 0.00438, time: 34.03294
[CW] ---------------------------
[CW] ---- Iteration:   798 ----
[CW] collect: return: 188.15710, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 0.06591, qf2_loss: 0.06742, policy_loss: -31.35940, policy_entropy: -5.98479, alpha: 0.00435, time: 34.12141
[CW] ---------------------------
[CW] ---- Iteration:   799 ----
[CW] collect: return: 199.91957, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 0.07176, qf2_loss: 0.07223, policy_loss: -31.29569, policy_entropy: -6.16918, alpha: 0.00439, time: 33.93732
[CW] ---------------------------
[CW] ---- Iteration:   800 ----
[CW] collect: return: 198.21445, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 0.07860, qf2_loss: 0.07820, policy_loss: -31.39163, policy_entropy: -6.10578, alpha: 0.00444, time: 33.76801
[CW] eval: return: 184.08983, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   801 ----
[CW] collect: return: 195.76682, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 0.07521, qf2_loss: 0.07565, policy_loss: -31.32882, policy_entropy: -6.26619, alpha: 0.00447, time: 34.15681
[CW] ---------------------------
[CW] ---- Iteration:   802 ----
[CW] collect: return: 186.64877, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 0.07965, qf2_loss: 0.08012, policy_loss: -31.35387, policy_entropy: -5.84153, alpha: 0.00452, time: 34.05977
[CW] ---------------------------
[CW] ---- Iteration:   803 ----
[CW] collect: return: 193.53287, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 0.06701, qf2_loss: 0.06776, policy_loss: -31.40666, policy_entropy: -5.73563, alpha: 0.00444, time: 33.95475
[CW] ---------------------------
[CW] ---- Iteration:   804 ----
[CW] collect: return: 182.47510, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 0.06952, qf2_loss: 0.06979, policy_loss: -31.37261, policy_entropy: -5.90397, alpha: 0.00439, time: 33.92473
[CW] ---------------------------
[CW] ---- Iteration:   805 ----
[CW] collect: return: 194.92010, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 0.07340, qf2_loss: 0.07376, policy_loss: -31.45207, policy_entropy: -5.97018, alpha: 0.00435, time: 34.13731
[CW] ---------------------------
[CW] ---- Iteration:   806 ----
[CW] collect: return: 185.70165, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 0.07713, qf2_loss: 0.07916, policy_loss: -31.41723, policy_entropy: -6.23617, alpha: 0.00438, time: 34.07287
[CW] ---------------------------
[CW] ---- Iteration:   807 ----
[CW] collect: return: 202.88254, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 0.07900, qf2_loss: 0.07821, policy_loss: -31.49730, policy_entropy: -6.10012, alpha: 0.00446, time: 34.03710
[CW] ---------------------------
[CW] ---- Iteration:   808 ----
[CW] collect: return: 188.34996, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 0.07452, qf2_loss: 0.07456, policy_loss: -31.54946, policy_entropy: -6.32735, alpha: 0.00452, time: 34.00530
[CW] ---------------------------
[CW] ---- Iteration:   809 ----
[CW] collect: return: 192.82173, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 0.06881, qf2_loss: 0.06978, policy_loss: -31.50254, policy_entropy: -6.31530, alpha: 0.00460, time: 35.64391
[CW] ---------------------------
[CW] ---- Iteration:   810 ----
[CW] collect: return: 199.14216, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 0.07647, qf2_loss: 0.07938, policy_loss: -31.45683, policy_entropy: -6.10283, alpha: 0.00468, time: 33.77976
[CW] ---------------------------
[CW] ---- Iteration:   811 ----
[CW] collect: return: 207.01310, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 0.07291, qf2_loss: 0.07228, policy_loss: -31.48950, policy_entropy: -6.14328, alpha: 0.00471, time: 33.97850
[CW] ---------------------------
[CW] ---- Iteration:   812 ----
[CW] collect: return: 196.83123, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 0.07014, qf2_loss: 0.07153, policy_loss: -31.64683, policy_entropy: -6.06159, alpha: 0.00476, time: 34.06588
[CW] ---------------------------
[CW] ---- Iteration:   813 ----
[CW] collect: return: 181.39738, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 0.07185, qf2_loss: 0.07237, policy_loss: -31.54452, policy_entropy: -5.93616, alpha: 0.00476, time: 34.25135
[CW] ---------------------------
[CW] ---- Iteration:   814 ----
[CW] collect: return: 197.80207, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 0.07611, qf2_loss: 0.07554, policy_loss: -31.65066, policy_entropy: -6.23923, alpha: 0.00478, time: 33.98779
[CW] ---------------------------
[CW] ---- Iteration:   815 ----
[CW] collect: return: 198.18570, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 0.07847, qf2_loss: 0.07735, policy_loss: -31.62876, policy_entropy: -6.18605, alpha: 0.00486, time: 34.04125
[CW] ---------------------------
[CW] ---- Iteration:   816 ----
[CW] collect: return: 193.10102, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 0.07377, qf2_loss: 0.07412, policy_loss: -31.65143, policy_entropy: -5.98507, alpha: 0.00490, time: 34.04220
[CW] ---------------------------
[CW] ---- Iteration:   817 ----
[CW] collect: return: 193.49903, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 0.06391, qf2_loss: 0.06441, policy_loss: -31.71242, policy_entropy: -5.90424, alpha: 0.00489, time: 34.29122
[CW] ---------------------------
[CW] ---- Iteration:   818 ----
[CW] collect: return: 192.65038, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 0.06761, qf2_loss: 0.06861, policy_loss: -31.71368, policy_entropy: -5.93986, alpha: 0.00485, time: 33.67525
[CW] ---------------------------
[CW] ---- Iteration:   819 ----
[CW] collect: return: 191.10115, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 0.07436, qf2_loss: 0.07676, policy_loss: -31.71385, policy_entropy: -5.76930, alpha: 0.00480, time: 33.75557
[CW] ---------------------------
[CW] ---- Iteration:   820 ----
[CW] collect: return: 192.73766, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 0.06609, qf2_loss: 0.06553, policy_loss: -31.81505, policy_entropy: -5.79744, alpha: 0.00472, time: 34.24817
[CW] eval: return: 192.63798, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   821 ----
[CW] collect: return: 198.67561, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 0.07153, qf2_loss: 0.07260, policy_loss: -31.77861, policy_entropy: -5.99050, alpha: 0.00468, time: 33.77849
[CW] ---------------------------
[CW] ---- Iteration:   822 ----
[CW] collect: return: 190.28114, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 0.07264, qf2_loss: 0.07335, policy_loss: -31.77577, policy_entropy: -5.93935, alpha: 0.00467, time: 33.92329
[CW] ---------------------------
[CW] ---- Iteration:   823 ----
[CW] collect: return: 199.44881, steps: 1000.00000, total_steps: 829000.00000
[CW] train: qf1_loss: 0.06756, qf2_loss: 0.06831, policy_loss: -31.82652, policy_entropy: -6.09661, alpha: 0.00467, time: 34.13252
[CW] ---------------------------
[CW] ---- Iteration:   824 ----
[CW] collect: return: 195.73184, steps: 1000.00000, total_steps: 830000.00000
[CW] train: qf1_loss: 0.06933, qf2_loss: 0.06985, policy_loss: -31.79179, policy_entropy: -6.05262, alpha: 0.00470, time: 33.91995
[CW] ---------------------------
[CW] ---- Iteration:   825 ----
[CW] collect: return: 204.07686, steps: 1000.00000, total_steps: 831000.00000
[CW] train: qf1_loss: 0.06780, qf2_loss: 0.06838, policy_loss: -31.89068, policy_entropy: -5.83014, alpha: 0.00468, time: 33.78039
[CW] ---------------------------
[CW] ---- Iteration:   826 ----
[CW] collect: return: 198.58581, steps: 1000.00000, total_steps: 832000.00000
[CW] train: qf1_loss: 0.06528, qf2_loss: 0.06655, policy_loss: -31.92634, policy_entropy: -6.14765, alpha: 0.00468, time: 33.64246
[CW] ---------------------------
[CW] ---- Iteration:   827 ----
[CW] collect: return: 169.98924, steps: 1000.00000, total_steps: 833000.00000
[CW] train: qf1_loss: 0.07771, qf2_loss: 0.07713, policy_loss: -31.89601, policy_entropy: -6.22149, alpha: 0.00474, time: 35.29046
[CW] ---------------------------
[CW] ---- Iteration:   828 ----
[CW] collect: return: 201.90857, steps: 1000.00000, total_steps: 834000.00000
[CW] train: qf1_loss: 0.07495, qf2_loss: 0.07628, policy_loss: -31.92359, policy_entropy: -6.40360, alpha: 0.00483, time: 34.19744
[CW] ---------------------------
[CW] ---- Iteration:   829 ----
[CW] collect: return: 198.02558, steps: 1000.00000, total_steps: 835000.00000
[CW] train: qf1_loss: 0.06783, qf2_loss: 0.06899, policy_loss: -31.96499, policy_entropy: -6.02030, alpha: 0.00493, time: 33.80615
[CW] ---------------------------
[CW] ---- Iteration:   830 ----
[CW] collect: return: 200.68666, steps: 1000.00000, total_steps: 836000.00000
[CW] train: qf1_loss: 0.06767, qf2_loss: 0.06823, policy_loss: -31.89193, policy_entropy: -5.77384, alpha: 0.00488, time: 33.86746
[CW] ---------------------------
[CW] ---- Iteration:   831 ----
[CW] collect: return: 175.67903, steps: 1000.00000, total_steps: 837000.00000
[CW] train: qf1_loss: 0.06909, qf2_loss: 0.06908, policy_loss: -31.93778, policy_entropy: -5.85717, alpha: 0.00482, time: 33.54757
[CW] ---------------------------
[CW] ---- Iteration:   832 ----
[CW] collect: return: 201.28488, steps: 1000.00000, total_steps: 838000.00000
[CW] train: qf1_loss: 0.06772, qf2_loss: 0.06892, policy_loss: -31.99467, policy_entropy: -6.12871, alpha: 0.00481, time: 33.84185
[CW] ---------------------------
[CW] ---- Iteration:   833 ----
[CW] collect: return: 195.05621, steps: 1000.00000, total_steps: 839000.00000
[CW] train: qf1_loss: 0.06442, qf2_loss: 0.06570, policy_loss: -31.96769, policy_entropy: -5.93325, alpha: 0.00485, time: 34.10249
[CW] ---------------------------
[CW] ---- Iteration:   834 ----
[CW] collect: return: 193.76880, steps: 1000.00000, total_steps: 840000.00000
[CW] train: qf1_loss: 0.06594, qf2_loss: 0.06619, policy_loss: -32.01879, policy_entropy: -6.04893, alpha: 0.00481, time: 34.15450
[CW] ---------------------------
[CW] ---- Iteration:   835 ----
[CW] collect: return: 204.53601, steps: 1000.00000, total_steps: 841000.00000
[CW] train: qf1_loss: 0.06994, qf2_loss: 0.06950, policy_loss: -32.05532, policy_entropy: -6.08372, alpha: 0.00485, time: 34.10281
[CW] ---------------------------
[CW] ---- Iteration:   836 ----
[CW] collect: return: 205.21792, steps: 1000.00000, total_steps: 842000.00000
[CW] train: qf1_loss: 0.06811, qf2_loss: 0.06825, policy_loss: -32.07941, policy_entropy: -5.94923, alpha: 0.00484, time: 34.32983
[CW] ---------------------------
[CW] ---- Iteration:   837 ----
[CW] collect: return: 194.31933, steps: 1000.00000, total_steps: 843000.00000
[CW] train: qf1_loss: 0.07129, qf2_loss: 0.07216, policy_loss: -32.05109, policy_entropy: -6.03236, alpha: 0.00485, time: 33.78060
[CW] ---------------------------
[CW] ---- Iteration:   838 ----
[CW] collect: return: 204.14144, steps: 1000.00000, total_steps: 844000.00000
[CW] train: qf1_loss: 0.06836, qf2_loss: 0.06912, policy_loss: -32.04046, policy_entropy: -5.99975, alpha: 0.00484, time: 34.12588
[CW] ---------------------------
[CW] ---- Iteration:   839 ----
[CW] collect: return: 181.73946, steps: 1000.00000, total_steps: 845000.00000
[CW] train: qf1_loss: 0.06430, qf2_loss: 0.06433, policy_loss: -32.06117, policy_entropy: -5.96399, alpha: 0.00484, time: 35.53060
[CW] ---------------------------
[CW] ---- Iteration:   840 ----
[CW] collect: return: 188.30349, steps: 1000.00000, total_steps: 846000.00000
[CW] train: qf1_loss: 0.06445, qf2_loss: 0.06465, policy_loss: -32.16030, policy_entropy: -5.77023, alpha: 0.00481, time: 33.57588
[CW] eval: return: 203.47829, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   841 ----
[CW] collect: return: 204.46723, steps: 1000.00000, total_steps: 847000.00000
[CW] train: qf1_loss: 0.06060, qf2_loss: 0.06035, policy_loss: -32.11916, policy_entropy: -6.07845, alpha: 0.00476, time: 33.76876
[CW] ---------------------------
[CW] ---- Iteration:   842 ----
[CW] collect: return: 208.51105, steps: 1000.00000, total_steps: 848000.00000
[CW] train: qf1_loss: 0.07630, qf2_loss: 0.07606, policy_loss: -32.15561, policy_entropy: -5.97725, alpha: 0.00479, time: 33.85238
[CW] ---------------------------
[CW] ---- Iteration:   843 ----
[CW] collect: return: 161.63382, steps: 1000.00000, total_steps: 849000.00000
[CW] train: qf1_loss: 0.06794, qf2_loss: 0.07029, policy_loss: -32.05393, policy_entropy: -5.93495, alpha: 0.00476, time: 33.57111
[CW] ---------------------------
[CW] ---- Iteration:   844 ----
[CW] collect: return: 206.06631, steps: 1000.00000, total_steps: 850000.00000
[CW] train: qf1_loss: 0.06286, qf2_loss: 0.06298, policy_loss: -32.14832, policy_entropy: -5.97921, alpha: 0.00475, time: 33.44417
[CW] ---------------------------
[CW] ---- Iteration:   845 ----
[CW] collect: return: 204.65179, steps: 1000.00000, total_steps: 851000.00000
[CW] train: qf1_loss: 0.06806, qf2_loss: 0.06901, policy_loss: -32.13972, policy_entropy: -5.88777, alpha: 0.00473, time: 33.57323
[CW] ---------------------------
[CW] ---- Iteration:   846 ----
[CW] collect: return: 214.26122, steps: 1000.00000, total_steps: 852000.00000
[CW] train: qf1_loss: 0.07315, qf2_loss: 0.07231, policy_loss: -32.16627, policy_entropy: -5.58691, alpha: 0.00467, time: 33.70440
[CW] ---------------------------
[CW] ---- Iteration:   847 ----
[CW] collect: return: 202.10120, steps: 1000.00000, total_steps: 853000.00000
[CW] train: qf1_loss: 0.06402, qf2_loss: 0.06461, policy_loss: -32.14619, policy_entropy: -5.95269, alpha: 0.00455, time: 33.59699
[CW] ---------------------------
[CW] ---- Iteration:   848 ----
[CW] collect: return: 214.19139, steps: 1000.00000, total_steps: 854000.00000
[CW] train: qf1_loss: 0.06151, qf2_loss: 0.06278, policy_loss: -32.13848, policy_entropy: -6.13560, alpha: 0.00457, time: 33.80857
[CW] ---------------------------
[CW] ---- Iteration:   849 ----
[CW] collect: return: 189.36308, steps: 1000.00000, total_steps: 855000.00000
[CW] train: qf1_loss: 0.06611, qf2_loss: 0.06678, policy_loss: -32.23043, policy_entropy: -6.12322, alpha: 0.00460, time: 33.63102
[CW] ---------------------------
[CW] ---- Iteration:   850 ----
[CW] collect: return: 189.84712, steps: 1000.00000, total_steps: 856000.00000
[CW] train: qf1_loss: 0.08116, qf2_loss: 0.08165, policy_loss: -32.20982, policy_entropy: -6.25852, alpha: 0.00467, time: 33.64548
[CW] ---------------------------
[CW] ---- Iteration:   851 ----
[CW] collect: return: 203.82678, steps: 1000.00000, total_steps: 857000.00000
[CW] train: qf1_loss: 0.06253, qf2_loss: 0.06326, policy_loss: -32.27548, policy_entropy: -6.05676, alpha: 0.00473, time: 33.57350
[CW] ---------------------------
[CW] ---- Iteration:   852 ----
[CW] collect: return: 206.25821, steps: 1000.00000, total_steps: 858000.00000
[CW] train: qf1_loss: 0.07139, qf2_loss: 0.07223, policy_loss: -32.27086, policy_entropy: -6.05720, alpha: 0.00476, time: 34.04185
[CW] ---------------------------
[CW] ---- Iteration:   853 ----
[CW] collect: return: 186.84497, steps: 1000.00000, total_steps: 859000.00000
[CW] train: qf1_loss: 0.06155, qf2_loss: 0.06205, policy_loss: -32.27491, policy_entropy: -6.18612, alpha: 0.00479, time: 34.24717
[CW] ---------------------------
[CW] ---- Iteration:   854 ----
[CW] collect: return: 198.58439, steps: 1000.00000, total_steps: 860000.00000
[CW] train: qf1_loss: 0.06040, qf2_loss: 0.06069, policy_loss: -32.27706, policy_entropy: -6.12578, alpha: 0.00485, time: 34.45178
[CW] ---------------------------
[CW] ---- Iteration:   855 ----
[CW] collect: return: 201.79080, steps: 1000.00000, total_steps: 861000.00000
[CW] train: qf1_loss: 0.06000, qf2_loss: 0.06010, policy_loss: -32.30001, policy_entropy: -6.18192, alpha: 0.00490, time: 33.52141
[CW] ---------------------------
[CW] ---- Iteration:   856 ----
[CW] collect: return: 192.80444, steps: 1000.00000, total_steps: 862000.00000
[CW] train: qf1_loss: 0.07453, qf2_loss: 0.07228, policy_loss: -32.24404, policy_entropy: -5.85149, alpha: 0.00491, time: 33.37794
[CW] ---------------------------
[CW] ---- Iteration:   857 ----
[CW] collect: return: 199.74734, steps: 1000.00000, total_steps: 863000.00000
[CW] train: qf1_loss: 0.06605, qf2_loss: 0.06881, policy_loss: -32.29039, policy_entropy: -6.12093, alpha: 0.00489, time: 34.07698
[CW] ---------------------------
[CW] ---- Iteration:   858 ----
[CW] collect: return: 185.87333, steps: 1000.00000, total_steps: 864000.00000
[CW] train: qf1_loss: 0.05929, qf2_loss: 0.05991, policy_loss: -32.28849, policy_entropy: -5.95206, alpha: 0.00491, time: 34.31897
[CW] ---------------------------
[CW] ---- Iteration:   859 ----
[CW] collect: return: 212.28298, steps: 1000.00000, total_steps: 865000.00000
[CW] train: qf1_loss: 0.05895, qf2_loss: 0.05961, policy_loss: -32.34402, policy_entropy: -6.15495, alpha: 0.00492, time: 34.11881
[CW] ---------------------------
[CW] ---- Iteration:   860 ----
[CW] collect: return: 197.86412, steps: 1000.00000, total_steps: 866000.00000
[CW] train: qf1_loss: 0.09211, qf2_loss: 0.09343, policy_loss: -32.33281, policy_entropy: -5.94176, alpha: 0.00497, time: 33.96172
[CW] eval: return: 205.44274, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   861 ----
[CW] collect: return: 210.21811, steps: 1000.00000, total_steps: 867000.00000
[CW] train: qf1_loss: 0.06879, qf2_loss: 0.07073, policy_loss: -32.40868, policy_entropy: -5.82662, alpha: 0.00491, time: 33.96135
[CW] ---------------------------
[CW] ---- Iteration:   862 ----
[CW] collect: return: 199.15900, steps: 1000.00000, total_steps: 868000.00000
[CW] train: qf1_loss: 0.06875, qf2_loss: 0.06937, policy_loss: -32.38869, policy_entropy: -5.89844, alpha: 0.00485, time: 33.94618
[CW] ---------------------------
[CW] ---- Iteration:   863 ----
[CW] collect: return: 215.49572, steps: 1000.00000, total_steps: 869000.00000
[CW] train: qf1_loss: 0.05985, qf2_loss: 0.06051, policy_loss: -32.40837, policy_entropy: -5.99595, alpha: 0.00483, time: 33.78528
[CW] ---------------------------
[CW] ---- Iteration:   864 ----
[CW] collect: return: 209.41185, steps: 1000.00000, total_steps: 870000.00000
[CW] train: qf1_loss: 0.06269, qf2_loss: 0.06362, policy_loss: -32.36065, policy_entropy: -6.14748, alpha: 0.00486, time: 33.96955
[CW] ---------------------------
[CW] ---- Iteration:   865 ----
[CW] collect: return: 212.70177, steps: 1000.00000, total_steps: 871000.00000
[CW] train: qf1_loss: 0.06049, qf2_loss: 0.06094, policy_loss: -32.45460, policy_entropy: -6.14220, alpha: 0.00492, time: 33.94234
[CW] ---------------------------
[CW] ---- Iteration:   866 ----
[CW] collect: return: 214.45205, steps: 1000.00000, total_steps: 872000.00000
[CW] train: qf1_loss: 0.06518, qf2_loss: 0.06513, policy_loss: -32.44728, policy_entropy: -6.24223, alpha: 0.00498, time: 33.66138
[CW] ---------------------------
[CW] ---- Iteration:   867 ----
[CW] collect: return: 205.22999, steps: 1000.00000, total_steps: 873000.00000
[CW] train: qf1_loss: 0.06192, qf2_loss: 0.06233, policy_loss: -32.40006, policy_entropy: -6.07840, alpha: 0.00507, time: 34.24666
[CW] ---------------------------
[CW] ---- Iteration:   868 ----
[CW] collect: return: 195.91728, steps: 1000.00000, total_steps: 874000.00000
[CW] train: qf1_loss: 0.06141, qf2_loss: 0.06239, policy_loss: -32.38724, policy_entropy: -5.98009, alpha: 0.00506, time: 34.42588
[CW] ---------------------------
[CW] ---- Iteration:   869 ----
[CW] collect: return: 208.93798, steps: 1000.00000, total_steps: 875000.00000
[CW] train: qf1_loss: 0.06555, qf2_loss: 0.06593, policy_loss: -32.46980, policy_entropy: -5.91920, alpha: 0.00502, time: 33.64082
[CW] ---------------------------
[CW] ---- Iteration:   870 ----
[CW] collect: return: 204.38876, steps: 1000.00000, total_steps: 876000.00000
[CW] train: qf1_loss: 0.07003, qf2_loss: 0.06915, policy_loss: -32.48910, policy_entropy: -6.13056, alpha: 0.00505, time: 33.94786
[CW] ---------------------------
[CW] ---- Iteration:   871 ----
[CW] collect: return: 205.13654, steps: 1000.00000, total_steps: 877000.00000
[CW] train: qf1_loss: 0.07152, qf2_loss: 0.07148, policy_loss: -32.54119, policy_entropy: -6.18653, alpha: 0.00512, time: 33.80501
[CW] ---------------------------
[CW] ---- Iteration:   872 ----
[CW] collect: return: 198.09471, steps: 1000.00000, total_steps: 878000.00000
[CW] train: qf1_loss: 0.06731, qf2_loss: 0.06884, policy_loss: -32.53496, policy_entropy: -5.99887, alpha: 0.00516, time: 34.10468
[CW] ---------------------------
[CW] ---- Iteration:   873 ----
[CW] collect: return: 207.61823, steps: 1000.00000, total_steps: 879000.00000
[CW] train: qf1_loss: 0.06495, qf2_loss: 0.06517, policy_loss: -32.48191, policy_entropy: -5.86066, alpha: 0.00513, time: 33.69858
[CW] ---------------------------
[CW] ---- Iteration:   874 ----
[CW] collect: return: 202.20576, steps: 1000.00000, total_steps: 880000.00000
[CW] train: qf1_loss: 0.06472, qf2_loss: 0.06477, policy_loss: -32.58684, policy_entropy: -5.93117, alpha: 0.00508, time: 33.66143
[CW] ---------------------------
[CW] ---- Iteration:   875 ----
[CW] collect: return: 205.09679, steps: 1000.00000, total_steps: 881000.00000
[CW] train: qf1_loss: 0.06685, qf2_loss: 0.06720, policy_loss: -32.61707, policy_entropy: -6.05603, alpha: 0.00507, time: 33.59578
[CW] ---------------------------
[CW] ---- Iteration:   876 ----
[CW] collect: return: 213.75554, steps: 1000.00000, total_steps: 882000.00000
[CW] train: qf1_loss: 0.06879, qf2_loss: 0.06938, policy_loss: -32.50707, policy_entropy: -6.18058, alpha: 0.00513, time: 34.12880
[CW] ---------------------------
[CW] ---- Iteration:   877 ----
[CW] collect: return: 214.59015, steps: 1000.00000, total_steps: 883000.00000
[CW] train: qf1_loss: 0.06079, qf2_loss: 0.06175, policy_loss: -32.54777, policy_entropy: -6.19509, alpha: 0.00519, time: 34.44899
[CW] ---------------------------
[CW] ---- Iteration:   878 ----
[CW] collect: return: 198.17756, steps: 1000.00000, total_steps: 884000.00000
[CW] train: qf1_loss: 0.06045, qf2_loss: 0.06178, policy_loss: -32.62031, policy_entropy: -6.05675, alpha: 0.00525, time: 33.66839
[CW] ---------------------------
[CW] ---- Iteration:   879 ----
[CW] collect: return: 209.50142, steps: 1000.00000, total_steps: 885000.00000
[CW] train: qf1_loss: 0.06525, qf2_loss: 0.06648, policy_loss: -32.60447, policy_entropy: -6.05374, alpha: 0.00529, time: 34.14693
[CW] ---------------------------
[CW] ---- Iteration:   880 ----
[CW] collect: return: 206.41060, steps: 1000.00000, total_steps: 886000.00000
[CW] train: qf1_loss: 0.07234, qf2_loss: 0.07183, policy_loss: -32.63566, policy_entropy: -6.12731, alpha: 0.00533, time: 34.13165
[CW] eval: return: 217.23788, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   881 ----
[CW] collect: return: 185.39449, steps: 1000.00000, total_steps: 887000.00000
[CW] train: qf1_loss: 0.07013, qf2_loss: 0.07220, policy_loss: -32.66428, policy_entropy: -6.06723, alpha: 0.00537, time: 33.64396
[CW] ---------------------------
[CW] ---- Iteration:   882 ----
[CW] collect: return: 219.43890, steps: 1000.00000, total_steps: 888000.00000
[CW] train: qf1_loss: 0.07034, qf2_loss: 0.06998, policy_loss: -32.66163, policy_entropy: -6.08612, alpha: 0.00543, time: 33.79527
[CW] ---------------------------
[CW] ---- Iteration:   883 ----
[CW] collect: return: 219.12476, steps: 1000.00000, total_steps: 889000.00000
[CW] train: qf1_loss: 0.06198, qf2_loss: 0.06229, policy_loss: -32.68636, policy_entropy: -5.91905, alpha: 0.00542, time: 33.56251
[CW] ---------------------------
[CW] ---- Iteration:   884 ----
[CW] collect: return: 221.68954, steps: 1000.00000, total_steps: 890000.00000
[CW] train: qf1_loss: 0.06329, qf2_loss: 0.06358, policy_loss: -32.75112, policy_entropy: -6.05945, alpha: 0.00539, time: 33.54030
[CW] ---------------------------
[CW] ---- Iteration:   885 ----
[CW] collect: return: 218.36153, steps: 1000.00000, total_steps: 891000.00000
[CW] train: qf1_loss: 0.06503, qf2_loss: 0.06568, policy_loss: -32.76077, policy_entropy: -5.98359, alpha: 0.00542, time: 33.77626
[CW] ---------------------------
[CW] ---- Iteration:   886 ----
[CW] collect: return: 193.55795, steps: 1000.00000, total_steps: 892000.00000
[CW] train: qf1_loss: 0.06334, qf2_loss: 0.06472, policy_loss: -32.76804, policy_entropy: -5.92332, alpha: 0.00539, time: 33.71841
[CW] ---------------------------
[CW] ---- Iteration:   887 ----
[CW] collect: return: 205.45982, steps: 1000.00000, total_steps: 893000.00000
[CW] train: qf1_loss: 0.07215, qf2_loss: 0.07048, policy_loss: -32.73999, policy_entropy: -6.01002, alpha: 0.00537, time: 33.82815
[CW] ---------------------------
[CW] ---- Iteration:   888 ----
[CW] collect: return: 206.98213, steps: 1000.00000, total_steps: 894000.00000
[CW] train: qf1_loss: 0.06774, qf2_loss: 0.06860, policy_loss: -32.69339, policy_entropy: -5.81133, alpha: 0.00534, time: 33.66970
[CW] ---------------------------
[CW] ---- Iteration:   889 ----
[CW] collect: return: 188.60965, steps: 1000.00000, total_steps: 895000.00000
[CW] train: qf1_loss: 0.06699, qf2_loss: 0.06954, policy_loss: -32.72474, policy_entropy: -5.95341, alpha: 0.00528, time: 33.64377
[CW] ---------------------------
[CW] ---- Iteration:   890 ----
[CW] collect: return: 206.35439, steps: 1000.00000, total_steps: 896000.00000
[CW] train: qf1_loss: 0.07230, qf2_loss: 0.07232, policy_loss: -32.74869, policy_entropy: -5.93456, alpha: 0.00525, time: 33.39984
[CW] ---------------------------
[CW] ---- Iteration:   891 ----
[CW] collect: return: 184.64114, steps: 1000.00000, total_steps: 897000.00000
[CW] train: qf1_loss: 0.06433, qf2_loss: 0.06600, policy_loss: -32.79233, policy_entropy: -6.14866, alpha: 0.00527, time: 33.32452
[CW] ---------------------------
[CW] ---- Iteration:   892 ----
[CW] collect: return: 223.24796, steps: 1000.00000, total_steps: 898000.00000
[CW] train: qf1_loss: 0.06792, qf2_loss: 0.06917, policy_loss: -32.84410, policy_entropy: -6.20053, alpha: 0.00537, time: 34.03609
[CW] ---------------------------
[CW] ---- Iteration:   893 ----
[CW] collect: return: 217.04585, steps: 1000.00000, total_steps: 899000.00000
[CW] train: qf1_loss: 0.07514, qf2_loss: 0.07313, policy_loss: -32.79331, policy_entropy: -6.15754, alpha: 0.00540, time: 34.31559
[CW] ---------------------------
[CW] ---- Iteration:   894 ----
[CW] collect: return: 212.12347, steps: 1000.00000, total_steps: 900000.00000
[CW] train: qf1_loss: 0.07051, qf2_loss: 0.07084, policy_loss: -32.81535, policy_entropy: -6.19914, alpha: 0.00552, time: 34.06398
[CW] ---------------------------
[CW] ---- Iteration:   895 ----
[CW] collect: return: 218.76011, steps: 1000.00000, total_steps: 901000.00000
[CW] train: qf1_loss: 0.07278, qf2_loss: 0.07297, policy_loss: -32.87581, policy_entropy: -6.11274, alpha: 0.00561, time: 34.38843
[CW] ---------------------------
[CW] ---- Iteration:   896 ----
[CW] collect: return: 216.25098, steps: 1000.00000, total_steps: 902000.00000
[CW] train: qf1_loss: 0.06304, qf2_loss: 0.06366, policy_loss: -32.81240, policy_entropy: -6.19927, alpha: 0.00566, time: 34.46998
[CW] ---------------------------
[CW] ---- Iteration:   897 ----
[CW] collect: return: 219.16959, steps: 1000.00000, total_steps: 903000.00000
[CW] train: qf1_loss: 0.06747, qf2_loss: 0.06823, policy_loss: -32.86879, policy_entropy: -6.00026, alpha: 0.00574, time: 34.28963
[CW] ---------------------------
[CW] ---- Iteration:   898 ----
[CW] collect: return: 205.58294, steps: 1000.00000, total_steps: 904000.00000
[CW] train: qf1_loss: 0.07518, qf2_loss: 0.07569, policy_loss: -32.85916, policy_entropy: -5.95668, alpha: 0.00572, time: 34.11444
[CW] ---------------------------
[CW] ---- Iteration:   899 ----
[CW] collect: return: 215.16972, steps: 1000.00000, total_steps: 905000.00000
[CW] train: qf1_loss: 0.07351, qf2_loss: 0.07367, policy_loss: -32.77675, policy_entropy: -5.95788, alpha: 0.00570, time: 34.34658
[CW] ---------------------------
[CW] ---- Iteration:   900 ----
[CW] collect: return: 224.59988, steps: 1000.00000, total_steps: 906000.00000
[CW] train: qf1_loss: 0.06827, qf2_loss: 0.06954, policy_loss: -32.93915, policy_entropy: -5.99904, alpha: 0.00568, time: 34.38159
[CW] eval: return: 190.46217, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   901 ----
[CW] collect: return: 221.01670, steps: 1000.00000, total_steps: 907000.00000
[CW] train: qf1_loss: 0.06951, qf2_loss: 0.07035, policy_loss: -32.92190, policy_entropy: -5.91389, alpha: 0.00566, time: 33.89831
[CW] ---------------------------
[CW] ---- Iteration:   902 ----
[CW] collect: return: 212.58016, steps: 1000.00000, total_steps: 908000.00000
[CW] train: qf1_loss: 0.07026, qf2_loss: 0.06982, policy_loss: -32.89417, policy_entropy: -5.94555, alpha: 0.00563, time: 33.80048
[CW] ---------------------------
[CW] ---- Iteration:   903 ----
[CW] collect: return: 215.53409, steps: 1000.00000, total_steps: 909000.00000
[CW] train: qf1_loss: 0.06811, qf2_loss: 0.06835, policy_loss: -32.91397, policy_entropy: -5.92925, alpha: 0.00559, time: 34.16654
[CW] ---------------------------
[CW] ---- Iteration:   904 ----
[CW] collect: return: 212.50168, steps: 1000.00000, total_steps: 910000.00000
[CW] train: qf1_loss: 0.07287, qf2_loss: 0.07320, policy_loss: -33.01039, policy_entropy: -6.12740, alpha: 0.00559, time: 34.21226
[CW] ---------------------------
[CW] ---- Iteration:   905 ----
[CW] collect: return: 218.86948, steps: 1000.00000, total_steps: 911000.00000
[CW] train: qf1_loss: 0.07400, qf2_loss: 0.07400, policy_loss: -32.96210, policy_entropy: -6.23715, alpha: 0.00568, time: 34.13479
[CW] ---------------------------
[CW] ---- Iteration:   906 ----
[CW] collect: return: 212.65956, steps: 1000.00000, total_steps: 912000.00000
[CW] train: qf1_loss: 0.07008, qf2_loss: 0.06998, policy_loss: -32.98418, policy_entropy: -6.09676, alpha: 0.00581, time: 34.27253
[CW] ---------------------------
[CW] ---- Iteration:   907 ----
[CW] collect: return: 211.56429, steps: 1000.00000, total_steps: 913000.00000
[CW] train: qf1_loss: 0.07235, qf2_loss: 0.07271, policy_loss: -33.05253, policy_entropy: -6.14241, alpha: 0.00585, time: 33.92796
[CW] ---------------------------
[CW] ---- Iteration:   908 ----
[CW] collect: return: 222.69432, steps: 1000.00000, total_steps: 914000.00000
[CW] train: qf1_loss: 0.07072, qf2_loss: 0.07229, policy_loss: -33.02902, policy_entropy: -6.12953, alpha: 0.00592, time: 34.14114
[CW] ---------------------------
[CW] ---- Iteration:   909 ----
[CW] collect: return: 219.30793, steps: 1000.00000, total_steps: 915000.00000
[CW] train: qf1_loss: 0.06443, qf2_loss: 0.06402, policy_loss: -33.09749, policy_entropy: -6.01324, alpha: 0.00596, time: 34.18952
[CW] ---------------------------
[CW] ---- Iteration:   910 ----
[CW] collect: return: 219.95001, steps: 1000.00000, total_steps: 916000.00000
[CW] train: qf1_loss: 0.07432, qf2_loss: 0.07395, policy_loss: -33.07884, policy_entropy: -5.81965, alpha: 0.00592, time: 33.87476
[CW] ---------------------------
[CW] ---- Iteration:   911 ----
[CW] collect: return: 212.17479, steps: 1000.00000, total_steps: 917000.00000
[CW] train: qf1_loss: 0.07523, qf2_loss: 0.07619, policy_loss: -33.14313, policy_entropy: -6.08772, alpha: 0.00589, time: 36.88791
[CW] ---------------------------
[CW] ---- Iteration:   912 ----
[CW] collect: return: 203.82137, steps: 1000.00000, total_steps: 918000.00000
[CW] train: qf1_loss: 0.06834, qf2_loss: 0.06914, policy_loss: -33.17491, policy_entropy: -6.09891, alpha: 0.00595, time: 34.32819
[CW] ---------------------------
[CW] ---- Iteration:   913 ----
[CW] collect: return: 209.37731, steps: 1000.00000, total_steps: 919000.00000
[CW] train: qf1_loss: 0.07678, qf2_loss: 0.07654, policy_loss: -33.17656, policy_entropy: -6.01821, alpha: 0.00598, time: 34.04278
[CW] ---------------------------
[CW] ---- Iteration:   914 ----
[CW] collect: return: 198.18050, steps: 1000.00000, total_steps: 920000.00000
[CW] train: qf1_loss: 0.06582, qf2_loss: 0.06606, policy_loss: -33.15837, policy_entropy: -5.94443, alpha: 0.00597, time: 33.98618
[CW] ---------------------------
[CW] ---- Iteration:   915 ----
[CW] collect: return: 223.77849, steps: 1000.00000, total_steps: 921000.00000
[CW] train: qf1_loss: 0.06416, qf2_loss: 0.06554, policy_loss: -33.17752, policy_entropy: -5.91771, alpha: 0.00592, time: 33.99389
[CW] ---------------------------
[CW] ---- Iteration:   916 ----
[CW] collect: return: 215.71402, steps: 1000.00000, total_steps: 922000.00000
[CW] train: qf1_loss: 0.06811, qf2_loss: 0.06917, policy_loss: -33.27268, policy_entropy: -6.09112, alpha: 0.00593, time: 34.77368
[CW] ---------------------------
[CW] ---- Iteration:   917 ----
[CW] collect: return: 205.70629, steps: 1000.00000, total_steps: 923000.00000
[CW] train: qf1_loss: 0.07281, qf2_loss: 0.07272, policy_loss: -33.22865, policy_entropy: -5.95832, alpha: 0.00595, time: 33.65921
[CW] ---------------------------
[CW] ---- Iteration:   918 ----
[CW] collect: return: 209.36418, steps: 1000.00000, total_steps: 924000.00000
[CW] train: qf1_loss: 0.08004, qf2_loss: 0.07990, policy_loss: -33.25836, policy_entropy: -6.08650, alpha: 0.00595, time: 33.28753
[CW] ---------------------------
[CW] ---- Iteration:   919 ----
[CW] collect: return: 210.86599, steps: 1000.00000, total_steps: 925000.00000
[CW] train: qf1_loss: 0.08269, qf2_loss: 0.08363, policy_loss: -33.23306, policy_entropy: -6.02187, alpha: 0.00598, time: 34.00233
[CW] ---------------------------
[CW] ---- Iteration:   920 ----
[CW] collect: return: 221.68140, steps: 1000.00000, total_steps: 926000.00000
[CW] train: qf1_loss: 0.07310, qf2_loss: 0.07266, policy_loss: -33.22734, policy_entropy: -6.04793, alpha: 0.00601, time: 34.12494
[CW] eval: return: 210.57590, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   921 ----
[CW] collect: return: 197.12059, steps: 1000.00000, total_steps: 927000.00000
[CW] train: qf1_loss: 0.06695, qf2_loss: 0.06741, policy_loss: -33.24025, policy_entropy: -5.87326, alpha: 0.00598, time: 33.99494
[CW] ---------------------------
[CW] ---- Iteration:   922 ----
[CW] collect: return: 221.58899, steps: 1000.00000, total_steps: 928000.00000
[CW] train: qf1_loss: 0.06313, qf2_loss: 0.06369, policy_loss: -33.33877, policy_entropy: -5.89931, alpha: 0.00592, time: 33.57830
[CW] ---------------------------
[CW] ---- Iteration:   923 ----
[CW] collect: return: 223.14283, steps: 1000.00000, total_steps: 929000.00000
[CW] train: qf1_loss: 0.07578, qf2_loss: 0.07456, policy_loss: -33.39978, policy_entropy: -5.90044, alpha: 0.00589, time: 33.82184
[CW] ---------------------------
[CW] ---- Iteration:   924 ----
[CW] collect: return: 222.28683, steps: 1000.00000, total_steps: 930000.00000
[CW] train: qf1_loss: 0.06652, qf2_loss: 0.06795, policy_loss: -33.38464, policy_entropy: -5.77070, alpha: 0.00580, time: 33.95789
[CW] ---------------------------
[CW] ---- Iteration:   925 ----
[CW] collect: return: 214.96772, steps: 1000.00000, total_steps: 931000.00000
[CW] train: qf1_loss: 0.07601, qf2_loss: 0.07703, policy_loss: -33.32955, policy_entropy: -6.00996, alpha: 0.00572, time: 34.09319
[CW] ---------------------------
[CW] ---- Iteration:   926 ----
[CW] collect: return: 212.60198, steps: 1000.00000, total_steps: 932000.00000
[CW] train: qf1_loss: 0.09299, qf2_loss: 0.09521, policy_loss: -33.37636, policy_entropy: -5.86929, alpha: 0.00572, time: 33.80141
[CW] ---------------------------
[CW] ---- Iteration:   927 ----
[CW] collect: return: 216.99273, steps: 1000.00000, total_steps: 933000.00000
[CW] train: qf1_loss: 0.06784, qf2_loss: 0.06775, policy_loss: -33.39855, policy_entropy: -6.05202, alpha: 0.00567, time: 33.97481
[CW] ---------------------------
[CW] ---- Iteration:   928 ----
[CW] collect: return: 220.27077, steps: 1000.00000, total_steps: 934000.00000
[CW] train: qf1_loss: 0.07060, qf2_loss: 0.07105, policy_loss: -33.49717, policy_entropy: -6.09720, alpha: 0.00571, time: 33.94588
[CW] ---------------------------
[CW] ---- Iteration:   929 ----
[CW] collect: return: 226.94117, steps: 1000.00000, total_steps: 935000.00000
[CW] train: qf1_loss: 0.07208, qf2_loss: 0.07192, policy_loss: -33.41246, policy_entropy: -6.02563, alpha: 0.00572, time: 34.84131
[CW] ---------------------------
[CW] ---- Iteration:   930 ----
[CW] collect: return: 220.74344, steps: 1000.00000, total_steps: 936000.00000
[CW] train: qf1_loss: 0.07520, qf2_loss: 0.07534, policy_loss: -33.49593, policy_entropy: -6.12834, alpha: 0.00579, time: 36.28530
[CW] ---------------------------
[CW] ---- Iteration:   931 ----
[CW] collect: return: 216.79556, steps: 1000.00000, total_steps: 937000.00000
[CW] train: qf1_loss: 0.06749, qf2_loss: 0.06797, policy_loss: -33.49833, policy_entropy: -6.07684, alpha: 0.00584, time: 33.85188
[CW] ---------------------------
[CW] ---- Iteration:   932 ----
[CW] collect: return: 212.15404, steps: 1000.00000, total_steps: 938000.00000
[CW] train: qf1_loss: 0.06653, qf2_loss: 0.06616, policy_loss: -33.53515, policy_entropy: -5.92914, alpha: 0.00584, time: 34.00702
[CW] ---------------------------
[CW] ---- Iteration:   933 ----
[CW] collect: return: 214.92042, steps: 1000.00000, total_steps: 939000.00000
[CW] train: qf1_loss: 0.06275, qf2_loss: 0.06463, policy_loss: -33.53915, policy_entropy: -6.00274, alpha: 0.00582, time: 33.99504
[CW] ---------------------------
[CW] ---- Iteration:   934 ----
[CW] collect: return: 222.88951, steps: 1000.00000, total_steps: 940000.00000
[CW] train: qf1_loss: 0.07000, qf2_loss: 0.06998, policy_loss: -33.50891, policy_entropy: -5.94617, alpha: 0.00580, time: 34.20181
[CW] ---------------------------
[CW] ---- Iteration:   935 ----
[CW] collect: return: 214.50022, steps: 1000.00000, total_steps: 941000.00000
[CW] train: qf1_loss: 0.06728, qf2_loss: 0.06808, policy_loss: -33.57630, policy_entropy: -6.04554, alpha: 0.00580, time: 34.63917
[CW] ---------------------------
[CW] ---- Iteration:   936 ----
[CW] collect: return: 211.79555, steps: 1000.00000, total_steps: 942000.00000
[CW] train: qf1_loss: 0.06651, qf2_loss: 0.06640, policy_loss: -33.54003, policy_entropy: -6.04075, alpha: 0.00583, time: 34.23655
[CW] ---------------------------
[CW] ---- Iteration:   937 ----
[CW] collect: return: 230.17129, steps: 1000.00000, total_steps: 943000.00000
[CW] train: qf1_loss: 0.07001, qf2_loss: 0.07069, policy_loss: -33.65014, policy_entropy: -5.91247, alpha: 0.00582, time: 33.70289
[CW] ---------------------------
[CW] ---- Iteration:   938 ----
[CW] collect: return: 220.80113, steps: 1000.00000, total_steps: 944000.00000
[CW] train: qf1_loss: 0.11132, qf2_loss: 0.11276, policy_loss: -33.58430, policy_entropy: -5.99916, alpha: 0.00578, time: 33.92241
[CW] ---------------------------
[CW] ---- Iteration:   939 ----
[CW] collect: return: 219.02785, steps: 1000.00000, total_steps: 945000.00000
[CW] train: qf1_loss: 0.07103, qf2_loss: 0.07107, policy_loss: -33.57123, policy_entropy: -6.17459, alpha: 0.00582, time: 34.18372
[CW] ---------------------------
[CW] ---- Iteration:   940 ----
[CW] collect: return: 223.56527, steps: 1000.00000, total_steps: 946000.00000
[CW] train: qf1_loss: 0.06524, qf2_loss: 0.06473, policy_loss: -33.62652, policy_entropy: -5.96556, alpha: 0.00589, time: 34.66153
[CW] eval: return: 214.54059, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   941 ----
[CW] collect: return: 221.09265, steps: 1000.00000, total_steps: 947000.00000
[CW] train: qf1_loss: 0.06358, qf2_loss: 0.06453, policy_loss: -33.74394, policy_entropy: -6.06522, alpha: 0.00588, time: 33.50900
[CW] ---------------------------
[CW] ---- Iteration:   942 ----
[CW] collect: return: 223.31047, steps: 1000.00000, total_steps: 948000.00000
[CW] train: qf1_loss: 0.06738, qf2_loss: 0.06897, policy_loss: -33.70905, policy_entropy: -5.98287, alpha: 0.00591, time: 34.08529
[CW] ---------------------------
[CW] ---- Iteration:   943 ----
[CW] collect: return: 213.63048, steps: 1000.00000, total_steps: 949000.00000
[CW] train: qf1_loss: 0.06738, qf2_loss: 0.06797, policy_loss: -33.69654, policy_entropy: -5.92540, alpha: 0.00586, time: 34.63294
[CW] ---------------------------
[CW] ---- Iteration:   944 ----
[CW] collect: return: 225.04354, steps: 1000.00000, total_steps: 950000.00000
[CW] train: qf1_loss: 0.06598, qf2_loss: 0.06621, policy_loss: -33.81172, policy_entropy: -6.06254, alpha: 0.00586, time: 34.23005
[CW] ---------------------------
[CW] ---- Iteration:   945 ----
[CW] collect: return: 230.13045, steps: 1000.00000, total_steps: 951000.00000
[CW] train: qf1_loss: 0.07526, qf2_loss: 0.07493, policy_loss: -33.68498, policy_entropy: -6.26988, alpha: 0.00594, time: 34.00415
[CW] ---------------------------
[CW] ---- Iteration:   946 ----
[CW] collect: return: 213.14615, steps: 1000.00000, total_steps: 952000.00000
[CW] train: qf1_loss: 0.07335, qf2_loss: 0.07318, policy_loss: -33.72917, policy_entropy: -6.18322, alpha: 0.00609, time: 33.87178
[CW] ---------------------------
[CW] ---- Iteration:   947 ----
[CW] collect: return: 211.07792, steps: 1000.00000, total_steps: 953000.00000
[CW] train: qf1_loss: 0.06758, qf2_loss: 0.06864, policy_loss: -33.79367, policy_entropy: -6.04790, alpha: 0.00615, time: 34.12427
[CW] ---------------------------
[CW] ---- Iteration:   948 ----
[CW] collect: return: 198.88065, steps: 1000.00000, total_steps: 954000.00000
[CW] train: qf1_loss: 0.05846, qf2_loss: 0.05890, policy_loss: -33.87207, policy_entropy: -6.02547, alpha: 0.00618, time: 34.25071
[CW] ---------------------------
[CW] ---- Iteration:   949 ----
[CW] collect: return: 191.53721, steps: 1000.00000, total_steps: 955000.00000
[CW] train: qf1_loss: 0.06264, qf2_loss: 0.06318, policy_loss: -33.76026, policy_entropy: -5.99224, alpha: 0.00616, time: 34.09426
[CW] ---------------------------
[CW] ---- Iteration:   950 ----
[CW] collect: return: 217.82794, steps: 1000.00000, total_steps: 956000.00000
[CW] train: qf1_loss: 0.06977, qf2_loss: 0.07109, policy_loss: -33.76887, policy_entropy: -6.05347, alpha: 0.00621, time: 34.13299
[CW] ---------------------------
[CW] ---- Iteration:   951 ----
[CW] collect: return: 211.33381, steps: 1000.00000, total_steps: 957000.00000
[CW] train: qf1_loss: 0.06436, qf2_loss: 0.06408, policy_loss: -33.74870, policy_entropy: -6.07242, alpha: 0.00623, time: 33.75843
[CW] ---------------------------
[CW] ---- Iteration:   952 ----
[CW] collect: return: 210.94289, steps: 1000.00000, total_steps: 958000.00000
[CW] train: qf1_loss: 0.08877, qf2_loss: 0.08847, policy_loss: -33.86368, policy_entropy: -5.93789, alpha: 0.00622, time: 34.43131
[CW] ---------------------------
[CW] ---- Iteration:   953 ----
[CW] collect: return: 213.18327, steps: 1000.00000, total_steps: 959000.00000
[CW] train: qf1_loss: 0.07235, qf2_loss: 0.07523, policy_loss: -33.80361, policy_entropy: -6.08773, alpha: 0.00623, time: 34.34465
[CW] ---------------------------
[CW] ---- Iteration:   954 ----
[CW] collect: return: 229.68974, steps: 1000.00000, total_steps: 960000.00000
[CW] train: qf1_loss: 0.06549, qf2_loss: 0.06580, policy_loss: -33.93138, policy_entropy: -5.99472, alpha: 0.00628, time: 34.24884
[CW] ---------------------------
[CW] ---- Iteration:   955 ----
[CW] collect: return: 227.24016, steps: 1000.00000, total_steps: 961000.00000
[CW] train: qf1_loss: 0.06883, qf2_loss: 0.06863, policy_loss: -33.85585, policy_entropy: -5.91434, alpha: 0.00624, time: 33.87414
[CW] ---------------------------
[CW] ---- Iteration:   956 ----
[CW] collect: return: 213.84174, steps: 1000.00000, total_steps: 962000.00000
[CW] train: qf1_loss: 0.07569, qf2_loss: 0.07506, policy_loss: -33.88911, policy_entropy: -6.13891, alpha: 0.00625, time: 33.74567
[CW] ---------------------------
[CW] ---- Iteration:   957 ----
[CW] collect: return: 220.07832, steps: 1000.00000, total_steps: 963000.00000
[CW] train: qf1_loss: 0.06929, qf2_loss: 0.06972, policy_loss: -33.89600, policy_entropy: -6.08923, alpha: 0.00630, time: 33.80828
[CW] ---------------------------
[CW] ---- Iteration:   958 ----
[CW] collect: return: 206.59339, steps: 1000.00000, total_steps: 964000.00000
[CW] train: qf1_loss: 0.07002, qf2_loss: 0.07059, policy_loss: -33.90375, policy_entropy: -6.08560, alpha: 0.00638, time: 34.25043
[CW] ---------------------------
[CW] ---- Iteration:   959 ----
[CW] collect: return: 219.39483, steps: 1000.00000, total_steps: 965000.00000
[CW] train: qf1_loss: 0.06636, qf2_loss: 0.06773, policy_loss: -33.96285, policy_entropy: -6.05571, alpha: 0.00641, time: 33.94983
[CW] ---------------------------
[CW] ---- Iteration:   960 ----
[CW] collect: return: 216.42163, steps: 1000.00000, total_steps: 966000.00000
[CW] train: qf1_loss: 0.07216, qf2_loss: 0.07189, policy_loss: -34.01985, policy_entropy: -6.02394, alpha: 0.00643, time: 34.16128
[CW] eval: return: 219.38809, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   961 ----
[CW] collect: return: 220.27729, steps: 1000.00000, total_steps: 967000.00000
[CW] train: qf1_loss: 0.07758, qf2_loss: 0.07559, policy_loss: -33.98159, policy_entropy: -6.22831, alpha: 0.00649, time: 33.87638
[CW] ---------------------------
[CW] ---- Iteration:   962 ----
[CW] collect: return: 224.12391, steps: 1000.00000, total_steps: 968000.00000
[CW] train: qf1_loss: 0.07317, qf2_loss: 0.07331, policy_loss: -33.97929, policy_entropy: -6.04504, alpha: 0.00661, time: 34.06892
[CW] ---------------------------
[CW] ---- Iteration:   963 ----
[CW] collect: return: 223.43011, steps: 1000.00000, total_steps: 969000.00000
[CW] train: qf1_loss: 0.07355, qf2_loss: 0.07480, policy_loss: -33.93094, policy_entropy: -6.05303, alpha: 0.00665, time: 33.89354
[CW] ---------------------------
[CW] ---- Iteration:   964 ----
[CW] collect: return: 227.87288, steps: 1000.00000, total_steps: 970000.00000
[CW] train: qf1_loss: 0.07437, qf2_loss: 0.07451, policy_loss: -34.03345, policy_entropy: -6.03078, alpha: 0.00664, time: 34.22304
[CW] ---------------------------
[CW] ---- Iteration:   965 ----
[CW] collect: return: 222.74655, steps: 1000.00000, total_steps: 971000.00000
[CW] train: qf1_loss: 0.07152, qf2_loss: 0.07101, policy_loss: -34.04933, policy_entropy: -6.07570, alpha: 0.00669, time: 33.70893
[CW] ---------------------------
[CW] ---- Iteration:   966 ----
[CW] collect: return: 211.31681, steps: 1000.00000, total_steps: 972000.00000
[CW] train: qf1_loss: 0.07609, qf2_loss: 0.07712, policy_loss: -34.11963, policy_entropy: -5.88888, alpha: 0.00667, time: 33.93069
[CW] ---------------------------
[CW] ---- Iteration:   967 ----
[CW] collect: return: 221.22066, steps: 1000.00000, total_steps: 973000.00000
[CW] train: qf1_loss: 0.07397, qf2_loss: 0.07315, policy_loss: -34.15770, policy_entropy: -6.00762, alpha: 0.00666, time: 34.19657
[CW] ---------------------------
[CW] ---- Iteration:   968 ----
[CW] collect: return: 221.07098, steps: 1000.00000, total_steps: 974000.00000
[CW] train: qf1_loss: 0.07166, qf2_loss: 0.07279, policy_loss: -34.11389, policy_entropy: -6.02195, alpha: 0.00666, time: 34.26236
[CW] ---------------------------
[CW] ---- Iteration:   969 ----
[CW] collect: return: 222.29673, steps: 1000.00000, total_steps: 975000.00000
[CW] train: qf1_loss: 0.07804, qf2_loss: 0.07866, policy_loss: -34.10200, policy_entropy: -6.00712, alpha: 0.00665, time: 34.09056
[CW] ---------------------------
[CW] ---- Iteration:   970 ----
[CW] collect: return: 226.69827, steps: 1000.00000, total_steps: 976000.00000
[CW] train: qf1_loss: 0.07579, qf2_loss: 0.07669, policy_loss: -34.08937, policy_entropy: -5.87139, alpha: 0.00662, time: 34.76102
[CW] ---------------------------
[CW] ---- Iteration:   971 ----
[CW] collect: return: 225.32441, steps: 1000.00000, total_steps: 977000.00000
[CW] train: qf1_loss: 0.07464, qf2_loss: 0.07448, policy_loss: -34.24628, policy_entropy: -5.91108, alpha: 0.00658, time: 33.62528
[CW] ---------------------------
[CW] ---- Iteration:   972 ----
[CW] collect: return: 242.26510, steps: 1000.00000, total_steps: 978000.00000
[CW] train: qf1_loss: 0.07418, qf2_loss: 0.07630, policy_loss: -34.24434, policy_entropy: -5.97814, alpha: 0.00653, time: 34.07266
[CW] ---------------------------
[CW] ---- Iteration:   973 ----
[CW] collect: return: 218.12893, steps: 1000.00000, total_steps: 979000.00000
[CW] train: qf1_loss: 0.07820, qf2_loss: 0.07551, policy_loss: -34.17525, policy_entropy: -5.83818, alpha: 0.00647, time: 33.86236
[CW] ---------------------------
[CW] ---- Iteration:   974 ----
[CW] collect: return: 234.68174, steps: 1000.00000, total_steps: 980000.00000
[CW] train: qf1_loss: 0.07713, qf2_loss: 0.07802, policy_loss: -34.28531, policy_entropy: -5.85301, alpha: 0.00636, time: 33.56212
[CW] ---------------------------
[CW] ---- Iteration:   975 ----
[CW] collect: return: 224.27354, steps: 1000.00000, total_steps: 981000.00000
[CW] train: qf1_loss: 0.07270, qf2_loss: 0.07163, policy_loss: -34.28687, policy_entropy: -5.99576, alpha: 0.00632, time: 33.87130
[CW] ---------------------------
[CW] ---- Iteration:   976 ----
[CW] collect: return: 226.50242, steps: 1000.00000, total_steps: 982000.00000
[CW] train: qf1_loss: 0.07109, qf2_loss: 0.07189, policy_loss: -34.20849, policy_entropy: -6.05722, alpha: 0.00635, time: 34.45873
[CW] ---------------------------
[CW] ---- Iteration:   977 ----
[CW] collect: return: 229.19391, steps: 1000.00000, total_steps: 983000.00000
[CW] train: qf1_loss: 0.07433, qf2_loss: 0.07528, policy_loss: -34.31458, policy_entropy: -6.07276, alpha: 0.00638, time: 34.60605
[CW] ---------------------------
[CW] ---- Iteration:   978 ----
[CW] collect: return: 224.85101, steps: 1000.00000, total_steps: 984000.00000
[CW] train: qf1_loss: 0.07123, qf2_loss: 0.07197, policy_loss: -34.32329, policy_entropy: -6.04174, alpha: 0.00640, time: 34.39013
[CW] ---------------------------
[CW] ---- Iteration:   979 ----
[CW] collect: return: 27.08393, steps: 1000.00000, total_steps: 985000.00000
[CW] train: qf1_loss: 0.08190, qf2_loss: 0.08325, policy_loss: -34.33117, policy_entropy: -5.91814, alpha: 0.00641, time: 34.50624
[CW] ---------------------------
[CW] ---- Iteration:   980 ----
[CW] collect: return: 222.24647, steps: 1000.00000, total_steps: 986000.00000
[CW] train: qf1_loss: 0.08159, qf2_loss: 0.08031, policy_loss: -34.26142, policy_entropy: -5.88882, alpha: 0.00634, time: 34.31730
[CW] eval: return: 185.01997, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   981 ----
[CW] collect: return: 228.76370, steps: 1000.00000, total_steps: 987000.00000
[CW] train: qf1_loss: 0.07629, qf2_loss: 0.07723, policy_loss: -34.31578, policy_entropy: -5.92100, alpha: 0.00627, time: 34.15089
[CW] ---------------------------
[CW] ---- Iteration:   982 ----
[CW] collect: return: 26.02767, steps: 1000.00000, total_steps: 988000.00000
[CW] train: qf1_loss: 0.07885, qf2_loss: 0.07848, policy_loss: -34.42032, policy_entropy: -6.01416, alpha: 0.00624, time: 33.85460
[CW] ---------------------------
[CW] ---- Iteration:   983 ----
[CW] collect: return: 211.45673, steps: 1000.00000, total_steps: 989000.00000
[CW] train: qf1_loss: 0.07912, qf2_loss: 0.07924, policy_loss: -34.29235, policy_entropy: -6.21991, alpha: 0.00634, time: 35.12794
[CW] ---------------------------
[CW] ---- Iteration:   984 ----
[CW] collect: return: 222.34981, steps: 1000.00000, total_steps: 990000.00000
[CW] train: qf1_loss: 0.07551, qf2_loss: 0.07499, policy_loss: -34.48971, policy_entropy: -6.05444, alpha: 0.00641, time: 33.93973
[CW] ---------------------------
[CW] ---- Iteration:   985 ----
[CW] collect: return: 217.66096, steps: 1000.00000, total_steps: 991000.00000
[CW] train: qf1_loss: 0.08161, qf2_loss: 0.08167, policy_loss: -34.27148, policy_entropy: -5.99578, alpha: 0.00641, time: 34.13280
[CW] ---------------------------
[CW] ---- Iteration:   986 ----
[CW] collect: return: 31.44977, steps: 1000.00000, total_steps: 992000.00000
[CW] train: qf1_loss: 0.07034, qf2_loss: 0.07063, policy_loss: -34.40783, policy_entropy: -5.87352, alpha: 0.00640, time: 33.70729
[CW] ---------------------------
[CW] ---- Iteration:   987 ----
[CW] collect: return: 237.04077, steps: 1000.00000, total_steps: 993000.00000
[CW] train: qf1_loss: 0.09508, qf2_loss: 0.09689, policy_loss: -34.50309, policy_entropy: -5.90222, alpha: 0.00633, time: 34.08158
[CW] ---------------------------
[CW] ---- Iteration:   988 ----
[CW] collect: return: 217.25075, steps: 1000.00000, total_steps: 994000.00000
[CW] train: qf1_loss: 0.07728, qf2_loss: 0.07785, policy_loss: -34.47134, policy_entropy: -6.05285, alpha: 0.00628, time: 34.49667
[CW] ---------------------------
[CW] ---- Iteration:   989 ----
[CW] collect: return: 238.37406, steps: 1000.00000, total_steps: 995000.00000
[CW] train: qf1_loss: 0.07128, qf2_loss: 0.07134, policy_loss: -34.48297, policy_entropy: -6.08845, alpha: 0.00635, time: 34.27665
[CW] ---------------------------
[CW] ---- Iteration:   990 ----
[CW] collect: return: 225.00887, steps: 1000.00000, total_steps: 996000.00000
[CW] train: qf1_loss: 0.06947, qf2_loss: 0.06944, policy_loss: -34.53625, policy_entropy: -5.94985, alpha: 0.00637, time: 34.38818
[CW] ---------------------------
[CW] ---- Iteration:   991 ----
[CW] collect: return: 228.25575, steps: 1000.00000, total_steps: 997000.00000
[CW] train: qf1_loss: 0.09110, qf2_loss: 0.09025, policy_loss: -34.52049, policy_entropy: -6.01445, alpha: 0.00635, time: 34.11321
[CW] ---------------------------
[CW] ---- Iteration:   992 ----
[CW] collect: return: 224.15517, steps: 1000.00000, total_steps: 998000.00000
[CW] train: qf1_loss: 0.07236, qf2_loss: 0.07291, policy_loss: -34.63899, policy_entropy: -6.08971, alpha: 0.00637, time: 34.20461
[CW] ---------------------------
[CW] ---- Iteration:   993 ----
[CW] collect: return: 225.23512, steps: 1000.00000, total_steps: 999000.00000
[CW] train: qf1_loss: 0.07662, qf2_loss: 0.07615, policy_loss: -34.63313, policy_entropy: -6.12553, alpha: 0.00644, time: 34.61519
[CW] ---------------------------
[CW] ---- Iteration:   994 ----
[CW] collect: return: 208.11388, steps: 1000.00000, total_steps: 1000000.00000
[CW] train: qf1_loss: 0.07436, qf2_loss: 0.07437, policy_loss: -34.53983, policy_entropy: -5.92904, alpha: 0.00647, time: 34.38356
[CW] ---------------------------
[CW] ---- Iteration:   995 ----
[CW] collect: return: 230.34436, steps: 1000.00000, total_steps: 1001000.00000
[CW] train: qf1_loss: 0.07699, qf2_loss: 0.07825, policy_loss: -34.60360, policy_entropy: -5.83477, alpha: 0.00641, time: 34.54010
[CW] ---------------------------
[CW] ---- Iteration:   996 ----
[CW] collect: return: 212.15135, steps: 1000.00000, total_steps: 1002000.00000
[CW] train: qf1_loss: 0.06463, qf2_loss: 0.06505, policy_loss: -34.61548, policy_entropy: -5.84417, alpha: 0.00629, time: 34.89916
[CW] ---------------------------
[CW] ---- Iteration:   997 ----
[CW] collect: return: 221.28442, steps: 1000.00000, total_steps: 1003000.00000
[CW] train: qf1_loss: 0.11995, qf2_loss: 0.11962, policy_loss: -34.65137, policy_entropy: -6.18207, alpha: 0.00628, time: 34.94081
[CW] ---------------------------
[CW] ---- Iteration:   998 ----
[CW] collect: return: 175.85294, steps: 1000.00000, total_steps: 1004000.00000
[CW] train: qf1_loss: 0.10312, qf2_loss: 0.10624, policy_loss: -34.64462, policy_entropy: -6.17910, alpha: 0.00643, time: 34.79583
[CW] ---------------------------
[CW] ---- Iteration:   999 ----
[CW] collect: return: 219.13240, steps: 1000.00000, total_steps: 1005000.00000
[CW] train: qf1_loss: 0.07690, qf2_loss: 0.07743, policy_loss: -34.58615, policy_entropy: -6.03735, alpha: 0.00646, time: 34.69575
[CW] ---------------------------
[CW] ---- Iteration:  1000 ----
[CW] collect: return: 232.05678, steps: 1000.00000, total_steps: 1006000.00000
[CW] train: qf1_loss: 0.07468, qf2_loss: 0.07460, policy_loss: -34.72849, policy_entropy: -5.74870, alpha: 0.00643, time: 34.84763
[CW] eval: return: 223.82855, steps: 1000.00000
[CW] ---------------------------
