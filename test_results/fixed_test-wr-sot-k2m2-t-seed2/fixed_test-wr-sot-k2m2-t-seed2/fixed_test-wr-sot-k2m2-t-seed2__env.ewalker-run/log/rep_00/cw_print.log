[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 25.26105, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 3.46755, qf2_loss: 3.48014, policy_loss: -7.81565, policy_entropy: 4.09684, alpha: 0.98504, time: 68.68730
[CW] eval: return: 25.11452, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 24.58157, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.09359, qf2_loss: 0.09210, policy_loss: -8.50685, policy_entropy: 4.10054, alpha: 0.95626, time: 68.67343
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 24.21974, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.07663, qf2_loss: 0.07571, policy_loss: -9.18212, policy_entropy: 4.10160, alpha: 0.92871, time: 68.36887
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 24.35264, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.06925, qf2_loss: 0.06893, policy_loss: -10.05979, policy_entropy: 4.10202, alpha: 0.90231, time: 68.28513
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 21.19622, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06391, qf2_loss: 0.06413, policy_loss: -11.05633, policy_entropy: 4.10122, alpha: 0.87698, time: 68.19133
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 22.04905, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06096, qf2_loss: 0.06139, policy_loss: -12.14492, policy_entropy: 4.10232, alpha: 0.85266, time: 68.51216
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 27.38079, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.07889, qf2_loss: 0.08018, policy_loss: -13.29401, policy_entropy: 4.10172, alpha: 0.82930, time: 68.18103
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 24.38675, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.09516, qf2_loss: 0.09566, policy_loss: -14.48191, policy_entropy: 4.10083, alpha: 0.80682, time: 68.52592
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 22.69360, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.09529, qf2_loss: 0.09566, policy_loss: -15.68655, policy_entropy: 4.10056, alpha: 0.78519, time: 68.68416
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 25.61041, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.09372, qf2_loss: 0.09416, policy_loss: -16.89184, policy_entropy: 4.10136, alpha: 0.76435, time: 68.75441
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 22.51757, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.11697, qf2_loss: 0.11773, policy_loss: -18.08418, policy_entropy: 4.10152, alpha: 0.74426, time: 68.80930
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 22.19502, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.08641, qf2_loss: 0.08686, policy_loss: -19.25389, policy_entropy: 4.10254, alpha: 0.72488, time: 68.73099
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 27.02437, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.11793, qf2_loss: 0.11889, policy_loss: -20.40086, policy_entropy: 4.10124, alpha: 0.70616, time: 68.51955
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 22.11152, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.12210, qf2_loss: 0.12307, policy_loss: -21.52108, policy_entropy: 4.10143, alpha: 0.68809, time: 68.76537
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 24.31546, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.11881, qf2_loss: 0.11984, policy_loss: -22.61432, policy_entropy: 4.10173, alpha: 0.67061, time: 68.68276
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 22.81575, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.12088, qf2_loss: 0.12193, policy_loss: -23.68107, policy_entropy: 4.10129, alpha: 0.65371, time: 68.87172
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 22.89012, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.11795, qf2_loss: 0.11900, policy_loss: -24.71967, policy_entropy: 4.10138, alpha: 0.63736, time: 68.64996
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 27.54837, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.12170, qf2_loss: 0.12281, policy_loss: -25.72764, policy_entropy: 4.10133, alpha: 0.62152, time: 68.74295
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 25.67581, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.09745, qf2_loss: 0.09835, policy_loss: -26.71415, policy_entropy: 4.10255, alpha: 0.60618, time: 68.83831
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 23.61854, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.13437, qf2_loss: 0.13568, policy_loss: -27.66505, policy_entropy: 4.10079, alpha: 0.59131, time: 68.62800
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 24.55184, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.09676, qf2_loss: 0.09760, policy_loss: -28.59340, policy_entropy: 4.10187, alpha: 0.57689, time: 68.53118
[CW] eval: return: 25.34591, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 22.18682, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.15098, qf2_loss: 0.15254, policy_loss: -29.49270, policy_entropy: 4.10092, alpha: 0.56291, time: 68.51853
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 24.89821, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.17133, qf2_loss: 0.17296, policy_loss: -30.36801, policy_entropy: 4.10244, alpha: 0.54933, time: 68.46411
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 32.47255, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.05892, qf2_loss: 0.05925, policy_loss: -31.21606, policy_entropy: 4.10105, alpha: 0.53615, time: 68.36189
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 28.88910, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.17489, qf2_loss: 0.17650, policy_loss: -32.03937, policy_entropy: 4.10141, alpha: 0.52334, time: 68.49174
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 29.74108, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.06201, qf2_loss: 0.06232, policy_loss: -32.83710, policy_entropy: 4.10075, alpha: 0.51090, time: 68.31165
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 26.03255, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.12607, qf2_loss: 0.12710, policy_loss: -33.61091, policy_entropy: 4.10172, alpha: 0.49881, time: 68.50291
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 23.61051, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.13017, qf2_loss: 0.13116, policy_loss: -34.36034, policy_entropy: 4.10218, alpha: 0.48705, time: 68.31014
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 25.33384, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.12732, qf2_loss: 0.12827, policy_loss: -35.08500, policy_entropy: 4.09968, alpha: 0.47561, time: 68.42833
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 28.04612, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.13481, qf2_loss: 0.13577, policy_loss: -35.78988, policy_entropy: 4.10145, alpha: 0.46448, time: 68.63840
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 28.03570, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.12579, qf2_loss: 0.12666, policy_loss: -36.47032, policy_entropy: 4.10127, alpha: 0.45365, time: 68.43852
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 23.35202, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.17830, qf2_loss: 0.17966, policy_loss: -37.13035, policy_entropy: 4.10216, alpha: 0.44310, time: 68.51092
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 22.94654, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.03806, qf2_loss: 0.03816, policy_loss: -37.76649, policy_entropy: 4.10188, alpha: 0.43283, time: 68.58257
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 25.90721, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.20009, qf2_loss: 0.20154, policy_loss: -38.38122, policy_entropy: 4.10203, alpha: 0.42283, time: 68.36189
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 22.34383, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.06645, qf2_loss: 0.06681, policy_loss: -38.98225, policy_entropy: 4.10085, alpha: 0.41309, time: 68.42656
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 25.26518, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.11752, qf2_loss: 0.11816, policy_loss: -39.55919, policy_entropy: 4.10143, alpha: 0.40360, time: 68.59660
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 24.60548, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.15246, qf2_loss: 0.15366, policy_loss: -40.11416, policy_entropy: 4.10072, alpha: 0.39434, time: 68.50314
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 25.31542, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.11689, qf2_loss: 0.11768, policy_loss: -40.65198, policy_entropy: 4.10334, alpha: 0.38532, time: 68.35171
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 24.34558, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.12668, qf2_loss: 0.12762, policy_loss: -41.17271, policy_entropy: 4.10031, alpha: 0.37652, time: 68.62618
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 24.68650, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.11836, qf2_loss: 0.11922, policy_loss: -41.67435, policy_entropy: 4.10110, alpha: 0.36795, time: 68.75521
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 28.73809, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.18345, qf2_loss: 0.18509, policy_loss: -42.15819, policy_entropy: 4.10156, alpha: 0.35958, time: 68.83920
[CW] eval: return: 24.86019, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 24.09642, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.04183, qf2_loss: 0.04201, policy_loss: -42.62631, policy_entropy: 4.10158, alpha: 0.35142, time: 68.86333
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 24.90415, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.14194, qf2_loss: 0.14334, policy_loss: -43.07583, policy_entropy: 4.10137, alpha: 0.34346, time: 68.70317
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 22.95749, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.11835, qf2_loss: 0.11922, policy_loss: -43.51071, policy_entropy: 4.10071, alpha: 0.33569, time: 68.88306
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 21.08753, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.19412, qf2_loss: 0.19573, policy_loss: -43.93019, policy_entropy: 4.10145, alpha: 0.32811, time: 68.70227
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 27.32943, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.06204, qf2_loss: 0.06238, policy_loss: -44.32478, policy_entropy: 4.10184, alpha: 0.32070, time: 68.90141
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 21.80333, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.15837, qf2_loss: 0.15952, policy_loss: -44.71428, policy_entropy: 4.10135, alpha: 0.31348, time: 68.82429
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 25.18666, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.09126, qf2_loss: 0.09177, policy_loss: -45.08669, policy_entropy: 4.10092, alpha: 0.30643, time: 68.78710
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 21.78204, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.11279, qf2_loss: 0.11361, policy_loss: -45.43929, policy_entropy: 4.10057, alpha: 0.29954, time: 68.54374
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 24.32377, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.12307, qf2_loss: 0.12389, policy_loss: -45.78608, policy_entropy: 4.10175, alpha: 0.29281, time: 68.60705
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 23.26853, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.11338, qf2_loss: 0.11384, policy_loss: -46.11641, policy_entropy: 4.10141, alpha: 0.28625, time: 69.06052
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 26.04938, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.09892, qf2_loss: 0.09958, policy_loss: -46.43256, policy_entropy: 4.10249, alpha: 0.27983, time: 68.68363
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 25.45257, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.10810, qf2_loss: 0.10884, policy_loss: -46.73530, policy_entropy: 4.10123, alpha: 0.27357, time: 68.51620
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 22.56749, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.14226, qf2_loss: 0.14329, policy_loss: -47.02102, policy_entropy: 4.10121, alpha: 0.26745, time: 68.54967
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 21.89139, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.09056, qf2_loss: 0.09098, policy_loss: -47.29355, policy_entropy: 4.10157, alpha: 0.26147, time: 68.64320
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 28.65393, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.15681, qf2_loss: 0.15755, policy_loss: -47.56876, policy_entropy: 4.10144, alpha: 0.25563, time: 68.53214
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 23.79132, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.08776, qf2_loss: 0.08819, policy_loss: -47.81644, policy_entropy: 4.10160, alpha: 0.24993, time: 68.86085
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 28.09388, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.13935, qf2_loss: 0.14006, policy_loss: -48.05942, policy_entropy: 4.10160, alpha: 0.24435, time: 68.70076
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 24.75515, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.09585, qf2_loss: 0.09634, policy_loss: -48.28935, policy_entropy: 4.10157, alpha: 0.23890, time: 68.66398
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 32.17833, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.15767, qf2_loss: 0.15817, policy_loss: -48.51180, policy_entropy: 4.09954, alpha: 0.23358, time: 68.60085
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 24.86901, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.04189, qf2_loss: 0.04201, policy_loss: -48.71607, policy_entropy: 4.10167, alpha: 0.22838, time: 68.87219
[CW] eval: return: 26.26620, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 21.77062, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.15021, qf2_loss: 0.15095, policy_loss: -48.91522, policy_entropy: 4.10037, alpha: 0.22330, time: 69.03975
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 24.66132, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.08015, qf2_loss: 0.08045, policy_loss: -49.09914, policy_entropy: 4.10175, alpha: 0.21833, time: 72.38545
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 22.26393, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.16574, qf2_loss: 0.16644, policy_loss: -49.28106, policy_entropy: 4.10188, alpha: 0.21348, time: 69.62885
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 25.90479, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.07207, qf2_loss: 0.07243, policy_loss: -49.44486, policy_entropy: 4.10184, alpha: 0.20873, time: 69.68936
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 24.59652, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.11943, qf2_loss: 0.11996, policy_loss: -49.60485, policy_entropy: 4.10030, alpha: 0.20409, time: 69.66118
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 31.39519, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.13691, qf2_loss: 0.13754, policy_loss: -49.75475, policy_entropy: 4.10074, alpha: 0.19956, time: 69.48362
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 27.37034, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.07630, qf2_loss: 0.07655, policy_loss: -49.89245, policy_entropy: 4.10160, alpha: 0.19513, time: 69.49985
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 20.52642, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.10552, qf2_loss: 0.10584, policy_loss: -50.02312, policy_entropy: 4.10118, alpha: 0.19080, time: 69.39855
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 26.85102, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.10877, qf2_loss: 0.10924, policy_loss: -50.14873, policy_entropy: 4.10071, alpha: 0.18656, time: 69.46239
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 22.55433, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.08569, qf2_loss: 0.08613, policy_loss: -50.26095, policy_entropy: 4.10113, alpha: 0.18242, time: 69.43488
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 22.26851, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.12167, qf2_loss: 0.12190, policy_loss: -50.36335, policy_entropy: 4.10040, alpha: 0.17838, time: 69.32756
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 23.30534, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.12994, qf2_loss: 0.13061, policy_loss: -50.46246, policy_entropy: 4.10119, alpha: 0.17442, time: 69.27420
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 22.83707, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.06003, qf2_loss: 0.06034, policy_loss: -50.55349, policy_entropy: 4.10172, alpha: 0.17055, time: 69.07951
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 23.86350, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.14267, qf2_loss: 0.14334, policy_loss: -50.63286, policy_entropy: 4.10113, alpha: 0.16677, time: 69.24013
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 20.51653, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.09757, qf2_loss: 0.09803, policy_loss: -50.70656, policy_entropy: 4.10192, alpha: 0.16308, time: 69.21859
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 23.11589, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.07551, qf2_loss: 0.07578, policy_loss: -50.77535, policy_entropy: 4.10193, alpha: 0.15946, time: 69.01811
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 23.94165, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.09769, qf2_loss: 0.09812, policy_loss: -50.83474, policy_entropy: 4.10106, alpha: 0.15593, time: 68.71986
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 26.15460, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.10379, qf2_loss: 0.10431, policy_loss: -50.89024, policy_entropy: 4.10099, alpha: 0.15247, time: 68.72701
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 23.29812, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.12608, qf2_loss: 0.12640, policy_loss: -50.93282, policy_entropy: 4.10059, alpha: 0.14910, time: 68.70777
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 22.43552, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.06745, qf2_loss: 0.06765, policy_loss: -50.97207, policy_entropy: 4.10163, alpha: 0.14580, time: 68.75940
[CW] eval: return: 25.49231, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 25.86111, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.09492, qf2_loss: 0.09547, policy_loss: -51.01406, policy_entropy: 4.10073, alpha: 0.14257, time: 68.54007
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 21.90333, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.12617, qf2_loss: 0.12680, policy_loss: -51.03853, policy_entropy: 4.10178, alpha: 0.13941, time: 68.48367
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 22.87843, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.09792, qf2_loss: 0.09849, policy_loss: -51.05810, policy_entropy: 4.10022, alpha: 0.13632, time: 68.56918
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 28.02765, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.09730, qf2_loss: 0.09748, policy_loss: -51.07340, policy_entropy: 4.10100, alpha: 0.13331, time: 68.53487
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 24.81545, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.06456, qf2_loss: 0.06474, policy_loss: -51.08702, policy_entropy: 4.10146, alpha: 0.13035, time: 68.58678
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 24.53733, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.14248, qf2_loss: 0.14343, policy_loss: -51.09394, policy_entropy: 4.10179, alpha: 0.12747, time: 68.59032
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 27.34757, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.03919, qf2_loss: 0.03932, policy_loss: -51.09279, policy_entropy: 4.09954, alpha: 0.12465, time: 68.53836
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 23.30672, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.13831, qf2_loss: 0.13896, policy_loss: -51.08723, policy_entropy: 4.10114, alpha: 0.12189, time: 68.36484
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 31.32342, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.07613, qf2_loss: 0.07643, policy_loss: -51.07596, policy_entropy: 4.10102, alpha: 0.11919, time: 68.49255
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 27.22490, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.10847, qf2_loss: 0.10900, policy_loss: -51.05916, policy_entropy: 4.10191, alpha: 0.11655, time: 68.59168
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 20.18776, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.06301, qf2_loss: 0.06333, policy_loss: -51.04057, policy_entropy: 4.10088, alpha: 0.11398, time: 68.36695
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 25.15963, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.09565, qf2_loss: 0.09628, policy_loss: -51.01650, policy_entropy: 4.10118, alpha: 0.11145, time: 68.38992
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 23.92246, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.08811, qf2_loss: 0.08846, policy_loss: -50.98628, policy_entropy: 4.10053, alpha: 0.10899, time: 68.50141
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 24.58969, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.08866, qf2_loss: 0.08889, policy_loss: -50.95158, policy_entropy: 4.10025, alpha: 0.10658, time: 68.81132
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 21.58850, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.09287, qf2_loss: 0.09311, policy_loss: -50.91487, policy_entropy: 4.10135, alpha: 0.10422, time: 68.79422
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 24.24603, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.06691, qf2_loss: 0.06724, policy_loss: -50.87034, policy_entropy: 4.10054, alpha: 0.10192, time: 68.72853
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 25.83840, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.09775, qf2_loss: 0.09850, policy_loss: -50.82501, policy_entropy: 4.10149, alpha: 0.09966, time: 68.36306
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 26.89113, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.07637, qf2_loss: 0.07648, policy_loss: -50.77606, policy_entropy: 4.10196, alpha: 0.09746, time: 68.56434
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 22.86046, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.10145, qf2_loss: 0.10168, policy_loss: -50.72118, policy_entropy: 4.10038, alpha: 0.09530, time: 68.68008
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 25.39548, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.08098, qf2_loss: 0.08118, policy_loss: -50.66253, policy_entropy: 4.10029, alpha: 0.09320, time: 68.65402
[CW] eval: return: 25.12783, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 24.65220, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.09282, qf2_loss: 0.09322, policy_loss: -50.60161, policy_entropy: 4.10091, alpha: 0.09113, time: 68.45424
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 24.44155, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.07213, qf2_loss: 0.07230, policy_loss: -50.53765, policy_entropy: 4.09916, alpha: 0.08912, time: 68.58600
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 25.24849, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.07290, qf2_loss: 0.07308, policy_loss: -50.46854, policy_entropy: 4.10103, alpha: 0.08715, time: 68.38938
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 24.72566, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.07773, qf2_loss: 0.07796, policy_loss: -50.39443, policy_entropy: 4.10003, alpha: 0.08522, time: 68.39748
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 25.26058, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.08937, qf2_loss: 0.08943, policy_loss: -50.32088, policy_entropy: 4.10045, alpha: 0.08334, time: 68.33192
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 27.07653, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.10360, qf2_loss: 0.10389, policy_loss: -50.24128, policy_entropy: 4.10045, alpha: 0.08149, time: 68.77693
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 27.05739, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.03799, qf2_loss: 0.03801, policy_loss: -50.16080, policy_entropy: 4.09978, alpha: 0.07969, time: 68.43586
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 23.81044, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.16729, qf2_loss: 0.16714, policy_loss: -50.07292, policy_entropy: 4.10076, alpha: 0.07793, time: 68.38597
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 25.12146, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.02287, qf2_loss: 0.02336, policy_loss: -49.98915, policy_entropy: 4.10012, alpha: 0.07620, time: 68.67744
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 24.50357, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.08430, qf2_loss: 0.08459, policy_loss: -49.89897, policy_entropy: 4.10022, alpha: 0.07452, time: 68.44621
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 24.64953, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.06216, qf2_loss: 0.06222, policy_loss: -49.80111, policy_entropy: 4.09969, alpha: 0.07287, time: 68.41493
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 25.22818, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.13502, qf2_loss: 0.13383, policy_loss: -49.71284, policy_entropy: 4.09889, alpha: 0.07126, time: 68.51569
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 26.09110, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.01422, qf2_loss: 0.01427, policy_loss: -49.61553, policy_entropy: 4.09930, alpha: 0.06968, time: 68.62347
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 24.88920, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.05637, qf2_loss: 0.05671, policy_loss: -49.51741, policy_entropy: 4.10004, alpha: 0.06814, time: 68.97677
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 25.31846, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.13985, qf2_loss: 0.14053, policy_loss: -49.41001, policy_entropy: 4.09933, alpha: 0.06664, time: 68.67986
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 24.29107, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.03165, qf2_loss: 0.03171, policy_loss: -49.30813, policy_entropy: 4.10079, alpha: 0.06516, time: 68.64750
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 28.26390, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.10469, qf2_loss: 0.10489, policy_loss: -49.20299, policy_entropy: 4.10013, alpha: 0.06372, time: 68.70902
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 24.50483, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.04387, qf2_loss: 0.04390, policy_loss: -49.09594, policy_entropy: 4.10082, alpha: 0.06231, time: 68.49089
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 22.85857, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.08076, qf2_loss: 0.08072, policy_loss: -48.98366, policy_entropy: 4.09784, alpha: 0.06093, time: 68.45334
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 27.71570, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.07364, qf2_loss: 0.07355, policy_loss: -48.86883, policy_entropy: 4.09970, alpha: 0.05959, time: 68.52451
[CW] eval: return: 25.55896, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 27.87896, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.06970, qf2_loss: 0.06969, policy_loss: -48.75733, policy_entropy: 4.09855, alpha: 0.05827, time: 68.43646
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 24.10765, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.06684, qf2_loss: 0.06682, policy_loss: -48.63795, policy_entropy: 4.09874, alpha: 0.05698, time: 68.44551
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 24.33632, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.06588, qf2_loss: 0.06579, policy_loss: -48.52462, policy_entropy: 4.09880, alpha: 0.05572, time: 68.78242
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 24.19136, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.12392, qf2_loss: 0.12402, policy_loss: -48.40055, policy_entropy: 4.09926, alpha: 0.05449, time: 68.65597
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 24.27896, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.02040, qf2_loss: 0.02038, policy_loss: -48.27841, policy_entropy: 4.09974, alpha: 0.05328, time: 68.74270
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 25.08114, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.06971, qf2_loss: 0.06960, policy_loss: -48.15438, policy_entropy: 4.09888, alpha: 0.05210, time: 68.54579
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 23.20442, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.08205, qf2_loss: 0.08187, policy_loss: -48.03152, policy_entropy: 4.10011, alpha: 0.05095, time: 68.54650
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 26.24441, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.07523, qf2_loss: 0.07528, policy_loss: -47.90357, policy_entropy: 4.09784, alpha: 0.04983, time: 68.51705
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 23.77978, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.10988, qf2_loss: 0.10986, policy_loss: -47.77218, policy_entropy: 4.09893, alpha: 0.04872, time: 68.83560
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 23.78335, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.01418, qf2_loss: 0.01413, policy_loss: -47.63950, policy_entropy: 4.09889, alpha: 0.04765, time: 68.53976
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 23.43969, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.06549, qf2_loss: 0.06520, policy_loss: -47.50920, policy_entropy: 4.09856, alpha: 0.04659, time: 68.42630
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 24.51470, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.03822, qf2_loss: 0.03834, policy_loss: -47.37343, policy_entropy: 4.09950, alpha: 0.04556, time: 68.54946
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 25.70184, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.08352, qf2_loss: 0.08331, policy_loss: -47.24322, policy_entropy: 4.10064, alpha: 0.04456, time: 68.49692
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 25.68381, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.09194, qf2_loss: 0.09215, policy_loss: -47.10201, policy_entropy: 4.10040, alpha: 0.04357, time: 68.49022
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 25.43830, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.02459, qf2_loss: 0.02445, policy_loss: -46.97259, policy_entropy: 4.09961, alpha: 0.04261, time: 68.67712
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 32.42433, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.05809, qf2_loss: 0.05781, policy_loss: -46.83652, policy_entropy: 4.09720, alpha: 0.04166, time: 68.59956
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 27.00800, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.09838, qf2_loss: 0.09817, policy_loss: -46.69604, policy_entropy: 4.09792, alpha: 0.04074, time: 69.01366
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 26.51836, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.02679, qf2_loss: 0.02669, policy_loss: -46.55687, policy_entropy: 4.09793, alpha: 0.03984, time: 68.83477
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 27.00983, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.08024, qf2_loss: 0.08005, policy_loss: -46.41571, policy_entropy: 4.09740, alpha: 0.03896, time: 68.67457
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 23.50307, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.04320, qf2_loss: 0.04299, policy_loss: -46.27761, policy_entropy: 4.09929, alpha: 0.03810, time: 68.61920
[CW] eval: return: 24.70760, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 30.61992, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.06397, qf2_loss: 0.06396, policy_loss: -46.13453, policy_entropy: 4.09834, alpha: 0.03726, time: 68.36598
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 25.13191, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.05702, qf2_loss: 0.05706, policy_loss: -45.99528, policy_entropy: 4.09724, alpha: 0.03643, time: 68.17547
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 23.78934, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.07418, qf2_loss: 0.07419, policy_loss: -45.84992, policy_entropy: 4.09578, alpha: 0.03563, time: 68.52568
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 23.87380, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.04052, qf2_loss: 0.04061, policy_loss: -45.70544, policy_entropy: 4.09492, alpha: 0.03484, time: 68.78194
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 24.73101, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.05866, qf2_loss: 0.05851, policy_loss: -45.56151, policy_entropy: 4.09589, alpha: 0.03407, time: 68.53996
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 23.73882, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.05056, qf2_loss: 0.05047, policy_loss: -45.41694, policy_entropy: 4.09566, alpha: 0.03332, time: 68.48332
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 22.41232, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.07004, qf2_loss: 0.06986, policy_loss: -45.27036, policy_entropy: 4.09481, alpha: 0.03258, time: 68.88279
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 21.89206, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.03628, qf2_loss: 0.03634, policy_loss: -45.12546, policy_entropy: 4.09604, alpha: 0.03186, time: 70.22961
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 21.85074, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.07453, qf2_loss: 0.07450, policy_loss: -44.97511, policy_entropy: 4.09584, alpha: 0.03116, time: 68.87926
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 21.12875, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.04782, qf2_loss: 0.04795, policy_loss: -44.82708, policy_entropy: 4.09582, alpha: 0.03047, time: 69.17701
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 24.05139, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.06414, qf2_loss: 0.06395, policy_loss: -44.68016, policy_entropy: 4.09616, alpha: 0.02979, time: 68.65878
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 23.13401, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.09410, qf2_loss: 0.09434, policy_loss: -44.52757, policy_entropy: 4.09577, alpha: 0.02914, time: 68.82428
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 24.32268, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.02652, qf2_loss: 0.02689, policy_loss: -44.37899, policy_entropy: 4.09582, alpha: 0.02849, time: 68.96634
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 23.98210, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.03748, qf2_loss: 0.03719, policy_loss: -44.23158, policy_entropy: 4.09234, alpha: 0.02786, time: 69.69116
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 23.43481, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.04859, qf2_loss: 0.04833, policy_loss: -44.07789, policy_entropy: 4.09387, alpha: 0.02725, time: 68.81542
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 31.29643, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.05699, qf2_loss: 0.05689, policy_loss: -43.93297, policy_entropy: 4.09301, alpha: 0.02664, time: 68.79456
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 23.92598, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.05333, qf2_loss: 0.05318, policy_loss: -43.77865, policy_entropy: 4.09156, alpha: 0.02605, time: 68.59722
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 26.20283, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.05699, qf2_loss: 0.05693, policy_loss: -43.62782, policy_entropy: 4.08824, alpha: 0.02548, time: 68.47094
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 23.93937, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.05963, qf2_loss: 0.05961, policy_loss: -43.47436, policy_entropy: 4.09103, alpha: 0.02492, time: 68.68441
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 26.88168, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.04701, qf2_loss: 0.04708, policy_loss: -43.32366, policy_entropy: 4.08910, alpha: 0.02437, time: 68.76853
[CW] eval: return: 24.86912, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 25.13564, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.03658, qf2_loss: 0.03644, policy_loss: -43.17239, policy_entropy: 4.09217, alpha: 0.02383, time: 68.41836
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 24.49558, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.06515, qf2_loss: 0.06534, policy_loss: -43.01918, policy_entropy: 4.08810, alpha: 0.02330, time: 69.00706
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 24.14877, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.03455, qf2_loss: 0.03469, policy_loss: -42.86613, policy_entropy: 4.08869, alpha: 0.02279, time: 68.63523
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 22.60434, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.04760, qf2_loss: 0.04774, policy_loss: -42.71299, policy_entropy: 4.08787, alpha: 0.02228, time: 68.46741
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 24.28908, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.06166, qf2_loss: 0.06186, policy_loss: -42.56341, policy_entropy: 4.08030, alpha: 0.02179, time: 68.24364
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 25.53260, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.05866, qf2_loss: 0.05850, policy_loss: -42.40312, policy_entropy: 4.07810, alpha: 0.02131, time: 68.52898
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 25.72439, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.03567, qf2_loss: 0.03607, policy_loss: -42.25159, policy_entropy: 4.08047, alpha: 0.02084, time: 68.51937
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 25.14927, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.05053, qf2_loss: 0.05064, policy_loss: -42.10606, policy_entropy: 4.07808, alpha: 0.02038, time: 68.55303
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 29.04133, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.05024, qf2_loss: 0.05044, policy_loss: -41.95151, policy_entropy: 4.08334, alpha: 0.01993, time: 68.44616
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 25.09092, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.03498, qf2_loss: 0.03505, policy_loss: -41.79887, policy_entropy: 4.08201, alpha: 0.01949, time: 68.29529
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 28.74842, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.05202, qf2_loss: 0.05238, policy_loss: -41.64701, policy_entropy: 4.07768, alpha: 0.01906, time: 68.61226
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 34.04619, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.04686, qf2_loss: 0.04707, policy_loss: -41.49452, policy_entropy: 4.07321, alpha: 0.01864, time: 68.25144
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 24.65169, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.04529, qf2_loss: 0.04593, policy_loss: -41.33562, policy_entropy: 4.07288, alpha: 0.01822, time: 68.55467
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 23.26657, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.05012, qf2_loss: 0.04994, policy_loss: -41.18644, policy_entropy: 4.07937, alpha: 0.01782, time: 68.51241
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 24.60293, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.05778, qf2_loss: 0.05807, policy_loss: -41.03227, policy_entropy: 4.06895, alpha: 0.01743, time: 68.31931
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 22.73364, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.03732, qf2_loss: 0.03765, policy_loss: -40.87997, policy_entropy: 4.07628, alpha: 0.01704, time: 68.54142
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 24.44027, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.05357, qf2_loss: 0.05385, policy_loss: -40.72380, policy_entropy: 4.08045, alpha: 0.01667, time: 68.35449
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 22.69799, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.03416, qf2_loss: 0.03401, policy_loss: -40.57298, policy_entropy: 4.07382, alpha: 0.01630, time: 68.54442
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 24.69814, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.03711, qf2_loss: 0.03735, policy_loss: -40.41996, policy_entropy: 4.07536, alpha: 0.01594, time: 68.51110
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 22.74039, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.06180, qf2_loss: 0.06234, policy_loss: -40.26125, policy_entropy: 4.07431, alpha: 0.01559, time: 68.75998
[CW] eval: return: 25.05120, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 24.09796, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.02477, qf2_loss: 0.02502, policy_loss: -40.10606, policy_entropy: 4.06567, alpha: 0.01524, time: 68.75733
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 27.82385, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.04280, qf2_loss: 0.04313, policy_loss: -39.95559, policy_entropy: 4.05718, alpha: 0.01491, time: 68.65566
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 27.70040, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.05674, qf2_loss: 0.05712, policy_loss: -39.80639, policy_entropy: 4.05787, alpha: 0.01458, time: 68.34274
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 30.36656, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.04221, qf2_loss: 0.04250, policy_loss: -39.65415, policy_entropy: 4.05830, alpha: 0.01425, time: 68.37368
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 30.39668, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.04641, qf2_loss: 0.04646, policy_loss: -39.50414, policy_entropy: 4.04323, alpha: 0.01394, time: 68.62640
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 23.14778, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.03410, qf2_loss: 0.03424, policy_loss: -39.35088, policy_entropy: 4.06386, alpha: 0.01363, time: 68.66980
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 27.41472, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.05047, qf2_loss: 0.04951, policy_loss: -39.19684, policy_entropy: 4.05635, alpha: 0.01333, time: 68.54735
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 22.35380, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.03501, qf2_loss: 0.03646, policy_loss: -39.04764, policy_entropy: 4.04675, alpha: 0.01304, time: 68.56483
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 26.77934, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.05037, qf2_loss: 0.05062, policy_loss: -38.89855, policy_entropy: 4.03880, alpha: 0.01275, time: 68.38935
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 23.66731, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.03140, qf2_loss: 0.03206, policy_loss: -38.74800, policy_entropy: 4.03858, alpha: 0.01247, time: 68.54138
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 28.03823, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.05129, qf2_loss: 0.05199, policy_loss: -38.59835, policy_entropy: 4.00997, alpha: 0.01219, time: 68.54463
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 30.72715, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.03869, qf2_loss: 0.03889, policy_loss: -38.44392, policy_entropy: 4.01611, alpha: 0.01193, time: 68.35767
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 23.62830, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.04041, qf2_loss: 0.04075, policy_loss: -38.29610, policy_entropy: 4.01817, alpha: 0.01166, time: 68.45526
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 28.89928, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.04900, qf2_loss: 0.04875, policy_loss: -38.14306, policy_entropy: 4.03583, alpha: 0.01141, time: 68.83182
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 23.28979, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.03604, qf2_loss: 0.03667, policy_loss: -37.99086, policy_entropy: 3.99737, alpha: 0.01115, time: 68.77162
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 23.67113, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.03754, qf2_loss: 0.03787, policy_loss: -37.84433, policy_entropy: 4.03733, alpha: 0.01091, time: 68.71843
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 23.96014, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.04649, qf2_loss: 0.04754, policy_loss: -37.69508, policy_entropy: 4.03413, alpha: 0.01067, time: 68.42672
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 24.21155, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.03667, qf2_loss: 0.03637, policy_loss: -37.54628, policy_entropy: 3.97215, alpha: 0.01043, time: 68.52923
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 24.96493, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.03571, qf2_loss: 0.03620, policy_loss: -37.40006, policy_entropy: 3.97781, alpha: 0.01020, time: 68.79438
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 23.87952, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.05724, qf2_loss: 0.05737, policy_loss: -37.24793, policy_entropy: 3.99343, alpha: 0.00998, time: 69.44704
[CW] eval: return: 25.73424, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 22.09013, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.01886, qf2_loss: 0.01882, policy_loss: -37.10083, policy_entropy: 3.98432, alpha: 0.00976, time: 69.29381
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 27.25831, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.04219, qf2_loss: 0.04280, policy_loss: -36.94854, policy_entropy: 3.99060, alpha: 0.00955, time: 68.99210
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 20.69585, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.03982, qf2_loss: 0.04064, policy_loss: -36.80258, policy_entropy: 3.99660, alpha: 0.00933, time: 69.17464
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 25.10765, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.08315, qf2_loss: 0.08380, policy_loss: -36.66168, policy_entropy: 3.98583, alpha: 0.00913, time: 69.09098
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 24.93594, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.01925, qf2_loss: 0.02012, policy_loss: -36.51584, policy_entropy: 3.94680, alpha: 0.00893, time: 69.21702
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 21.54214, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.02151, qf2_loss: 0.02161, policy_loss: -36.36029, policy_entropy: 3.97106, alpha: 0.00873, time: 68.95371
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 21.22304, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.03009, qf2_loss: 0.03020, policy_loss: -36.21766, policy_entropy: 3.96474, alpha: 0.00854, time: 68.96840
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 25.68113, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.06960, qf2_loss: 0.06902, policy_loss: -36.07872, policy_entropy: 3.99763, alpha: 0.00835, time: 68.67881
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 24.28986, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.01718, qf2_loss: 0.01738, policy_loss: -35.93090, policy_entropy: 3.92512, alpha: 0.00817, time: 68.53367
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 25.94470, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.03732, qf2_loss: 0.03835, policy_loss: -35.78615, policy_entropy: 3.97359, alpha: 0.00799, time: 68.64998
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 22.01401, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.03482, qf2_loss: 0.03561, policy_loss: -35.64802, policy_entropy: 3.83348, alpha: 0.00781, time: 68.40139
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 21.69165, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.03841, qf2_loss: 0.03923, policy_loss: -35.50451, policy_entropy: 3.86106, alpha: 0.00764, time: 68.49228
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 26.92880, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.03034, qf2_loss: 0.03142, policy_loss: -35.35142, policy_entropy: 3.90370, alpha: 0.00748, time: 68.62795
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 22.68221, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 0.05187, qf2_loss: 0.05156, policy_loss: -35.21238, policy_entropy: 3.78854, alpha: 0.00731, time: 68.55425
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 24.97078, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.02917, qf2_loss: 0.02982, policy_loss: -35.07351, policy_entropy: 3.89649, alpha: 0.00715, time: 68.76551
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 29.72267, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.04112, qf2_loss: 0.04152, policy_loss: -34.92941, policy_entropy: 3.78707, alpha: 0.00700, time: 68.58453
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 24.60122, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.02961, qf2_loss: 0.03025, policy_loss: -34.79041, policy_entropy: 3.82835, alpha: 0.00684, time: 68.43216
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 31.54828, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.04046, qf2_loss: 0.04062, policy_loss: -34.64877, policy_entropy: 3.72218, alpha: 0.00669, time: 68.58161
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 17.80730, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.03932, qf2_loss: 0.03987, policy_loss: -34.50933, policy_entropy: 3.76505, alpha: 0.00655, time: 68.66700
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 17.06780, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.03390, qf2_loss: 0.03449, policy_loss: -34.36834, policy_entropy: 3.21942, alpha: 0.00641, time: 68.59889
[CW] eval: return: 21.68409, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 16.13424, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.04058, qf2_loss: 0.04105, policy_loss: -34.23278, policy_entropy: 3.33406, alpha: 0.00628, time: 68.69479
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 13.77643, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.03742, qf2_loss: 0.03840, policy_loss: -34.09130, policy_entropy: 3.45638, alpha: 0.00614, time: 68.54931
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 22.86315, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.03283, qf2_loss: 0.03323, policy_loss: -33.96732, policy_entropy: 2.93829, alpha: 0.00601, time: 68.43812
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 15.06950, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.03708, qf2_loss: 0.03724, policy_loss: -33.82243, policy_entropy: 3.20500, alpha: 0.00589, time: 68.33810
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 25.92093, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.03129, qf2_loss: 0.03203, policy_loss: -33.68379, policy_entropy: 2.79678, alpha: 0.00577, time: 68.61804
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 25.88970, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.04422, qf2_loss: 0.04449, policy_loss: -33.55918, policy_entropy: 2.53392, alpha: 0.00566, time: 68.76582
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 13.67922, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.03359, qf2_loss: 0.03376, policy_loss: -33.41196, policy_entropy: 3.17515, alpha: 0.00554, time: 68.50344
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 14.52852, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 0.03356, qf2_loss: 0.03384, policy_loss: -33.28066, policy_entropy: 2.57262, alpha: 0.00542, time: 68.54993
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 26.15971, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.03329, qf2_loss: 0.03348, policy_loss: -33.13780, policy_entropy: 2.85717, alpha: 0.00532, time: 68.84035
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 25.78137, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.06926, qf2_loss: 0.06990, policy_loss: -33.00548, policy_entropy: 2.79066, alpha: 0.00520, time: 68.83111
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 25.95582, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 0.02631, qf2_loss: 0.02622, policy_loss: -32.87184, policy_entropy: 3.10161, alpha: 0.00509, time: 68.83630
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 15.71336, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.05740, qf2_loss: 0.05688, policy_loss: -32.73532, policy_entropy: 2.86407, alpha: 0.00499, time: 68.77179
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 27.79125, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.02718, qf2_loss: 0.02719, policy_loss: -32.61072, policy_entropy: 2.72873, alpha: 0.00488, time: 68.54408
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 14.73009, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.03709, qf2_loss: 0.03664, policy_loss: -32.47316, policy_entropy: 2.30466, alpha: 0.00478, time: 68.49145
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 13.69903, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.04139, qf2_loss: 0.04183, policy_loss: -32.34149, policy_entropy: 2.35730, alpha: 0.00469, time: 68.67949
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 15.97234, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.03345, qf2_loss: 0.03303, policy_loss: -32.20929, policy_entropy: 2.37201, alpha: 0.00459, time: 68.61155
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 13.51650, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 0.05060, qf2_loss: 0.05067, policy_loss: -32.09487, policy_entropy: 1.30343, alpha: 0.00450, time: 68.37030
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 25.09563, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 0.04984, qf2_loss: 0.05017, policy_loss: -31.95709, policy_entropy: 0.86720, alpha: 0.00442, time: 68.64131
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 26.15982, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 0.03659, qf2_loss: 0.03602, policy_loss: -31.81911, policy_entropy: 2.06003, alpha: 0.00434, time: 68.64943
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 14.19575, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.05014, qf2_loss: 0.04914, policy_loss: -31.70478, policy_entropy: 1.69803, alpha: 0.00425, time: 68.50145
[CW] eval: return: 25.93241, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 26.78656, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 0.06017, qf2_loss: 0.05960, policy_loss: -31.58816, policy_entropy: 1.31867, alpha: 0.00417, time: 68.84524
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 26.19674, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 0.06572, qf2_loss: 0.06560, policy_loss: -31.45905, policy_entropy: 1.77351, alpha: 0.00409, time: 68.60859
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 19.59583, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 0.05984, qf2_loss: 0.05812, policy_loss: -31.34449, policy_entropy: 1.52963, alpha: 0.00401, time: 68.57758
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 26.03543, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 0.08032, qf2_loss: 0.07917, policy_loss: -31.21748, policy_entropy: 0.97427, alpha: 0.00393, time: 68.87916
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 28.86104, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 0.06173, qf2_loss: 0.06030, policy_loss: -31.10382, policy_entropy: 0.15328, alpha: 0.00386, time: 68.54713
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 47.04394, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 0.10045, qf2_loss: 0.09886, policy_loss: -31.00818, policy_entropy: -1.78769, alpha: 0.00381, time: 69.02345
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 42.25217, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 0.09241, qf2_loss: 0.09041, policy_loss: -30.90541, policy_entropy: -4.53414, alpha: 0.00378, time: 68.73890
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 47.80757, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 0.11116, qf2_loss: 0.11137, policy_loss: -30.83058, policy_entropy: -6.19420, alpha: 0.00377, time: 68.65189
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 37.00756, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 0.10194, qf2_loss: 0.09877, policy_loss: -30.75502, policy_entropy: -6.35625, alpha: 0.00377, time: 68.71202
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 40.16271, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 0.14339, qf2_loss: 0.14389, policy_loss: -30.68767, policy_entropy: -6.68816, alpha: 0.00378, time: 68.86397
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 43.48481, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 0.14115, qf2_loss: 0.13931, policy_loss: -30.63735, policy_entropy: -7.02994, alpha: 0.00379, time: 68.95715
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 45.20966, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 0.12846, qf2_loss: 0.12552, policy_loss: -30.59777, policy_entropy: -7.98884, alpha: 0.00381, time: 68.64424
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 49.71265, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 0.15307, qf2_loss: 0.14911, policy_loss: -30.57352, policy_entropy: -7.94487, alpha: 0.00384, time: 69.68713
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 42.35235, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 0.15632, qf2_loss: 0.15455, policy_loss: -30.53098, policy_entropy: -8.04858, alpha: 0.00386, time: 69.15414
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 48.84537, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 0.16190, qf2_loss: 0.15918, policy_loss: -30.55079, policy_entropy: -9.69174, alpha: 0.00391, time: 68.74273
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 40.94224, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 0.17661, qf2_loss: 0.17440, policy_loss: -30.56514, policy_entropy: -9.36747, alpha: 0.00398, time: 68.52620
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 52.29768, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 0.15886, qf2_loss: 0.15824, policy_loss: -30.62003, policy_entropy: -9.42717, alpha: 0.00404, time: 68.76778
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 70.15948, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 0.14736, qf2_loss: 0.14482, policy_loss: -30.71113, policy_entropy: -9.10899, alpha: 0.00410, time: 68.71614
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 64.75623, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 0.14290, qf2_loss: 0.14218, policy_loss: -30.80589, policy_entropy: -10.01939, alpha: 0.00418, time: 69.99166
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 42.51356, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 0.18501, qf2_loss: 0.18563, policy_loss: -30.99847, policy_entropy: -10.51373, alpha: 0.00427, time: 69.55201
[CW] eval: return: 24.15355, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 24.19787, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 0.20011, qf2_loss: 0.20261, policy_loss: -31.09941, policy_entropy: -9.01832, alpha: 0.00437, time: 68.79927
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 11.44091, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 0.15078, qf2_loss: 0.15130, policy_loss: -31.23948, policy_entropy: -10.37469, alpha: 0.00446, time: 68.71221
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 56.36705, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 0.18623, qf2_loss: 0.18724, policy_loss: -31.32760, policy_entropy: -10.05932, alpha: 0.00458, time: 68.78968
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 24.56598, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 0.18471, qf2_loss: 0.18483, policy_loss: -31.43558, policy_entropy: -9.61441, alpha: 0.00470, time: 68.82970
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 5.74505, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 0.20491, qf2_loss: 0.20488, policy_loss: -31.45453, policy_entropy: -9.01193, alpha: 0.00480, time: 68.70738
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 6.33822, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 0.19564, qf2_loss: 0.19568, policy_loss: -31.49175, policy_entropy: -7.47787, alpha: 0.00488, time: 68.61322
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 22.96630, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 0.21768, qf2_loss: 0.21745, policy_loss: -31.47053, policy_entropy: -6.80538, alpha: 0.00493, time: 68.48369
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 26.40009, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 0.23494, qf2_loss: 0.23576, policy_loss: -31.37171, policy_entropy: -5.79714, alpha: 0.00494, time: 68.48508
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 23.37109, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 0.22901, qf2_loss: 0.22849, policy_loss: -31.33908, policy_entropy: -6.73586, alpha: 0.00494, time: 68.71273
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 23.63529, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 0.22496, qf2_loss: 0.22474, policy_loss: -31.29647, policy_entropy: -6.20838, alpha: 0.00497, time: 68.68244
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 22.80982, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 0.22084, qf2_loss: 0.22086, policy_loss: -31.19358, policy_entropy: -5.93672, alpha: 0.00497, time: 68.68020
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 51.40748, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 0.21370, qf2_loss: 0.21570, policy_loss: -31.09944, policy_entropy: -5.76256, alpha: 0.00496, time: 69.05917
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 21.05988, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 0.16946, qf2_loss: 0.16978, policy_loss: -31.06829, policy_entropy: -5.15566, alpha: 0.00494, time: 68.92900
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 19.63274, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 0.15659, qf2_loss: 0.15690, policy_loss: -31.07432, policy_entropy: -5.08083, alpha: 0.00490, time: 68.77572
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 27.94519, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 0.14625, qf2_loss: 0.14618, policy_loss: -31.08760, policy_entropy: -4.10358, alpha: 0.00482, time: 68.84388
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 17.82593, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 0.15905, qf2_loss: 0.15995, policy_loss: -30.97964, policy_entropy: -3.37051, alpha: 0.00472, time: 68.99054
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 19.22388, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 0.18837, qf2_loss: 0.18908, policy_loss: -31.03272, policy_entropy: -2.59884, alpha: 0.00458, time: 68.75049
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 27.92375, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 0.14180, qf2_loss: 0.14163, policy_loss: -30.94126, policy_entropy: -2.61701, alpha: 0.00443, time: 68.66807
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 18.50442, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 0.13969, qf2_loss: 0.13933, policy_loss: -30.88612, policy_entropy: -2.71189, alpha: 0.00428, time: 68.70444
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 18.11405, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 0.13565, qf2_loss: 0.13570, policy_loss: -30.88554, policy_entropy: -3.74457, alpha: 0.00417, time: 68.85566
[CW] eval: return: 23.93716, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 22.34838, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 0.16515, qf2_loss: 0.16644, policy_loss: -30.79071, policy_entropy: -2.95135, alpha: 0.00408, time: 68.68777
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 22.72996, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 0.14841, qf2_loss: 0.14517, policy_loss: -30.73365, policy_entropy: -3.24944, alpha: 0.00397, time: 68.54136
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 21.52592, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 0.10510, qf2_loss: 0.10420, policy_loss: -30.74062, policy_entropy: -4.60696, alpha: 0.00390, time: 68.94406
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 34.58649, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 0.12088, qf2_loss: 0.11946, policy_loss: -30.58218, policy_entropy: -3.92279, alpha: 0.00384, time: 69.05888
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 26.81124, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 0.09911, qf2_loss: 0.09805, policy_loss: -30.57893, policy_entropy: -3.41736, alpha: 0.00376, time: 69.27913
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 23.51447, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 0.10515, qf2_loss: 0.10515, policy_loss: -30.54961, policy_entropy: -2.67541, alpha: 0.00366, time: 68.80686
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 14.40247, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 0.10326, qf2_loss: 0.10107, policy_loss: -30.49233, policy_entropy: -3.58643, alpha: 0.00357, time: 68.78385
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 44.50450, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 0.19005, qf2_loss: 0.19277, policy_loss: -30.30776, policy_entropy: -4.33806, alpha: 0.00350, time: 68.74706
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 14.93017, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 0.07464, qf2_loss: 0.07336, policy_loss: -30.25780, policy_entropy: -1.86072, alpha: 0.00342, time: 68.52157
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 4.46121, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 0.06704, qf2_loss: 0.06686, policy_loss: -30.13683, policy_entropy: -2.40635, alpha: 0.00331, time: 68.84834
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 5.60382, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 0.07206, qf2_loss: 0.07099, policy_loss: -30.03858, policy_entropy: -2.60929, alpha: 0.00321, time: 68.99877
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 5.54504, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 0.12604, qf2_loss: 0.12544, policy_loss: -29.93753, policy_entropy: -2.23585, alpha: 0.00313, time: 68.64467
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 41.56917, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 0.07500, qf2_loss: 0.07607, policy_loss: -29.84530, policy_entropy: -3.28834, alpha: 0.00304, time: 68.70146
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 43.54066, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 0.06136, qf2_loss: 0.06083, policy_loss: -29.75879, policy_entropy: -3.31261, alpha: 0.00298, time: 69.04595
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 44.04974, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 0.06479, qf2_loss: 0.06519, policy_loss: -29.68135, policy_entropy: -3.94200, alpha: 0.00292, time: 68.88008
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 43.81449, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 0.05826, qf2_loss: 0.05769, policy_loss: -29.59635, policy_entropy: -4.12869, alpha: 0.00288, time: 69.09095
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 42.36960, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 0.04569, qf2_loss: 0.04540, policy_loss: -29.47841, policy_entropy: -4.78869, alpha: 0.00284, time: 68.83512
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 23.29662, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 0.05503, qf2_loss: 0.05452, policy_loss: -29.45461, policy_entropy: -3.57135, alpha: 0.00280, time: 68.93300
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 20.75693, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 0.08499, qf2_loss: 0.08651, policy_loss: -29.24218, policy_entropy: -3.94692, alpha: 0.00275, time: 69.00622
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 23.98206, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 0.04875, qf2_loss: 0.04880, policy_loss: -29.18392, policy_entropy: -4.17905, alpha: 0.00270, time: 68.90755
[CW] eval: return: 26.57189, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 27.33765, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 0.04585, qf2_loss: 0.04682, policy_loss: -29.12729, policy_entropy: -4.78098, alpha: 0.00267, time: 69.08212
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 20.06306, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 0.03612, qf2_loss: 0.03592, policy_loss: -29.09565, policy_entropy: -4.72931, alpha: 0.00264, time: 68.86778
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 23.60935, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 0.03658, qf2_loss: 0.03640, policy_loss: -28.93543, policy_entropy: -5.24728, alpha: 0.00261, time: 68.83669
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 43.12514, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 0.04878, qf2_loss: 0.04883, policy_loss: -28.84433, policy_entropy: -4.17594, alpha: 0.00258, time: 68.95243
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 22.23748, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 0.07506, qf2_loss: 0.07688, policy_loss: -28.81117, policy_entropy: -4.64507, alpha: 0.00254, time: 69.14177
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 22.32022, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 0.03425, qf2_loss: 0.03385, policy_loss: -28.69570, policy_entropy: -4.16152, alpha: 0.00250, time: 69.43868
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 22.58742, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 0.03727, qf2_loss: 0.03664, policy_loss: -28.50901, policy_entropy: -4.29689, alpha: 0.00246, time: 68.56802
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 21.39635, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 0.05313, qf2_loss: 0.05300, policy_loss: -28.43020, policy_entropy: -4.30245, alpha: 0.00242, time: 68.40391
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 24.72649, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 0.03332, qf2_loss: 0.03331, policy_loss: -28.32424, policy_entropy: -4.24734, alpha: 0.00238, time: 68.69523
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 21.75476, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 0.03282, qf2_loss: 0.03272, policy_loss: -28.23293, policy_entropy: -4.97233, alpha: 0.00234, time: 68.89144
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 19.83962, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 0.03820, qf2_loss: 0.03876, policy_loss: -28.15432, policy_entropy: -4.27294, alpha: 0.00231, time: 69.86710
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 25.51031, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 0.05860, qf2_loss: 0.05990, policy_loss: -27.99805, policy_entropy: -3.66017, alpha: 0.00226, time: 68.68333
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 24.96867, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 0.03536, qf2_loss: 0.03540, policy_loss: -27.93165, policy_entropy: -4.93317, alpha: 0.00222, time: 68.82353
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 39.30477, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 0.05455, qf2_loss: 0.05579, policy_loss: -27.85537, policy_entropy: -6.46553, alpha: 0.00220, time: 68.74834
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 24.16870, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 0.04426, qf2_loss: 0.04512, policy_loss: -27.76274, policy_entropy: -6.04552, alpha: 0.00222, time: 68.61137
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 32.21538, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 0.03364, qf2_loss: 0.03385, policy_loss: -27.66220, policy_entropy: -5.95545, alpha: 0.00222, time: 68.89065
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 27.40026, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 0.03145, qf2_loss: 0.03147, policy_loss: -27.54882, policy_entropy: -5.60394, alpha: 0.00221, time: 68.75918
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 39.21538, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 0.04688, qf2_loss: 0.04701, policy_loss: -27.49586, policy_entropy: -5.88666, alpha: 0.00220, time: 68.97372
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 65.36341, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 0.04153, qf2_loss: 0.04177, policy_loss: -27.36402, policy_entropy: -6.05805, alpha: 0.00221, time: 69.06773
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 40.23639, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 0.03214, qf2_loss: 0.03324, policy_loss: -27.27686, policy_entropy: -4.66287, alpha: 0.00219, time: 68.91849
[CW] eval: return: 44.32463, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 38.81914, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 0.03461, qf2_loss: 0.03492, policy_loss: -27.20243, policy_entropy: -4.44093, alpha: 0.00214, time: 68.92411
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 25.63441, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 0.03588, qf2_loss: 0.03556, policy_loss: -27.11113, policy_entropy: -4.38767, alpha: 0.00210, time: 68.57484
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 26.40186, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 0.04691, qf2_loss: 0.04686, policy_loss: -27.03380, policy_entropy: -4.97006, alpha: 0.00206, time: 68.47651
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 64.73914, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 0.04428, qf2_loss: 0.04526, policy_loss: -26.90370, policy_entropy: -4.99744, alpha: 0.00203, time: 68.96845
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 40.58903, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 0.04756, qf2_loss: 0.04819, policy_loss: -26.86594, policy_entropy: -5.78695, alpha: 0.00202, time: 68.67189
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 33.09212, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 0.04863, qf2_loss: 0.04898, policy_loss: -26.71231, policy_entropy: -6.03412, alpha: 0.00200, time: 68.95699
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 68.71650, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 0.03633, qf2_loss: 0.03604, policy_loss: -26.64157, policy_entropy: -6.09066, alpha: 0.00201, time: 68.50851
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 110.96864, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 0.04777, qf2_loss: 0.04875, policy_loss: -26.66278, policy_entropy: -5.02715, alpha: 0.00199, time: 68.80109
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 114.10285, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 0.04881, qf2_loss: 0.04921, policy_loss: -26.56417, policy_entropy: -6.81669, alpha: 0.00198, time: 68.62488
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 43.73258, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 0.04842, qf2_loss: 0.04910, policy_loss: -26.46026, policy_entropy: -5.83353, alpha: 0.00201, time: 68.47053
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 91.95135, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 0.04414, qf2_loss: 0.04537, policy_loss: -26.39397, policy_entropy: -6.41557, alpha: 0.00201, time: 68.84554
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 41.73223, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 0.05721, qf2_loss: 0.05652, policy_loss: -26.33314, policy_entropy: -8.27483, alpha: 0.00204, time: 68.49419
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 78.36435, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 0.08266, qf2_loss: 0.08405, policy_loss: -26.29893, policy_entropy: -8.23289, alpha: 0.00214, time: 68.84420
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 70.08684, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 0.04818, qf2_loss: 0.04832, policy_loss: -26.19658, policy_entropy: -8.10111, alpha: 0.00220, time: 68.90676
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 70.24164, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 0.04946, qf2_loss: 0.04934, policy_loss: -26.26600, policy_entropy: -8.97923, alpha: 0.00230, time: 69.06151
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 44.19952, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 0.07817, qf2_loss: 0.07894, policy_loss: -26.13247, policy_entropy: -8.62195, alpha: 0.00240, time: 68.70802
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 66.12914, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 0.06449, qf2_loss: 0.06632, policy_loss: -26.13750, policy_entropy: -8.60097, alpha: 0.00251, time: 68.66102
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 43.59047, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 0.05891, qf2_loss: 0.05878, policy_loss: -26.00974, policy_entropy: -6.75078, alpha: 0.00257, time: 68.76608
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 83.13841, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 0.07100, qf2_loss: 0.06939, policy_loss: -25.94877, policy_entropy: -7.05186, alpha: 0.00262, time: 68.82396
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 54.82983, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 0.08183, qf2_loss: 0.08552, policy_loss: -25.91358, policy_entropy: -6.08873, alpha: 0.00265, time: 68.59946
[CW] eval: return: 47.57019, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 40.32660, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 0.07063, qf2_loss: 0.07160, policy_loss: -25.91603, policy_entropy: -5.93887, alpha: 0.00264, time: 68.82445
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 42.79639, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 0.06155, qf2_loss: 0.06157, policy_loss: -25.83445, policy_entropy: -5.73144, alpha: 0.00264, time: 68.64351
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 47.22533, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 0.07062, qf2_loss: 0.07022, policy_loss: -25.73896, policy_entropy: -5.49586, alpha: 0.00262, time: 68.85699
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 51.54883, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 0.09792, qf2_loss: 0.09832, policy_loss: -25.72686, policy_entropy: -5.88294, alpha: 0.00260, time: 68.73926
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 39.71291, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 0.08712, qf2_loss: 0.08828, policy_loss: -25.62321, policy_entropy: -5.94491, alpha: 0.00259, time: 68.66285
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 44.17637, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 0.07351, qf2_loss: 0.07355, policy_loss: -25.55870, policy_entropy: -5.78097, alpha: 0.00259, time: 68.91974
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 54.46119, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 0.09037, qf2_loss: 0.09091, policy_loss: -25.53958, policy_entropy: -5.19804, alpha: 0.00257, time: 68.99484
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 45.06247, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 0.10614, qf2_loss: 0.10935, policy_loss: -25.53590, policy_entropy: -5.86386, alpha: 0.00253, time: 68.68176
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 49.05117, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 0.07408, qf2_loss: 0.07465, policy_loss: -25.47798, policy_entropy: -5.76561, alpha: 0.00252, time: 68.97403
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 45.05702, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 0.09181, qf2_loss: 0.09268, policy_loss: -25.46247, policy_entropy: -6.90424, alpha: 0.00254, time: 69.15212
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 44.87996, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 0.07904, qf2_loss: 0.07811, policy_loss: -25.41543, policy_entropy: -7.04183, alpha: 0.00259, time: 68.93979
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 40.50145, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 0.08381, qf2_loss: 0.08647, policy_loss: -25.38732, policy_entropy: -6.83103, alpha: 0.00266, time: 69.02625
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 43.54741, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 0.09718, qf2_loss: 0.09766, policy_loss: -25.31263, policy_entropy: -7.12532, alpha: 0.00273, time: 68.65141
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 44.88501, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 0.08241, qf2_loss: 0.08349, policy_loss: -25.26104, policy_entropy: -7.43834, alpha: 0.00282, time: 69.01865
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 52.13343, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 0.08367, qf2_loss: 0.08532, policy_loss: -25.18764, policy_entropy: -7.36608, alpha: 0.00292, time: 68.89987
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 67.35189, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 0.11290, qf2_loss: 0.11120, policy_loss: -25.28011, policy_entropy: -7.49835, alpha: 0.00302, time: 68.88609
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 43.83339, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 0.09015, qf2_loss: 0.09131, policy_loss: -25.17094, policy_entropy: -7.14432, alpha: 0.00313, time: 69.13242
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 51.39006, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 0.11884, qf2_loss: 0.11863, policy_loss: -25.16314, policy_entropy: -6.90019, alpha: 0.00322, time: 69.02713
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 56.97124, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 0.09257, qf2_loss: 0.09248, policy_loss: -25.13302, policy_entropy: -6.37449, alpha: 0.00327, time: 69.84008
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 63.13066, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 0.08793, qf2_loss: 0.08904, policy_loss: -25.06214, policy_entropy: -7.63903, alpha: 0.00335, time: 68.70741
[CW] eval: return: 50.88750, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 44.57488, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 0.10475, qf2_loss: 0.10316, policy_loss: -25.12826, policy_entropy: -7.31994, alpha: 0.00349, time: 68.80321
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 81.04430, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 0.10019, qf2_loss: 0.09984, policy_loss: -25.06452, policy_entropy: -6.80856, alpha: 0.00360, time: 68.80923
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 49.22332, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 0.08801, qf2_loss: 0.08919, policy_loss: -24.99240, policy_entropy: -7.14503, alpha: 0.00370, time: 69.01088
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 111.02524, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 0.08826, qf2_loss: 0.08959, policy_loss: -25.07238, policy_entropy: -6.61972, alpha: 0.00380, time: 69.80989
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 43.61505, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 0.08484, qf2_loss: 0.08408, policy_loss: -24.99425, policy_entropy: -6.89599, alpha: 0.00388, time: 68.91317
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 14.26460, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 0.08131, qf2_loss: 0.08075, policy_loss: -24.94372, policy_entropy: -6.03479, alpha: 0.00394, time: 68.97911
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 53.07539, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 0.11790, qf2_loss: 0.11673, policy_loss: -24.92174, policy_entropy: -6.19157, alpha: 0.00395, time: 68.75199
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 91.24171, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 0.09245, qf2_loss: 0.09193, policy_loss: -24.96377, policy_entropy: -6.62396, alpha: 0.00401, time: 68.72066
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 56.99933, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 0.09370, qf2_loss: 0.09368, policy_loss: -24.85245, policy_entropy: -5.80249, alpha: 0.00404, time: 68.67569
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 65.56703, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 0.12170, qf2_loss: 0.12299, policy_loss: -24.87509, policy_entropy: -5.42993, alpha: 0.00400, time: 68.88925
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 62.82303, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 0.09313, qf2_loss: 0.09202, policy_loss: -24.76730, policy_entropy: -5.36358, alpha: 0.00390, time: 69.16636
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 66.35405, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 0.09736, qf2_loss: 0.09552, policy_loss: -24.87347, policy_entropy: -5.51203, alpha: 0.00383, time: 69.04124
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 60.07237, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 0.10159, qf2_loss: 0.10226, policy_loss: -24.84307, policy_entropy: -5.65840, alpha: 0.00378, time: 69.54587
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 56.93027, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 0.08430, qf2_loss: 0.08370, policy_loss: -24.79986, policy_entropy: -5.67273, alpha: 0.00372, time: 68.86787
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 55.23748, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 0.07652, qf2_loss: 0.07655, policy_loss: -24.75155, policy_entropy: -5.83207, alpha: 0.00371, time: 68.86070
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 77.16563, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 0.08336, qf2_loss: 0.08249, policy_loss: -24.73627, policy_entropy: -5.93771, alpha: 0.00369, time: 69.13933
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 80.38487, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 0.09460, qf2_loss: 0.09364, policy_loss: -24.71971, policy_entropy: -5.60579, alpha: 0.00367, time: 69.10603
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 47.76020, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 0.08640, qf2_loss: 0.08564, policy_loss: -24.66346, policy_entropy: -5.68282, alpha: 0.00360, time: 68.84462
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 157.59008, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 0.12742, qf2_loss: 0.12810, policy_loss: -24.69797, policy_entropy: -6.00643, alpha: 0.00359, time: 68.88974
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 133.00197, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 0.08178, qf2_loss: 0.08138, policy_loss: -24.58340, policy_entropy: -5.98315, alpha: 0.00358, time: 68.93646
[CW] eval: return: 96.90915, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 52.12691, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 0.08032, qf2_loss: 0.08094, policy_loss: -24.69626, policy_entropy: -6.27105, alpha: 0.00359, time: 68.86359
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 28.12494, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 0.08274, qf2_loss: 0.08150, policy_loss: -24.58583, policy_entropy: -6.71621, alpha: 0.00367, time: 68.98383
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 52.88055, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 0.07713, qf2_loss: 0.07681, policy_loss: -24.55963, policy_entropy: -6.12297, alpha: 0.00374, time: 68.80043
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 87.67803, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 0.09821, qf2_loss: 0.09881, policy_loss: -24.55076, policy_entropy: -6.32381, alpha: 0.00376, time: 68.83591
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 107.42640, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 0.09248, qf2_loss: 0.09237, policy_loss: -24.52041, policy_entropy: -5.85683, alpha: 0.00379, time: 68.69393
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 118.25577, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 0.07920, qf2_loss: 0.07763, policy_loss: -24.55616, policy_entropy: -5.91792, alpha: 0.00376, time: 68.91164
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 55.36735, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 0.08056, qf2_loss: 0.08021, policy_loss: -24.53904, policy_entropy: -5.88481, alpha: 0.00376, time: 68.79611
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 50.25615, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 0.07850, qf2_loss: 0.07851, policy_loss: -24.44191, policy_entropy: -5.49569, alpha: 0.00371, time: 68.64110
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 53.34075, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 0.08227, qf2_loss: 0.08261, policy_loss: -24.39158, policy_entropy: -5.21879, alpha: 0.00361, time: 68.96684
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 113.19136, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 0.08729, qf2_loss: 0.08634, policy_loss: -24.38176, policy_entropy: -5.39144, alpha: 0.00352, time: 68.68284
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 83.57991, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 0.07530, qf2_loss: 0.07447, policy_loss: -24.40242, policy_entropy: -5.69470, alpha: 0.00344, time: 68.83616
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 85.25677, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 0.08893, qf2_loss: 0.08858, policy_loss: -24.35860, policy_entropy: -5.50043, alpha: 0.00340, time: 68.62248
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 111.17386, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 0.07611, qf2_loss: 0.07654, policy_loss: -24.32153, policy_entropy: -5.52129, alpha: 0.00333, time: 68.69334
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 107.80244, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 0.09066, qf2_loss: 0.08959, policy_loss: -24.29839, policy_entropy: -5.68489, alpha: 0.00328, time: 68.79586
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 112.52270, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 0.10044, qf2_loss: 0.10093, policy_loss: -24.30747, policy_entropy: -5.47579, alpha: 0.00323, time: 68.77692
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 67.13332, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 0.09168, qf2_loss: 0.08979, policy_loss: -24.24555, policy_entropy: -5.46854, alpha: 0.00317, time: 69.63457
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 54.31468, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 0.08530, qf2_loss: 0.08516, policy_loss: -24.27054, policy_entropy: -5.88182, alpha: 0.00313, time: 69.09331
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 52.73957, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 0.07812, qf2_loss: 0.07767, policy_loss: -24.17011, policy_entropy: -6.14133, alpha: 0.00314, time: 68.83999
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 108.03356, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 0.08290, qf2_loss: 0.08276, policy_loss: -24.12546, policy_entropy: -5.38683, alpha: 0.00311, time: 68.96786
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 103.62264, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 0.08104, qf2_loss: 0.08126, policy_loss: -24.17944, policy_entropy: -5.77304, alpha: 0.00306, time: 69.09633
[CW] eval: return: 92.59279, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 112.94077, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 0.07584, qf2_loss: 0.07642, policy_loss: -24.17917, policy_entropy: -6.29030, alpha: 0.00306, time: 68.86123
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 123.89544, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 0.09369, qf2_loss: 0.09395, policy_loss: -24.17970, policy_entropy: -6.30360, alpha: 0.00310, time: 68.87384
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 87.33714, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 0.08095, qf2_loss: 0.08068, policy_loss: -24.06353, policy_entropy: -6.69407, alpha: 0.00315, time: 68.81205
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 44.50466, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 0.08498, qf2_loss: 0.08469, policy_loss: -24.03973, policy_entropy: -6.54867, alpha: 0.00324, time: 68.87272
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 108.84226, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 0.06827, qf2_loss: 0.06791, policy_loss: -24.06857, policy_entropy: -6.40973, alpha: 0.00329, time: 68.78552
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 128.51036, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 0.07428, qf2_loss: 0.07369, policy_loss: -24.01427, policy_entropy: -6.34928, alpha: 0.00334, time: 69.01731
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 72.07657, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 0.11231, qf2_loss: 0.11206, policy_loss: -24.06205, policy_entropy: -5.87582, alpha: 0.00337, time: 68.96912
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 123.07381, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 0.07085, qf2_loss: 0.06997, policy_loss: -24.03003, policy_entropy: -5.78552, alpha: 0.00335, time: 68.77847
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 123.20050, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 0.07553, qf2_loss: 0.07500, policy_loss: -23.98853, policy_entropy: -6.19175, alpha: 0.00333, time: 68.77879
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 107.18976, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 0.10654, qf2_loss: 0.10774, policy_loss: -23.95834, policy_entropy: -5.74416, alpha: 0.00334, time: 68.89823
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 85.30046, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 0.08494, qf2_loss: 0.08198, policy_loss: -23.95478, policy_entropy: -5.64101, alpha: 0.00330, time: 69.15848
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 126.28077, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 0.09101, qf2_loss: 0.09262, policy_loss: -23.95760, policy_entropy: -5.41001, alpha: 0.00324, time: 68.99972
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 115.74145, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 0.11526, qf2_loss: 0.11680, policy_loss: -23.93967, policy_entropy: -5.65238, alpha: 0.00318, time: 68.89680
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 121.46601, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 0.28682, qf2_loss: 0.28524, policy_loss: -23.92199, policy_entropy: -6.40573, alpha: 0.00316, time: 68.64868
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 117.31183, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 0.09621, qf2_loss: 0.09473, policy_loss: -23.85571, policy_entropy: -6.59245, alpha: 0.00322, time: 68.97670
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 19.87807, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 0.06806, qf2_loss: 0.06793, policy_loss: -23.84062, policy_entropy: -6.70895, alpha: 0.00331, time: 68.89923
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 113.36264, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 0.06377, qf2_loss: 0.06377, policy_loss: -23.84495, policy_entropy: -6.38306, alpha: 0.00338, time: 68.92085
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 115.57887, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 0.06727, qf2_loss: 0.06823, policy_loss: -23.77446, policy_entropy: -6.36824, alpha: 0.00342, time: 68.92682
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 118.45479, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 0.06857, qf2_loss: 0.06827, policy_loss: -23.84437, policy_entropy: -6.58007, alpha: 0.00349, time: 68.89624
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 91.22905, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 0.06405, qf2_loss: 0.06417, policy_loss: -23.73305, policy_entropy: -6.42093, alpha: 0.00356, time: 68.69194
[CW] eval: return: 105.84128, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 85.20061, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 0.06714, qf2_loss: 0.06726, policy_loss: -23.80153, policy_entropy: -6.34417, alpha: 0.00362, time: 68.86828
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 134.35344, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 0.08237, qf2_loss: 0.08144, policy_loss: -23.70469, policy_entropy: -6.40557, alpha: 0.00368, time: 68.64213
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 96.14603, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 0.07260, qf2_loss: 0.07319, policy_loss: -23.68286, policy_entropy: -6.14755, alpha: 0.00371, time: 68.73093
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 135.18972, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 0.07626, qf2_loss: 0.07672, policy_loss: -23.76326, policy_entropy: -6.38770, alpha: 0.00378, time: 68.85339
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 132.50465, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 0.09060, qf2_loss: 0.08957, policy_loss: -23.73453, policy_entropy: -6.06026, alpha: 0.00380, time: 68.84134
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 63.46535, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 0.08246, qf2_loss: 0.08360, policy_loss: -23.73681, policy_entropy: -6.16358, alpha: 0.00382, time: 68.92065
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 108.98659, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 0.07267, qf2_loss: 0.07295, policy_loss: -23.66626, policy_entropy: -6.17188, alpha: 0.00387, time: 68.79948
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 89.75261, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 0.07900, qf2_loss: 0.07944, policy_loss: -23.66025, policy_entropy: -6.50122, alpha: 0.00391, time: 68.92181
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 128.43975, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 0.09192, qf2_loss: 0.09298, policy_loss: -23.66109, policy_entropy: -6.39746, alpha: 0.00401, time: 68.72122
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 125.96687, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 0.09901, qf2_loss: 0.09853, policy_loss: -23.68055, policy_entropy: -6.34718, alpha: 0.00408, time: 68.92367
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 118.15462, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 0.08414, qf2_loss: 0.08302, policy_loss: -23.64793, policy_entropy: -6.02284, alpha: 0.00413, time: 68.77155
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 102.83592, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 0.07515, qf2_loss: 0.07637, policy_loss: -23.57263, policy_entropy: -6.06600, alpha: 0.00414, time: 68.87439
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 119.69756, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 0.08415, qf2_loss: 0.08286, policy_loss: -23.56823, policy_entropy: -5.89068, alpha: 0.00412, time: 68.81734
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 109.14575, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 0.11370, qf2_loss: 0.11572, policy_loss: -23.63133, policy_entropy: -5.95121, alpha: 0.00411, time: 68.83546
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 32.59505, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 0.07857, qf2_loss: 0.07809, policy_loss: -23.59161, policy_entropy: -6.03665, alpha: 0.00413, time: 68.88352
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 109.00963, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 0.08553, qf2_loss: 0.08604, policy_loss: -23.61917, policy_entropy: -5.63619, alpha: 0.00410, time: 69.02513
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 95.62107, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 0.08999, qf2_loss: 0.08862, policy_loss: -23.64773, policy_entropy: -5.48580, alpha: 0.00399, time: 68.96618
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 103.31375, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 0.08606, qf2_loss: 0.08543, policy_loss: -23.60277, policy_entropy: -5.78987, alpha: 0.00392, time: 68.79884
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 114.12787, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 0.08470, qf2_loss: 0.08509, policy_loss: -23.60367, policy_entropy: -5.89428, alpha: 0.00390, time: 68.96172
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 105.23288, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 0.07755, qf2_loss: 0.07744, policy_loss: -23.56349, policy_entropy: -5.96614, alpha: 0.00388, time: 69.15061
[CW] eval: return: 104.60336, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 114.58903, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 0.09124, qf2_loss: 0.09346, policy_loss: -23.56654, policy_entropy: -5.72285, alpha: 0.00385, time: 68.99918
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 112.64902, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 0.11624, qf2_loss: 0.11649, policy_loss: -23.57164, policy_entropy: -5.54166, alpha: 0.00379, time: 69.04187
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 123.00027, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 0.09782, qf2_loss: 0.09855, policy_loss: -23.55309, policy_entropy: -6.12217, alpha: 0.00375, time: 68.92749
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 135.15508, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 0.09508, qf2_loss: 0.09505, policy_loss: -23.57887, policy_entropy: -6.14383, alpha: 0.00377, time: 68.68346
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 155.37242, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 0.08334, qf2_loss: 0.08236, policy_loss: -23.54513, policy_entropy: -6.29850, alpha: 0.00382, time: 68.78939
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 122.92254, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 0.07776, qf2_loss: 0.07814, policy_loss: -23.56643, policy_entropy: -5.87314, alpha: 0.00384, time: 68.82378
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 122.19305, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 0.08355, qf2_loss: 0.08356, policy_loss: -23.58910, policy_entropy: -5.94853, alpha: 0.00382, time: 69.26414
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 155.21947, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 0.07795, qf2_loss: 0.07862, policy_loss: -23.55690, policy_entropy: -6.12578, alpha: 0.00381, time: 69.21679
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 119.32857, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 0.07361, qf2_loss: 0.07597, policy_loss: -23.57535, policy_entropy: -6.15095, alpha: 0.00384, time: 69.90127
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 140.13141, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 0.09153, qf2_loss: 0.09080, policy_loss: -23.64449, policy_entropy: -6.25518, alpha: 0.00390, time: 69.33167
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 135.10939, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 0.08290, qf2_loss: 0.08108, policy_loss: -23.65768, policy_entropy: -6.20542, alpha: 0.00393, time: 69.07225
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 132.71883, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 0.06859, qf2_loss: 0.06895, policy_loss: -23.60366, policy_entropy: -6.13568, alpha: 0.00397, time: 68.92263
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 112.41063, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 0.08292, qf2_loss: 0.08229, policy_loss: -23.64511, policy_entropy: -5.78974, alpha: 0.00398, time: 69.01088
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 131.68387, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 0.07662, qf2_loss: 0.07595, policy_loss: -23.56056, policy_entropy: -5.49134, alpha: 0.00388, time: 69.25499
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 152.13010, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 0.07259, qf2_loss: 0.07318, policy_loss: -23.63901, policy_entropy: -5.67918, alpha: 0.00382, time: 68.93002
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 130.67153, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 0.08062, qf2_loss: 0.07838, policy_loss: -23.57456, policy_entropy: -5.85973, alpha: 0.00377, time: 68.98632
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 131.59162, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 0.06995, qf2_loss: 0.07037, policy_loss: -23.55765, policy_entropy: -5.82163, alpha: 0.00375, time: 68.81046
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 138.50872, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 0.07659, qf2_loss: 0.07692, policy_loss: -23.57216, policy_entropy: -5.80802, alpha: 0.00370, time: 68.94509
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 156.22484, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 0.06854, qf2_loss: 0.06817, policy_loss: -23.64626, policy_entropy: -5.53530, alpha: 0.00365, time: 68.91379
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 145.31755, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 0.07959, qf2_loss: 0.07964, policy_loss: -23.64785, policy_entropy: -5.93050, alpha: 0.00358, time: 68.73420
[CW] eval: return: 137.93519, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 139.09720, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 0.07687, qf2_loss: 0.07756, policy_loss: -23.60378, policy_entropy: -5.69590, alpha: 0.00357, time: 68.97646
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 140.31807, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 0.06970, qf2_loss: 0.06970, policy_loss: -23.55476, policy_entropy: -5.62632, alpha: 0.00348, time: 68.90031
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 133.88392, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 0.08296, qf2_loss: 0.08232, policy_loss: -23.58127, policy_entropy: -5.92857, alpha: 0.00346, time: 69.17541
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 153.62948, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 0.07868, qf2_loss: 0.07941, policy_loss: -23.63839, policy_entropy: -5.93648, alpha: 0.00344, time: 69.00433
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 142.79775, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 0.07981, qf2_loss: 0.08012, policy_loss: -23.63457, policy_entropy: -5.91357, alpha: 0.00344, time: 69.27142
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 143.07905, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 0.08976, qf2_loss: 0.08999, policy_loss: -23.51351, policy_entropy: -5.95438, alpha: 0.00342, time: 69.03118
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 147.94285, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 0.07705, qf2_loss: 0.07566, policy_loss: -23.65462, policy_entropy: -6.27296, alpha: 0.00344, time: 69.09506
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 151.09401, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 0.06335, qf2_loss: 0.06285, policy_loss: -23.56903, policy_entropy: -6.09781, alpha: 0.00347, time: 68.78915
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 129.21295, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 0.10857, qf2_loss: 0.11056, policy_loss: -23.55057, policy_entropy: -6.07301, alpha: 0.00349, time: 69.06082
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 128.48571, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 0.07373, qf2_loss: 0.07304, policy_loss: -23.57207, policy_entropy: -5.71505, alpha: 0.00347, time: 69.31912
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 155.56434, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 0.06603, qf2_loss: 0.06609, policy_loss: -23.68278, policy_entropy: -5.85369, alpha: 0.00343, time: 69.17556
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 144.91941, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 0.07157, qf2_loss: 0.07165, policy_loss: -23.61945, policy_entropy: -6.11779, alpha: 0.00342, time: 69.17281
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 150.03929, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 0.06639, qf2_loss: 0.06630, policy_loss: -23.65101, policy_entropy: -6.32250, alpha: 0.00347, time: 68.84280
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 151.75051, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 0.09951, qf2_loss: 0.09868, policy_loss: -23.66813, policy_entropy: -6.21368, alpha: 0.00352, time: 68.98531
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 160.73494, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 0.08101, qf2_loss: 0.08191, policy_loss: -23.64897, policy_entropy: -5.79680, alpha: 0.00352, time: 69.18447
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 163.53866, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 0.07376, qf2_loss: 0.07417, policy_loss: -23.62243, policy_entropy: -5.78331, alpha: 0.00348, time: 69.04016
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 155.11676, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 0.08447, qf2_loss: 0.08651, policy_loss: -23.60849, policy_entropy: -5.69733, alpha: 0.00343, time: 69.10773
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 154.26913, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 0.08110, qf2_loss: 0.08073, policy_loss: -23.71539, policy_entropy: -6.03571, alpha: 0.00340, time: 69.07988
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 146.62880, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 0.09398, qf2_loss: 0.09516, policy_loss: -23.71041, policy_entropy: -6.06317, alpha: 0.00342, time: 68.81881
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 139.32885, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 0.07543, qf2_loss: 0.07520, policy_loss: -23.71763, policy_entropy: -5.85750, alpha: 0.00340, time: 68.95874
[CW] eval: return: 151.82614, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 159.18500, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 0.06945, qf2_loss: 0.06948, policy_loss: -23.69574, policy_entropy: -5.96164, alpha: 0.00338, time: 68.84787
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 164.07636, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 0.09935, qf2_loss: 0.09985, policy_loss: -23.65906, policy_entropy: -6.20787, alpha: 0.00339, time: 69.17050
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 136.16941, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 0.13392, qf2_loss: 0.13474, policy_loss: -23.75716, policy_entropy: -6.29071, alpha: 0.00346, time: 69.06143
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 164.09272, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 0.07234, qf2_loss: 0.07276, policy_loss: -23.73977, policy_entropy: -6.03198, alpha: 0.00349, time: 69.15020
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 161.14725, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 0.08470, qf2_loss: 0.08420, policy_loss: -23.78754, policy_entropy: -6.16290, alpha: 0.00350, time: 68.93102
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 159.50076, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 0.13101, qf2_loss: 0.13399, policy_loss: -23.79958, policy_entropy: -6.21020, alpha: 0.00354, time: 68.82210
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 158.62200, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 0.08180, qf2_loss: 0.08070, policy_loss: -23.75194, policy_entropy: -5.76241, alpha: 0.00353, time: 69.03933
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 159.80591, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 0.07000, qf2_loss: 0.06890, policy_loss: -23.76609, policy_entropy: -5.87742, alpha: 0.00351, time: 69.21853
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 146.08163, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 0.06580, qf2_loss: 0.06561, policy_loss: -23.79634, policy_entropy: -5.88865, alpha: 0.00348, time: 69.23096
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 178.10908, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 0.08001, qf2_loss: 0.07978, policy_loss: -23.85599, policy_entropy: -6.12506, alpha: 0.00349, time: 68.91568
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 130.87799, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 0.09250, qf2_loss: 0.09146, policy_loss: -23.78593, policy_entropy: -6.15455, alpha: 0.00351, time: 68.95614
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 142.02692, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 0.07224, qf2_loss: 0.07300, policy_loss: -23.80626, policy_entropy: -6.20457, alpha: 0.00356, time: 69.38581
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 143.90214, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 0.07728, qf2_loss: 0.07683, policy_loss: -23.84925, policy_entropy: -5.96284, alpha: 0.00356, time: 69.11781
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 172.48487, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 0.09448, qf2_loss: 0.09543, policy_loss: -23.90014, policy_entropy: -6.14999, alpha: 0.00357, time: 68.98184
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 162.92685, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 0.07777, qf2_loss: 0.07731, policy_loss: -23.92056, policy_entropy: -5.91849, alpha: 0.00359, time: 68.80672
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 172.35585, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 0.08232, qf2_loss: 0.08240, policy_loss: -23.90358, policy_entropy: -6.09583, alpha: 0.00358, time: 68.95417
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 158.08737, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 0.08387, qf2_loss: 0.08367, policy_loss: -23.89213, policy_entropy: -5.95868, alpha: 0.00360, time: 68.99784
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 149.74033, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 0.07838, qf2_loss: 0.07793, policy_loss: -23.99904, policy_entropy: -5.95311, alpha: 0.00359, time: 68.68934
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 160.05480, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 0.10365, qf2_loss: 0.10314, policy_loss: -23.96600, policy_entropy: -6.07093, alpha: 0.00358, time: 68.93188
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 141.37052, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 0.08988, qf2_loss: 0.08935, policy_loss: -23.89219, policy_entropy: -6.03638, alpha: 0.00359, time: 68.73803
[CW] eval: return: 158.40151, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 116.79099, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 0.08881, qf2_loss: 0.08869, policy_loss: -23.98141, policy_entropy: -5.88524, alpha: 0.00358, time: 69.17319
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 192.01172, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 0.10169, qf2_loss: 0.10254, policy_loss: -23.91502, policy_entropy: -5.79067, alpha: 0.00355, time: 68.74137
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 160.82000, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 0.11065, qf2_loss: 0.11015, policy_loss: -23.97040, policy_entropy: -6.19062, alpha: 0.00354, time: 68.81180
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 166.92607, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 0.09399, qf2_loss: 0.09348, policy_loss: -24.01507, policy_entropy: -5.90619, alpha: 0.00355, time: 68.49496
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 25.20057, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 0.08129, qf2_loss: 0.08238, policy_loss: -23.97743, policy_entropy: -6.20496, alpha: 0.00358, time: 69.03357
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 157.48925, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 0.09272, qf2_loss: 0.09212, policy_loss: -24.01393, policy_entropy: -5.93120, alpha: 0.00359, time: 68.92672
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 170.18175, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 0.08662, qf2_loss: 0.08662, policy_loss: -23.99968, policy_entropy: -6.04226, alpha: 0.00358, time: 68.99656
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 171.42069, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 0.09342, qf2_loss: 0.09261, policy_loss: -24.00798, policy_entropy: -5.98124, alpha: 0.00359, time: 69.10937
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 166.09947, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 0.08281, qf2_loss: 0.08104, policy_loss: -24.06056, policy_entropy: -6.22501, alpha: 0.00362, time: 69.26133
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 167.21903, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 0.08582, qf2_loss: 0.08630, policy_loss: -24.13586, policy_entropy: -5.78063, alpha: 0.00361, time: 68.86185
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 163.30298, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 0.08728, qf2_loss: 0.08676, policy_loss: -24.13821, policy_entropy: -5.93872, alpha: 0.00358, time: 68.90482
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 192.23369, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 0.11272, qf2_loss: 0.11381, policy_loss: -24.07140, policy_entropy: -6.36903, alpha: 0.00361, time: 69.95604
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 177.26137, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 0.09794, qf2_loss: 0.09843, policy_loss: -24.16552, policy_entropy: -6.15619, alpha: 0.00366, time: 69.10720
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 155.51940, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 0.08919, qf2_loss: 0.08855, policy_loss: -24.19899, policy_entropy: -6.09951, alpha: 0.00370, time: 68.95487
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 179.15389, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 0.09003, qf2_loss: 0.09046, policy_loss: -24.17780, policy_entropy: -6.10706, alpha: 0.00373, time: 68.89066
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 149.59970, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 0.09916, qf2_loss: 0.09869, policy_loss: -24.21542, policy_entropy: -6.24456, alpha: 0.00377, time: 69.13170
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 170.64164, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 0.10842, qf2_loss: 0.10730, policy_loss: -24.17700, policy_entropy: -5.96553, alpha: 0.00380, time: 69.38279
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 191.71290, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 0.09248, qf2_loss: 0.09387, policy_loss: -24.20624, policy_entropy: -6.22093, alpha: 0.00380, time: 68.82061
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 175.38222, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 0.09602, qf2_loss: 0.09719, policy_loss: -24.21855, policy_entropy: -6.19478, alpha: 0.00385, time: 68.86852
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 162.59848, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 0.08863, qf2_loss: 0.08932, policy_loss: -24.17122, policy_entropy: -5.91588, alpha: 0.00388, time: 68.96407
[CW] eval: return: 165.60084, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 149.41717, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 0.10615, qf2_loss: 0.10487, policy_loss: -24.25303, policy_entropy: -6.08949, alpha: 0.00386, time: 69.10769
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 172.06764, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 0.09991, qf2_loss: 0.10027, policy_loss: -24.30095, policy_entropy: -6.02592, alpha: 0.00391, time: 69.17558
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 161.42488, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 0.12049, qf2_loss: 0.11869, policy_loss: -24.31324, policy_entropy: -5.94449, alpha: 0.00388, time: 69.14767
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 167.38437, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 0.10264, qf2_loss: 0.10281, policy_loss: -24.31275, policy_entropy: -6.10435, alpha: 0.00387, time: 68.91404
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 159.44622, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 0.13772, qf2_loss: 0.14125, policy_loss: -24.33088, policy_entropy: -6.06092, alpha: 0.00392, time: 68.81981
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 178.89768, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 0.11662, qf2_loss: 0.11413, policy_loss: -24.32503, policy_entropy: -6.03604, alpha: 0.00392, time: 68.72277
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 171.91218, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 0.09752, qf2_loss: 0.09726, policy_loss: -24.34574, policy_entropy: -5.98398, alpha: 0.00393, time: 69.95690
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 165.37043, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 0.11893, qf2_loss: 0.11749, policy_loss: -24.36995, policy_entropy: -5.91550, alpha: 0.00392, time: 69.11013
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 168.64114, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 0.10420, qf2_loss: 0.10492, policy_loss: -24.38567, policy_entropy: -5.98480, alpha: 0.00390, time: 68.97250
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 174.73550, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 0.10060, qf2_loss: 0.10020, policy_loss: -24.32979, policy_entropy: -5.94902, alpha: 0.00389, time: 68.87417
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 160.18281, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 0.10423, qf2_loss: 0.10593, policy_loss: -24.46443, policy_entropy: -6.05680, alpha: 0.00391, time: 69.19192
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 170.67561, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 0.11342, qf2_loss: 0.11208, policy_loss: -24.45526, policy_entropy: -6.05615, alpha: 0.00391, time: 68.80290
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 163.02069, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 0.09434, qf2_loss: 0.09434, policy_loss: -24.41547, policy_entropy: -6.07572, alpha: 0.00392, time: 68.82481
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 173.78785, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 0.18404, qf2_loss: 0.18510, policy_loss: -24.45405, policy_entropy: -6.21741, alpha: 0.00394, time: 69.00543
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 185.11756, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 0.09380, qf2_loss: 0.09391, policy_loss: -24.52909, policy_entropy: -6.25525, alpha: 0.00403, time: 68.85692
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 164.97808, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 0.09708, qf2_loss: 0.09625, policy_loss: -24.52423, policy_entropy: -5.74334, alpha: 0.00403, time: 69.00302
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 172.53297, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 0.10868, qf2_loss: 0.10791, policy_loss: -24.58148, policy_entropy: -5.94862, alpha: 0.00397, time: 69.50275
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 175.29600, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 0.09998, qf2_loss: 0.09963, policy_loss: -24.55598, policy_entropy: -5.90296, alpha: 0.00396, time: 69.30003
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 162.74842, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 0.10727, qf2_loss: 0.10852, policy_loss: -24.51590, policy_entropy: -6.03647, alpha: 0.00396, time: 68.98423
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 157.26237, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 0.10870, qf2_loss: 0.10944, policy_loss: -24.57408, policy_entropy: -6.20174, alpha: 0.00398, time: 68.72661
[CW] eval: return: 166.12923, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 155.67858, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 0.11847, qf2_loss: 0.11744, policy_loss: -24.57084, policy_entropy: -5.93512, alpha: 0.00399, time: 68.83597
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 165.23884, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 0.11220, qf2_loss: 0.11296, policy_loss: -24.67282, policy_entropy: -6.23465, alpha: 0.00401, time: 68.81950
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 170.93554, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 0.11714, qf2_loss: 0.11673, policy_loss: -24.69019, policy_entropy: -6.23121, alpha: 0.00407, time: 69.17402
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 158.80145, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 0.10941, qf2_loss: 0.10911, policy_loss: -24.66109, policy_entropy: -6.24690, alpha: 0.00413, time: 69.10559
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 163.14214, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 0.12667, qf2_loss: 0.12606, policy_loss: -24.75585, policy_entropy: -6.54804, alpha: 0.00423, time: 69.16422
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 175.27063, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 0.12488, qf2_loss: 0.12753, policy_loss: -24.69488, policy_entropy: -6.24592, alpha: 0.00433, time: 69.16683
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 179.80782, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 0.11704, qf2_loss: 0.11543, policy_loss: -24.72285, policy_entropy: -6.38159, alpha: 0.00441, time: 69.03698
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 164.45134, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 0.12232, qf2_loss: 0.12239, policy_loss: -24.66168, policy_entropy: -6.00511, alpha: 0.00447, time: 68.95002
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 157.95924, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 0.11179, qf2_loss: 0.11383, policy_loss: -24.84783, policy_entropy: -5.75012, alpha: 0.00444, time: 68.93525
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 164.26926, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 0.12642, qf2_loss: 0.12450, policy_loss: -24.82838, policy_entropy: -5.85293, alpha: 0.00439, time: 68.78630
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 180.47910, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 0.11972, qf2_loss: 0.11922, policy_loss: -24.84802, policy_entropy: -5.86046, alpha: 0.00435, time: 68.58690
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 172.43191, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 0.16213, qf2_loss: 0.16473, policy_loss: -24.92702, policy_entropy: -6.16573, alpha: 0.00435, time: 68.60541
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 167.45042, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 0.11949, qf2_loss: 0.11837, policy_loss: -24.88847, policy_entropy: -5.82833, alpha: 0.00434, time: 68.87428
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 174.57789, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 0.11487, qf2_loss: 0.11333, policy_loss: -24.94990, policy_entropy: -5.97321, alpha: 0.00434, time: 68.68771
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 178.63666, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 0.11014, qf2_loss: 0.11174, policy_loss: -24.96697, policy_entropy: -5.85470, alpha: 0.00431, time: 68.81055
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 181.81047, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 0.12004, qf2_loss: 0.12046, policy_loss: -24.93732, policy_entropy: -5.92842, alpha: 0.00429, time: 68.80851
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 178.35555, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 0.11770, qf2_loss: 0.11783, policy_loss: -24.83944, policy_entropy: -5.64407, alpha: 0.00423, time: 68.75886
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 178.52451, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 0.10996, qf2_loss: 0.10960, policy_loss: -25.03371, policy_entropy: -5.98642, alpha: 0.00419, time: 69.20801
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 159.65902, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 0.12153, qf2_loss: 0.12214, policy_loss: -25.10052, policy_entropy: -5.92670, alpha: 0.00419, time: 68.87979
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 171.61395, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 0.10790, qf2_loss: 0.10712, policy_loss: -25.03937, policy_entropy: -5.88223, alpha: 0.00416, time: 68.76938
[CW] eval: return: 174.03948, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 176.40176, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 0.13191, qf2_loss: 0.13199, policy_loss: -25.11916, policy_entropy: -6.19585, alpha: 0.00415, time: 69.81560
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 170.79402, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 0.13125, qf2_loss: 0.13287, policy_loss: -25.05497, policy_entropy: -6.09705, alpha: 0.00421, time: 69.83547
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 182.15856, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 0.11264, qf2_loss: 0.11147, policy_loss: -25.14930, policy_entropy: -5.74321, alpha: 0.00419, time: 69.31824
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 175.09581, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 0.10806, qf2_loss: 0.10792, policy_loss: -25.10946, policy_entropy: -6.03921, alpha: 0.00415, time: 69.41952
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 180.54464, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 0.11059, qf2_loss: 0.11045, policy_loss: -25.12819, policy_entropy: -6.00085, alpha: 0.00416, time: 70.08135
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 115.01632, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 0.13977, qf2_loss: 0.14063, policy_loss: -25.14289, policy_entropy: -5.89084, alpha: 0.00416, time: 69.59677
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 189.55158, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 0.12780, qf2_loss: 0.12922, policy_loss: -25.31868, policy_entropy: -6.03082, alpha: 0.00414, time: 69.31160
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 189.74039, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 0.12683, qf2_loss: 0.12653, policy_loss: -25.29258, policy_entropy: -5.93261, alpha: 0.00414, time: 69.69619
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 175.53098, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 0.12123, qf2_loss: 0.12103, policy_loss: -25.30630, policy_entropy: -6.10256, alpha: 0.00414, time: 69.90634
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 164.02225, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 0.11637, qf2_loss: 0.11594, policy_loss: -25.32080, policy_entropy: -6.01470, alpha: 0.00415, time: 69.88788
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 167.52875, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 0.11447, qf2_loss: 0.11456, policy_loss: -25.35299, policy_entropy: -5.95553, alpha: 0.00415, time: 69.82215
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 184.14603, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 0.10617, qf2_loss: 0.10547, policy_loss: -25.45078, policy_entropy: -6.07889, alpha: 0.00416, time: 72.03537
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 184.75014, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 0.17255, qf2_loss: 0.17544, policy_loss: -25.48268, policy_entropy: -6.27725, alpha: 0.00420, time: 69.63963
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 169.50527, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 0.11000, qf2_loss: 0.11095, policy_loss: -25.38217, policy_entropy: -6.00083, alpha: 0.00423, time: 69.80490
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 174.71594, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 0.12646, qf2_loss: 0.12524, policy_loss: -25.41102, policy_entropy: -6.07482, alpha: 0.00423, time: 69.86815
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 147.71487, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 0.12433, qf2_loss: 0.12308, policy_loss: -25.49502, policy_entropy: -6.01724, alpha: 0.00425, time: 69.87184
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 172.34484, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 0.10965, qf2_loss: 0.10862, policy_loss: -25.50196, policy_entropy: -6.01054, alpha: 0.00427, time: 69.86251
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 181.99514, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 0.11288, qf2_loss: 0.11225, policy_loss: -25.48091, policy_entropy: -6.09554, alpha: 0.00426, time: 70.01458
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 183.62965, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 0.11930, qf2_loss: 0.11854, policy_loss: -25.56397, policy_entropy: -5.95960, alpha: 0.00429, time: 69.84770
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 185.61122, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 0.14729, qf2_loss: 0.14914, policy_loss: -25.58995, policy_entropy: -6.16943, alpha: 0.00428, time: 69.75001
[CW] eval: return: 178.14152, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 169.34134, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 0.15149, qf2_loss: 0.15117, policy_loss: -25.52139, policy_entropy: -6.56178, alpha: 0.00439, time: 69.82498
[CW] ---------------------------
