{"collect/return": 169.34134482219815, "collect/steps": 1000.0, "collect/total_steps": 587000.0, "train/qf1_loss": 0.1514891117066145, "train/qf2_loss": 0.1511660809069872, "train/policy_loss": -25.521388301849367, "train/policy_entropy": -6.561776576042175, "train/alpha": 0.00439149038400501, "train/time": 69.82497692108154, "eval/return": 178.1415197366383, "eval/steps": 1000.0, "_timestamp": 1678754556.1505146, "_runtime": 43151.815853595734, "_step": 581}