{"collect/return": 335.513621392136, "collect/steps": 1000.0, "collect/total_steps": 840000.0, "train/qf1_loss": 6.161212129592895, "train/qf2_loss": 6.180771396160126, "train/policy_loss": -257.73310455322263, "train/policy_entropy": -0.9993840539455414, "train/alpha": 0.20588534072041512, "train/time": 32.69035768508911, "eval/return": 334.384532070304, "eval/steps": 1000.0, "_timestamp": 1678403969.0111809, "_runtime": 28701.093946933746, "_step": 834}