{"collect/return": 841.6385987843387, "collect/steps": 1000.0, "collect/total_steps": 555000.0, "train/qf1_loss": 45.18926446914673, "train/qf2_loss": 45.47943849563599, "train/policy_loss": -461.37043334960936, "train/policy_entropy": -1.0070070230960846, "train/alpha": 0.6797912961244583, "train/time": 49.68905019760132, "eval/return": 828.5000869757438, "eval/steps": 1000.0, "_timestamp": 1678341467.5137703, "_runtime": 28649.523418426514, "_step": 549}