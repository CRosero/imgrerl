{"collect/return": 839.5805197106092, "collect/steps": 1000.0, "collect/total_steps": 845000.0, "train/qf1_loss": 17.98829577922821, "train/qf2_loss": 17.667382926940917, "train/policy_loss": -677.9276416015625, "train/policy_entropy": -1.007369504570961, "train/alpha": 0.33183132112026215, "train/time": 32.75379037857056, "eval/return": 834.0939775217296, "eval/steps": 1000.0, "_timestamp": 1678385764.1701708, "_runtime": 28661.763559818268, "_step": 839}