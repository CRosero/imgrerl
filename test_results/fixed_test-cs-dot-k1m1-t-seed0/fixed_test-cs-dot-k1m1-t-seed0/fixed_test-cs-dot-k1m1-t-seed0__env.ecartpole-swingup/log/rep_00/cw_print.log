[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 153.66153, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 0.99351, qf2_loss: 1.00721, policy_loss: -2.32755, policy_entropy: 0.68255, alpha: 0.98504, time: 37.64254
[CW] eval: return: 134.10749, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 131.03425, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.06167, qf2_loss: 0.06184, policy_loss: -2.75581, policy_entropy: 0.68191, alpha: 0.95626, time: 32.23287
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 199.90091, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.06536, qf2_loss: 0.06594, policy_loss: -3.32143, policy_entropy: 0.67881, alpha: 0.92874, time: 32.85849
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 91.92535, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.07741, qf2_loss: 0.07813, policy_loss: -3.77273, policy_entropy: 0.67634, alpha: 0.90240, time: 33.17468
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 167.06507, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.10155, qf2_loss: 0.10360, policy_loss: -4.41910, policy_entropy: 0.67358, alpha: 0.87714, time: 32.95377
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 267.59531, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.18341, qf2_loss: 0.18280, policy_loss: -5.33793, policy_entropy: 0.66712, alpha: 0.85291, time: 33.05894
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 50.22440, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.18526, qf2_loss: 0.18424, policy_loss: -5.82891, policy_entropy: 0.66276, alpha: 0.82969, time: 33.16827
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 42.33996, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.24329, qf2_loss: 0.24261, policy_loss: -6.35302, policy_entropy: 0.66117, alpha: 0.80736, time: 33.05493
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 83.53069, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.21673, qf2_loss: 0.21502, policy_loss: -6.78363, policy_entropy: 0.66108, alpha: 0.78585, time: 32.96695
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 154.43874, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.28770, qf2_loss: 0.28700, policy_loss: -7.40607, policy_entropy: 0.65762, alpha: 0.76512, time: 32.84686
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 180.95305, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.32597, qf2_loss: 0.32383, policy_loss: -8.24932, policy_entropy: 0.64963, alpha: 0.74517, time: 32.97719
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 100.91285, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.37307, qf2_loss: 0.37129, policy_loss: -8.83962, policy_entropy: 0.64260, alpha: 0.72597, time: 32.98346
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 245.80666, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.55183, qf2_loss: 0.55181, policy_loss: -9.70190, policy_entropy: 0.62786, alpha: 0.70752, time: 32.93824
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 72.65715, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.48079, qf2_loss: 0.48072, policy_loss: -10.16226, policy_entropy: 0.61298, alpha: 0.68980, time: 32.85759
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 214.25939, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.54806, qf2_loss: 0.55341, policy_loss: -11.01283, policy_entropy: 0.58760, alpha: 0.67281, time: 32.93012
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 200.29418, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.78943, qf2_loss: 0.79261, policy_loss: -12.01882, policy_entropy: 0.55876, alpha: 0.65657, time: 33.11506
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 224.68559, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.68871, qf2_loss: 0.69645, policy_loss: -12.81831, policy_entropy: 0.52305, alpha: 0.64105, time: 33.19471
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 237.24768, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.89177, qf2_loss: 0.89494, policy_loss: -13.76706, policy_entropy: 0.49266, alpha: 0.62624, time: 33.37952
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 354.54071, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.91237, qf2_loss: 0.92237, policy_loss: -15.00077, policy_entropy: 0.45458, alpha: 0.61205, time: 32.91740
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 232.92819, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 1.15733, qf2_loss: 1.17020, policy_loss: -15.92429, policy_entropy: 0.42022, alpha: 0.59849, time: 33.12444
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 223.91692, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 1.13008, qf2_loss: 1.14446, policy_loss: -17.04744, policy_entropy: 0.39287, alpha: 0.58546, time: 33.05967
[CW] eval: return: 192.73962, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 173.93791, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 1.04045, qf2_loss: 1.05353, policy_loss: -17.84482, policy_entropy: 0.36271, alpha: 0.57290, time: 33.06742
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 193.76723, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 1.60305, qf2_loss: 1.61818, policy_loss: -18.96404, policy_entropy: 0.33880, alpha: 0.56079, time: 32.95917
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 196.90760, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 1.56639, qf2_loss: 1.58242, policy_loss: -19.98809, policy_entropy: 0.31593, alpha: 0.54905, time: 33.00861
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 205.73084, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 1.41095, qf2_loss: 1.43203, policy_loss: -20.96749, policy_entropy: 0.29857, alpha: 0.53766, time: 33.09777
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 291.59842, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 1.53862, qf2_loss: 1.55335, policy_loss: -21.95986, policy_entropy: 0.27642, alpha: 0.52656, time: 32.94226
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 213.47340, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 1.53152, qf2_loss: 1.54437, policy_loss: -23.06791, policy_entropy: 0.25035, alpha: 0.51583, time: 33.17609
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 184.82573, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 1.37367, qf2_loss: 1.39357, policy_loss: -23.90052, policy_entropy: 0.24094, alpha: 0.50542, time: 32.95050
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 225.12421, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 1.49233, qf2_loss: 1.49960, policy_loss: -24.75157, policy_entropy: 0.22731, alpha: 0.49519, time: 33.09948
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 304.36064, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 1.57522, qf2_loss: 1.59580, policy_loss: -26.04606, policy_entropy: 0.20584, alpha: 0.48522, time: 33.53661
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 200.86289, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 1.59218, qf2_loss: 1.60813, policy_loss: -26.75666, policy_entropy: 0.18561, alpha: 0.47553, time: 32.92420
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 200.63990, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 1.67315, qf2_loss: 1.68115, policy_loss: -27.96665, policy_entropy: 0.17524, alpha: 0.46608, time: 32.91731
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 224.22235, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 1.53699, qf2_loss: 1.55311, policy_loss: -28.75425, policy_entropy: 0.14356, alpha: 0.45688, time: 33.10847
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 211.04929, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 1.61455, qf2_loss: 1.62318, policy_loss: -29.81311, policy_entropy: 0.12652, alpha: 0.44800, time: 33.12919
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 244.31047, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 1.63023, qf2_loss: 1.64645, policy_loss: -30.86996, policy_entropy: 0.10978, alpha: 0.43932, time: 33.00653
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 367.06873, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 1.82756, qf2_loss: 1.84448, policy_loss: -31.70021, policy_entropy: 0.08419, alpha: 0.43087, time: 32.92451
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 289.55949, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 1.78347, qf2_loss: 1.78974, policy_loss: -33.04363, policy_entropy: 0.06045, alpha: 0.42269, time: 33.21001
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 277.59146, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 1.93276, qf2_loss: 1.94212, policy_loss: -33.95260, policy_entropy: 0.03347, alpha: 0.41476, time: 32.83914
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 295.38072, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 1.98012, qf2_loss: 1.97535, policy_loss: -35.25032, policy_entropy: 0.00841, alpha: 0.40704, time: 32.92156
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 269.57843, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 1.95907, qf2_loss: 1.96571, policy_loss: -36.30135, policy_entropy: -0.01815, alpha: 0.39961, time: 32.99802
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 246.95682, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 2.07902, qf2_loss: 2.09517, policy_loss: -37.41403, policy_entropy: -0.04160, alpha: 0.39238, time: 33.00215
[CW] eval: return: 316.72171, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 193.52795, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 2.13893, qf2_loss: 2.14149, policy_loss: -38.17943, policy_entropy: -0.06895, alpha: 0.38537, time: 32.71382
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 193.21306, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 2.32928, qf2_loss: 2.33375, policy_loss: -39.25334, policy_entropy: -0.08858, alpha: 0.37853, time: 33.21134
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 287.66669, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 2.23225, qf2_loss: 2.25365, policy_loss: -40.52894, policy_entropy: -0.11677, alpha: 0.37192, time: 33.05414
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 291.71812, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 2.52638, qf2_loss: 2.54122, policy_loss: -41.44548, policy_entropy: -0.13587, alpha: 0.36545, time: 32.91258
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 288.37584, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 2.51858, qf2_loss: 2.53069, policy_loss: -42.45360, policy_entropy: -0.14710, alpha: 0.35918, time: 32.92432
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 287.39506, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 2.58758, qf2_loss: 2.60398, policy_loss: -43.96160, policy_entropy: -0.17663, alpha: 0.35298, time: 32.72313
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 329.94424, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 2.40784, qf2_loss: 2.41926, policy_loss: -44.79701, policy_entropy: -0.19769, alpha: 0.34702, time: 32.94729
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 248.76978, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 2.56216, qf2_loss: 2.57894, policy_loss: -46.20444, policy_entropy: -0.20839, alpha: 0.34116, time: 32.97662
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 312.37765, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 2.89535, qf2_loss: 2.91713, policy_loss: -47.16016, policy_entropy: -0.22718, alpha: 0.33540, time: 33.00724
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 243.06852, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 2.86078, qf2_loss: 2.87606, policy_loss: -48.35510, policy_entropy: -0.24669, alpha: 0.32980, time: 33.13707
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 263.22690, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 2.71095, qf2_loss: 2.71933, policy_loss: -49.57816, policy_entropy: -0.27099, alpha: 0.32433, time: 33.05607
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 281.50472, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 2.62572, qf2_loss: 2.64907, policy_loss: -50.57962, policy_entropy: -0.27701, alpha: 0.31899, time: 33.07898
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 346.55935, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 2.53732, qf2_loss: 2.56460, policy_loss: -51.71446, policy_entropy: -0.31492, alpha: 0.31378, time: 33.05877
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 276.98489, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 2.59568, qf2_loss: 2.63422, policy_loss: -52.63490, policy_entropy: -0.32470, alpha: 0.30873, time: 33.06310
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 225.27877, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 2.76070, qf2_loss: 2.79349, policy_loss: -53.88327, policy_entropy: -0.33193, alpha: 0.30376, time: 33.08588
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 454.24416, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 3.59861, qf2_loss: 3.66215, policy_loss: -54.94668, policy_entropy: -0.34771, alpha: 0.29883, time: 33.64942
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 364.97137, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 2.60573, qf2_loss: 2.61754, policy_loss: -56.29013, policy_entropy: -0.36535, alpha: 0.29402, time: 33.01040
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 283.06578, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 2.50976, qf2_loss: 2.53107, policy_loss: -57.20333, policy_entropy: -0.37498, alpha: 0.28931, time: 32.98872
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 225.10077, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 2.85163, qf2_loss: 2.87511, policy_loss: -58.30921, policy_entropy: -0.37093, alpha: 0.28462, time: 33.00783
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 274.74126, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 2.71134, qf2_loss: 2.73585, policy_loss: -59.63109, policy_entropy: -0.36235, alpha: 0.27987, time: 32.87189
[CW] eval: return: 251.94484, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 245.53484, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 2.84306, qf2_loss: 2.87856, policy_loss: -60.52296, policy_entropy: -0.38640, alpha: 0.27513, time: 32.75174
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 298.79462, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 2.60298, qf2_loss: 2.62503, policy_loss: -61.56166, policy_entropy: -0.40676, alpha: 0.27060, time: 33.02471
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 252.25603, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 2.89502, qf2_loss: 2.92927, policy_loss: -62.36452, policy_entropy: -0.41926, alpha: 0.26614, time: 32.89277
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 362.11142, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 2.95213, qf2_loss: 2.95469, policy_loss: -63.86608, policy_entropy: -0.42942, alpha: 0.26179, time: 33.08460
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 277.16361, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 3.05094, qf2_loss: 3.08879, policy_loss: -64.54044, policy_entropy: -0.43427, alpha: 0.25747, time: 32.81362
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 273.65397, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 2.94378, qf2_loss: 2.98087, policy_loss: -65.46031, policy_entropy: -0.47263, alpha: 0.25335, time: 33.05188
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 332.14510, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 2.70717, qf2_loss: 2.73343, policy_loss: -66.95346, policy_entropy: -0.46845, alpha: 0.24931, time: 32.98229
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 338.59812, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 2.91784, qf2_loss: 2.96474, policy_loss: -68.06795, policy_entropy: -0.50597, alpha: 0.24538, time: 33.08367
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 294.24301, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 3.16902, qf2_loss: 3.20386, policy_loss: -69.51101, policy_entropy: -0.52089, alpha: 0.24163, time: 32.93721
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 294.97254, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 3.66638, qf2_loss: 3.68810, policy_loss: -70.18454, policy_entropy: -0.53145, alpha: 0.23798, time: 33.07392
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 243.13676, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 3.00831, qf2_loss: 3.03156, policy_loss: -71.18288, policy_entropy: -0.57054, alpha: 0.23442, time: 33.22011
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 371.15808, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 3.23629, qf2_loss: 3.26736, policy_loss: -72.18754, policy_entropy: -0.57168, alpha: 0.23108, time: 33.15591
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 362.87140, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 3.36711, qf2_loss: 3.37982, policy_loss: -73.09171, policy_entropy: -0.60752, alpha: 0.22783, time: 32.98190
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 416.32590, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 3.25775, qf2_loss: 3.26929, policy_loss: -74.35142, policy_entropy: -0.60946, alpha: 0.22467, time: 32.98551
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 384.97686, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 3.52673, qf2_loss: 3.55528, policy_loss: -75.42936, policy_entropy: -0.62728, alpha: 0.22158, time: 33.18679
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 341.25815, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 3.33517, qf2_loss: 3.35773, policy_loss: -76.27176, policy_entropy: -0.63788, alpha: 0.21855, time: 33.07853
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 274.99236, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 3.18771, qf2_loss: 3.21834, policy_loss: -77.44699, policy_entropy: -0.65788, alpha: 0.21563, time: 33.09950
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 336.91665, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 3.12400, qf2_loss: 3.14313, policy_loss: -78.23689, policy_entropy: -0.68598, alpha: 0.21285, time: 33.04871
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 221.93896, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 3.56134, qf2_loss: 3.61555, policy_loss: -79.22567, policy_entropy: -0.69254, alpha: 0.21019, time: 33.29764
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 393.50235, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 3.15934, qf2_loss: 3.15563, policy_loss: -80.76660, policy_entropy: -0.70970, alpha: 0.20760, time: 33.05652
[CW] eval: return: 354.05456, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 360.52876, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 3.23642, qf2_loss: 3.25949, policy_loss: -81.49471, policy_entropy: -0.73136, alpha: 0.20511, time: 32.77570
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 288.61520, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 3.36960, qf2_loss: 3.40763, policy_loss: -82.48391, policy_entropy: -0.71953, alpha: 0.20268, time: 32.92404
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 418.39960, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 3.34102, qf2_loss: 3.35529, policy_loss: -83.55197, policy_entropy: -0.74929, alpha: 0.20024, time: 32.99169
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 355.74611, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 3.46637, qf2_loss: 3.47065, policy_loss: -84.55179, policy_entropy: -0.75324, alpha: 0.19790, time: 32.95858
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 379.26551, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 4.18171, qf2_loss: 4.21458, policy_loss: -85.74022, policy_entropy: -0.77105, alpha: 0.19561, time: 33.02232
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 354.15596, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 3.88392, qf2_loss: 3.91633, policy_loss: -86.45060, policy_entropy: -0.74718, alpha: 0.19332, time: 32.95883
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 408.78546, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 6.24062, qf2_loss: 6.33385, policy_loss: -88.05400, policy_entropy: -0.74121, alpha: 0.19084, time: 32.80000
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 382.54793, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 3.09480, qf2_loss: 3.11981, policy_loss: -88.78330, policy_entropy: -0.78255, alpha: 0.18843, time: 32.99571
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 304.30205, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 3.10519, qf2_loss: 3.11697, policy_loss: -90.16458, policy_entropy: -0.79690, alpha: 0.18629, time: 32.84380
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 393.28957, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 3.69945, qf2_loss: 3.74870, policy_loss: -91.11691, policy_entropy: -0.80306, alpha: 0.18418, time: 33.01229
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 360.84911, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 3.38880, qf2_loss: 3.42492, policy_loss: -92.31164, policy_entropy: -0.84259, alpha: 0.18231, time: 33.25996
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 394.40152, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 3.24671, qf2_loss: 3.26915, policy_loss: -93.32538, policy_entropy: -0.86832, alpha: 0.18069, time: 32.95056
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 360.60908, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 3.80221, qf2_loss: 3.83661, policy_loss: -94.27864, policy_entropy: -0.86957, alpha: 0.17927, time: 33.22684
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 372.28644, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 3.50023, qf2_loss: 3.52305, policy_loss: -96.00289, policy_entropy: -0.87954, alpha: 0.17778, time: 33.10010
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 359.92507, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 3.54564, qf2_loss: 3.57228, policy_loss: -96.75938, policy_entropy: -0.89104, alpha: 0.17646, time: 33.16843
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 406.69218, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 3.84524, qf2_loss: 3.85190, policy_loss: -97.63186, policy_entropy: -0.92070, alpha: 0.17525, time: 32.94086
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 374.59501, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 4.04008, qf2_loss: 4.07273, policy_loss: -99.17495, policy_entropy: -0.92667, alpha: 0.17430, time: 33.17116
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 380.04716, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 3.61668, qf2_loss: 3.63774, policy_loss: -100.02213, policy_entropy: -0.94058, alpha: 0.17340, time: 32.95076
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 374.17071, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 3.70680, qf2_loss: 3.72374, policy_loss: -100.99800, policy_entropy: -0.94854, alpha: 0.17263, time: 33.07238
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 407.03772, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 3.74618, qf2_loss: 3.74986, policy_loss: -102.18426, policy_entropy: -0.96280, alpha: 0.17200, time: 32.91462
[CW] eval: return: 395.60557, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 496.28847, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 4.73738, qf2_loss: 4.76845, policy_loss: -103.53638, policy_entropy: -0.99280, alpha: 0.17154, time: 32.81808
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 518.85615, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 3.73377, qf2_loss: 3.75432, policy_loss: -104.75082, policy_entropy: -1.00656, alpha: 0.17158, time: 32.90757
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 325.23789, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 3.83053, qf2_loss: 3.83851, policy_loss: -105.47925, policy_entropy: -1.01741, alpha: 0.17180, time: 33.17141
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 421.28790, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 3.84958, qf2_loss: 3.85161, policy_loss: -106.87714, policy_entropy: -1.02274, alpha: 0.17214, time: 32.94994
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 386.78068, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 4.38636, qf2_loss: 4.42333, policy_loss: -108.04896, policy_entropy: -1.01532, alpha: 0.17247, time: 32.99733
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 427.93336, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 4.19503, qf2_loss: 4.21488, policy_loss: -108.92516, policy_entropy: -1.03083, alpha: 0.17290, time: 32.95743
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 368.50175, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 4.17342, qf2_loss: 4.14300, policy_loss: -110.25406, policy_entropy: -1.03957, alpha: 0.17364, time: 32.79452
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 375.77252, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 4.28357, qf2_loss: 4.31546, policy_loss: -111.27835, policy_entropy: -1.03381, alpha: 0.17443, time: 33.08161
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 401.04689, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 4.30874, qf2_loss: 4.32366, policy_loss: -112.18012, policy_entropy: -1.04501, alpha: 0.17522, time: 33.21730
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 433.80514, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 4.11934, qf2_loss: 4.14948, policy_loss: -113.51134, policy_entropy: -1.03931, alpha: 0.17626, time: 32.84102
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 346.63898, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 4.31459, qf2_loss: 4.36129, policy_loss: -114.44402, policy_entropy: -1.03589, alpha: 0.17722, time: 32.99594
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 370.51042, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 4.94539, qf2_loss: 4.99162, policy_loss: -115.65412, policy_entropy: -1.02892, alpha: 0.17814, time: 33.31994
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 319.41136, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 4.03723, qf2_loss: 4.03805, policy_loss: -115.86751, policy_entropy: -1.04571, alpha: 0.17913, time: 33.13686
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 365.10256, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 3.95926, qf2_loss: 3.98985, policy_loss: -117.45138, policy_entropy: -1.03928, alpha: 0.18037, time: 33.24313
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 444.21614, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 4.14942, qf2_loss: 4.15720, policy_loss: -118.30229, policy_entropy: -1.03459, alpha: 0.18155, time: 33.01598
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 421.54078, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 4.57565, qf2_loss: 4.62550, policy_loss: -119.00254, policy_entropy: -1.02313, alpha: 0.18255, time: 33.06200
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 438.53814, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 4.30382, qf2_loss: 4.31902, policy_loss: -121.22537, policy_entropy: -1.02257, alpha: 0.18335, time: 33.03680
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 396.65172, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 4.56445, qf2_loss: 4.61052, policy_loss: -122.10273, policy_entropy: -1.03169, alpha: 0.18424, time: 32.86362
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 492.29636, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 4.38977, qf2_loss: 4.42366, policy_loss: -123.28464, policy_entropy: -1.01994, alpha: 0.18532, time: 33.11913
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 522.52189, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 4.58519, qf2_loss: 4.61568, policy_loss: -124.28210, policy_entropy: -1.02165, alpha: 0.18635, time: 32.83541
[CW] eval: return: 379.42955, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 360.23816, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 6.23795, qf2_loss: 6.29049, policy_loss: -125.32561, policy_entropy: -1.03223, alpha: 0.18741, time: 32.80088
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 467.82528, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 5.44262, qf2_loss: 5.47107, policy_loss: -127.01332, policy_entropy: -1.02744, alpha: 0.18839, time: 32.91593
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 346.39680, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 4.29693, qf2_loss: 4.32340, policy_loss: -127.34731, policy_entropy: -1.03421, alpha: 0.18999, time: 32.97858
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 439.38691, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 4.34778, qf2_loss: 4.34479, policy_loss: -128.32349, policy_entropy: -1.03087, alpha: 0.19169, time: 32.95874
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 361.42382, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 4.26015, qf2_loss: 4.27235, policy_loss: -129.79298, policy_entropy: -1.02776, alpha: 0.19328, time: 32.96364
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 508.61901, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 4.58330, qf2_loss: 4.62777, policy_loss: -130.63713, policy_entropy: -1.02683, alpha: 0.19455, time: 33.05164
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 392.37327, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 5.07968, qf2_loss: 5.08297, policy_loss: -132.03523, policy_entropy: -1.03114, alpha: 0.19627, time: 33.04936
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 422.73788, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 5.56674, qf2_loss: 5.62132, policy_loss: -132.98273, policy_entropy: -1.02264, alpha: 0.19820, time: 32.85335
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 429.43580, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 4.47870, qf2_loss: 4.50027, policy_loss: -133.27878, policy_entropy: -1.02662, alpha: 0.19924, time: 32.99382
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 441.94072, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 4.20889, qf2_loss: 4.25027, policy_loss: -134.80569, policy_entropy: -1.02359, alpha: 0.20113, time: 32.82231
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 518.25453, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 4.51002, qf2_loss: 4.54661, policy_loss: -135.99574, policy_entropy: -1.01248, alpha: 0.20242, time: 33.04442
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 444.60923, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 20.40264, qf2_loss: 20.42238, policy_loss: -136.64491, policy_entropy: -0.98625, alpha: 0.20269, time: 33.02957
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 415.17444, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 6.23285, qf2_loss: 6.22637, policy_loss: -138.85955, policy_entropy: -1.01372, alpha: 0.20214, time: 33.24693
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 450.01666, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 4.73000, qf2_loss: 4.79613, policy_loss: -139.42398, policy_entropy: -1.03903, alpha: 0.20407, time: 32.96434
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 438.13687, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 4.77035, qf2_loss: 4.79230, policy_loss: -140.82790, policy_entropy: -1.03625, alpha: 0.20679, time: 33.04789
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 454.98558, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 4.62683, qf2_loss: 4.68631, policy_loss: -141.41197, policy_entropy: -1.04165, alpha: 0.21007, time: 33.09078
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 534.24370, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 4.48678, qf2_loss: 4.49426, policy_loss: -143.02378, policy_entropy: -1.04593, alpha: 0.21331, time: 32.88459
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 468.19855, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 4.55889, qf2_loss: 4.54065, policy_loss: -144.27432, policy_entropy: -1.04849, alpha: 0.21750, time: 33.12466
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 343.93045, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 4.35722, qf2_loss: 4.37356, policy_loss: -145.10474, policy_entropy: -1.03094, alpha: 0.22137, time: 33.03886
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 481.74851, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 4.51232, qf2_loss: 4.54107, policy_loss: -146.34038, policy_entropy: -1.03025, alpha: 0.22408, time: 33.02688
[CW] eval: return: 445.06123, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 447.19770, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 4.58237, qf2_loss: 4.62425, policy_loss: -147.67256, policy_entropy: -1.00718, alpha: 0.22613, time: 33.11254
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 523.48116, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 5.11776, qf2_loss: 5.11838, policy_loss: -148.30786, policy_entropy: -1.03157, alpha: 0.22743, time: 32.70574
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 432.99941, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 5.15823, qf2_loss: 5.16579, policy_loss: -149.59245, policy_entropy: -1.02133, alpha: 0.23026, time: 33.17996
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 447.80255, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 5.03083, qf2_loss: 5.06921, policy_loss: -150.68307, policy_entropy: -1.03645, alpha: 0.23324, time: 32.93456
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 527.41860, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 4.72370, qf2_loss: 4.74304, policy_loss: -151.85567, policy_entropy: -1.01996, alpha: 0.23639, time: 33.37932
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 446.50553, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 5.04809, qf2_loss: 5.07690, policy_loss: -152.36421, policy_entropy: -1.03489, alpha: 0.23925, time: 32.81511
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 459.00238, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 5.75705, qf2_loss: 5.81859, policy_loss: -153.80609, policy_entropy: -1.02911, alpha: 0.24207, time: 33.19088
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 472.69208, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 5.46103, qf2_loss: 5.48383, policy_loss: -154.73826, policy_entropy: -1.01796, alpha: 0.24504, time: 33.38538
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 421.53420, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 6.45979, qf2_loss: 6.49774, policy_loss: -155.79258, policy_entropy: -1.02066, alpha: 0.24732, time: 33.22148
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 484.03133, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 5.69393, qf2_loss: 5.74019, policy_loss: -157.26424, policy_entropy: -1.02364, alpha: 0.25004, time: 33.14765
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 547.11184, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 5.29631, qf2_loss: 5.30055, policy_loss: -157.28517, policy_entropy: -1.02517, alpha: 0.25272, time: 32.95045
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 457.87812, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 5.49954, qf2_loss: 5.44840, policy_loss: -159.26296, policy_entropy: -1.01224, alpha: 0.25490, time: 33.02305
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 458.55549, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 5.98714, qf2_loss: 6.02875, policy_loss: -160.50165, policy_entropy: -1.01836, alpha: 0.25686, time: 33.11274
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 531.98755, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 5.64892, qf2_loss: 5.65173, policy_loss: -160.90535, policy_entropy: -1.01816, alpha: 0.25955, time: 32.82063
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 468.07351, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 5.18815, qf2_loss: 5.20846, policy_loss: -162.34849, policy_entropy: -1.02314, alpha: 0.26233, time: 33.09403
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 465.64159, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 5.47710, qf2_loss: 5.50463, policy_loss: -163.50978, policy_entropy: -1.01407, alpha: 0.26398, time: 33.01174
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 445.52567, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 5.55532, qf2_loss: 5.57296, policy_loss: -164.32769, policy_entropy: -1.01057, alpha: 0.26600, time: 33.01123
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 467.86617, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 5.26593, qf2_loss: 5.26291, policy_loss: -165.81321, policy_entropy: -1.01271, alpha: 0.26750, time: 33.20562
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 528.67524, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 5.71254, qf2_loss: 5.74176, policy_loss: -167.16621, policy_entropy: -1.00621, alpha: 0.26891, time: 33.03281
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 500.62862, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 5.92030, qf2_loss: 5.97177, policy_loss: -168.66048, policy_entropy: -1.01615, alpha: 0.26990, time: 32.87725
[CW] eval: return: 488.62986, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 438.10826, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 5.66208, qf2_loss: 5.70205, policy_loss: -169.20746, policy_entropy: -1.00485, alpha: 0.27177, time: 33.25849
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 492.40208, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 5.41763, qf2_loss: 5.42690, policy_loss: -170.12667, policy_entropy: -1.01916, alpha: 0.27281, time: 32.83038
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 425.99008, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 6.70841, qf2_loss: 6.73832, policy_loss: -170.87537, policy_entropy: -0.99995, alpha: 0.27461, time: 32.69822
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 376.21094, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 6.53527, qf2_loss: 6.56685, policy_loss: -172.34908, policy_entropy: -0.98936, alpha: 0.27397, time: 33.18114
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 554.69519, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 6.36946, qf2_loss: 6.36024, policy_loss: -172.80461, policy_entropy: -1.00785, alpha: 0.27368, time: 33.15041
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 614.97725, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 6.56578, qf2_loss: 6.57192, policy_loss: -173.76737, policy_entropy: -1.02077, alpha: 0.27523, time: 33.06874
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 475.14319, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 5.92243, qf2_loss: 5.94827, policy_loss: -175.24264, policy_entropy: -1.03127, alpha: 0.27927, time: 33.19324
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 483.08338, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 6.73223, qf2_loss: 6.77739, policy_loss: -176.35413, policy_entropy: -1.00352, alpha: 0.28142, time: 33.08893
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 544.94740, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 8.14382, qf2_loss: 8.13991, policy_loss: -177.52076, policy_entropy: -0.99642, alpha: 0.28148, time: 32.87081
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 543.94432, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 7.02576, qf2_loss: 7.03519, policy_loss: -179.16098, policy_entropy: -1.00477, alpha: 0.28190, time: 33.01507
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 445.11733, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 6.56313, qf2_loss: 6.63576, policy_loss: -179.77171, policy_entropy: -1.00315, alpha: 0.28230, time: 32.98627
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 568.94381, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 6.72514, qf2_loss: 6.75455, policy_loss: -180.88364, policy_entropy: -1.01244, alpha: 0.28375, time: 32.92013
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 590.42506, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 6.17761, qf2_loss: 6.26399, policy_loss: -180.90243, policy_entropy: -1.00423, alpha: 0.28486, time: 32.93855
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 456.77240, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 6.50234, qf2_loss: 6.50800, policy_loss: -183.01887, policy_entropy: -1.00632, alpha: 0.28600, time: 32.98025
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 531.00726, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 6.94144, qf2_loss: 6.98887, policy_loss: -184.34889, policy_entropy: -1.01098, alpha: 0.28687, time: 32.98949
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 537.22747, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 6.83934, qf2_loss: 6.90311, policy_loss: -185.50556, policy_entropy: -1.00480, alpha: 0.28790, time: 33.21353
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 467.27408, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 7.18431, qf2_loss: 7.25764, policy_loss: -186.25445, policy_entropy: -1.00942, alpha: 0.28864, time: 33.19632
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 527.38488, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 8.46354, qf2_loss: 8.46176, policy_loss: -186.94615, policy_entropy: -0.99987, alpha: 0.29074, time: 33.15907
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 519.87279, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 7.55212, qf2_loss: 7.60186, policy_loss: -188.02892, policy_entropy: -1.00232, alpha: 0.29037, time: 33.18766
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 582.11086, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 6.68688, qf2_loss: 6.75554, policy_loss: -189.70789, policy_entropy: -1.01850, alpha: 0.29197, time: 33.05198
[CW] eval: return: 539.15866, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 447.66575, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 7.73769, qf2_loss: 7.73288, policy_loss: -190.94695, policy_entropy: -1.02383, alpha: 0.29547, time: 32.98410
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 467.79215, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 6.91594, qf2_loss: 6.95606, policy_loss: -190.73509, policy_entropy: -1.02029, alpha: 0.29892, time: 32.90428
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 563.90671, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 7.37595, qf2_loss: 7.36980, policy_loss: -192.65742, policy_entropy: -1.00503, alpha: 0.30181, time: 32.93247
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 554.98707, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 7.86942, qf2_loss: 7.86887, policy_loss: -193.49562, policy_entropy: -1.01086, alpha: 0.30289, time: 33.02724
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 416.89598, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 7.46897, qf2_loss: 7.51936, policy_loss: -194.87036, policy_entropy: -1.01491, alpha: 0.30519, time: 33.01411
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 465.25824, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 8.62435, qf2_loss: 8.64149, policy_loss: -195.85808, policy_entropy: -1.01965, alpha: 0.30855, time: 32.88471
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 522.29986, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 6.98334, qf2_loss: 7.02676, policy_loss: -196.68340, policy_entropy: -1.02298, alpha: 0.31230, time: 33.05618
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 441.25226, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 7.35769, qf2_loss: 7.36432, policy_loss: -197.14229, policy_entropy: -1.00521, alpha: 0.31487, time: 33.26723
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 464.08398, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 7.76155, qf2_loss: 7.79639, policy_loss: -199.88843, policy_entropy: -1.01277, alpha: 0.31682, time: 33.08297
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 513.62366, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 10.25154, qf2_loss: 10.23877, policy_loss: -200.54552, policy_entropy: -1.00764, alpha: 0.31833, time: 33.11003
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 453.36216, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 9.02065, qf2_loss: 8.99735, policy_loss: -200.81153, policy_entropy: -1.00489, alpha: 0.32039, time: 32.89857
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 482.47048, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 9.44032, qf2_loss: 9.51775, policy_loss: -201.88797, policy_entropy: -1.00851, alpha: 0.32169, time: 32.92108
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 492.64107, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 8.38218, qf2_loss: 8.43461, policy_loss: -203.37967, policy_entropy: -1.01396, alpha: 0.32339, time: 33.19492
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 532.96117, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 8.53196, qf2_loss: 8.51969, policy_loss: -204.11269, policy_entropy: -1.01318, alpha: 0.32582, time: 33.02876
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 545.02044, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 8.17312, qf2_loss: 8.20685, policy_loss: -205.26043, policy_entropy: -1.00065, alpha: 0.32795, time: 33.19142
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 546.55289, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 8.44313, qf2_loss: 8.44834, policy_loss: -206.21339, policy_entropy: -1.01647, alpha: 0.33028, time: 32.96066
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 471.88226, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 8.79108, qf2_loss: 8.75386, policy_loss: -206.41238, policy_entropy: -1.00796, alpha: 0.33293, time: 32.97773
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 495.94471, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 8.87916, qf2_loss: 8.93722, policy_loss: -208.52836, policy_entropy: -0.99299, alpha: 0.33279, time: 33.32179
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 502.99461, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 8.61436, qf2_loss: 8.61219, policy_loss: -209.85496, policy_entropy: -0.99519, alpha: 0.33178, time: 33.08862
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 475.02724, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 9.26368, qf2_loss: 9.33914, policy_loss: -209.42280, policy_entropy: -1.00295, alpha: 0.33098, time: 33.16827
[CW] eval: return: 517.13409, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 545.99944, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 11.31311, qf2_loss: 11.38091, policy_loss: -211.09402, policy_entropy: -0.99213, alpha: 0.33067, time: 32.83717
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 475.99120, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 9.31976, qf2_loss: 9.28611, policy_loss: -212.33274, policy_entropy: -1.00005, alpha: 0.33052, time: 32.91275
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 542.94293, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 9.59079, qf2_loss: 9.64879, policy_loss: -212.93749, policy_entropy: -1.01382, alpha: 0.33064, time: 32.97837
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 586.99580, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 7.85533, qf2_loss: 7.89485, policy_loss: -214.24953, policy_entropy: -1.01654, alpha: 0.33397, time: 32.96885
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 553.07154, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 8.86976, qf2_loss: 8.91090, policy_loss: -216.13624, policy_entropy: -1.01130, alpha: 0.33765, time: 32.96377
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 584.63645, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 9.26429, qf2_loss: 9.30826, policy_loss: -217.05256, policy_entropy: -1.00763, alpha: 0.33966, time: 33.02257
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 545.76461, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 8.43608, qf2_loss: 8.51619, policy_loss: -218.28682, policy_entropy: -1.00886, alpha: 0.34167, time: 32.84284
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 618.13402, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 8.52689, qf2_loss: 8.56864, policy_loss: -219.77656, policy_entropy: -0.99270, alpha: 0.34167, time: 32.81465
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 554.54460, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 9.35731, qf2_loss: 9.36773, policy_loss: -219.16746, policy_entropy: -1.00511, alpha: 0.34130, time: 32.93945
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 623.78757, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 9.37907, qf2_loss: 9.43729, policy_loss: -221.31025, policy_entropy: -1.00568, alpha: 0.34278, time: 32.99028
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 428.80493, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 9.25442, qf2_loss: 9.25474, policy_loss: -223.08162, policy_entropy: -1.01012, alpha: 0.34497, time: 32.83446
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 548.14696, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 8.26821, qf2_loss: 8.29893, policy_loss: -222.73876, policy_entropy: -1.00314, alpha: 0.34615, time: 32.85737
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 522.89720, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 8.45404, qf2_loss: 8.41393, policy_loss: -223.94419, policy_entropy: -1.00162, alpha: 0.34649, time: 32.83392
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 546.75677, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 9.63634, qf2_loss: 9.71100, policy_loss: -225.42888, policy_entropy: -1.00587, alpha: 0.34737, time: 32.89212
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 531.53944, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 10.05404, qf2_loss: 10.10447, policy_loss: -225.88596, policy_entropy: -0.99795, alpha: 0.34781, time: 32.99320
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 551.34419, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 9.87021, qf2_loss: 9.90845, policy_loss: -226.79614, policy_entropy: -1.01341, alpha: 0.34932, time: 32.91142
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 556.61215, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 8.51048, qf2_loss: 8.46982, policy_loss: -228.11187, policy_entropy: -0.99474, alpha: 0.35159, time: 32.86228
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 546.61787, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 9.85364, qf2_loss: 9.94447, policy_loss: -229.55770, policy_entropy: -1.01146, alpha: 0.35103, time: 33.07053
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 558.46608, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 12.04951, qf2_loss: 12.06044, policy_loss: -230.41216, policy_entropy: -0.97694, alpha: 0.35017, time: 32.79876
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 627.06770, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 10.82266, qf2_loss: 10.80126, policy_loss: -231.96990, policy_entropy: -0.99751, alpha: 0.34783, time: 32.82017
[CW] eval: return: 514.87574, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 540.44299, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 12.35764, qf2_loss: 12.51576, policy_loss: -232.48588, policy_entropy: -1.00537, alpha: 0.34770, time: 32.69025
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 545.36178, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 8.28360, qf2_loss: 8.24698, policy_loss: -233.35149, policy_entropy: -1.00540, alpha: 0.34870, time: 32.77864
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 581.69070, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 7.85803, qf2_loss: 7.87478, policy_loss: -234.84820, policy_entropy: -1.01988, alpha: 0.35196, time: 33.88202
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 550.17227, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 7.77739, qf2_loss: 7.83706, policy_loss: -236.16916, policy_entropy: -1.01574, alpha: 0.35554, time: 34.15035
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 548.14415, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 8.77598, qf2_loss: 8.82112, policy_loss: -236.93673, policy_entropy: -1.01019, alpha: 0.35899, time: 32.88239
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 528.57837, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 8.84942, qf2_loss: 8.88089, policy_loss: -237.79676, policy_entropy: -0.99659, alpha: 0.35894, time: 34.35939
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 537.29114, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 8.78550, qf2_loss: 8.77399, policy_loss: -238.55083, policy_entropy: -1.00451, alpha: 0.35933, time: 32.85048
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 555.80779, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 8.68799, qf2_loss: 8.82870, policy_loss: -239.78446, policy_entropy: -0.99524, alpha: 0.36003, time: 33.08145
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 647.57595, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 8.87866, qf2_loss: 8.91062, policy_loss: -241.29728, policy_entropy: -0.99938, alpha: 0.35953, time: 32.85857
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 581.10630, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 11.29655, qf2_loss: 11.31895, policy_loss: -240.94992, policy_entropy: -1.00354, alpha: 0.35871, time: 32.91776
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 542.06608, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 9.60506, qf2_loss: 9.71410, policy_loss: -243.13823, policy_entropy: -0.99499, alpha: 0.35933, time: 32.95798
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 552.83301, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 9.07142, qf2_loss: 9.06726, policy_loss: -242.34951, policy_entropy: -1.01664, alpha: 0.36122, time: 34.39874
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 548.85110, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 8.96966, qf2_loss: 8.99526, policy_loss: -244.93034, policy_entropy: -1.01240, alpha: 0.36451, time: 32.98824
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 547.56828, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 10.10387, qf2_loss: 10.20245, policy_loss: -245.43326, policy_entropy: -0.99910, alpha: 0.36653, time: 33.02255
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 620.32940, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 9.57842, qf2_loss: 9.57012, policy_loss: -246.99503, policy_entropy: -0.99979, alpha: 0.36546, time: 33.12171
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 549.80289, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 10.03389, qf2_loss: 10.02124, policy_loss: -248.35267, policy_entropy: -1.00710, alpha: 0.36606, time: 32.87435
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 569.02362, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 10.81803, qf2_loss: 10.82124, policy_loss: -247.35386, policy_entropy: -1.00665, alpha: 0.36870, time: 32.96632
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 473.76632, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 10.88561, qf2_loss: 10.97022, policy_loss: -249.15387, policy_entropy: -1.00718, alpha: 0.36890, time: 32.82924
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 589.38012, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 9.06790, qf2_loss: 9.14392, policy_loss: -249.89333, policy_entropy: -1.00758, alpha: 0.37090, time: 33.07202
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 614.64055, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 8.38301, qf2_loss: 8.40964, policy_loss: -251.29696, policy_entropy: -1.01812, alpha: 0.37441, time: 32.85081
[CW] eval: return: 576.81782, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 551.86142, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 10.61496, qf2_loss: 10.57965, policy_loss: -252.19642, policy_entropy: -1.00331, alpha: 0.37684, time: 32.90519
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 610.96953, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 12.92830, qf2_loss: 13.01568, policy_loss: -253.94691, policy_entropy: -0.99883, alpha: 0.37717, time: 33.10335
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 548.62682, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 9.58872, qf2_loss: 9.64349, policy_loss: -254.49704, policy_entropy: -0.99740, alpha: 0.37709, time: 32.79735
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 557.02605, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 9.21806, qf2_loss: 9.23747, policy_loss: -256.29200, policy_entropy: -1.00794, alpha: 0.37718, time: 32.85517
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 602.78915, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 10.23633, qf2_loss: 10.30701, policy_loss: -256.50013, policy_entropy: -1.00649, alpha: 0.37921, time: 33.09083
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 617.63964, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 9.98489, qf2_loss: 10.02887, policy_loss: -257.70089, policy_entropy: -1.00700, alpha: 0.38109, time: 33.11496
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 583.81661, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 9.93261, qf2_loss: 10.02984, policy_loss: -258.20573, policy_entropy: -0.99851, alpha: 0.38259, time: 33.16932
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 552.69755, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 12.74819, qf2_loss: 12.80080, policy_loss: -259.33362, policy_entropy: -0.99956, alpha: 0.38244, time: 32.97350
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 541.28410, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 10.52034, qf2_loss: 10.48531, policy_loss: -260.46926, policy_entropy: -1.01271, alpha: 0.38240, time: 32.75137
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 619.97862, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 9.91469, qf2_loss: 9.99208, policy_loss: -261.16193, policy_entropy: -1.00052, alpha: 0.38497, time: 32.85106
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 697.46626, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 10.35926, qf2_loss: 10.45314, policy_loss: -263.26604, policy_entropy: -0.99452, alpha: 0.38442, time: 32.72224
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 532.14604, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 10.23660, qf2_loss: 10.37929, policy_loss: -263.68176, policy_entropy: -1.00133, alpha: 0.38381, time: 32.87854
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 550.10676, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 10.89097, qf2_loss: 10.97852, policy_loss: -263.88175, policy_entropy: -1.00553, alpha: 0.38422, time: 32.79437
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 613.85876, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 12.16989, qf2_loss: 12.23462, policy_loss: -265.39271, policy_entropy: -0.99641, alpha: 0.38530, time: 32.87507
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 622.91374, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 9.91620, qf2_loss: 10.01380, policy_loss: -267.03740, policy_entropy: -1.00926, alpha: 0.38467, time: 32.86314
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 597.89140, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 9.56956, qf2_loss: 9.67274, policy_loss: -265.91880, policy_entropy: -1.01053, alpha: 0.38730, time: 32.81032
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 540.38643, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 10.37459, qf2_loss: 10.38011, policy_loss: -268.76031, policy_entropy: -1.00309, alpha: 0.38993, time: 32.85000
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 620.94660, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 11.83965, qf2_loss: 11.92325, policy_loss: -270.03819, policy_entropy: -1.00650, alpha: 0.39132, time: 32.84504
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 615.08153, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 12.05995, qf2_loss: 12.09932, policy_loss: -270.59338, policy_entropy: -0.99671, alpha: 0.39160, time: 32.76756
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 688.94219, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 9.57954, qf2_loss: 9.61422, policy_loss: -270.60214, policy_entropy: -1.00957, alpha: 0.39216, time: 32.84549
[CW] eval: return: 622.22405, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 574.29460, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 10.12636, qf2_loss: 10.11850, policy_loss: -272.17457, policy_entropy: -1.01579, alpha: 0.39568, time: 32.94571
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 593.02996, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 10.58889, qf2_loss: 10.57262, policy_loss: -273.35898, policy_entropy: -1.00776, alpha: 0.39770, time: 32.59743
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 631.93929, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 12.29474, qf2_loss: 12.38243, policy_loss: -274.21361, policy_entropy: -0.99865, alpha: 0.39908, time: 32.69080
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 560.18282, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 12.11117, qf2_loss: 12.17422, policy_loss: -274.63464, policy_entropy: -0.99927, alpha: 0.39820, time: 32.75053
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 620.36136, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 11.57272, qf2_loss: 11.56497, policy_loss: -274.62973, policy_entropy: -1.00540, alpha: 0.39906, time: 32.68000
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 691.22165, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 10.67918, qf2_loss: 10.65119, policy_loss: -276.47141, policy_entropy: -1.01013, alpha: 0.40162, time: 32.74194
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 542.35511, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 9.74284, qf2_loss: 9.88446, policy_loss: -278.32094, policy_entropy: -1.00529, alpha: 0.40385, time: 32.74874
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 691.97993, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 10.11419, qf2_loss: 10.14870, policy_loss: -279.37948, policy_entropy: -1.00281, alpha: 0.40596, time: 32.65557
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 668.93395, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 12.34654, qf2_loss: 12.41871, policy_loss: -279.25146, policy_entropy: -1.00643, alpha: 0.40679, time: 32.86981
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 548.09891, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 14.35020, qf2_loss: 14.47076, policy_loss: -279.15902, policy_entropy: -0.99037, alpha: 0.40635, time: 32.72902
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 618.55551, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 11.72434, qf2_loss: 11.79685, policy_loss: -281.18942, policy_entropy: -1.00282, alpha: 0.40441, time: 32.93365
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 619.59797, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 10.05023, qf2_loss: 10.21260, policy_loss: -281.41228, policy_entropy: -1.00585, alpha: 0.40550, time: 33.40358
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 691.93812, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 11.54867, qf2_loss: 11.65236, policy_loss: -282.64495, policy_entropy: -1.00965, alpha: 0.40868, time: 32.97153
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 600.27570, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 11.84653, qf2_loss: 11.99438, policy_loss: -283.80378, policy_entropy: -1.00929, alpha: 0.40993, time: 32.55692
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 738.77309, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 10.55013, qf2_loss: 10.69489, policy_loss: -283.35331, policy_entropy: -1.01104, alpha: 0.41287, time: 32.72283
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 610.37600, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 10.08777, qf2_loss: 10.21123, policy_loss: -285.84922, policy_entropy: -1.01129, alpha: 0.41677, time: 32.67904
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 643.61059, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 10.44300, qf2_loss: 10.42928, policy_loss: -288.21255, policy_entropy: -1.00826, alpha: 0.41913, time: 32.76310
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 691.74795, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 11.85887, qf2_loss: 11.92069, policy_loss: -287.56144, policy_entropy: -1.00637, alpha: 0.42038, time: 32.70737
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 840.47400, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 11.50416, qf2_loss: 11.55646, policy_loss: -289.37411, policy_entropy: -0.99743, alpha: 0.42214, time: 32.69128
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 603.75888, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 12.02304, qf2_loss: 12.16096, policy_loss: -289.61930, policy_entropy: -1.00350, alpha: 0.42145, time: 32.68068
[CW] eval: return: 645.41717, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 694.55383, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 10.98169, qf2_loss: 11.03154, policy_loss: -290.63478, policy_entropy: -1.00635, alpha: 0.42313, time: 32.54439
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 693.53942, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 10.60697, qf2_loss: 10.65317, policy_loss: -292.27668, policy_entropy: -1.01358, alpha: 0.42623, time: 32.79863
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 596.19316, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 11.46724, qf2_loss: 11.57881, policy_loss: -292.76673, policy_entropy: -1.00445, alpha: 0.42862, time: 32.36244
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 736.62467, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 12.25853, qf2_loss: 12.23003, policy_loss: -292.35560, policy_entropy: -1.00714, alpha: 0.43027, time: 32.64646
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 675.60774, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 11.61821, qf2_loss: 11.78879, policy_loss: -293.16282, policy_entropy: -1.00828, alpha: 0.43215, time: 32.83119
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 614.22766, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 11.06258, qf2_loss: 11.06231, policy_loss: -294.82390, policy_entropy: -1.01467, alpha: 0.43577, time: 32.66023
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 745.03037, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 11.34829, qf2_loss: 11.49127, policy_loss: -295.12814, policy_entropy: -1.00914, alpha: 0.43964, time: 32.80812
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 845.33118, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 12.07445, qf2_loss: 12.07301, policy_loss: -297.01404, policy_entropy: -0.99713, alpha: 0.44105, time: 32.78927
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 607.54377, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 12.55238, qf2_loss: 12.62720, policy_loss: -297.46964, policy_entropy: -0.99641, alpha: 0.43826, time: 32.79040
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 581.04920, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 15.04191, qf2_loss: 15.18859, policy_loss: -298.87611, policy_entropy: -0.99343, alpha: 0.43834, time: 32.79212
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 736.73419, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 10.94420, qf2_loss: 10.98905, policy_loss: -299.16462, policy_entropy: -1.01419, alpha: 0.43801, time: 32.76827
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 611.95792, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 10.83265, qf2_loss: 10.81399, policy_loss: -300.64556, policy_entropy: -1.00297, alpha: 0.44029, time: 32.75101
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 851.90285, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 12.10721, qf2_loss: 12.01367, policy_loss: -301.04230, policy_entropy: -0.99348, alpha: 0.44095, time: 32.92543
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 684.37264, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 12.44750, qf2_loss: 12.60805, policy_loss: -302.98324, policy_entropy: -1.00681, alpha: 0.44072, time: 32.59831
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 614.44437, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 11.58116, qf2_loss: 11.63726, policy_loss: -303.66419, policy_entropy: -1.00955, alpha: 0.44254, time: 32.57899
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 842.10469, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 12.16758, qf2_loss: 12.24172, policy_loss: -304.47807, policy_entropy: -1.00824, alpha: 0.44583, time: 32.52427
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 660.97825, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 16.08930, qf2_loss: 16.14515, policy_loss: -305.42268, policy_entropy: -0.99738, alpha: 0.44737, time: 32.74573
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 753.02817, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 11.59895, qf2_loss: 11.74980, policy_loss: -306.30212, policy_entropy: -0.99525, alpha: 0.44617, time: 32.56563
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 766.34863, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 13.26622, qf2_loss: 13.35935, policy_loss: -307.35008, policy_entropy: -1.01240, alpha: 0.44593, time: 32.79135
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 690.51020, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 14.05899, qf2_loss: 14.07230, policy_loss: -308.53954, policy_entropy: -1.00063, alpha: 0.44920, time: 32.60432
[CW] eval: return: 679.40248, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 768.42027, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 12.12609, qf2_loss: 12.13970, policy_loss: -309.21640, policy_entropy: -1.00621, alpha: 0.44928, time: 32.63439
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 852.78883, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 12.06278, qf2_loss: 12.03046, policy_loss: -311.50653, policy_entropy: -1.01393, alpha: 0.45324, time: 32.75662
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 751.30887, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 13.03497, qf2_loss: 13.13124, policy_loss: -310.61588, policy_entropy: -0.99805, alpha: 0.45535, time: 32.44612
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 770.97721, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 12.11367, qf2_loss: 12.01345, policy_loss: -311.59891, policy_entropy: -1.02138, alpha: 0.45683, time: 32.75136
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 604.88789, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 14.89271, qf2_loss: 14.91860, policy_loss: -314.01222, policy_entropy: -1.00158, alpha: 0.46099, time: 32.45385
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 748.01253, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 14.68884, qf2_loss: 14.78257, policy_loss: -313.45775, policy_entropy: -0.99673, alpha: 0.46055, time: 32.90287
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 850.21710, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 12.77902, qf2_loss: 12.85607, policy_loss: -314.56973, policy_entropy: -1.00210, alpha: 0.45999, time: 32.61450
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 753.66547, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 13.24090, qf2_loss: 13.35591, policy_loss: -316.23530, policy_entropy: -0.99759, alpha: 0.45994, time: 32.67200
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 847.10332, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 12.49614, qf2_loss: 12.56486, policy_loss: -316.08768, policy_entropy: -1.00496, alpha: 0.46050, time: 32.65697
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 680.67457, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 13.68864, qf2_loss: 13.75209, policy_loss: -317.78002, policy_entropy: -1.00679, alpha: 0.46319, time: 32.65980
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 600.47993, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 12.82406, qf2_loss: 12.89281, policy_loss: -318.08339, policy_entropy: -1.00390, alpha: 0.46582, time: 32.73451
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 764.75897, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 13.24799, qf2_loss: 13.29805, policy_loss: -322.31332, policy_entropy: -1.00279, alpha: 0.46611, time: 32.67643
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 851.56166, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 13.02843, qf2_loss: 13.07678, policy_loss: -320.67937, policy_entropy: -1.01275, alpha: 0.46791, time: 32.69382
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 856.81746, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 13.04784, qf2_loss: 13.12697, policy_loss: -322.82464, policy_entropy: -1.01165, alpha: 0.47229, time: 32.82827
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 855.78888, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 21.67735, qf2_loss: 21.60156, policy_loss: -323.59740, policy_entropy: -0.98628, alpha: 0.47195, time: 32.80513
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 677.79217, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 14.25600, qf2_loss: 14.38068, policy_loss: -323.75122, policy_entropy: -1.00327, alpha: 0.46900, time: 32.63949
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 853.22214, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 12.25198, qf2_loss: 12.35684, policy_loss: -325.35739, policy_entropy: -1.01316, alpha: 0.47084, time: 32.90575
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 850.90331, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 12.82727, qf2_loss: 12.80128, policy_loss: -326.49665, policy_entropy: -1.00531, alpha: 0.47519, time: 32.66313
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 849.73491, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 14.25133, qf2_loss: 14.38698, policy_loss: -328.20474, policy_entropy: -1.01407, alpha: 0.47869, time: 32.74318
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 847.32433, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 14.38361, qf2_loss: 14.50705, policy_loss: -328.29909, policy_entropy: -0.99945, alpha: 0.47957, time: 32.82458
[CW] eval: return: 803.13955, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 846.78246, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 16.53506, qf2_loss: 16.64333, policy_loss: -329.81185, policy_entropy: -1.01609, alpha: 0.48160, time: 32.64991
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 766.25327, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 18.78004, qf2_loss: 18.68391, policy_loss: -329.18437, policy_entropy: -0.99840, alpha: 0.48667, time: 32.69228
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 855.37374, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 15.86647, qf2_loss: 15.86860, policy_loss: -330.52042, policy_entropy: -1.00913, alpha: 0.48622, time: 32.69048
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 759.55663, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 12.42263, qf2_loss: 12.48548, policy_loss: -332.59225, policy_entropy: -1.01643, alpha: 0.49132, time: 32.59411
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 850.92111, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 13.46169, qf2_loss: 13.62665, policy_loss: -334.84824, policy_entropy: -1.00400, alpha: 0.49556, time: 32.59312
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 853.61907, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 20.40218, qf2_loss: 20.49990, policy_loss: -334.36317, policy_entropy: -1.00008, alpha: 0.49853, time: 32.98571
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 614.79797, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 16.14862, qf2_loss: 16.33085, policy_loss: -335.52942, policy_entropy: -0.99346, alpha: 0.49346, time: 32.61291
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 839.34109, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 13.51623, qf2_loss: 13.56634, policy_loss: -336.12408, policy_entropy: -1.01935, alpha: 0.49566, time: 32.71709
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 545.00997, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 13.87453, qf2_loss: 13.89016, policy_loss: -337.26028, policy_entropy: -0.99868, alpha: 0.49986, time: 32.65830
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 648.96436, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 15.52738, qf2_loss: 15.46382, policy_loss: -339.17278, policy_entropy: -1.00799, alpha: 0.50011, time: 32.62788
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 849.74498, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 18.85673, qf2_loss: 18.82466, policy_loss: -339.53440, policy_entropy: -0.99872, alpha: 0.50311, time: 32.48972
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 829.78106, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 25.57812, qf2_loss: 25.77684, policy_loss: -339.93871, policy_entropy: -0.98318, alpha: 0.49953, time: 32.53113
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 853.27891, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 15.39020, qf2_loss: 15.45028, policy_loss: -342.44511, policy_entropy: -1.00167, alpha: 0.49686, time: 32.80694
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 762.08660, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 15.15020, qf2_loss: 15.12791, policy_loss: -343.00582, policy_entropy: -1.01651, alpha: 0.50002, time: 32.50365
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 854.35499, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 14.89478, qf2_loss: 15.01275, policy_loss: -344.63657, policy_entropy: -1.00961, alpha: 0.50375, time: 32.79218
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 854.56722, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 15.97149, qf2_loss: 15.92640, policy_loss: -345.88912, policy_entropy: -1.00622, alpha: 0.50625, time: 32.72585
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 847.13048, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 15.23746, qf2_loss: 15.26009, policy_loss: -346.90950, policy_entropy: -1.00978, alpha: 0.50979, time: 32.54287
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 854.68966, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 15.90496, qf2_loss: 15.90263, policy_loss: -348.49016, policy_entropy: -1.00037, alpha: 0.51176, time: 36.00946
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 845.71864, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 17.70033, qf2_loss: 17.67766, policy_loss: -348.30841, policy_entropy: -0.99681, alpha: 0.51066, time: 32.65494
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 855.90181, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 16.16114, qf2_loss: 16.13847, policy_loss: -350.64420, policy_entropy: -1.01039, alpha: 0.51110, time: 32.65864
[CW] eval: return: 833.55406, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 853.59631, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 19.73786, qf2_loss: 19.69385, policy_loss: -352.39475, policy_entropy: -0.99466, alpha: 0.51413, time: 32.81074
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 846.23854, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 18.76692, qf2_loss: 18.79377, policy_loss: -353.62217, policy_entropy: -1.00827, alpha: 0.51395, time: 32.78924
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 855.60767, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 16.08086, qf2_loss: 16.04485, policy_loss: -352.70267, policy_entropy: -1.00729, alpha: 0.51651, time: 32.78960
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 846.25929, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 17.43502, qf2_loss: 17.25259, policy_loss: -356.15072, policy_entropy: -1.00908, alpha: 0.51921, time: 32.60731
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 837.89424, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 16.82010, qf2_loss: 16.91796, policy_loss: -354.59820, policy_entropy: -1.00772, alpha: 0.52219, time: 32.83257
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 849.20879, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 19.61628, qf2_loss: 19.84878, policy_loss: -358.09816, policy_entropy: -1.00272, alpha: 0.52422, time: 32.63362
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 684.81206, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 19.83639, qf2_loss: 19.59695, policy_loss: -358.08521, policy_entropy: -1.00284, alpha: 0.52559, time: 32.65831
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 783.17191, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 20.91762, qf2_loss: 21.04312, policy_loss: -359.67442, policy_entropy: -0.99473, alpha: 0.52461, time: 33.02904
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 849.67318, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 16.95592, qf2_loss: 16.90743, policy_loss: -361.56790, policy_entropy: -1.00295, alpha: 0.52538, time: 32.86689
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 761.00149, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 20.55468, qf2_loss: 20.69790, policy_loss: -362.94035, policy_entropy: -1.00394, alpha: 0.52698, time: 32.76397
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 765.86339, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 17.62476, qf2_loss: 17.69828, policy_loss: -361.55548, policy_entropy: -1.00799, alpha: 0.52901, time: 32.68634
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 852.88951, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 18.07849, qf2_loss: 18.00870, policy_loss: -363.73328, policy_entropy: -0.99186, alpha: 0.52943, time: 32.69817
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 838.67895, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 21.50616, qf2_loss: 21.62674, policy_loss: -366.17318, policy_entropy: -0.99777, alpha: 0.52742, time: 32.66070
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 610.50098, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 20.85726, qf2_loss: 20.84569, policy_loss: -366.40864, policy_entropy: -1.00384, alpha: 0.52596, time: 32.58482
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 852.05941, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 19.49087, qf2_loss: 19.50496, policy_loss: -367.62741, policy_entropy: -0.98578, alpha: 0.52516, time: 32.61583
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 854.97372, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 17.43178, qf2_loss: 17.37419, policy_loss: -368.06373, policy_entropy: -1.01157, alpha: 0.52436, time: 32.84598
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 850.60488, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 17.50689, qf2_loss: 17.65196, policy_loss: -368.71768, policy_entropy: -1.00894, alpha: 0.52747, time: 32.66890
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 849.55102, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 18.88135, qf2_loss: 18.71478, policy_loss: -370.91411, policy_entropy: -1.00434, alpha: 0.53098, time: 32.86869
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 852.49962, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 19.43583, qf2_loss: 19.50283, policy_loss: -372.69176, policy_entropy: -1.00541, alpha: 0.53258, time: 32.54354
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 834.13139, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 22.13105, qf2_loss: 22.07100, policy_loss: -372.79538, policy_entropy: -0.99092, alpha: 0.53355, time: 32.66849
[CW] eval: return: 740.44667, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 601.91925, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 18.68467, qf2_loss: 18.75545, policy_loss: -374.36643, policy_entropy: -1.00707, alpha: 0.53230, time: 32.66930
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 838.68143, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 18.59944, qf2_loss: 18.63834, policy_loss: -373.80897, policy_entropy: -1.00622, alpha: 0.53611, time: 32.69497
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 852.35053, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 18.38439, qf2_loss: 18.38238, policy_loss: -375.62358, policy_entropy: -0.99516, alpha: 0.53418, time: 32.67073
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 796.98797, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 18.73986, qf2_loss: 18.82372, policy_loss: -377.08311, policy_entropy: -1.00935, alpha: 0.53481, time: 32.35466
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 758.56257, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 20.99496, qf2_loss: 20.99460, policy_loss: -379.02108, policy_entropy: -1.00164, alpha: 0.53777, time: 32.70377
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 837.36365, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 21.88652, qf2_loss: 21.91266, policy_loss: -379.62168, policy_entropy: -1.00578, alpha: 0.53931, time: 32.49244
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 840.23859, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 20.95460, qf2_loss: 21.10034, policy_loss: -380.78108, policy_entropy: -0.99661, alpha: 0.53923, time: 32.72992
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 836.56127, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 19.10930, qf2_loss: 19.16699, policy_loss: -380.92313, policy_entropy: -1.01186, alpha: 0.53992, time: 32.72946
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 854.84899, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 23.32048, qf2_loss: 23.42388, policy_loss: -382.80638, policy_entropy: -1.01198, alpha: 0.54637, time: 32.78672
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 846.08315, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 20.76899, qf2_loss: 20.77865, policy_loss: -384.09306, policy_entropy: -0.98911, alpha: 0.54733, time: 32.71949
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 837.29436, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 21.84107, qf2_loss: 22.03084, policy_loss: -383.32688, policy_entropy: -1.00072, alpha: 0.54460, time: 32.48176
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 854.35408, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 25.79409, qf2_loss: 25.72774, policy_loss: -385.27678, policy_entropy: -0.99788, alpha: 0.54411, time: 32.61904
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 846.81646, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 25.03650, qf2_loss: 24.96390, policy_loss: -389.59678, policy_entropy: -1.00565, alpha: 0.54560, time: 32.67285
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 753.48614, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 20.33661, qf2_loss: 20.44571, policy_loss: -388.65845, policy_entropy: -0.99811, alpha: 0.54677, time: 32.89329
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 845.79389, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 17.56669, qf2_loss: 17.70580, policy_loss: -392.02822, policy_entropy: -0.99716, alpha: 0.54578, time: 32.85328
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 854.24392, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 18.68544, qf2_loss: 18.82803, policy_loss: -388.49284, policy_entropy: -1.00494, alpha: 0.54469, time: 32.76085
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 852.67718, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 21.10486, qf2_loss: 21.36484, policy_loss: -389.46228, policy_entropy: -1.00007, alpha: 0.54499, time: 32.77391
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 852.27994, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 21.18345, qf2_loss: 21.33458, policy_loss: -391.96020, policy_entropy: -1.00245, alpha: 0.54712, time: 32.75441
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 850.67423, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 21.35514, qf2_loss: 21.29020, policy_loss: -394.21337, policy_entropy: -1.00404, alpha: 0.54731, time: 32.71709
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 850.69086, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 38.11391, qf2_loss: 38.58381, policy_loss: -394.91957, policy_entropy: -0.97771, alpha: 0.54699, time: 32.68891
[CW] eval: return: 606.91409, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 551.65006, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 50.53542, qf2_loss: 50.60705, policy_loss: -393.31254, policy_entropy: -0.98975, alpha: 0.53646, time: 32.63703
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 837.23160, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 26.83733, qf2_loss: 26.51001, policy_loss: -397.93152, policy_entropy: -1.01309, alpha: 0.53782, time: 32.71184
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 832.31893, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 19.76183, qf2_loss: 19.89086, policy_loss: -396.22877, policy_entropy: -1.01277, alpha: 0.54260, time: 32.58007
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 848.76154, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 19.40233, qf2_loss: 19.22275, policy_loss: -400.60645, policy_entropy: -0.99897, alpha: 0.54529, time: 32.62653
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 851.57265, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 18.91344, qf2_loss: 18.86667, policy_loss: -402.68250, policy_entropy: -1.00642, alpha: 0.54606, time: 32.77841
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 847.71531, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 22.12364, qf2_loss: 22.17879, policy_loss: -401.46539, policy_entropy: -1.00416, alpha: 0.54956, time: 33.69869
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 846.66042, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 21.51487, qf2_loss: 21.76914, policy_loss: -402.88071, policy_entropy: -1.00390, alpha: 0.54975, time: 32.78095
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 843.09986, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 20.48704, qf2_loss: 20.25526, policy_loss: -402.60101, policy_entropy: -1.00077, alpha: 0.55102, time: 32.71855
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 848.05771, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 21.02815, qf2_loss: 21.19907, policy_loss: -406.59812, policy_entropy: -0.99978, alpha: 0.55075, time: 32.68774
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 847.22354, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 21.91638, qf2_loss: 22.00011, policy_loss: -407.28430, policy_entropy: -1.00419, alpha: 0.55269, time: 32.55255
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 852.93199, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 21.96067, qf2_loss: 21.86819, policy_loss: -406.60117, policy_entropy: -0.99561, alpha: 0.55259, time: 32.57250
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 846.84447, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 21.49214, qf2_loss: 21.79035, policy_loss: -408.30489, policy_entropy: -1.01720, alpha: 0.55519, time: 32.68538
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 847.30132, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 21.57834, qf2_loss: 21.85253, policy_loss: -409.96805, policy_entropy: -1.00130, alpha: 0.55864, time: 33.11787
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 849.74863, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 23.33184, qf2_loss: 23.38621, policy_loss: -409.54682, policy_entropy: -1.00158, alpha: 0.55926, time: 32.77650
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 836.02907, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 22.22447, qf2_loss: 22.36120, policy_loss: -413.34376, policy_entropy: -1.00847, alpha: 0.55948, time: 32.92842
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 845.65969, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 23.31093, qf2_loss: 23.45109, policy_loss: -413.22439, policy_entropy: -0.99521, alpha: 0.56150, time: 33.02195
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 849.84866, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 23.93682, qf2_loss: 23.82242, policy_loss: -414.08611, policy_entropy: -1.00587, alpha: 0.56282, time: 32.67841
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 848.06721, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 22.68276, qf2_loss: 22.68609, policy_loss: -412.74371, policy_entropy: -0.99781, alpha: 0.56229, time: 32.64945
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 850.89934, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 24.34095, qf2_loss: 24.70258, policy_loss: -418.20961, policy_entropy: -0.99783, alpha: 0.56218, time: 32.60549
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 807.63653, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 28.03105, qf2_loss: 27.90442, policy_loss: -419.00137, policy_entropy: -1.00412, alpha: 0.56300, time: 32.61391
[CW] eval: return: 546.81447, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 562.18208, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 48.07039, qf2_loss: 48.05107, policy_loss: -420.03170, policy_entropy: -0.98593, alpha: 0.55996, time: 32.82499
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 789.70235, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 31.12005, qf2_loss: 31.01181, policy_loss: -421.81523, policy_entropy: -1.00409, alpha: 0.55738, time: 32.69564
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 848.25084, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 22.74469, qf2_loss: 22.65558, policy_loss: -420.08561, policy_entropy: -1.00867, alpha: 0.55960, time: 32.97298
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 851.45262, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 20.57663, qf2_loss: 20.64245, policy_loss: -420.08695, policy_entropy: -1.00582, alpha: 0.56128, time: 32.51968
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 836.39897, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 20.79169, qf2_loss: 20.88310, policy_loss: -422.48674, policy_entropy: -1.00148, alpha: 0.56406, time: 32.47462
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 849.86013, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 22.72129, qf2_loss: 22.77477, policy_loss: -423.84451, policy_entropy: -0.99234, alpha: 0.56532, time: 32.74547
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 844.57835, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 21.59740, qf2_loss: 21.61524, policy_loss: -427.97915, policy_entropy: -0.99539, alpha: 0.56163, time: 32.63391
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 561.06024, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 22.56524, qf2_loss: 22.43173, policy_loss: -425.92418, policy_entropy: -1.00243, alpha: 0.56075, time: 32.76671
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 852.47507, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 22.51224, qf2_loss: 22.57217, policy_loss: -425.55558, policy_entropy: -1.00559, alpha: 0.56250, time: 34.94424
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 846.05388, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 25.84355, qf2_loss: 25.74464, policy_loss: -426.94458, policy_entropy: -1.00096, alpha: 0.56320, time: 32.61298
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 848.28165, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 33.81186, qf2_loss: 33.84839, policy_loss: -427.49773, policy_entropy: -0.99789, alpha: 0.56327, time: 32.65035
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 843.50101, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 30.84609, qf2_loss: 31.09326, policy_loss: -430.25632, policy_entropy: -1.00188, alpha: 0.56279, time: 32.60306
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 767.62662, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 26.43720, qf2_loss: 26.09471, policy_loss: -431.52056, policy_entropy: -0.98407, alpha: 0.56046, time: 32.89417
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 843.21522, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 24.96558, qf2_loss: 25.18444, policy_loss: -431.86114, policy_entropy: -1.00048, alpha: 0.55784, time: 32.69327
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 683.06541, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 25.49314, qf2_loss: 25.69600, policy_loss: -431.93791, policy_entropy: -1.01009, alpha: 0.55941, time: 32.67123
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 836.91496, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 23.08035, qf2_loss: 23.13263, policy_loss: -436.48450, policy_entropy: -1.00468, alpha: 0.56216, time: 32.96205
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 848.31644, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 26.15340, qf2_loss: 26.15270, policy_loss: -433.41519, policy_entropy: -1.00501, alpha: 0.56520, time: 32.58026
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 847.65926, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 25.90662, qf2_loss: 26.03544, policy_loss: -437.75175, policy_entropy: -0.99985, alpha: 0.56499, time: 32.74145
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 831.49314, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 23.27974, qf2_loss: 23.16651, policy_loss: -437.57353, policy_entropy: -0.99991, alpha: 0.56591, time: 32.57346
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 846.69434, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 23.25290, qf2_loss: 23.32367, policy_loss: -437.09447, policy_entropy: -1.00410, alpha: 0.56481, time: 32.73740
[CW] eval: return: 846.81691, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 843.33859, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 31.10616, qf2_loss: 31.03550, policy_loss: -439.54001, policy_entropy: -0.99111, alpha: 0.56471, time: 32.81407
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 838.40170, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 40.17768, qf2_loss: 40.23270, policy_loss: -437.80676, policy_entropy: -1.00841, alpha: 0.56595, time: 32.72221
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 842.80360, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 38.90820, qf2_loss: 38.83750, policy_loss: -443.29650, policy_entropy: -0.98779, alpha: 0.56513, time: 32.82833
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 846.72011, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 29.84094, qf2_loss: 29.82229, policy_loss: -443.12565, policy_entropy: -0.99724, alpha: 0.56253, time: 32.49041
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 852.29369, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 29.31510, qf2_loss: 29.38480, policy_loss: -443.34821, policy_entropy: -1.01113, alpha: 0.56318, time: 32.57139
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 848.20868, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 24.84414, qf2_loss: 24.69931, policy_loss: -445.68484, policy_entropy: -1.00846, alpha: 0.56703, time: 32.89401
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 851.34850, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 23.74512, qf2_loss: 23.81122, policy_loss: -445.27067, policy_entropy: -1.00502, alpha: 0.56928, time: 32.55536
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 847.52499, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 25.17761, qf2_loss: 25.32563, policy_loss: -448.81149, policy_entropy: -1.00624, alpha: 0.57194, time: 32.78794
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 846.48073, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 24.72332, qf2_loss: 24.66011, policy_loss: -447.14001, policy_entropy: -0.99277, alpha: 0.57115, time: 32.77819
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 835.75356, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 23.77832, qf2_loss: 23.90992, policy_loss: -448.16976, policy_entropy: -0.99038, alpha: 0.56804, time: 32.74414
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 845.84468, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 23.82340, qf2_loss: 23.88967, policy_loss: -449.97462, policy_entropy: -0.99204, alpha: 0.56542, time: 32.77185
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 839.35541, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 24.30516, qf2_loss: 24.38067, policy_loss: -449.97345, policy_entropy: -1.00069, alpha: 0.56395, time: 32.82658
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 849.65714, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 24.30186, qf2_loss: 24.21690, policy_loss: -452.44010, policy_entropy: -0.99792, alpha: 0.56292, time: 32.78558
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 848.34457, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 27.87557, qf2_loss: 27.95686, policy_loss: -453.85878, policy_entropy: -1.00099, alpha: 0.56245, time: 32.80727
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 850.60322, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 25.90806, qf2_loss: 25.92682, policy_loss: -454.37626, policy_entropy: -0.99066, alpha: 0.56178, time: 32.68450
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 843.06547, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 25.32190, qf2_loss: 25.43814, policy_loss: -454.73952, policy_entropy: -1.00420, alpha: 0.56060, time: 32.70396
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 848.93340, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 28.06803, qf2_loss: 28.13205, policy_loss: -459.44222, policy_entropy: -0.99688, alpha: 0.56055, time: 32.60894
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 580.47125, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 29.15278, qf2_loss: 29.14572, policy_loss: -455.62675, policy_entropy: -0.99644, alpha: 0.55978, time: 33.09652
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 851.33228, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 27.51517, qf2_loss: 27.95492, policy_loss: -456.85458, policy_entropy: -1.00070, alpha: 0.55939, time: 32.69344
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 842.00015, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 29.22548, qf2_loss: 29.03090, policy_loss: -456.70790, policy_entropy: -1.00380, alpha: 0.56000, time: 32.53087
[CW] eval: return: 850.37028, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 848.29861, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 26.94195, qf2_loss: 26.80688, policy_loss: -461.50249, policy_entropy: -1.00769, alpha: 0.56154, time: 32.82470
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 843.62838, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 25.19192, qf2_loss: 25.00169, policy_loss: -463.50454, policy_entropy: -0.99632, alpha: 0.56278, time: 32.76720
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 835.73750, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 27.58514, qf2_loss: 27.49791, policy_loss: -463.37785, policy_entropy: -0.99012, alpha: 0.55985, time: 32.84588
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 844.64968, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 31.58886, qf2_loss: 31.55695, policy_loss: -464.15049, policy_entropy: -0.99727, alpha: 0.55676, time: 32.76899
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 849.75377, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 33.32924, qf2_loss: 33.26412, policy_loss: -463.32731, policy_entropy: -1.00541, alpha: 0.55880, time: 32.64236
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 844.37885, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 94.16259, qf2_loss: 94.18167, policy_loss: -464.31269, policy_entropy: -0.98821, alpha: 0.55737, time: 32.81229
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 399.26404, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 72.49521, qf2_loss: 72.29615, policy_loss: -465.06408, policy_entropy: -0.97370, alpha: 0.55082, time: 32.59024
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 844.84739, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 29.64545, qf2_loss: 29.50619, policy_loss: -469.82225, policy_entropy: -0.96934, alpha: 0.54474, time: 32.53082
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 841.12133, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 25.65481, qf2_loss: 25.51546, policy_loss: -470.94919, policy_entropy: -0.98312, alpha: 0.53658, time: 32.64216
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 845.61407, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 25.55190, qf2_loss: 25.42169, policy_loss: -472.81933, policy_entropy: -0.99409, alpha: 0.53293, time: 32.62081
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 849.43206, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 24.18411, qf2_loss: 24.25875, policy_loss: -471.18084, policy_entropy: -1.00363, alpha: 0.53289, time: 32.53791
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 827.98357, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 24.85449, qf2_loss: 24.93236, policy_loss: -471.54671, policy_entropy: -1.00445, alpha: 0.53368, time: 35.38925
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 847.86344, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 25.04313, qf2_loss: 24.87163, policy_loss: -472.80380, policy_entropy: -1.01444, alpha: 0.53533, time: 32.67400
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 843.64608, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 24.56455, qf2_loss: 24.59675, policy_loss: -474.81904, policy_entropy: -1.00023, alpha: 0.53849, time: 32.75726
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 830.74356, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 24.74487, qf2_loss: 24.59763, policy_loss: -475.31764, policy_entropy: -1.01526, alpha: 0.54124, time: 33.27850
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 838.17366, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 24.79678, qf2_loss: 24.76642, policy_loss: -474.88861, policy_entropy: -1.01048, alpha: 0.54442, time: 32.59656
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 842.96796, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 23.46598, qf2_loss: 23.71089, policy_loss: -477.20075, policy_entropy: -1.01164, alpha: 0.54519, time: 32.68651
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 831.78387, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 25.41690, qf2_loss: 25.43699, policy_loss: -478.12340, policy_entropy: -1.00382, alpha: 0.54883, time: 32.52355
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 851.31182, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 27.74929, qf2_loss: 27.53648, policy_loss: -479.50789, policy_entropy: -1.00365, alpha: 0.55090, time: 32.68632
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 844.26311, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 25.35911, qf2_loss: 25.23523, policy_loss: -477.74405, policy_entropy: -1.00402, alpha: 0.55152, time: 32.87900
[CW] eval: return: 822.79311, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 741.70772, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 27.25943, qf2_loss: 27.28685, policy_loss: -480.80600, policy_entropy: -1.00462, alpha: 0.55300, time: 32.82420
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 847.30904, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 29.22725, qf2_loss: 29.99592, policy_loss: -483.03450, policy_entropy: -1.00149, alpha: 0.55515, time: 32.72989
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 844.13582, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 30.50926, qf2_loss: 29.91378, policy_loss: -484.23467, policy_entropy: -0.98528, alpha: 0.55308, time: 32.50940
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 839.55127, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 27.77505, qf2_loss: 27.74149, policy_loss: -483.02201, policy_entropy: -1.00403, alpha: 0.55171, time: 32.52304
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 847.33258, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 25.95579, qf2_loss: 26.03240, policy_loss: -486.70601, policy_entropy: -0.99057, alpha: 0.55115, time: 32.28366
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 850.10330, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 28.93212, qf2_loss: 28.91116, policy_loss: -486.15663, policy_entropy: -1.00279, alpha: 0.54844, time: 32.76335
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 837.33769, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 28.98013, qf2_loss: 29.04798, policy_loss: -488.17883, policy_entropy: -0.99844, alpha: 0.54972, time: 32.70569
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 848.69510, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 26.10869, qf2_loss: 26.04598, policy_loss: -487.60858, policy_entropy: -0.99987, alpha: 0.54912, time: 32.52342
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 850.81659, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 27.59267, qf2_loss: 27.30530, policy_loss: -487.82324, policy_entropy: -1.00343, alpha: 0.54846, time: 32.92575
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 848.38290, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 29.23689, qf2_loss: 29.01129, policy_loss: -490.21013, policy_entropy: -0.99269, alpha: 0.54911, time: 32.77349
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 839.96983, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 58.25990, qf2_loss: 58.32387, policy_loss: -489.36393, policy_entropy: -1.01191, alpha: 0.55006, time: 32.79577
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 756.23599, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 62.56386, qf2_loss: 62.86857, policy_loss: -492.67782, policy_entropy: -0.97907, alpha: 0.55024, time: 32.80729
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 849.64249, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 27.68928, qf2_loss: 27.95873, policy_loss: -494.78742, policy_entropy: -0.97223, alpha: 0.54248, time: 32.88369
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 845.02947, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 25.03488, qf2_loss: 25.10089, policy_loss: -491.68957, policy_entropy: -1.00556, alpha: 0.53860, time: 32.59475
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 845.91008, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 25.28107, qf2_loss: 25.08027, policy_loss: -492.57799, policy_entropy: -1.01877, alpha: 0.54107, time: 32.46621
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 847.04372, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 25.01326, qf2_loss: 25.12116, policy_loss: -496.27004, policy_entropy: -0.99632, alpha: 0.54365, time: 32.70436
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 849.11083, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 23.88215, qf2_loss: 23.93194, policy_loss: -497.07042, policy_entropy: -0.99479, alpha: 0.54293, time: 32.68104
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 841.89824, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 26.30974, qf2_loss: 26.16018, policy_loss: -498.42026, policy_entropy: -1.00061, alpha: 0.54093, time: 32.71672
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 844.66003, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 28.41595, qf2_loss: 28.70964, policy_loss: -497.90339, policy_entropy: -1.00277, alpha: 0.54247, time: 32.49000
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 849.78251, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 29.04642, qf2_loss: 28.44889, policy_loss: -500.64984, policy_entropy: -1.00117, alpha: 0.54265, time: 32.57837
[CW] eval: return: 831.01134, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 842.95456, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 26.66157, qf2_loss: 26.59494, policy_loss: -498.72702, policy_entropy: -1.00056, alpha: 0.54292, time: 32.81433
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 849.34735, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 26.13054, qf2_loss: 26.29635, policy_loss: -501.13191, policy_entropy: -0.99925, alpha: 0.54330, time: 32.75771
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 845.28766, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 24.96742, qf2_loss: 25.20308, policy_loss: -504.78758, policy_entropy: -1.00494, alpha: 0.54449, time: 32.81017
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 844.22295, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 22.95680, qf2_loss: 23.04815, policy_loss: -506.73696, policy_entropy: -0.99668, alpha: 0.54433, time: 32.61521
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 851.35586, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 26.00236, qf2_loss: 25.97856, policy_loss: -507.02998, policy_entropy: -0.99390, alpha: 0.54152, time: 32.53292
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 847.56882, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 28.75907, qf2_loss: 28.63626, policy_loss: -504.53126, policy_entropy: -1.00969, alpha: 0.54325, time: 32.57714
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 845.46530, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 27.20174, qf2_loss: 27.32337, policy_loss: -506.27904, policy_entropy: -1.00142, alpha: 0.54473, time: 32.70118
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 847.58858, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 27.24947, qf2_loss: 27.40722, policy_loss: -508.28124, policy_entropy: -0.99168, alpha: 0.54404, time: 32.47311
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 846.55318, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 27.25826, qf2_loss: 27.26038, policy_loss: -507.96681, policy_entropy: -1.00022, alpha: 0.54196, time: 32.87489
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 841.41255, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 28.30979, qf2_loss: 28.30565, policy_loss: -509.08340, policy_entropy: -1.00714, alpha: 0.54388, time: 32.93312
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 760.76821, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 27.02600, qf2_loss: 26.70611, policy_loss: -509.33063, policy_entropy: -1.01393, alpha: 0.54576, time: 32.76078
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 850.12559, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 27.64455, qf2_loss: 27.62646, policy_loss: -509.21914, policy_entropy: -0.99601, alpha: 0.54770, time: 32.62166
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 848.88285, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 26.81290, qf2_loss: 27.09851, policy_loss: -513.60596, policy_entropy: -0.98853, alpha: 0.54565, time: 32.69085
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 846.57010, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 50.95583, qf2_loss: 49.39579, policy_loss: -511.60217, policy_entropy: -0.99412, alpha: 0.54396, time: 32.63250
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 834.36023, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 65.39288, qf2_loss: 66.58106, policy_loss: -513.29717, policy_entropy: -0.98250, alpha: 0.54072, time: 32.55520
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 755.89413, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 42.36653, qf2_loss: 42.45823, policy_loss: -511.72410, policy_entropy: -0.97851, alpha: 0.53512, time: 32.93459
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 848.74692, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 25.48208, qf2_loss: 25.42930, policy_loss: -515.61161, policy_entropy: -0.99229, alpha: 0.53089, time: 32.93000
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 849.54782, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 26.38775, qf2_loss: 26.41804, policy_loss: -514.50107, policy_entropy: -0.99308, alpha: 0.52953, time: 32.89243
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 848.13812, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 24.05367, qf2_loss: 24.33538, policy_loss: -518.21139, policy_entropy: -1.00031, alpha: 0.52970, time: 32.93653
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 849.55016, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 25.66849, qf2_loss: 25.61049, policy_loss: -519.08024, policy_entropy: -1.00045, alpha: 0.52925, time: 32.69552
[CW] eval: return: 847.63319, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 848.57278, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 27.36426, qf2_loss: 27.64521, policy_loss: -516.47651, policy_entropy: -1.00792, alpha: 0.52970, time: 32.51972
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 842.36367, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 25.43035, qf2_loss: 25.45486, policy_loss: -519.06608, policy_entropy: -1.00920, alpha: 0.53327, time: 33.01663
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 847.46592, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 24.87199, qf2_loss: 24.98185, policy_loss: -520.65069, policy_entropy: -1.00584, alpha: 0.53429, time: 32.78241
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 847.58332, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 25.20662, qf2_loss: 25.47624, policy_loss: -520.91776, policy_entropy: -1.00495, alpha: 0.53533, time: 32.61414
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 847.43178, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 25.01245, qf2_loss: 25.16662, policy_loss: -522.15713, policy_entropy: -1.00471, alpha: 0.53690, time: 32.56506
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 845.19186, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 24.02053, qf2_loss: 23.90957, policy_loss: -522.88800, policy_entropy: -0.99041, alpha: 0.53675, time: 32.65898
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 846.00159, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 28.30473, qf2_loss: 27.84270, policy_loss: -524.12911, policy_entropy: -1.00047, alpha: 0.53559, time: 32.68765
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 849.13451, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 26.52287, qf2_loss: 26.72423, policy_loss: -524.43809, policy_entropy: -1.00749, alpha: 0.53659, time: 34.78173
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 849.90192, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 24.21294, qf2_loss: 24.10558, policy_loss: -527.04089, policy_entropy: -1.00155, alpha: 0.53718, time: 32.49019
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 846.59150, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 27.52787, qf2_loss: 27.47515, policy_loss: -525.30093, policy_entropy: -1.00680, alpha: 0.53803, time: 32.83226
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 830.63811, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 28.06155, qf2_loss: 28.21163, policy_loss: -528.29356, policy_entropy: -1.00276, alpha: 0.53992, time: 32.55896
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 850.19671, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 26.22451, qf2_loss: 26.36879, policy_loss: -527.51907, policy_entropy: -0.99756, alpha: 0.53950, time: 32.74017
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 836.89034, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 27.35930, qf2_loss: 26.61379, policy_loss: -530.65810, policy_entropy: -0.98621, alpha: 0.53726, time: 32.66525
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 847.23899, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 24.85544, qf2_loss: 25.16545, policy_loss: -532.39155, policy_entropy: -0.98610, alpha: 0.53420, time: 32.74039
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 847.90887, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 29.45219, qf2_loss: 29.48153, policy_loss: -529.64505, policy_entropy: -0.99218, alpha: 0.53133, time: 32.94857
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 846.07669, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 26.01066, qf2_loss: 26.08604, policy_loss: -531.43154, policy_entropy: -1.00173, alpha: 0.53026, time: 32.75420
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 839.32301, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 25.48261, qf2_loss: 25.39461, policy_loss: -529.79733, policy_entropy: -1.00544, alpha: 0.53123, time: 32.71922
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 846.09638, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 27.02832, qf2_loss: 27.02551, policy_loss: -533.60096, policy_entropy: -0.99844, alpha: 0.53203, time: 32.40471
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 847.93059, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 29.78759, qf2_loss: 29.78095, policy_loss: -531.16467, policy_entropy: -1.00162, alpha: 0.53254, time: 32.74730
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 845.30086, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 27.50357, qf2_loss: 27.54787, policy_loss: -535.13520, policy_entropy: -0.99089, alpha: 0.53217, time: 32.94109
[CW] eval: return: 845.41223, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 845.49198, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 25.24049, qf2_loss: 25.23536, policy_loss: -535.97731, policy_entropy: -1.00624, alpha: 0.53069, time: 32.60539
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 821.14675, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 29.43331, qf2_loss: 29.56914, policy_loss: -534.48420, policy_entropy: -1.00720, alpha: 0.53212, time: 32.97138
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 842.03544, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 27.93224, qf2_loss: 27.76660, policy_loss: -535.35512, policy_entropy: -1.00034, alpha: 0.53321, time: 32.50828
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 836.15395, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 24.33446, qf2_loss: 24.41007, policy_loss: -538.84180, policy_entropy: -1.00318, alpha: 0.53446, time: 32.66969
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 840.49253, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 25.60969, qf2_loss: 25.40683, policy_loss: -537.59041, policy_entropy: -1.00666, alpha: 0.53459, time: 32.70757
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 850.41889, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 24.44098, qf2_loss: 24.56249, policy_loss: -537.49492, policy_entropy: -1.00063, alpha: 0.53648, time: 32.45923
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 848.63825, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 26.71048, qf2_loss: 26.54886, policy_loss: -541.56923, policy_entropy: -0.99436, alpha: 0.53547, time: 32.87882
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 844.01133, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 26.50444, qf2_loss: 26.80762, policy_loss: -543.23173, policy_entropy: -0.97899, alpha: 0.53234, time: 32.74186
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 819.14640, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 32.30207, qf2_loss: 31.90948, policy_loss: -542.84600, policy_entropy: -0.99120, alpha: 0.52873, time: 32.90253
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 842.23470, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 33.41464, qf2_loss: 33.81156, policy_loss: -540.68394, policy_entropy: -0.99834, alpha: 0.52692, time: 32.73209
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 767.48644, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 25.69168, qf2_loss: 25.50086, policy_loss: -545.19884, policy_entropy: -0.99246, alpha: 0.52650, time: 32.56038
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 830.60510, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 25.80272, qf2_loss: 25.72290, policy_loss: -543.47553, policy_entropy: -1.00130, alpha: 0.52571, time: 32.81748
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 838.01912, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 23.97150, qf2_loss: 24.19275, policy_loss: -544.75638, policy_entropy: -0.98722, alpha: 0.52398, time: 32.66471
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 844.16743, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 24.27787, qf2_loss: 24.40840, policy_loss: -546.54838, policy_entropy: -0.98688, alpha: 0.52111, time: 32.79662
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 843.70805, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 24.56616, qf2_loss: 24.54530, policy_loss: -545.95763, policy_entropy: -1.00733, alpha: 0.52016, time: 32.90766
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 848.44001, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 27.38264, qf2_loss: 27.38601, policy_loss: -545.97773, policy_entropy: -0.99116, alpha: 0.52119, time: 32.81416
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 845.21493, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 24.35358, qf2_loss: 24.55294, policy_loss: -548.10317, policy_entropy: -0.99053, alpha: 0.51698, time: 33.04703
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 830.75740, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 24.63780, qf2_loss: 24.81643, policy_loss: -549.83099, policy_entropy: -1.00041, alpha: 0.51577, time: 32.67159
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 850.19771, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 24.92564, qf2_loss: 25.17810, policy_loss: -550.27671, policy_entropy: -0.99078, alpha: 0.51590, time: 32.67105
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 849.49994, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 28.26361, qf2_loss: 28.22461, policy_loss: -549.14102, policy_entropy: -1.01116, alpha: 0.51561, time: 32.75942
[CW] eval: return: 837.41525, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 850.72083, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 29.26178, qf2_loss: 29.23507, policy_loss: -550.82444, policy_entropy: -0.99618, alpha: 0.51756, time: 32.70958
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 845.93525, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 25.03246, qf2_loss: 25.08258, policy_loss: -552.66975, policy_entropy: -0.98791, alpha: 0.51597, time: 32.72757
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 841.87648, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 24.57024, qf2_loss: 24.76258, policy_loss: -554.24981, policy_entropy: -0.98895, alpha: 0.51323, time: 32.65754
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 842.69219, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 25.66532, qf2_loss: 25.59145, policy_loss: -553.57073, policy_entropy: -0.99593, alpha: 0.51214, time: 32.87828
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 828.51288, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 23.16230, qf2_loss: 23.13942, policy_loss: -555.42748, policy_entropy: -0.98876, alpha: 0.50993, time: 32.50932
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 846.37191, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 25.62190, qf2_loss: 25.84892, policy_loss: -557.42573, policy_entropy: -0.99856, alpha: 0.50804, time: 32.51656
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 849.50485, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 27.75451, qf2_loss: 27.83319, policy_loss: -554.53575, policy_entropy: -1.00146, alpha: 0.50816, time: 32.67367
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 834.61458, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 25.51662, qf2_loss: 25.58020, policy_loss: -557.37173, policy_entropy: -1.00212, alpha: 0.50803, time: 32.69262
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 847.28517, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 28.54125, qf2_loss: 28.34640, policy_loss: -557.08375, policy_entropy: -1.00616, alpha: 0.50918, time: 33.13175
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 846.93513, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 26.63963, qf2_loss: 26.77294, policy_loss: -557.59368, policy_entropy: -1.00343, alpha: 0.51098, time: 33.04163
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 690.57084, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 24.66515, qf2_loss: 24.61232, policy_loss: -559.48363, policy_entropy: -0.99401, alpha: 0.51032, time: 32.90783
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 842.90141, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 51.22069, qf2_loss: 51.40837, policy_loss: -556.37350, policy_entropy: -1.01832, alpha: 0.51143, time: 32.77493
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 826.85459, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 36.60144, qf2_loss: 36.79474, policy_loss: -560.14944, policy_entropy: -0.96713, alpha: 0.51128, time: 32.77882
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 845.23954, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 26.25401, qf2_loss: 26.50793, policy_loss: -561.21549, policy_entropy: -0.97999, alpha: 0.50499, time: 32.74486
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 841.32271, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 23.35232, qf2_loss: 23.38976, policy_loss: -561.80091, policy_entropy: -0.98601, alpha: 0.50198, time: 32.67159
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 845.13630, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 22.00374, qf2_loss: 21.81881, policy_loss: -561.45378, policy_entropy: -0.99754, alpha: 0.49962, time: 33.47755
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 849.09124, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 21.66549, qf2_loss: 21.79228, policy_loss: -563.50824, policy_entropy: -0.99700, alpha: 0.49888, time: 32.72328
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 838.31721, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 22.49814, qf2_loss: 22.54592, policy_loss: -561.74227, policy_entropy: -1.00013, alpha: 0.49923, time: 32.87511
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 844.79606, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 23.74161, qf2_loss: 23.67139, policy_loss: -563.88647, policy_entropy: -0.98992, alpha: 0.49816, time: 32.93680
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 755.31749, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 22.90148, qf2_loss: 22.97158, policy_loss: -566.24490, policy_entropy: -0.98471, alpha: 0.49526, time: 32.69128
[CW] eval: return: 844.31524, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 848.95263, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 24.34562, qf2_loss: 24.13199, policy_loss: -566.18498, policy_entropy: -1.00010, alpha: 0.49424, time: 33.01710
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 844.22395, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 21.80992, qf2_loss: 21.88986, policy_loss: -567.18484, policy_entropy: -1.00653, alpha: 0.49402, time: 32.64006
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 845.10867, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 23.21675, qf2_loss: 23.81634, policy_loss: -566.57300, policy_entropy: -1.00136, alpha: 0.49579, time: 33.11735
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 850.20400, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 24.16134, qf2_loss: 24.06701, policy_loss: -567.74232, policy_entropy: -1.00518, alpha: 0.49519, time: 32.82943
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 847.21929, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 22.71404, qf2_loss: 22.68539, policy_loss: -568.17706, policy_entropy: -1.01158, alpha: 0.49679, time: 32.83637
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 850.08079, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 24.70616, qf2_loss: 24.57839, policy_loss: -568.32307, policy_entropy: -1.00386, alpha: 0.49871, time: 32.98681
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 843.49146, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 34.16893, qf2_loss: 34.38726, policy_loss: -571.12385, policy_entropy: -0.98992, alpha: 0.49919, time: 40.60241
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 844.20616, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 26.16611, qf2_loss: 25.98305, policy_loss: -570.27771, policy_entropy: -0.99340, alpha: 0.49642, time: 32.81723
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 847.20134, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 21.93483, qf2_loss: 22.08974, policy_loss: -570.97213, policy_entropy: -1.00063, alpha: 0.49533, time: 32.73011
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 845.99903, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 22.87641, qf2_loss: 23.15075, policy_loss: -574.03164, policy_entropy: -0.97998, alpha: 0.49448, time: 33.87083
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 847.10878, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 22.98877, qf2_loss: 22.83081, policy_loss: -569.83075, policy_entropy: -1.00692, alpha: 0.49411, time: 32.87999
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 842.86032, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 25.58924, qf2_loss: 25.36093, policy_loss: -571.47614, policy_entropy: -0.99688, alpha: 0.49337, time: 32.56622
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 759.75762, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 22.94422, qf2_loss: 23.23393, policy_loss: -573.98098, policy_entropy: -0.98852, alpha: 0.49331, time: 33.06750
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 839.41886, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 23.18575, qf2_loss: 23.12102, policy_loss: -571.13014, policy_entropy: -1.00345, alpha: 0.49144, time: 32.54370
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 847.94835, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 22.78377, qf2_loss: 22.88948, policy_loss: -572.89991, policy_entropy: -1.00361, alpha: 0.49138, time: 33.06835
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 842.45609, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 31.01826, qf2_loss: 31.02280, policy_loss: -573.17255, policy_entropy: -1.01246, alpha: 0.49325, time: 32.76590
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 840.85742, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 24.76742, qf2_loss: 24.66464, policy_loss: -576.43055, policy_entropy: -0.99130, alpha: 0.49369, time: 32.82033
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 845.30965, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 23.18072, qf2_loss: 23.30275, policy_loss: -576.74344, policy_entropy: -1.00183, alpha: 0.49337, time: 32.82081
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 843.55312, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 25.31998, qf2_loss: 25.39382, policy_loss: -574.11640, policy_entropy: -0.99817, alpha: 0.49365, time: 32.63695
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 840.49640, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 25.50182, qf2_loss: 25.37203, policy_loss: -577.16629, policy_entropy: -0.99411, alpha: 0.49141, time: 32.96973
[CW] eval: return: 842.36919, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 840.77726, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 23.57058, qf2_loss: 24.16206, policy_loss: -579.15077, policy_entropy: -0.98574, alpha: 0.49133, time: 32.61400
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 849.71283, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 22.63108, qf2_loss: 22.61745, policy_loss: -578.80667, policy_entropy: -0.99012, alpha: 0.48802, time: 32.61717
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 843.26101, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 22.93908, qf2_loss: 22.80844, policy_loss: -579.99606, policy_entropy: -1.00252, alpha: 0.48747, time: 32.66936
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 844.19161, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 25.64160, qf2_loss: 25.64335, policy_loss: -581.26669, policy_entropy: -0.97437, alpha: 0.48643, time: 32.72114
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 841.09825, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 26.87865, qf2_loss: 26.89780, policy_loss: -581.27829, policy_entropy: -0.98575, alpha: 0.48259, time: 33.00925
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 840.76965, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 32.59121, qf2_loss: 32.82686, policy_loss: -581.64834, policy_entropy: -0.98471, alpha: 0.48030, time: 32.56533
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 837.58304, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 28.55264, qf2_loss: 28.72515, policy_loss: -583.68190, policy_entropy: -0.98430, alpha: 0.47633, time: 32.87809
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 847.25773, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 26.97544, qf2_loss: 26.33189, policy_loss: -581.24057, policy_entropy: -1.01932, alpha: 0.47626, time: 32.71506
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 828.22851, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 23.25426, qf2_loss: 23.11627, policy_loss: -581.55185, policy_entropy: -1.00078, alpha: 0.47921, time: 32.81027
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 843.39690, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 21.56070, qf2_loss: 21.47749, policy_loss: -581.38490, policy_entropy: -1.01529, alpha: 0.47891, time: 32.87793
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 824.55959, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 23.79378, qf2_loss: 24.13730, policy_loss: -586.93348, policy_entropy: -0.99516, alpha: 0.48144, time: 32.79316
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 839.31951, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 22.74971, qf2_loss: 22.78314, policy_loss: -582.49328, policy_entropy: -1.00634, alpha: 0.48083, time: 32.74003
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 835.43104, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 25.69533, qf2_loss: 25.81706, policy_loss: -584.45801, policy_entropy: -0.98622, alpha: 0.48020, time: 33.03908
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 840.01283, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 22.98201, qf2_loss: 22.99434, policy_loss: -584.49346, policy_entropy: -0.99668, alpha: 0.47846, time: 32.66611
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 847.12634, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 23.15659, qf2_loss: 23.17228, policy_loss: -584.55976, policy_entropy: -1.00688, alpha: 0.47972, time: 32.70718
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 843.36247, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 20.93700, qf2_loss: 21.07531, policy_loss: -586.84352, policy_entropy: -0.99330, alpha: 0.48004, time: 32.83965
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 846.93532, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 24.52297, qf2_loss: 24.42633, policy_loss: -588.14963, policy_entropy: -0.98869, alpha: 0.47808, time: 32.92855
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 847.76381, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 24.87424, qf2_loss: 24.81207, policy_loss: -587.29379, policy_entropy: -0.98996, alpha: 0.47586, time: 32.67811
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 842.86953, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 37.93475, qf2_loss: 38.14809, policy_loss: -589.95641, policy_entropy: -0.98398, alpha: 0.47386, time: 32.61530
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 646.29704, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 85.79160, qf2_loss: 85.85853, policy_loss: -590.28263, policy_entropy: -0.98244, alpha: 0.47093, time: 32.75879
[CW] eval: return: 830.06529, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 824.38702, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 32.51766, qf2_loss: 32.76541, policy_loss: -589.30887, policy_entropy: -0.96905, alpha: 0.46772, time: 32.71668
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 836.08152, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 23.64375, qf2_loss: 23.71324, policy_loss: -590.22089, policy_entropy: -0.97920, alpha: 0.46320, time: 32.90065
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 844.78693, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 20.85256, qf2_loss: 20.99910, policy_loss: -592.83763, policy_entropy: -0.98199, alpha: 0.45987, time: 32.77220
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 840.56742, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 21.39783, qf2_loss: 21.48530, policy_loss: -590.94282, policy_entropy: -0.99687, alpha: 0.45849, time: 32.64640
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 839.88156, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 22.61735, qf2_loss: 22.77025, policy_loss: -592.12417, policy_entropy: -0.99371, alpha: 0.45739, time: 32.73502
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 607.96384, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 20.38538, qf2_loss: 20.29506, policy_loss: -594.06943, policy_entropy: -0.99429, alpha: 0.45719, time: 32.60602
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 843.37801, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 19.17314, qf2_loss: 19.36637, policy_loss: -594.06854, policy_entropy: -0.99634, alpha: 0.45600, time: 33.28013
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 829.73283, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 21.68430, qf2_loss: 21.56640, policy_loss: -595.19626, policy_entropy: -0.99238, alpha: 0.45460, time: 32.65755
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 844.48649, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 20.42171, qf2_loss: 20.28725, policy_loss: -596.14732, policy_entropy: -0.99563, alpha: 0.45464, time: 32.87760
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 846.90912, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 21.65916, qf2_loss: 21.86936, policy_loss: -596.58833, policy_entropy: -0.99040, alpha: 0.45376, time: 32.59314
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 847.58440, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 19.66552, qf2_loss: 19.93274, policy_loss: -594.73111, policy_entropy: -1.00239, alpha: 0.45263, time: 32.52262
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 841.10065, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 21.55022, qf2_loss: 21.84191, policy_loss: -595.67526, policy_entropy: -1.01048, alpha: 0.45317, time: 32.63734
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 845.66670, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 20.81686, qf2_loss: 20.70693, policy_loss: -597.04777, policy_entropy: -0.99476, alpha: 0.45470, time: 32.69703
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 843.86945, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 22.74919, qf2_loss: 22.47733, policy_loss: -598.49227, policy_entropy: -0.99239, alpha: 0.45308, time: 32.65511
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 849.95862, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 21.08048, qf2_loss: 20.99591, policy_loss: -597.47500, policy_entropy: -1.00099, alpha: 0.45172, time: 32.66243
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 837.15609, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 20.69663, qf2_loss: 20.65840, policy_loss: -597.58402, policy_entropy: -1.01095, alpha: 0.45316, time: 32.75460
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 846.59260, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 23.33942, qf2_loss: 23.60864, policy_loss: -597.92759, policy_entropy: -0.99662, alpha: 0.45367, time: 32.98210
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 844.48279, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 21.96996, qf2_loss: 21.89699, policy_loss: -598.41713, policy_entropy: -0.99652, alpha: 0.45392, time: 32.56002
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 840.62601, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 22.75226, qf2_loss: 23.01809, policy_loss: -600.23565, policy_entropy: -1.00464, alpha: 0.45377, time: 32.64629
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 838.73374, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 24.10439, qf2_loss: 24.15343, policy_loss: -598.71056, policy_entropy: -1.01903, alpha: 0.45479, time: 32.53782
[CW] eval: return: 837.67142, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 840.81136, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 21.38869, qf2_loss: 21.37892, policy_loss: -603.62000, policy_entropy: -0.96817, alpha: 0.45507, time: 32.88262
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 848.38792, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 21.88867, qf2_loss: 21.96178, policy_loss: -601.55142, policy_entropy: -1.00385, alpha: 0.45270, time: 33.07640
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 837.74560, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 22.58380, qf2_loss: 22.09678, policy_loss: -602.25295, policy_entropy: -0.99822, alpha: 0.45285, time: 32.64203
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 842.99299, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 24.15840, qf2_loss: 24.60019, policy_loss: -603.08049, policy_entropy: -0.99891, alpha: 0.45195, time: 32.44686
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 839.48758, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 24.17975, qf2_loss: 24.28228, policy_loss: -602.17134, policy_entropy: -1.00176, alpha: 0.45234, time: 32.65868
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 841.75719, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 23.35728, qf2_loss: 23.15856, policy_loss: -604.83958, policy_entropy: -0.98868, alpha: 0.45082, time: 32.47483
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 845.37569, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 20.97522, qf2_loss: 21.20353, policy_loss: -604.94990, policy_entropy: -1.00103, alpha: 0.45082, time: 32.43031
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 848.82243, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 22.53273, qf2_loss: 22.23997, policy_loss: -605.10261, policy_entropy: -0.99848, alpha: 0.45052, time: 32.56450
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 600.66045, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 20.55937, qf2_loss: 20.63961, policy_loss: -605.86539, policy_entropy: -0.98533, alpha: 0.44983, time: 32.66720
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 849.98431, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 20.72138, qf2_loss: 20.41399, policy_loss: -606.06817, policy_entropy: -0.99237, alpha: 0.44705, time: 32.67383
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 842.44876, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 22.98526, qf2_loss: 23.19939, policy_loss: -606.19097, policy_entropy: -0.99582, alpha: 0.44634, time: 32.79294
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 839.54095, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 22.05918, qf2_loss: 22.28150, policy_loss: -606.81383, policy_entropy: -0.99104, alpha: 0.44584, time: 32.60847
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 843.87159, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 24.13543, qf2_loss: 24.14481, policy_loss: -604.42953, policy_entropy: -1.01308, alpha: 0.44558, time: 32.79442
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 843.91923, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 23.45192, qf2_loss: 23.31626, policy_loss: -606.62929, policy_entropy: -0.99067, alpha: 0.44663, time: 32.67237
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 834.45088, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 23.74974, qf2_loss: 23.43090, policy_loss: -610.08753, policy_entropy: -0.97852, alpha: 0.44320, time: 32.51204
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 840.16702, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 23.74672, qf2_loss: 24.01232, policy_loss: -610.43059, policy_entropy: -0.97732, alpha: 0.44122, time: 32.60694
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 839.20252, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 24.63281, qf2_loss: 24.63956, policy_loss: -611.41150, policy_entropy: -0.99434, alpha: 0.43891, time: 32.65098
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 840.22246, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 23.44784, qf2_loss: 23.45750, policy_loss: -609.43499, policy_entropy: -0.99904, alpha: 0.43831, time: 32.61348
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 850.26485, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 21.64526, qf2_loss: 21.64926, policy_loss: -607.93087, policy_entropy: -1.01227, alpha: 0.43870, time: 32.41551
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 847.70308, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 19.77870, qf2_loss: 19.96262, policy_loss: -610.39061, policy_entropy: -1.00064, alpha: 0.44018, time: 36.18958
[CW] eval: return: 845.47914, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 847.42422, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 19.40830, qf2_loss: 19.29627, policy_loss: -612.16331, policy_entropy: -1.00812, alpha: 0.44086, time: 32.60690
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 844.72669, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 22.60498, qf2_loss: 22.51341, policy_loss: -611.26472, policy_entropy: -1.00338, alpha: 0.44134, time: 32.72244
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 843.02779, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 22.51888, qf2_loss: 22.73198, policy_loss: -612.41674, policy_entropy: -1.00263, alpha: 0.44150, time: 32.59742
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 848.21284, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 19.89178, qf2_loss: 19.93416, policy_loss: -615.06827, policy_entropy: -0.98656, alpha: 0.44139, time: 32.53812
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 847.97682, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 20.24006, qf2_loss: 19.97226, policy_loss: -614.85975, policy_entropy: -0.98485, alpha: 0.43948, time: 32.68003
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 845.00209, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 19.36183, qf2_loss: 19.26378, policy_loss: -615.04617, policy_entropy: -0.99603, alpha: 0.43821, time: 32.39762
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 846.16193, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 20.35021, qf2_loss: 20.58234, policy_loss: -614.23976, policy_entropy: -1.00027, alpha: 0.43809, time: 32.77513
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 848.39854, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 39.80083, qf2_loss: 39.33617, policy_loss: -616.60234, policy_entropy: -0.98771, alpha: 0.43671, time: 32.75618
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 603.88021, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 119.56078, qf2_loss: 119.35840, policy_loss: -615.74016, policy_entropy: -0.97398, alpha: 0.43546, time: 32.74151
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 739.37117, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 43.33548, qf2_loss: 43.80165, policy_loss: -616.15385, policy_entropy: -0.94965, alpha: 0.43008, time: 32.74531
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 841.47282, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 21.12359, qf2_loss: 21.09730, policy_loss: -619.42012, policy_entropy: -0.94117, alpha: 0.42257, time: 32.52652
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 843.04052, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 20.68382, qf2_loss: 20.61207, policy_loss: -616.31436, policy_entropy: -0.98675, alpha: 0.41809, time: 32.54218
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 845.44497, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 20.49623, qf2_loss: 20.52542, policy_loss: -618.96303, policy_entropy: -0.98638, alpha: 0.41600, time: 32.65982
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 842.38378, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 19.34569, qf2_loss: 19.33874, policy_loss: -618.64094, policy_entropy: -0.99744, alpha: 0.41489, time: 32.50732
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 841.91740, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 18.36187, qf2_loss: 18.26071, policy_loss: -620.98173, policy_entropy: -0.99790, alpha: 0.41524, time: 32.56778
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 747.74776, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 18.69244, qf2_loss: 18.57608, policy_loss: -620.66503, policy_entropy: -1.01207, alpha: 0.41568, time: 32.42112
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 847.16076, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 19.19785, qf2_loss: 19.06954, policy_loss: -621.47292, policy_entropy: -1.00725, alpha: 0.41628, time: 32.57337
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 844.87530, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 18.69090, qf2_loss: 18.83133, policy_loss: -618.52151, policy_entropy: -1.01834, alpha: 0.41772, time: 32.68701
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 845.09161, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 19.90691, qf2_loss: 19.73550, policy_loss: -622.81383, policy_entropy: -1.00750, alpha: 0.41925, time: 32.72159
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 835.73671, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 18.80257, qf2_loss: 18.90682, policy_loss: -621.75546, policy_entropy: -1.01718, alpha: 0.42159, time: 32.57300
[CW] eval: return: 846.55598, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 847.83359, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 18.81004, qf2_loss: 18.81966, policy_loss: -622.60245, policy_entropy: -1.00861, alpha: 0.42294, time: 32.88466
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 842.00997, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 19.67807, qf2_loss: 19.40840, policy_loss: -621.97667, policy_entropy: -1.01401, alpha: 0.42416, time: 32.58518
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 850.00545, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 18.67466, qf2_loss: 18.62082, policy_loss: -623.34185, policy_entropy: -1.00608, alpha: 0.42567, time: 32.69258
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 845.85021, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 19.23362, qf2_loss: 19.56401, policy_loss: -624.49977, policy_entropy: -1.00519, alpha: 0.42617, time: 32.68080
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 839.15634, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 18.39112, qf2_loss: 18.51267, policy_loss: -623.49849, policy_entropy: -1.01105, alpha: 0.42671, time: 32.51171
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 836.91918, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 21.13265, qf2_loss: 20.89938, policy_loss: -626.44372, policy_entropy: -0.99850, alpha: 0.42719, time: 32.50946
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 845.16290, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 19.79261, qf2_loss: 19.82591, policy_loss: -624.28401, policy_entropy: -1.00197, alpha: 0.42709, time: 32.53981
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 848.87131, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 21.51333, qf2_loss: 21.78220, policy_loss: -626.73857, policy_entropy: -0.99015, alpha: 0.42761, time: 33.02855
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 841.42653, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 24.87576, qf2_loss: 24.59355, policy_loss: -627.18020, policy_entropy: -0.98496, alpha: 0.42586, time: 32.71973
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 843.07981, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 21.22675, qf2_loss: 21.51341, policy_loss: -625.45882, policy_entropy: -1.01044, alpha: 0.42589, time: 32.89746
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 850.12514, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 19.79696, qf2_loss: 20.18629, policy_loss: -627.03420, policy_entropy: -0.98916, alpha: 0.42540, time: 32.48183
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 843.40312, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 18.97349, qf2_loss: 19.10097, policy_loss: -627.73431, policy_entropy: -0.99821, alpha: 0.42466, time: 32.50142
[CW] ---------------------------
[CW] ---- Iteration:   673 ----
[CW] collect: return: 840.97967, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 19.07408, qf2_loss: 19.02221, policy_loss: -625.93336, policy_entropy: -1.01032, alpha: 0.42517, time: 32.77959
[CW] ---------------------------
[CW] ---- Iteration:   674 ----
[CW] collect: return: 847.03413, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 19.76574, qf2_loss: 19.66565, policy_loss: -629.44247, policy_entropy: -0.98632, alpha: 0.42515, time: 32.60872
[CW] ---------------------------
[CW] ---- Iteration:   675 ----
[CW] collect: return: 844.91627, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 21.75854, qf2_loss: 21.90926, policy_loss: -623.23612, policy_entropy: -1.02696, alpha: 0.42591, time: 32.68821
[CW] ---------------------------
[CW] ---- Iteration:   676 ----
[CW] collect: return: 846.24787, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 20.28545, qf2_loss: 20.33415, policy_loss: -629.95662, policy_entropy: -0.98292, alpha: 0.42663, time: 32.77426
[CW] ---------------------------
[CW] ---- Iteration:   677 ----
[CW] collect: return: 840.42383, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 19.44015, qf2_loss: 19.07520, policy_loss: -630.45301, policy_entropy: -0.97298, alpha: 0.42420, time: 32.58522
[CW] ---------------------------
[CW] ---- Iteration:   678 ----
[CW] collect: return: 845.25519, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 18.83555, qf2_loss: 18.75103, policy_loss: -632.54477, policy_entropy: -0.97261, alpha: 0.42124, time: 32.62272
[CW] ---------------------------
[CW] ---- Iteration:   679 ----
[CW] collect: return: 847.24841, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 21.71044, qf2_loss: 21.97822, policy_loss: -627.38130, policy_entropy: -1.01618, alpha: 0.41998, time: 32.57084
[CW] ---------------------------
[CW] ---- Iteration:   680 ----
[CW] collect: return: 841.93236, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 23.47635, qf2_loss: 23.34838, policy_loss: -631.12029, policy_entropy: -0.98171, alpha: 0.41946, time: 32.54800
[CW] eval: return: 844.76369, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   681 ----
[CW] collect: return: 847.25459, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 21.36418, qf2_loss: 21.42717, policy_loss: -631.57558, policy_entropy: -0.96168, alpha: 0.41694, time: 35.21702
[CW] ---------------------------
[CW] ---- Iteration:   682 ----
[CW] collect: return: 839.62114, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 22.39607, qf2_loss: 21.94103, policy_loss: -630.59359, policy_entropy: -0.98916, alpha: 0.41288, time: 32.76282
[CW] ---------------------------
[CW] ---- Iteration:   683 ----
[CW] collect: return: 844.81062, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 22.73209, qf2_loss: 22.70184, policy_loss: -631.31665, policy_entropy: -1.01111, alpha: 0.41321, time: 32.74367
[CW] ---------------------------
[CW] ---- Iteration:   684 ----
[CW] collect: return: 844.29192, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 22.73811, qf2_loss: 22.88747, policy_loss: -629.93306, policy_entropy: -1.00822, alpha: 0.41402, time: 33.06662
[CW] ---------------------------
[CW] ---- Iteration:   685 ----
[CW] collect: return: 843.67558, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 20.93872, qf2_loss: 21.03247, policy_loss: -631.27746, policy_entropy: -0.98921, alpha: 0.41459, time: 32.79552
[CW] ---------------------------
[CW] ---- Iteration:   686 ----
[CW] collect: return: 849.45889, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 20.51700, qf2_loss: 20.28601, policy_loss: -632.00755, policy_entropy: -1.00133, alpha: 0.41339, time: 32.39384
[CW] ---------------------------
[CW] ---- Iteration:   687 ----
[CW] collect: return: 841.11281, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 21.56428, qf2_loss: 21.55944, policy_loss: -634.24570, policy_entropy: -0.98820, alpha: 0.41363, time: 32.54473
[CW] ---------------------------
[CW] ---- Iteration:   688 ----
[CW] collect: return: 848.30898, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 19.86226, qf2_loss: 19.84847, policy_loss: -634.76458, policy_entropy: -0.97774, alpha: 0.41103, time: 32.62046
[CW] ---------------------------
[CW] ---- Iteration:   689 ----
[CW] collect: return: 844.04800, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 19.36778, qf2_loss: 19.38548, policy_loss: -635.68858, policy_entropy: -0.97784, alpha: 0.40901, time: 32.62244
[CW] ---------------------------
[CW] ---- Iteration:   690 ----
[CW] collect: return: 847.76995, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 20.47797, qf2_loss: 20.08644, policy_loss: -633.54777, policy_entropy: -0.99976, alpha: 0.40751, time: 33.06557
[CW] ---------------------------
[CW] ---- Iteration:   691 ----
[CW] collect: return: 836.70553, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 23.64726, qf2_loss: 23.69471, policy_loss: -631.70976, policy_entropy: -1.00710, alpha: 0.40724, time: 32.48751
[CW] ---------------------------
[CW] ---- Iteration:   692 ----
[CW] collect: return: 846.97967, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 18.44967, qf2_loss: 18.52053, policy_loss: -635.74020, policy_entropy: -1.00794, alpha: 0.40916, time: 33.01239
[CW] ---------------------------
[CW] ---- Iteration:   693 ----
[CW] collect: return: 834.91685, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 22.29730, qf2_loss: 22.64388, policy_loss: -636.66910, policy_entropy: -0.98551, alpha: 0.40891, time: 32.62594
[CW] ---------------------------
[CW] ---- Iteration:   694 ----
[CW] collect: return: 842.91856, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 33.16328, qf2_loss: 33.06019, policy_loss: -634.38827, policy_entropy: -1.00906, alpha: 0.40811, time: 32.49419
[CW] ---------------------------
[CW] ---- Iteration:   695 ----
[CW] collect: return: 825.68273, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 52.80101, qf2_loss: 52.82354, policy_loss: -632.57428, policy_entropy: -1.01552, alpha: 0.40959, time: 32.57274
[CW] ---------------------------
[CW] ---- Iteration:   696 ----
[CW] collect: return: 850.07016, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 27.95808, qf2_loss: 27.66329, policy_loss: -637.68238, policy_entropy: -0.97028, alpha: 0.40879, time: 32.53269
[CW] ---------------------------
[CW] ---- Iteration:   697 ----
[CW] collect: return: 844.82801, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 18.94174, qf2_loss: 18.87327, policy_loss: -638.68670, policy_entropy: -0.96365, alpha: 0.40481, time: 32.79839
[CW] ---------------------------
[CW] ---- Iteration:   698 ----
[CW] collect: return: 849.26360, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 18.18727, qf2_loss: 18.15659, policy_loss: -636.27134, policy_entropy: -0.99861, alpha: 0.40259, time: 32.65647
[CW] ---------------------------
[CW] ---- Iteration:   699 ----
[CW] collect: return: 836.30343, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 19.48497, qf2_loss: 19.72112, policy_loss: -636.58180, policy_entropy: -0.98818, alpha: 0.40233, time: 32.69626
[CW] ---------------------------
[CW] ---- Iteration:   700 ----
[CW] collect: return: 842.83089, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 16.57490, qf2_loss: 16.63228, policy_loss: -637.37539, policy_entropy: -1.00983, alpha: 0.40240, time: 32.48414
[CW] eval: return: 843.23032, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   701 ----
[CW] collect: return: 838.60483, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 17.74029, qf2_loss: 17.78218, policy_loss: -640.01504, policy_entropy: -0.97962, alpha: 0.40172, time: 32.40651
[CW] ---------------------------
[CW] ---- Iteration:   702 ----
[CW] collect: return: 842.12551, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 18.42872, qf2_loss: 18.43514, policy_loss: -639.84044, policy_entropy: -0.97806, alpha: 0.39900, time: 32.75861
[CW] ---------------------------
[CW] ---- Iteration:   703 ----
[CW] collect: return: 844.26261, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 19.50193, qf2_loss: 19.63441, policy_loss: -640.69625, policy_entropy: -0.97173, alpha: 0.39642, time: 32.40981
[CW] ---------------------------
[CW] ---- Iteration:   704 ----
[CW] collect: return: 843.36325, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 18.02993, qf2_loss: 18.15355, policy_loss: -640.04237, policy_entropy: -0.97618, alpha: 0.39385, time: 32.50442
[CW] ---------------------------
[CW] ---- Iteration:   705 ----
[CW] collect: return: 844.53218, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 17.21117, qf2_loss: 16.95370, policy_loss: -643.74708, policy_entropy: -0.96953, alpha: 0.39142, time: 32.56894
[CW] ---------------------------
[CW] ---- Iteration:   706 ----
[CW] collect: return: 849.94375, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 17.47833, qf2_loss: 17.59218, policy_loss: -638.69659, policy_entropy: -1.00127, alpha: 0.38960, time: 32.70149
[CW] ---------------------------
[CW] ---- Iteration:   707 ----
[CW] collect: return: 847.17259, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 20.35344, qf2_loss: 20.18855, policy_loss: -640.11132, policy_entropy: -1.00118, alpha: 0.38899, time: 32.57914
[CW] ---------------------------
[CW] ---- Iteration:   708 ----
[CW] collect: return: 824.86508, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 23.70624, qf2_loss: 23.50325, policy_loss: -640.09292, policy_entropy: -1.00996, alpha: 0.38966, time: 32.57635
[CW] ---------------------------
[CW] ---- Iteration:   709 ----
[CW] collect: return: 841.16200, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 21.55504, qf2_loss: 21.69932, policy_loss: -641.39928, policy_entropy: -0.99311, alpha: 0.38967, time: 32.46566
[CW] ---------------------------
[CW] ---- Iteration:   710 ----
[CW] collect: return: 841.90685, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 19.38506, qf2_loss: 19.53124, policy_loss: -641.69311, policy_entropy: -1.00186, alpha: 0.38966, time: 32.58489
[CW] ---------------------------
[CW] ---- Iteration:   711 ----
[CW] collect: return: 682.74575, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 18.94415, qf2_loss: 18.83919, policy_loss: -645.08382, policy_entropy: -0.97503, alpha: 0.38901, time: 32.49989
[CW] ---------------------------
[CW] ---- Iteration:   712 ----
[CW] collect: return: 779.15584, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 20.07222, qf2_loss: 19.97028, policy_loss: -640.82548, policy_entropy: -1.02152, alpha: 0.38858, time: 32.55628
[CW] ---------------------------
[CW] ---- Iteration:   713 ----
[CW] collect: return: 844.54662, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 18.30657, qf2_loss: 18.50445, policy_loss: -643.31382, policy_entropy: -1.00233, alpha: 0.39025, time: 32.59789
[CW] ---------------------------
[CW] ---- Iteration:   714 ----
[CW] collect: return: 841.30928, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 20.60777, qf2_loss: 20.49308, policy_loss: -644.23058, policy_entropy: -0.99825, alpha: 0.38997, time: 32.59261
[CW] ---------------------------
[CW] ---- Iteration:   715 ----
[CW] collect: return: 611.04962, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 24.32174, qf2_loss: 24.41799, policy_loss: -640.98221, policy_entropy: -1.03780, alpha: 0.39124, time: 32.46124
[CW] ---------------------------
[CW] ---- Iteration:   716 ----
[CW] collect: return: 848.38651, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 21.37271, qf2_loss: 21.17902, policy_loss: -642.85675, policy_entropy: -1.00420, alpha: 0.39388, time: 32.53991
[CW] ---------------------------
[CW] ---- Iteration:   717 ----
[CW] collect: return: 842.62601, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 23.11700, qf2_loss: 22.49186, policy_loss: -645.07807, policy_entropy: -1.00411, alpha: 0.39400, time: 32.57213
[CW] ---------------------------
[CW] ---- Iteration:   718 ----
[CW] collect: return: 845.66001, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 21.45729, qf2_loss: 21.83213, policy_loss: -645.03708, policy_entropy: -0.99493, alpha: 0.39396, time: 32.67111
[CW] ---------------------------
[CW] ---- Iteration:   719 ----
[CW] collect: return: 844.65730, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 19.67485, qf2_loss: 19.74817, policy_loss: -646.00287, policy_entropy: -0.99388, alpha: 0.39356, time: 32.84765
[CW] ---------------------------
[CW] ---- Iteration:   720 ----
[CW] collect: return: 848.17493, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 17.98032, qf2_loss: 17.98669, policy_loss: -644.74759, policy_entropy: -1.00656, alpha: 0.39461, time: 32.64189
[CW] eval: return: 845.15241, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   721 ----
[CW] collect: return: 847.90759, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 20.13329, qf2_loss: 20.17286, policy_loss: -646.97483, policy_entropy: -0.99479, alpha: 0.39396, time: 32.71078
[CW] ---------------------------
[CW] ---- Iteration:   722 ----
[CW] collect: return: 840.26021, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 25.79078, qf2_loss: 25.98882, policy_loss: -648.59803, policy_entropy: -0.98473, alpha: 0.39199, time: 32.61565
[CW] ---------------------------
[CW] ---- Iteration:   723 ----
[CW] collect: return: 741.58324, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 38.40834, qf2_loss: 37.85261, policy_loss: -645.86048, policy_entropy: -1.01524, alpha: 0.39288, time: 32.52855
[CW] ---------------------------
[CW] ---- Iteration:   724 ----
[CW] collect: return: 840.69527, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 26.04979, qf2_loss: 26.19412, policy_loss: -650.28610, policy_entropy: -0.97738, alpha: 0.39222, time: 32.60080
[CW] ---------------------------
[CW] ---- Iteration:   725 ----
[CW] collect: return: 842.44937, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 21.83173, qf2_loss: 21.75966, policy_loss: -646.02444, policy_entropy: -1.00050, alpha: 0.39092, time: 32.62316
[CW] ---------------------------
[CW] ---- Iteration:   726 ----
[CW] collect: return: 835.03669, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 19.03041, qf2_loss: 18.96972, policy_loss: -649.96103, policy_entropy: -0.97544, alpha: 0.38987, time: 32.61401
[CW] ---------------------------
[CW] ---- Iteration:   727 ----
[CW] collect: return: 848.07935, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 17.18471, qf2_loss: 17.42089, policy_loss: -649.73294, policy_entropy: -0.97346, alpha: 0.38691, time: 32.59867
[CW] ---------------------------
[CW] ---- Iteration:   728 ----
[CW] collect: return: 844.24650, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 17.71109, qf2_loss: 17.73381, policy_loss: -649.86062, policy_entropy: -0.98684, alpha: 0.38481, time: 32.60546
[CW] ---------------------------
[CW] ---- Iteration:   729 ----
[CW] collect: return: 843.99190, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 19.90187, qf2_loss: 19.73089, policy_loss: -648.33317, policy_entropy: -1.00036, alpha: 0.38486, time: 32.57414
[CW] ---------------------------
[CW] ---- Iteration:   730 ----
[CW] collect: return: 841.58277, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 21.46833, qf2_loss: 21.58294, policy_loss: -650.69333, policy_entropy: -1.01220, alpha: 0.38474, time: 32.59876
[CW] ---------------------------
[CW] ---- Iteration:   731 ----
[CW] collect: return: 846.31917, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 21.80582, qf2_loss: 21.36380, policy_loss: -647.28211, policy_entropy: -1.01511, alpha: 0.38623, time: 33.13179
[CW] ---------------------------
[CW] ---- Iteration:   732 ----
[CW] collect: return: 848.69881, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 17.48787, qf2_loss: 17.63946, policy_loss: -649.29775, policy_entropy: -0.99791, alpha: 0.38766, time: 32.67726
[CW] ---------------------------
[CW] ---- Iteration:   733 ----
[CW] collect: return: 840.27153, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 17.49535, qf2_loss: 17.92492, policy_loss: -650.89746, policy_entropy: -0.99464, alpha: 0.38689, time: 32.43296
[CW] ---------------------------
[CW] ---- Iteration:   734 ----
[CW] collect: return: 841.73643, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 17.76183, qf2_loss: 17.78054, policy_loss: -652.49555, policy_entropy: -0.99213, alpha: 0.38551, time: 32.65250
[CW] ---------------------------
[CW] ---- Iteration:   735 ----
[CW] collect: return: 843.50244, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 18.99675, qf2_loss: 19.00709, policy_loss: -652.24201, policy_entropy: -0.97768, alpha: 0.38538, time: 32.53830
[CW] ---------------------------
[CW] ---- Iteration:   736 ----
[CW] collect: return: 831.69335, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 17.14464, qf2_loss: 16.86959, policy_loss: -651.05441, policy_entropy: -1.00354, alpha: 0.38417, time: 32.69645
[CW] ---------------------------
[CW] ---- Iteration:   737 ----
[CW] collect: return: 835.97897, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 17.91345, qf2_loss: 17.75740, policy_loss: -652.49063, policy_entropy: -0.99889, alpha: 0.38340, time: 32.66357
[CW] ---------------------------
[CW] ---- Iteration:   738 ----
[CW] collect: return: 849.02398, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 18.37727, qf2_loss: 18.27139, policy_loss: -649.96977, policy_entropy: -1.01091, alpha: 0.38426, time: 32.55414
[CW] ---------------------------
[CW] ---- Iteration:   739 ----
[CW] collect: return: 829.52727, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 19.31743, qf2_loss: 19.40837, policy_loss: -651.04847, policy_entropy: -1.00505, alpha: 0.38481, time: 32.65061
[CW] ---------------------------
[CW] ---- Iteration:   740 ----
[CW] collect: return: 843.00351, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 20.65577, qf2_loss: 20.55249, policy_loss: -651.28353, policy_entropy: -1.00646, alpha: 0.38572, time: 33.18635
[CW] eval: return: 845.34548, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   741 ----
[CW] collect: return: 847.94415, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 18.55243, qf2_loss: 18.57614, policy_loss: -653.31226, policy_entropy: -0.99057, alpha: 0.38595, time: 32.84775
[CW] ---------------------------
[CW] ---- Iteration:   742 ----
[CW] collect: return: 843.47827, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 17.81397, qf2_loss: 17.53545, policy_loss: -654.26784, policy_entropy: -0.98366, alpha: 0.38443, time: 32.68872
[CW] ---------------------------
[CW] ---- Iteration:   743 ----
[CW] collect: return: 843.35946, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 24.45074, qf2_loss: 24.64254, policy_loss: -649.69377, policy_entropy: -1.04673, alpha: 0.38571, time: 32.49109
[CW] ---------------------------
[CW] ---- Iteration:   744 ----
[CW] collect: return: 838.40066, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 20.95575, qf2_loss: 20.77677, policy_loss: -658.13744, policy_entropy: -0.95781, alpha: 0.38598, time: 32.57685
[CW] ---------------------------
[CW] ---- Iteration:   745 ----
[CW] collect: return: 845.59343, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 18.77629, qf2_loss: 18.92094, policy_loss: -654.77861, policy_entropy: -0.99585, alpha: 0.38332, time: 32.62378
[CW] ---------------------------
[CW] ---- Iteration:   746 ----
[CW] collect: return: 845.82339, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 19.06474, qf2_loss: 18.99482, policy_loss: -653.94106, policy_entropy: -0.99984, alpha: 0.38316, time: 32.81931
[CW] ---------------------------
[CW] ---- Iteration:   747 ----
[CW] collect: return: 841.72131, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 19.11156, qf2_loss: 19.59908, policy_loss: -656.26374, policy_entropy: -0.98207, alpha: 0.38293, time: 32.33500
[CW] ---------------------------
[CW] ---- Iteration:   748 ----
[CW] collect: return: 823.08713, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 24.29921, qf2_loss: 24.05743, policy_loss: -657.28311, policy_entropy: -0.96629, alpha: 0.37941, time: 32.50783
[CW] ---------------------------
[CW] ---- Iteration:   749 ----
[CW] collect: return: 840.78025, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 19.14033, qf2_loss: 19.07565, policy_loss: -655.01124, policy_entropy: -0.99682, alpha: 0.37744, time: 32.67741
[CW] ---------------------------
[CW] ---- Iteration:   750 ----
[CW] collect: return: 848.67749, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 16.13649, qf2_loss: 16.23847, policy_loss: -657.24156, policy_entropy: -0.98227, alpha: 0.37749, time: 32.52989
[CW] ---------------------------
[CW] ---- Iteration:   751 ----
[CW] collect: return: 846.95067, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 17.65121, qf2_loss: 17.62108, policy_loss: -657.60405, policy_entropy: -0.98561, alpha: 0.37639, time: 32.74838
[CW] ---------------------------
[CW] ---- Iteration:   752 ----
[CW] collect: return: 844.95035, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 17.02739, qf2_loss: 16.89048, policy_loss: -657.79714, policy_entropy: -0.97521, alpha: 0.37440, time: 32.41616
[CW] ---------------------------
[CW] ---- Iteration:   753 ----
[CW] collect: return: 840.64266, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 16.22518, qf2_loss: 16.13891, policy_loss: -657.51701, policy_entropy: -0.99463, alpha: 0.37234, time: 32.72588
[CW] ---------------------------
[CW] ---- Iteration:   754 ----
[CW] collect: return: 849.57365, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 18.07101, qf2_loss: 18.16917, policy_loss: -656.56807, policy_entropy: -1.00676, alpha: 0.37195, time: 32.97039
[CW] ---------------------------
[CW] ---- Iteration:   755 ----
[CW] collect: return: 833.27849, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 20.94712, qf2_loss: 20.82344, policy_loss: -658.79669, policy_entropy: -0.98545, alpha: 0.37210, time: 32.46763
[CW] ---------------------------
[CW] ---- Iteration:   756 ----
[CW] collect: return: 851.01866, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 20.63611, qf2_loss: 20.75139, policy_loss: -657.34603, policy_entropy: -1.00765, alpha: 0.37157, time: 32.63769
[CW] ---------------------------
[CW] ---- Iteration:   757 ----
[CW] collect: return: 835.38043, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 18.00331, qf2_loss: 17.85111, policy_loss: -661.22385, policy_entropy: -0.97376, alpha: 0.37061, time: 32.65286
[CW] ---------------------------
[CW] ---- Iteration:   758 ----
[CW] collect: return: 843.76906, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 18.72670, qf2_loss: 18.89355, policy_loss: -658.70520, policy_entropy: -0.98760, alpha: 0.36949, time: 32.77361
[CW] ---------------------------
[CW] ---- Iteration:   759 ----
[CW] collect: return: 843.01695, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 16.87898, qf2_loss: 17.21429, policy_loss: -660.11636, policy_entropy: -0.98261, alpha: 0.36778, time: 32.56888
[CW] ---------------------------
[CW] ---- Iteration:   760 ----
[CW] collect: return: 844.89419, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 20.97893, qf2_loss: 20.59824, policy_loss: -660.59576, policy_entropy: -0.99211, alpha: 0.36666, time: 32.60839
[CW] eval: return: 843.62876, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   761 ----
[CW] collect: return: 849.58136, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 23.70854, qf2_loss: 23.50555, policy_loss: -662.83523, policy_entropy: -0.96762, alpha: 0.36483, time: 32.73395
[CW] ---------------------------
[CW] ---- Iteration:   762 ----
[CW] collect: return: 847.06086, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 20.60979, qf2_loss: 20.55518, policy_loss: -660.48211, policy_entropy: -1.00591, alpha: 0.36384, time: 32.56996
[CW] ---------------------------
[CW] ---- Iteration:   763 ----
[CW] collect: return: 840.50762, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 17.50480, qf2_loss: 17.57472, policy_loss: -659.55195, policy_entropy: -0.99849, alpha: 0.36404, time: 32.62513
[CW] ---------------------------
[CW] ---- Iteration:   764 ----
[CW] collect: return: 844.34094, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 17.23875, qf2_loss: 17.27338, policy_loss: -660.99674, policy_entropy: -0.98817, alpha: 0.36281, time: 32.58567
[CW] ---------------------------
[CW] ---- Iteration:   765 ----
[CW] collect: return: 839.77343, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 17.35127, qf2_loss: 17.07826, policy_loss: -661.84545, policy_entropy: -1.00231, alpha: 0.36249, time: 32.72962
[CW] ---------------------------
[CW] ---- Iteration:   766 ----
[CW] collect: return: 845.20164, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 18.81966, qf2_loss: 19.26423, policy_loss: -660.56181, policy_entropy: -1.00576, alpha: 0.36252, time: 32.62303
[CW] ---------------------------
[CW] ---- Iteration:   767 ----
[CW] collect: return: 846.56229, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 17.41068, qf2_loss: 17.65591, policy_loss: -661.01822, policy_entropy: -0.99386, alpha: 0.36258, time: 32.57147
[CW] ---------------------------
[CW] ---- Iteration:   768 ----
[CW] collect: return: 843.82937, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 20.58517, qf2_loss: 20.32495, policy_loss: -663.03300, policy_entropy: -0.97314, alpha: 0.36182, time: 32.42859
[CW] ---------------------------
[CW] ---- Iteration:   769 ----
[CW] collect: return: 838.14379, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 17.79203, qf2_loss: 17.79411, policy_loss: -663.26135, policy_entropy: -0.98304, alpha: 0.35924, time: 32.34017
[CW] ---------------------------
[CW] ---- Iteration:   770 ----
[CW] collect: return: 834.98748, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 20.17622, qf2_loss: 20.29641, policy_loss: -663.27557, policy_entropy: -1.00089, alpha: 0.35859, time: 32.96860
[CW] ---------------------------
[CW] ---- Iteration:   771 ----
[CW] collect: return: 847.42790, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 18.62355, qf2_loss: 18.49422, policy_loss: -660.05096, policy_entropy: -1.00906, alpha: 0.35876, time: 32.61068
[CW] ---------------------------
[CW] ---- Iteration:   772 ----
[CW] collect: return: 843.77330, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 19.74735, qf2_loss: 19.52145, policy_loss: -660.08394, policy_entropy: -1.00956, alpha: 0.35968, time: 32.34342
[CW] ---------------------------
[CW] ---- Iteration:   773 ----
[CW] collect: return: 839.97730, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 18.14689, qf2_loss: 18.37760, policy_loss: -663.25854, policy_entropy: -0.98503, alpha: 0.35989, time: 32.85543
[CW] ---------------------------
[CW] ---- Iteration:   774 ----
[CW] collect: return: 839.26674, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 18.78585, qf2_loss: 18.52586, policy_loss: -662.45598, policy_entropy: -1.00665, alpha: 0.35955, time: 32.57048
[CW] ---------------------------
[CW] ---- Iteration:   775 ----
[CW] collect: return: 847.15213, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 18.56060, qf2_loss: 18.76100, policy_loss: -665.91614, policy_entropy: -0.96275, alpha: 0.35764, time: 32.68838
[CW] ---------------------------
[CW] ---- Iteration:   776 ----
[CW] collect: return: 845.60562, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 18.65733, qf2_loss: 18.68308, policy_loss: -662.08775, policy_entropy: -1.00616, alpha: 0.35657, time: 32.55615
[CW] ---------------------------
[CW] ---- Iteration:   777 ----
[CW] collect: return: 837.95282, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 18.58745, qf2_loss: 18.55328, policy_loss: -666.51931, policy_entropy: -0.97050, alpha: 0.35521, time: 32.39932
[CW] ---------------------------
[CW] ---- Iteration:   778 ----
[CW] collect: return: 842.51051, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 18.02871, qf2_loss: 17.95320, policy_loss: -665.63365, policy_entropy: -0.98653, alpha: 0.35292, time: 33.02448
[CW] ---------------------------
[CW] ---- Iteration:   779 ----
[CW] collect: return: 843.36129, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 19.90977, qf2_loss: 19.65942, policy_loss: -662.25389, policy_entropy: -1.01350, alpha: 0.35337, time: 32.61126
[CW] ---------------------------
[CW] ---- Iteration:   780 ----
[CW] collect: return: 837.34848, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 18.09798, qf2_loss: 17.92524, policy_loss: -666.20952, policy_entropy: -0.99443, alpha: 0.35382, time: 32.95597
[CW] eval: return: 842.97355, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   781 ----
[CW] collect: return: 839.47426, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 20.43302, qf2_loss: 20.57173, policy_loss: -667.46332, policy_entropy: -0.96852, alpha: 0.35215, time: 32.53433
[CW] ---------------------------
[CW] ---- Iteration:   782 ----
[CW] collect: return: 846.11259, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 18.00290, qf2_loss: 18.01856, policy_loss: -666.71927, policy_entropy: -0.98432, alpha: 0.34963, time: 32.70452
[CW] ---------------------------
[CW] ---- Iteration:   783 ----
[CW] collect: return: 840.27246, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 18.47005, qf2_loss: 18.25206, policy_loss: -668.00094, policy_entropy: -0.97131, alpha: 0.34831, time: 32.72081
[CW] ---------------------------
[CW] ---- Iteration:   784 ----
[CW] collect: return: 843.31640, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 18.41983, qf2_loss: 18.61580, policy_loss: -670.31820, policy_entropy: -0.97135, alpha: 0.34551, time: 32.53434
[CW] ---------------------------
[CW] ---- Iteration:   785 ----
[CW] collect: return: 845.41383, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 18.10732, qf2_loss: 17.96149, policy_loss: -665.92607, policy_entropy: -1.01591, alpha: 0.34506, time: 32.80061
[CW] ---------------------------
[CW] ---- Iteration:   786 ----
[CW] collect: return: 844.85167, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 17.70315, qf2_loss: 17.55476, policy_loss: -667.65625, policy_entropy: -0.98865, alpha: 0.34514, time: 32.60597
[CW] ---------------------------
[CW] ---- Iteration:   787 ----
[CW] collect: return: 835.33696, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 18.05783, qf2_loss: 18.39328, policy_loss: -668.04581, policy_entropy: -0.99822, alpha: 0.34489, time: 32.68246
[CW] ---------------------------
[CW] ---- Iteration:   788 ----
[CW] collect: return: 837.73798, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 17.78065, qf2_loss: 17.61983, policy_loss: -667.75992, policy_entropy: -0.98330, alpha: 0.34426, time: 32.40566
[CW] ---------------------------
[CW] ---- Iteration:   789 ----
[CW] collect: return: 848.32311, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 18.45796, qf2_loss: 18.38444, policy_loss: -668.24012, policy_entropy: -0.97086, alpha: 0.34197, time: 32.63075
[CW] ---------------------------
[CW] ---- Iteration:   790 ----
[CW] collect: return: 567.22778, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 18.37350, qf2_loss: 18.47636, policy_loss: -669.15100, policy_entropy: -0.98925, alpha: 0.33984, time: 32.72026
[CW] ---------------------------
[CW] ---- Iteration:   791 ----
[CW] collect: return: 838.34766, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 16.83270, qf2_loss: 16.98570, policy_loss: -667.20057, policy_entropy: -1.02669, alpha: 0.34109, time: 32.52567
[CW] ---------------------------
[CW] ---- Iteration:   792 ----
[CW] collect: return: 845.73569, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 20.05948, qf2_loss: 19.80836, policy_loss: -666.53777, policy_entropy: -1.02425, alpha: 0.34311, time: 32.56489
[CW] ---------------------------
[CW] ---- Iteration:   793 ----
[CW] collect: return: 837.42119, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 26.18218, qf2_loss: 26.17280, policy_loss: -669.22556, policy_entropy: -0.99323, alpha: 0.34439, time: 32.64741
[CW] ---------------------------
[CW] ---- Iteration:   794 ----
[CW] collect: return: 841.64951, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 27.11439, qf2_loss: 26.67435, policy_loss: -669.40107, policy_entropy: -0.98148, alpha: 0.34276, time: 32.70307
[CW] ---------------------------
[CW] ---- Iteration:   795 ----
[CW] collect: return: 837.83701, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 21.46985, qf2_loss: 21.68626, policy_loss: -668.66732, policy_entropy: -1.00372, alpha: 0.34224, time: 32.78809
[CW] ---------------------------
[CW] ---- Iteration:   796 ----
[CW] collect: return: 846.14837, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 16.46029, qf2_loss: 16.53203, policy_loss: -668.22778, policy_entropy: -1.00309, alpha: 0.34264, time: 32.64908
[CW] ---------------------------
[CW] ---- Iteration:   797 ----
[CW] collect: return: 847.49945, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 15.98924, qf2_loss: 16.18784, policy_loss: -671.45575, policy_entropy: -0.98306, alpha: 0.34137, time: 35.68512
[CW] ---------------------------
[CW] ---- Iteration:   798 ----
[CW] collect: return: 827.47764, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 21.84195, qf2_loss: 21.50273, policy_loss: -669.11890, policy_entropy: -1.00562, alpha: 0.34059, time: 32.50083
[CW] ---------------------------
[CW] ---- Iteration:   799 ----
[CW] collect: return: 836.58516, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 20.50372, qf2_loss: 20.68834, policy_loss: -670.38582, policy_entropy: -0.97905, alpha: 0.34099, time: 32.81142
[CW] ---------------------------
[CW] ---- Iteration:   800 ----
[CW] collect: return: 846.57214, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 15.76358, qf2_loss: 15.81252, policy_loss: -670.40115, policy_entropy: -0.99712, alpha: 0.33954, time: 32.62563
[CW] eval: return: 839.48511, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   801 ----
[CW] collect: return: 840.47485, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 15.59734, qf2_loss: 15.71067, policy_loss: -670.75815, policy_entropy: -0.98713, alpha: 0.33927, time: 32.54215
[CW] ---------------------------
[CW] ---- Iteration:   802 ----
[CW] collect: return: 843.93187, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 16.11063, qf2_loss: 16.09544, policy_loss: -672.42619, policy_entropy: -0.98238, alpha: 0.33760, time: 32.52798
[CW] ---------------------------
[CW] ---- Iteration:   803 ----
[CW] collect: return: 833.84001, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 15.97727, qf2_loss: 15.93884, policy_loss: -672.93543, policy_entropy: -1.00285, alpha: 0.33705, time: 32.64969
[CW] ---------------------------
[CW] ---- Iteration:   804 ----
[CW] collect: return: 842.06607, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 18.55501, qf2_loss: 18.01244, policy_loss: -668.23859, policy_entropy: -1.02904, alpha: 0.33836, time: 32.64117
[CW] ---------------------------
[CW] ---- Iteration:   805 ----
[CW] collect: return: 821.20136, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 18.44360, qf2_loss: 18.58377, policy_loss: -671.11118, policy_entropy: -1.00524, alpha: 0.34018, time: 32.68210
[CW] ---------------------------
[CW] ---- Iteration:   806 ----
[CW] collect: return: 843.52496, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 16.40968, qf2_loss: 16.59304, policy_loss: -672.89685, policy_entropy: -0.99170, alpha: 0.33945, time: 32.78965
[CW] ---------------------------
[CW] ---- Iteration:   807 ----
[CW] collect: return: 749.59196, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 17.97209, qf2_loss: 18.12574, policy_loss: -669.74760, policy_entropy: -1.01907, alpha: 0.34032, time: 32.50172
[CW] ---------------------------
[CW] ---- Iteration:   808 ----
[CW] collect: return: 844.10074, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 17.35783, qf2_loss: 17.32242, policy_loss: -669.00195, policy_entropy: -1.02795, alpha: 0.34148, time: 32.53323
[CW] ---------------------------
[CW] ---- Iteration:   809 ----
[CW] collect: return: 832.38804, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 15.82058, qf2_loss: 15.74672, policy_loss: -673.09420, policy_entropy: -0.99976, alpha: 0.34357, time: 32.67855
[CW] ---------------------------
[CW] ---- Iteration:   810 ----
[CW] collect: return: 846.22095, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 16.58102, qf2_loss: 16.54642, policy_loss: -674.95889, policy_entropy: -0.98141, alpha: 0.34250, time: 32.60201
[CW] ---------------------------
[CW] ---- Iteration:   811 ----
[CW] collect: return: 680.50866, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 18.43163, qf2_loss: 18.44381, policy_loss: -673.60908, policy_entropy: -0.99176, alpha: 0.34105, time: 32.52319
[CW] ---------------------------
[CW] ---- Iteration:   812 ----
[CW] collect: return: 848.55443, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 18.69452, qf2_loss: 18.75860, policy_loss: -675.12671, policy_entropy: -0.98193, alpha: 0.34010, time: 32.56190
[CW] ---------------------------
[CW] ---- Iteration:   813 ----
[CW] collect: return: 831.01615, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 17.55190, qf2_loss: 17.44006, policy_loss: -675.61105, policy_entropy: -0.99333, alpha: 0.33881, time: 32.55238
[CW] ---------------------------
[CW] ---- Iteration:   814 ----
[CW] collect: return: 838.31735, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 23.54266, qf2_loss: 23.27818, policy_loss: -674.04847, policy_entropy: -1.00713, alpha: 0.33861, time: 32.59599
[CW] ---------------------------
[CW] ---- Iteration:   815 ----
[CW] collect: return: 838.71332, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 22.39065, qf2_loss: 22.36604, policy_loss: -675.74995, policy_entropy: -0.98704, alpha: 0.33891, time: 32.73784
[CW] ---------------------------
[CW] ---- Iteration:   816 ----
[CW] collect: return: 832.98686, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 18.60502, qf2_loss: 18.50756, policy_loss: -672.27739, policy_entropy: -1.00321, alpha: 0.33873, time: 32.55140
[CW] ---------------------------
[CW] ---- Iteration:   817 ----
[CW] collect: return: 845.34956, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 16.69380, qf2_loss: 16.86861, policy_loss: -675.16909, policy_entropy: -0.99903, alpha: 0.33898, time: 32.58952
[CW] ---------------------------
[CW] ---- Iteration:   818 ----
[CW] collect: return: 837.14325, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 17.63829, qf2_loss: 17.72164, policy_loss: -673.83480, policy_entropy: -0.99734, alpha: 0.33878, time: 32.50239
[CW] ---------------------------
[CW] ---- Iteration:   819 ----
[CW] collect: return: 846.27774, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 21.36938, qf2_loss: 21.55825, policy_loss: -674.86154, policy_entropy: -0.99351, alpha: 0.33860, time: 32.58023
[CW] ---------------------------
[CW] ---- Iteration:   820 ----
[CW] collect: return: 609.19446, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 19.58673, qf2_loss: 19.40575, policy_loss: -674.49669, policy_entropy: -1.00012, alpha: 0.33799, time: 32.58610
[CW] eval: return: 834.09398, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   821 ----
[CW] collect: return: 843.35848, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 16.75194, qf2_loss: 16.83366, policy_loss: -676.14542, policy_entropy: -0.98304, alpha: 0.33773, time: 32.53034
[CW] ---------------------------
[CW] ---- Iteration:   822 ----
[CW] collect: return: 843.57196, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 16.40751, qf2_loss: 16.57376, policy_loss: -674.19221, policy_entropy: -0.99315, alpha: 0.33618, time: 32.81066
[CW] ---------------------------
[CW] ---- Iteration:   823 ----
[CW] collect: return: 845.36300, steps: 1000.00000, total_steps: 829000.00000
[CW] train: qf1_loss: 15.93627, qf2_loss: 15.78818, policy_loss: -676.32216, policy_entropy: -1.00779, alpha: 0.33612, time: 32.55476
[CW] ---------------------------
[CW] ---- Iteration:   824 ----
[CW] collect: return: 843.33116, steps: 1000.00000, total_steps: 830000.00000
[CW] train: qf1_loss: 15.81845, qf2_loss: 15.69890, policy_loss: -676.12557, policy_entropy: -1.00058, alpha: 0.33689, time: 32.52008
[CW] ---------------------------
[CW] ---- Iteration:   825 ----
[CW] collect: return: 843.74024, steps: 1000.00000, total_steps: 831000.00000
[CW] train: qf1_loss: 16.55559, qf2_loss: 16.57590, policy_loss: -675.39890, policy_entropy: -1.00505, alpha: 0.33653, time: 32.53359
[CW] ---------------------------
[CW] ---- Iteration:   826 ----
[CW] collect: return: 842.06818, steps: 1000.00000, total_steps: 832000.00000
[CW] train: qf1_loss: 17.81085, qf2_loss: 17.59428, policy_loss: -674.41991, policy_entropy: -1.00027, alpha: 0.33700, time: 32.34168
[CW] ---------------------------
[CW] ---- Iteration:   827 ----
[CW] collect: return: 840.00932, steps: 1000.00000, total_steps: 833000.00000
[CW] train: qf1_loss: 22.47524, qf2_loss: 22.46008, policy_loss: -676.03753, policy_entropy: -1.01589, alpha: 0.33721, time: 32.81553
[CW] ---------------------------
[CW] ---- Iteration:   828 ----
[CW] collect: return: 839.26688, steps: 1000.00000, total_steps: 834000.00000
[CW] train: qf1_loss: 31.50870, qf2_loss: 31.63280, policy_loss: -675.37861, policy_entropy: -1.03328, alpha: 0.33913, time: 32.29174
[CW] ---------------------------
[CW] ---- Iteration:   829 ----
[CW] collect: return: 836.91047, steps: 1000.00000, total_steps: 835000.00000
[CW] train: qf1_loss: 37.09485, qf2_loss: 37.73630, policy_loss: -678.01723, policy_entropy: -0.97476, alpha: 0.33989, time: 32.58054
[CW] ---------------------------
[CW] ---- Iteration:   830 ----
[CW] collect: return: 827.64810, steps: 1000.00000, total_steps: 836000.00000
[CW] train: qf1_loss: 21.05568, qf2_loss: 20.96111, policy_loss: -674.92119, policy_entropy: -0.98837, alpha: 0.33822, time: 32.51550
[CW] ---------------------------
[CW] ---- Iteration:   831 ----
[CW] collect: return: 841.09937, steps: 1000.00000, total_steps: 837000.00000
[CW] train: qf1_loss: 20.17823, qf2_loss: 20.09418, policy_loss: -676.33431, policy_entropy: -0.99266, alpha: 0.33763, time: 32.75342
[CW] ---------------------------
[CW] ---- Iteration:   832 ----
[CW] collect: return: 826.66167, steps: 1000.00000, total_steps: 838000.00000
[CW] train: qf1_loss: 19.75923, qf2_loss: 19.71121, policy_loss: -677.78983, policy_entropy: -0.98747, alpha: 0.33658, time: 32.71710
[CW] ---------------------------
[CW] ---- Iteration:   833 ----
[CW] collect: return: 840.92350, steps: 1000.00000, total_steps: 839000.00000
[CW] train: qf1_loss: 15.49141, qf2_loss: 15.33324, policy_loss: -678.68682, policy_entropy: -0.97424, alpha: 0.33542, time: 32.49767
[CW] ---------------------------
[CW] ---- Iteration:   834 ----
[CW] collect: return: 845.42066, steps: 1000.00000, total_steps: 840000.00000
[CW] train: qf1_loss: 15.26091, qf2_loss: 15.26224, policy_loss: -679.00068, policy_entropy: -0.97966, alpha: 0.33354, time: 32.96280
[CW] ---------------------------
[CW] ---- Iteration:   835 ----
[CW] collect: return: 846.09028, steps: 1000.00000, total_steps: 841000.00000
[CW] train: qf1_loss: 14.93674, qf2_loss: 15.00570, policy_loss: -679.85615, policy_entropy: -1.00085, alpha: 0.33225, time: 32.60082
[CW] ---------------------------
[CW] ---- Iteration:   836 ----
[CW] collect: return: 848.58227, steps: 1000.00000, total_steps: 842000.00000
[CW] train: qf1_loss: 16.81642, qf2_loss: 16.81324, policy_loss: -678.54768, policy_entropy: -1.00128, alpha: 0.33294, time: 32.65346
[CW] ---------------------------
[CW] ---- Iteration:   837 ----
[CW] collect: return: 847.47107, steps: 1000.00000, total_steps: 843000.00000
[CW] train: qf1_loss: 15.88290, qf2_loss: 15.77299, policy_loss: -676.18512, policy_entropy: -1.00207, alpha: 0.33259, time: 32.67252
[CW] ---------------------------
[CW] ---- Iteration:   838 ----
[CW] collect: return: 838.21065, steps: 1000.00000, total_steps: 844000.00000
[CW] train: qf1_loss: 18.83198, qf2_loss: 18.73863, policy_loss: -680.04080, policy_entropy: -0.98309, alpha: 0.33279, time: 33.98874
[CW] ---------------------------
[CW] ---- Iteration:   839 ----
[CW] collect: return: 839.58052, steps: 1000.00000, total_steps: 845000.00000
[CW] train: qf1_loss: 17.98830, qf2_loss: 17.66738, policy_loss: -677.92764, policy_entropy: -1.00737, alpha: 0.33183, time: 32.75379
[CW] ---------------------------
