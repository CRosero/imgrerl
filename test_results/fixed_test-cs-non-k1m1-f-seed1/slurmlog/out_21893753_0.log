Hostname: uc2n904.localdomain
Found slurm config: SLURM
Seeting default slurm config
/home/kit/stud/uprnr/imgrerl/test_results/fixed_test-cs-non-k1m1-f-seed1/fixed_test-cs-non-k1m1-f-seed1/fixed_test-cs-non-k1m1-f-seed1__env.ecartpole-swingup/log/rep_00
[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
False
params: 
 {'env': {'env': 'cartpole-swingup'}} 

additionalVars: 
 {'seed': 1, 'agent': {'image_augmentation_K': 1, 'image_augmentation_M': 1, 'image_augmentation_type': <AugmentationType.NONE: 1>, 'image_augmentation_actor_critic_same_aug': False}}
conf_dict: 
 --------Config-------- 
seed: 1
cuda_id: 0
Subconfig: env
	env: cartpole-swingup
	obs_type: image
Subconfig: agent
	algo_name: sac
	encoder: lstm
	action_embedding_size: 32
	observ_embedding_size: 0
	reward_embedding_size: 32
	rnn_hidden_size: 512
	dqn_layers: [512, 512, 512]
	policy_layers: [512, 512, 512]
	lr: 0.0003
	gamma: 0.99
	tau: 0.005
	entropy_alpha: 1.0
	image_augmentation_type: AugmentationType.NONE
	image_augmentation_K: 1
	image_augmentation_M: 1
	image_augmentation_actor_critic_same_aug: False
Subconfig: rl
	sampled_seq_len: 64
	buffer_size: 1000000.0
	batch_size: 32
	rollouts_per_iter: 1
	num_updates_per_iter: 100
	num_init_rollouts: 5
Subconfig: eval
	interval: 20
	num_rollouts: 20
---------------------- 
 
Init feature extractor:  1 ;  32 ;  <function relu at 0x14c8094567a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x14c8094567a0>
Init feature extractor:  1 ;  164 ;  <function relu at 0x14c8094567a0>
Critic: 
Critic_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (current_shortcut_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=164, bias=True)
  )
  (qf1): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
  (qf2): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
)
Init feature extractor:  1 ;  32 ;  <function relu at 0x14c8094567a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x14c8094567a0>
Actor: 
Actor_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (policy): TanhGaussianPolicy(
    (fc0): Linear(in_features=612, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
    (last_fc_log_std): Linear(in_features=512, out_features=1, bias=True)
  )
)
buffer RAM usage: 11.46 GB
Collecting train stats
[CW] ---- Iteration:     0 ----
[CW] collect: return: 70.11390, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.13108, qf2_loss: 1.12680, policy_loss: -2.62198, policy_entropy: 0.68279, alpha: 0.98504, time: 42.14456
[CW] eval: return: 115.67697, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     1 ----
[CW] collect: return: 117.13035, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.07120, qf2_loss: 0.07176, policy_loss: -3.04740, policy_entropy: 0.67957, alpha: 0.95628, time: 33.14063
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     2 ----
[CW] collect: return: 112.34075, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.05832, qf2_loss: 0.05949, policy_loss: -3.48924, policy_entropy: 0.67443, alpha: 0.92879, time: 33.04619
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     3 ----
[CW] collect: return: 134.79538, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.05746, qf2_loss: 0.05864, policy_loss: -4.01459, policy_entropy: 0.67039, alpha: 0.90248, time: 33.10416
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     4 ----
[CW] collect: return: 107.26021, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.07094, qf2_loss: 0.07168, policy_loss: -4.57493, policy_entropy: 0.66560, alpha: 0.87727, time: 33.26096
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     5 ----
[CW] collect: return: 107.76788, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.11253, qf2_loss: 0.11336, policy_loss: -5.25229, policy_entropy: 0.66106, alpha: 0.85309, time: 33.05044
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     6 ----
[CW] collect: return: 222.52574, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.12215, qf2_loss: 0.12386, policy_loss: -6.07503, policy_entropy: 0.65173, alpha: 0.82990, time: 33.09663
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     7 ----
[CW] collect: return: 142.91211, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.18215, qf2_loss: 0.18394, policy_loss: -6.77199, policy_entropy: 0.64902, alpha: 0.80765, time: 33.67505
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     8 ----
[CW] collect: return: 205.88347, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.19152, qf2_loss: 0.19392, policy_loss: -7.49249, policy_entropy: 0.64367, alpha: 0.78622, time: 33.48447
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     9 ----
[CW] collect: return: 134.41786, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.20001, qf2_loss: 0.20126, policy_loss: -8.33941, policy_entropy: 0.63337, alpha: 0.76561, time: 33.53841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    10 ----
[CW] collect: return: 109.72524, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.24678, qf2_loss: 0.24744, policy_loss: -8.89257, policy_entropy: 0.62973, alpha: 0.74580, time: 33.49605
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    11 ----
[CW] collect: return: 218.50030, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.27633, qf2_loss: 0.27704, policy_loss: -9.87362, policy_entropy: 0.61745, alpha: 0.72670, time: 33.45376
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    12 ----
[CW] collect: return: 90.17353, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.28325, qf2_loss: 0.28214, policy_loss: -10.30158, policy_entropy: 0.60704, alpha: 0.70835, time: 33.53819
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    13 ----
[CW] collect: return: 137.41847, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.33340, qf2_loss: 0.33487, policy_loss: -11.04438, policy_entropy: 0.59879, alpha: 0.69066, time: 33.53708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    14 ----
[CW] collect: return: 169.03940, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.32720, qf2_loss: 0.32814, policy_loss: -11.75602, policy_entropy: 0.59298, alpha: 0.67358, time: 33.52478
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    15 ----
[CW] collect: return: 229.75087, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.42147, qf2_loss: 0.41900, policy_loss: -12.66428, policy_entropy: 0.57849, alpha: 0.65707, time: 33.63941
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    16 ----
[CW] collect: return: 106.39514, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.47921, qf2_loss: 0.48029, policy_loss: -13.18270, policy_entropy: 0.56748, alpha: 0.64117, time: 33.36180
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    17 ----
[CW] collect: return: 88.50184, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.46715, qf2_loss: 0.47061, policy_loss: -13.94781, policy_entropy: 0.55524, alpha: 0.62581, time: 33.31103
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    18 ----
[CW] collect: return: 217.84588, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.50268, qf2_loss: 0.50162, policy_loss: -14.66497, policy_entropy: 0.54407, alpha: 0.61097, time: 33.31116
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    19 ----
[CW] collect: return: 182.80191, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.47002, qf2_loss: 0.46856, policy_loss: -15.29156, policy_entropy: 0.53391, alpha: 0.59660, time: 33.07501
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    20 ----
[CW] collect: return: 253.09226, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.80558, qf2_loss: 0.80532, policy_loss: -16.26103, policy_entropy: 0.51612, alpha: 0.58272, time: 32.65962
[CW] eval: return: 158.14595, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    21 ----
[CW] collect: return: 116.38499, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.48375, qf2_loss: 0.47881, policy_loss: -16.95541, policy_entropy: 0.49770, alpha: 0.56933, time: 32.87766
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    22 ----
[CW] collect: return: 183.91140, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.53181, qf2_loss: 0.53162, policy_loss: -17.90079, policy_entropy: 0.47810, alpha: 0.55639, time: 33.45099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    23 ----
[CW] collect: return: 225.82083, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.70433, qf2_loss: 0.70248, policy_loss: -18.68933, policy_entropy: 0.45545, alpha: 0.54393, time: 33.43488
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    24 ----
[CW] collect: return: 226.53125, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.57180, qf2_loss: 0.56922, policy_loss: -19.63832, policy_entropy: 0.41822, alpha: 0.53195, time: 33.40515
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    25 ----
[CW] collect: return: 144.68739, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.65726, qf2_loss: 0.65165, policy_loss: -20.58417, policy_entropy: 0.38170, alpha: 0.52049, time: 33.49152
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    26 ----
[CW] collect: return: 188.01603, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.95605, qf2_loss: 0.95237, policy_loss: -21.29513, policy_entropy: 0.36171, alpha: 0.50948, time: 33.61283
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    27 ----
[CW] collect: return: 203.54750, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.59340, qf2_loss: 0.58855, policy_loss: -22.31251, policy_entropy: 0.33387, alpha: 0.49880, time: 33.81352
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    28 ----
[CW] collect: return: 189.93709, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.60877, qf2_loss: 0.60035, policy_loss: -22.87822, policy_entropy: 0.30320, alpha: 0.48853, time: 33.63626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    29 ----
[CW] collect: return: 183.09090, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.67836, qf2_loss: 0.67638, policy_loss: -23.49170, policy_entropy: 0.27775, alpha: 0.47858, time: 33.55923
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    30 ----
[CW] collect: return: 225.51316, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.93237, qf2_loss: 0.92731, policy_loss: -24.80608, policy_entropy: 0.25018, alpha: 0.46898, time: 33.32782
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    31 ----
[CW] collect: return: 190.13198, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 1.01741, qf2_loss: 1.01204, policy_loss: -25.81847, policy_entropy: 0.22772, alpha: 0.45965, time: 32.60369
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    32 ----
[CW] collect: return: 274.99326, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.91305, qf2_loss: 0.91004, policy_loss: -26.60438, policy_entropy: 0.18748, alpha: 0.45066, time: 33.06552
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    33 ----
[CW] collect: return: 273.62051, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.96971, qf2_loss: 0.96978, policy_loss: -27.60081, policy_entropy: 0.15708, alpha: 0.44202, time: 33.09924
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    34 ----
[CW] collect: return: 230.40038, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 1.24440, qf2_loss: 1.22636, policy_loss: -28.66269, policy_entropy: 0.13546, alpha: 0.43363, time: 33.22552
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    35 ----
[CW] collect: return: 227.95173, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 1.02804, qf2_loss: 1.02535, policy_loss: -29.25121, policy_entropy: 0.08190, alpha: 0.42555, time: 33.59453
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    36 ----
[CW] collect: return: 213.76625, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.91808, qf2_loss: 0.91042, policy_loss: -30.51651, policy_entropy: 0.07062, alpha: 0.41778, time: 33.48772
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    37 ----
[CW] collect: return: 220.98008, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 1.13111, qf2_loss: 1.12129, policy_loss: -31.27524, policy_entropy: 0.04731, alpha: 0.41019, time: 33.37784
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    38 ----
[CW] collect: return: 248.71898, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 1.12952, qf2_loss: 1.12772, policy_loss: -32.44020, policy_entropy: 0.01517, alpha: 0.40279, time: 33.47930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    39 ----
[CW] collect: return: 288.61630, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 1.02826, qf2_loss: 1.03024, policy_loss: -33.07977, policy_entropy: -0.01270, alpha: 0.39561, time: 33.50598
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    40 ----
[CW] collect: return: 217.69100, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 1.09185, qf2_loss: 1.08146, policy_loss: -33.99415, policy_entropy: -0.04770, alpha: 0.38872, time: 33.24320
[CW] eval: return: 247.73394, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    41 ----
[CW] collect: return: 186.14123, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 1.05689, qf2_loss: 1.05697, policy_loss: -34.91489, policy_entropy: -0.05596, alpha: 0.38200, time: 33.35489
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    42 ----
[CW] collect: return: 230.24256, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 1.13474, qf2_loss: 1.13989, policy_loss: -35.50919, policy_entropy: -0.11169, alpha: 0.37546, time: 33.59755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    43 ----
[CW] collect: return: 232.28144, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 1.55198, qf2_loss: 1.54890, policy_loss: -37.15234, policy_entropy: -0.11383, alpha: 0.36917, time: 33.85093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    44 ----
[CW] collect: return: 202.27073, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 1.20805, qf2_loss: 1.19457, policy_loss: -37.48445, policy_entropy: -0.15450, alpha: 0.36301, time: 33.64413
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    45 ----
[CW] collect: return: 273.54831, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 1.26313, qf2_loss: 1.25519, policy_loss: -38.53401, policy_entropy: -0.18524, alpha: 0.35708, time: 33.71282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    46 ----
[CW] collect: return: 206.79540, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 1.23032, qf2_loss: 1.23033, policy_loss: -39.94063, policy_entropy: -0.20195, alpha: 0.35131, time: 33.64905
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    47 ----
[CW] collect: return: 256.54107, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 1.45130, qf2_loss: 1.43735, policy_loss: -40.87631, policy_entropy: -0.21490, alpha: 0.34565, time: 33.68671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    48 ----
[CW] collect: return: 253.59127, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 1.22056, qf2_loss: 1.20848, policy_loss: -41.50714, policy_entropy: -0.22960, alpha: 0.34004, time: 33.79778
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    49 ----
[CW] collect: return: 209.81557, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 1.25154, qf2_loss: 1.22531, policy_loss: -42.80890, policy_entropy: -0.23467, alpha: 0.33452, time: 33.52368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    50 ----
[CW] collect: return: 261.99784, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 1.35614, qf2_loss: 1.36429, policy_loss: -43.19868, policy_entropy: -0.25958, alpha: 0.32906, time: 33.71102
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    51 ----
[CW] collect: return: 202.83354, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 1.43058, qf2_loss: 1.42203, policy_loss: -44.53565, policy_entropy: -0.26992, alpha: 0.32378, time: 33.56167
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    52 ----
[CW] collect: return: 258.31184, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 1.31758, qf2_loss: 1.29690, policy_loss: -45.22003, policy_entropy: -0.29874, alpha: 0.31855, time: 33.37301
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    53 ----
[CW] collect: return: 196.07369, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 1.50598, qf2_loss: 1.50137, policy_loss: -46.33317, policy_entropy: -0.31082, alpha: 0.31347, time: 33.58256
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    54 ----
[CW] collect: return: 214.15965, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 1.80754, qf2_loss: 1.79580, policy_loss: -47.10676, policy_entropy: -0.31016, alpha: 0.30847, time: 33.65988
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    55 ----
[CW] collect: return: 338.72218, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 1.28540, qf2_loss: 1.28049, policy_loss: -48.30993, policy_entropy: -0.32930, alpha: 0.30345, time: 33.46009
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    56 ----
[CW] collect: return: 239.02048, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 1.64591, qf2_loss: 1.63423, policy_loss: -49.19147, policy_entropy: -0.35408, alpha: 0.29859, time: 33.77014
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    57 ----
[CW] collect: return: 198.67012, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 1.49032, qf2_loss: 1.48965, policy_loss: -49.93338, policy_entropy: -0.37139, alpha: 0.29387, time: 33.39840
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    58 ----
[CW] collect: return: 238.26412, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 1.47146, qf2_loss: 1.47520, policy_loss: -51.05247, policy_entropy: -0.38454, alpha: 0.28925, time: 33.37927
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    59 ----
[CW] collect: return: 220.82533, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 1.46006, qf2_loss: 1.45265, policy_loss: -51.92678, policy_entropy: -0.39711, alpha: 0.28469, time: 33.41878
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    60 ----
[CW] collect: return: 233.76147, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 1.95710, qf2_loss: 1.95942, policy_loss: -52.99344, policy_entropy: -0.43223, alpha: 0.28034, time: 33.88730
[CW] eval: return: 233.40942, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    61 ----
[CW] collect: return: 377.65813, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 1.56898, qf2_loss: 1.55910, policy_loss: -53.69677, policy_entropy: -0.45062, alpha: 0.27611, time: 33.63308
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    62 ----
[CW] collect: return: 286.27387, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 1.56663, qf2_loss: 1.55805, policy_loss: -54.84277, policy_entropy: -0.45214, alpha: 0.27191, time: 33.63528
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    63 ----
[CW] collect: return: 264.17359, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 2.07230, qf2_loss: 2.06635, policy_loss: -55.74627, policy_entropy: -0.47680, alpha: 0.26784, time: 33.65373
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    64 ----
[CW] collect: return: 203.60328, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 1.64378, qf2_loss: 1.62269, policy_loss: -56.50943, policy_entropy: -0.50980, alpha: 0.26392, time: 33.63780
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    65 ----
[CW] collect: return: 231.75225, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 1.86884, qf2_loss: 1.86893, policy_loss: -57.05840, policy_entropy: -0.52635, alpha: 0.26015, time: 33.57102
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    66 ----
[CW] collect: return: 268.99404, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 1.58994, qf2_loss: 1.58212, policy_loss: -58.42635, policy_entropy: -0.54067, alpha: 0.25649, time: 33.33798
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    67 ----
[CW] collect: return: 241.19487, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 1.66127, qf2_loss: 1.65938, policy_loss: -58.97990, policy_entropy: -0.56985, alpha: 0.25295, time: 33.63522
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    68 ----
[CW] collect: return: 258.65657, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 1.78299, qf2_loss: 1.76611, policy_loss: -59.85922, policy_entropy: -0.57105, alpha: 0.24948, time: 33.58979
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    69 ----
[CW] collect: return: 209.82295, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 2.03369, qf2_loss: 2.02854, policy_loss: -61.00741, policy_entropy: -0.56825, alpha: 0.24599, time: 33.16309
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    70 ----
[CW] collect: return: 204.77425, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 1.57418, qf2_loss: 1.55665, policy_loss: -61.49869, policy_entropy: -0.59852, alpha: 0.24258, time: 32.98005
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    71 ----
[CW] collect: return: 202.46297, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 1.79150, qf2_loss: 1.77803, policy_loss: -62.36028, policy_entropy: -0.61525, alpha: 0.23930, time: 33.40677
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    72 ----
[CW] collect: return: 207.87338, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 1.94406, qf2_loss: 1.91088, policy_loss: -63.06690, policy_entropy: -0.62019, alpha: 0.23609, time: 33.30473
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    73 ----
[CW] collect: return: 214.63352, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 1.76746, qf2_loss: 1.72927, policy_loss: -64.23287, policy_entropy: -0.62877, alpha: 0.23295, time: 33.40865
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    74 ----
[CW] collect: return: 303.15198, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 2.05614, qf2_loss: 2.02769, policy_loss: -65.00234, policy_entropy: -0.64327, alpha: 0.22973, time: 33.47944
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    75 ----
[CW] collect: return: 238.13813, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 1.58200, qf2_loss: 1.56454, policy_loss: -65.81503, policy_entropy: -0.66984, alpha: 0.22670, time: 33.70386
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    76 ----
[CW] collect: return: 209.89754, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 2.22151, qf2_loss: 2.21405, policy_loss: -66.61359, policy_entropy: -0.65668, alpha: 0.22377, time: 33.56954
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    77 ----
[CW] collect: return: 223.15655, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 1.58980, qf2_loss: 1.58350, policy_loss: -67.63702, policy_entropy: -0.67164, alpha: 0.22076, time: 33.56917
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    78 ----
[CW] collect: return: 206.00238, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 1.62088, qf2_loss: 1.60873, policy_loss: -68.24230, policy_entropy: -0.66958, alpha: 0.21775, time: 33.62824
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    79 ----
[CW] collect: return: 268.09341, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 1.69597, qf2_loss: 1.68307, policy_loss: -68.88003, policy_entropy: -0.68251, alpha: 0.21477, time: 33.56630
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    80 ----
[CW] collect: return: 235.78094, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 1.60557, qf2_loss: 1.59537, policy_loss: -69.80776, policy_entropy: -0.67383, alpha: 0.21177, time: 33.64630
[CW] eval: return: 281.04058, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    81 ----
[CW] collect: return: 228.70363, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 1.89318, qf2_loss: 1.89151, policy_loss: -70.69644, policy_entropy: -0.70817, alpha: 0.20886, time: 33.50974
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    82 ----
[CW] collect: return: 274.55318, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 2.09342, qf2_loss: 2.08244, policy_loss: -71.37133, policy_entropy: -0.71176, alpha: 0.20612, time: 33.51634
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    83 ----
[CW] collect: return: 308.40867, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 2.03516, qf2_loss: 2.01862, policy_loss: -72.05460, policy_entropy: -0.72974, alpha: 0.20344, time: 33.42812
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    84 ----
[CW] collect: return: 306.33165, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 1.88619, qf2_loss: 1.87513, policy_loss: -73.17959, policy_entropy: -0.73843, alpha: 0.20082, time: 33.50174
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    85 ----
[CW] collect: return: 264.35981, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 2.02492, qf2_loss: 2.01169, policy_loss: -74.29013, policy_entropy: -0.76924, alpha: 0.19833, time: 33.39915
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    86 ----
[CW] collect: return: 231.41687, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 2.13606, qf2_loss: 2.12216, policy_loss: -74.95145, policy_entropy: -0.75703, alpha: 0.19599, time: 33.48204
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    87 ----
[CW] collect: return: 324.86498, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 1.76248, qf2_loss: 1.74302, policy_loss: -75.81319, policy_entropy: -0.77411, alpha: 0.19355, time: 33.38562
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    88 ----
[CW] collect: return: 261.71808, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 1.64441, qf2_loss: 1.63181, policy_loss: -76.18777, policy_entropy: -0.81210, alpha: 0.19139, time: 32.99814
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    89 ----
[CW] collect: return: 220.69161, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 1.97254, qf2_loss: 1.95901, policy_loss: -77.07848, policy_entropy: -0.82584, alpha: 0.18941, time: 33.30184
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    90 ----
[CW] collect: return: 201.43843, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 2.08329, qf2_loss: 2.09638, policy_loss: -78.32479, policy_entropy: -0.83119, alpha: 0.18752, time: 33.48487
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    91 ----
[CW] collect: return: 209.85967, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 2.31339, qf2_loss: 2.30594, policy_loss: -78.68610, policy_entropy: -0.81342, alpha: 0.18557, time: 33.35718
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    92 ----
[CW] collect: return: 297.49077, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 2.24634, qf2_loss: 2.24079, policy_loss: -79.58808, policy_entropy: -0.84363, alpha: 0.18362, time: 32.87771
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    93 ----
[CW] collect: return: 238.80426, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 1.86999, qf2_loss: 1.87086, policy_loss: -80.54726, policy_entropy: -0.85202, alpha: 0.18179, time: 32.84958
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    94 ----
[CW] collect: return: 213.88831, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 1.81310, qf2_loss: 1.81178, policy_loss: -81.56908, policy_entropy: -0.86486, alpha: 0.18009, time: 33.35951
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    95 ----
[CW] collect: return: 222.92153, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 1.89066, qf2_loss: 1.86594, policy_loss: -82.24256, policy_entropy: -0.87283, alpha: 0.17845, time: 33.66209
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    96 ----
[CW] collect: return: 245.57720, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 1.72619, qf2_loss: 1.71293, policy_loss: -82.94738, policy_entropy: -0.87428, alpha: 0.17693, time: 33.62916
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    97 ----
[CW] collect: return: 363.94619, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 1.89803, qf2_loss: 1.90025, policy_loss: -83.59852, policy_entropy: -0.88065, alpha: 0.17533, time: 33.49758
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    98 ----
[CW] collect: return: 287.01398, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 2.05928, qf2_loss: 2.05009, policy_loss: -84.11649, policy_entropy: -0.88199, alpha: 0.17373, time: 33.63821
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    99 ----
[CW] collect: return: 214.62305, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 1.84307, qf2_loss: 1.83386, policy_loss: -84.52959, policy_entropy: -0.90761, alpha: 0.17229, time: 33.55560
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   100 ----
[CW] collect: return: 245.94805, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 2.24480, qf2_loss: 2.24480, policy_loss: -86.08586, policy_entropy: -0.89408, alpha: 0.17086, time: 33.33332
[CW] eval: return: 228.52989, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   101 ----
[CW] collect: return: 182.88077, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 2.17736, qf2_loss: 2.19555, policy_loss: -86.48145, policy_entropy: -0.89510, alpha: 0.16933, time: 33.40507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   102 ----
[CW] collect: return: 229.89383, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 2.40495, qf2_loss: 2.40207, policy_loss: -87.22619, policy_entropy: -0.91671, alpha: 0.16797, time: 33.80487
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   103 ----
[CW] collect: return: 220.56033, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 3.52658, qf2_loss: 3.51298, policy_loss: -88.00715, policy_entropy: -0.89505, alpha: 0.16654, time: 33.58402
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   104 ----
[CW] collect: return: 191.90936, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 2.05401, qf2_loss: 2.04627, policy_loss: -89.13472, policy_entropy: -0.92685, alpha: 0.16514, time: 33.50594
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   105 ----
[CW] collect: return: 305.00379, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 1.97258, qf2_loss: 1.96827, policy_loss: -89.61963, policy_entropy: -0.93983, alpha: 0.16403, time: 33.46343
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   106 ----
[CW] collect: return: 253.99018, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 1.87586, qf2_loss: 1.85328, policy_loss: -90.11044, policy_entropy: -0.93555, alpha: 0.16306, time: 33.24562
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   107 ----
[CW] collect: return: 254.23818, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 2.12029, qf2_loss: 2.10850, policy_loss: -91.16559, policy_entropy: -0.93206, alpha: 0.16184, time: 33.52743
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   108 ----
[CW] collect: return: 283.71280, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 2.43709, qf2_loss: 2.43645, policy_loss: -91.58917, policy_entropy: -0.96241, alpha: 0.16093, time: 33.29449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   109 ----
[CW] collect: return: 286.18196, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 2.07762, qf2_loss: 2.09165, policy_loss: -92.22307, policy_entropy: -0.95315, alpha: 0.16017, time: 33.24225
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   110 ----
[CW] collect: return: 270.69260, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 2.04938, qf2_loss: 2.05977, policy_loss: -93.78377, policy_entropy: -0.96505, alpha: 0.15933, time: 33.51692
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   111 ----
[CW] collect: return: 316.78157, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 2.06837, qf2_loss: 2.03085, policy_loss: -93.88602, policy_entropy: -0.97189, alpha: 0.15866, time: 36.89035
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   112 ----
[CW] collect: return: 268.16615, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 2.14281, qf2_loss: 2.13480, policy_loss: -94.58913, policy_entropy: -0.97970, alpha: 0.15820, time: 33.37921
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   113 ----
[CW] collect: return: 257.02355, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 2.44920, qf2_loss: 2.44210, policy_loss: -95.53965, policy_entropy: -0.98485, alpha: 0.15783, time: 33.22176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   114 ----
[CW] collect: return: 234.76521, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 2.27018, qf2_loss: 2.25216, policy_loss: -96.08808, policy_entropy: -0.98275, alpha: 0.15746, time: 33.61337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   115 ----
[CW] collect: return: 260.63223, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 2.17009, qf2_loss: 2.15397, policy_loss: -96.77501, policy_entropy: -0.99229, alpha: 0.15709, time: 33.31169
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   116 ----
[CW] collect: return: 364.39058, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 2.36362, qf2_loss: 2.33557, policy_loss: -97.19988, policy_entropy: -1.01262, alpha: 0.15720, time: 33.66841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   117 ----
[CW] collect: return: 260.32548, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 2.32755, qf2_loss: 2.32256, policy_loss: -98.04813, policy_entropy: -0.98524, alpha: 0.15722, time: 33.33370
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   118 ----
[CW] collect: return: 333.14428, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 2.47319, qf2_loss: 2.47233, policy_loss: -98.82525, policy_entropy: -1.00478, alpha: 0.15713, time: 33.53614
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   119 ----
[CW] collect: return: 281.45552, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 2.24588, qf2_loss: 2.21875, policy_loss: -100.31242, policy_entropy: -0.99988, alpha: 0.15705, time: 33.55625
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   120 ----
[CW] collect: return: 323.68159, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 2.48856, qf2_loss: 2.46583, policy_loss: -100.44017, policy_entropy: -1.01168, alpha: 0.15735, time: 33.41332
[CW] eval: return: 266.05886, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   121 ----
[CW] collect: return: 235.54708, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 2.85051, qf2_loss: 2.80593, policy_loss: -101.62839, policy_entropy: -1.01369, alpha: 0.15753, time: 33.43682
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   122 ----
[CW] collect: return: 241.59186, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 2.81907, qf2_loss: 2.78962, policy_loss: -102.24351, policy_entropy: -1.00942, alpha: 0.15816, time: 33.61105
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   123 ----
[CW] collect: return: 245.38579, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 2.30785, qf2_loss: 2.29805, policy_loss: -102.53914, policy_entropy: -1.01254, alpha: 0.15838, time: 33.63132
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   124 ----
[CW] collect: return: 242.62786, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 2.30623, qf2_loss: 2.26030, policy_loss: -103.67441, policy_entropy: -1.02778, alpha: 0.15902, time: 33.43941
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   125 ----
[CW] collect: return: 204.97980, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 2.40050, qf2_loss: 2.37196, policy_loss: -104.46376, policy_entropy: -1.01363, alpha: 0.15977, time: 33.33508
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   126 ----
[CW] collect: return: 274.66098, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 2.36910, qf2_loss: 2.33296, policy_loss: -105.24359, policy_entropy: -1.01936, alpha: 0.16028, time: 33.18436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   127 ----
[CW] collect: return: 254.10716, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 2.44699, qf2_loss: 2.43601, policy_loss: -105.33342, policy_entropy: -1.01760, alpha: 0.16112, time: 32.96849
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   128 ----
[CW] collect: return: 250.25381, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 2.57552, qf2_loss: 2.57056, policy_loss: -106.26384, policy_entropy: -1.01906, alpha: 0.16199, time: 33.27041
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   129 ----
[CW] collect: return: 271.47750, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 2.59254, qf2_loss: 2.58217, policy_loss: -106.62937, policy_entropy: -1.03372, alpha: 0.16303, time: 33.16304
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   130 ----
[CW] collect: return: 283.96839, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 2.78498, qf2_loss: 2.72407, policy_loss: -107.51593, policy_entropy: -1.01544, alpha: 0.16437, time: 33.55708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   131 ----
[CW] collect: return: 375.11913, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 2.58226, qf2_loss: 2.59077, policy_loss: -108.21856, policy_entropy: -1.01475, alpha: 0.16503, time: 33.63737
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   132 ----
[CW] collect: return: 209.76193, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 2.44658, qf2_loss: 2.41311, policy_loss: -108.68542, policy_entropy: -1.01872, alpha: 0.16582, time: 33.32569
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   133 ----
[CW] collect: return: 293.84839, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 2.28533, qf2_loss: 2.26859, policy_loss: -109.92554, policy_entropy: -1.01616, alpha: 0.16668, time: 33.46761
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   134 ----
[CW] collect: return: 305.97833, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 2.59156, qf2_loss: 2.56353, policy_loss: -110.40239, policy_entropy: -1.00095, alpha: 0.16721, time: 33.42113
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   135 ----
[CW] collect: return: 227.26789, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 2.64821, qf2_loss: 2.62692, policy_loss: -110.64195, policy_entropy: -1.00214, alpha: 0.16725, time: 33.76996
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   136 ----
[CW] collect: return: 286.12045, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 2.79440, qf2_loss: 2.78541, policy_loss: -111.68943, policy_entropy: -1.00866, alpha: 0.16766, time: 33.61452
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   137 ----
[CW] collect: return: 243.15433, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 2.62346, qf2_loss: 2.55604, policy_loss: -112.27033, policy_entropy: -0.98809, alpha: 0.16764, time: 33.40244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   138 ----
[CW] collect: return: 232.29667, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 3.08082, qf2_loss: 3.06518, policy_loss: -113.31409, policy_entropy: -0.99878, alpha: 0.16726, time: 33.49354
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   139 ----
[CW] collect: return: 190.73618, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 2.74721, qf2_loss: 2.73776, policy_loss: -113.69571, policy_entropy: -0.98565, alpha: 0.16667, time: 33.15261
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   140 ----
[CW] collect: return: 290.65596, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 2.56084, qf2_loss: 2.55221, policy_loss: -114.62776, policy_entropy: -1.00668, alpha: 0.16613, time: 33.40507
[CW] eval: return: 272.67036, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   141 ----
[CW] collect: return: 280.32959, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 2.63697, qf2_loss: 2.63827, policy_loss: -114.89156, policy_entropy: -0.99170, alpha: 0.16619, time: 32.99789
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   142 ----
[CW] collect: return: 227.51788, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 3.17980, qf2_loss: 3.18265, policy_loss: -115.51893, policy_entropy: -0.99945, alpha: 0.16576, time: 33.12915
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   143 ----
[CW] collect: return: 219.88560, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 2.60817, qf2_loss: 2.59889, policy_loss: -115.97534, policy_entropy: -1.00613, alpha: 0.16634, time: 33.29334
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   144 ----
[CW] collect: return: 277.51870, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 2.57669, qf2_loss: 2.57312, policy_loss: -116.44898, policy_entropy: -0.98445, alpha: 0.16581, time: 33.18427
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   145 ----
[CW] collect: return: 274.81634, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 2.79166, qf2_loss: 2.73963, policy_loss: -117.22751, policy_entropy: -1.00803, alpha: 0.16573, time: 33.45964
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   146 ----
[CW] collect: return: 209.93830, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 2.67855, qf2_loss: 2.67837, policy_loss: -118.31704, policy_entropy: -0.99738, alpha: 0.16589, time: 33.60095
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   147 ----
[CW] collect: return: 248.06776, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 2.87690, qf2_loss: 2.85093, policy_loss: -119.19476, policy_entropy: -0.99757, alpha: 0.16567, time: 33.60349
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   148 ----
[CW] collect: return: 301.19985, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 2.82786, qf2_loss: 2.82583, policy_loss: -119.29686, policy_entropy: -1.00452, alpha: 0.16571, time: 33.51977
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   149 ----
[CW] collect: return: 265.73950, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 3.15061, qf2_loss: 3.13934, policy_loss: -119.97054, policy_entropy: -1.00152, alpha: 0.16573, time: 33.51240
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   150 ----
[CW] collect: return: 301.93732, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 2.66866, qf2_loss: 2.66019, policy_loss: -120.47070, policy_entropy: -1.00013, alpha: 0.16598, time: 33.68283
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   151 ----
[CW] collect: return: 214.30996, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 2.48573, qf2_loss: 2.47751, policy_loss: -121.63196, policy_entropy: -1.00817, alpha: 0.16643, time: 33.72138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   152 ----
[CW] collect: return: 251.49497, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 2.71045, qf2_loss: 2.73204, policy_loss: -122.06435, policy_entropy: -1.00380, alpha: 0.16685, time: 33.87302
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   153 ----
[CW] collect: return: 299.44072, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 2.88495, qf2_loss: 2.90951, policy_loss: -122.64221, policy_entropy: -1.00247, alpha: 0.16705, time: 33.58711
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   154 ----
[CW] collect: return: 233.74288, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 3.44729, qf2_loss: 3.38789, policy_loss: -123.29362, policy_entropy: -1.00175, alpha: 0.16721, time: 33.71470
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   155 ----
[CW] collect: return: 200.68479, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 4.34199, qf2_loss: 4.30570, policy_loss: -124.03451, policy_entropy: -1.00175, alpha: 0.16758, time: 33.90679
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   156 ----
[CW] collect: return: 266.40513, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 2.38962, qf2_loss: 2.37921, policy_loss: -123.94087, policy_entropy: -1.00500, alpha: 0.16768, time: 33.68057
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   157 ----
[CW] collect: return: 308.85411, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 2.52549, qf2_loss: 2.50012, policy_loss: -124.94144, policy_entropy: -1.00147, alpha: 0.16784, time: 33.47033
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   158 ----
[CW] collect: return: 311.38724, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 3.08049, qf2_loss: 3.06716, policy_loss: -125.16076, policy_entropy: -1.02335, alpha: 0.16876, time: 33.57295
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   159 ----
[CW] collect: return: 192.86803, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 3.35148, qf2_loss: 3.32837, policy_loss: -125.75046, policy_entropy: -1.00052, alpha: 0.16969, time: 33.62497
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   160 ----
[CW] collect: return: 363.22709, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 2.88272, qf2_loss: 2.82903, policy_loss: -126.71736, policy_entropy: -0.99177, alpha: 0.16969, time: 33.51609
[CW] eval: return: 285.64460, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   161 ----
[CW] collect: return: 218.85816, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 2.79311, qf2_loss: 2.79446, policy_loss: -127.41512, policy_entropy: -1.00775, alpha: 0.16944, time: 33.49568
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   162 ----
[CW] collect: return: 304.53372, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 2.57588, qf2_loss: 2.55909, policy_loss: -127.31821, policy_entropy: -1.00378, alpha: 0.17024, time: 33.41748
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   163 ----
[CW] collect: return: 224.18901, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 3.25408, qf2_loss: 3.19858, policy_loss: -127.93422, policy_entropy: -0.99514, alpha: 0.17018, time: 33.36547
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   164 ----
[CW] collect: return: 312.76031, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 3.21116, qf2_loss: 3.19654, policy_loss: -128.67905, policy_entropy: -0.99449, alpha: 0.16977, time: 33.46987
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   165 ----
[CW] collect: return: 287.66252, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 2.97422, qf2_loss: 2.94944, policy_loss: -129.80019, policy_entropy: -0.99388, alpha: 0.16895, time: 33.59990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   166 ----
[CW] collect: return: 170.87804, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 2.76757, qf2_loss: 2.74874, policy_loss: -130.01687, policy_entropy: -0.99901, alpha: 0.16849, time: 33.77827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   167 ----
[CW] collect: return: 288.09792, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 2.67213, qf2_loss: 2.64344, policy_loss: -130.28445, policy_entropy: -0.99890, alpha: 0.16876, time: 33.72464
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   168 ----
[CW] collect: return: 312.92966, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 3.00099, qf2_loss: 2.99365, policy_loss: -131.25642, policy_entropy: -1.01415, alpha: 0.16921, time: 33.69189
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   169 ----
[CW] collect: return: 202.26573, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 4.88328, qf2_loss: 4.87497, policy_loss: -131.54588, policy_entropy: -0.99231, alpha: 0.16986, time: 33.59202
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   170 ----
[CW] collect: return: 339.39441, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 3.17496, qf2_loss: 3.13712, policy_loss: -132.37186, policy_entropy: -0.99377, alpha: 0.16884, time: 33.26044
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   171 ----
[CW] collect: return: 225.93123, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 2.68241, qf2_loss: 2.66999, policy_loss: -132.91291, policy_entropy: -0.99323, alpha: 0.16809, time: 33.75814
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   172 ----
[CW] collect: return: 269.60143, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 3.18151, qf2_loss: 3.15598, policy_loss: -133.06688, policy_entropy: -0.98441, alpha: 0.16771, time: 33.65185
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   173 ----
[CW] collect: return: 276.35497, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 2.95815, qf2_loss: 2.90327, policy_loss: -133.68274, policy_entropy: -1.01111, alpha: 0.16680, time: 33.43616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   174 ----
[CW] collect: return: 249.79937, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 3.42754, qf2_loss: 3.37373, policy_loss: -134.29889, policy_entropy: -1.00333, alpha: 0.16803, time: 33.46229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   175 ----
[CW] collect: return: 315.89753, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 3.00498, qf2_loss: 2.95494, policy_loss: -135.09062, policy_entropy: -0.99854, alpha: 0.16805, time: 33.30065
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   176 ----
[CW] collect: return: 285.68478, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 3.22325, qf2_loss: 3.19167, policy_loss: -135.75736, policy_entropy: -1.00198, alpha: 0.16772, time: 33.06244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   177 ----
[CW] collect: return: 256.78587, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 3.31082, qf2_loss: 3.27802, policy_loss: -135.90570, policy_entropy: -1.01407, alpha: 0.16880, time: 33.32755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   178 ----
[CW] collect: return: 215.39760, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 3.07765, qf2_loss: 3.02719, policy_loss: -135.84758, policy_entropy: -0.99880, alpha: 0.16918, time: 33.70292
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   179 ----
[CW] collect: return: 346.73720, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 3.34929, qf2_loss: 3.35015, policy_loss: -137.08164, policy_entropy: -1.01131, alpha: 0.16960, time: 33.45325
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   180 ----
[CW] collect: return: 208.46621, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 3.29650, qf2_loss: 3.29526, policy_loss: -137.50843, policy_entropy: -0.99343, alpha: 0.17006, time: 33.39679
[CW] eval: return: 288.13571, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   181 ----
[CW] collect: return: 261.80009, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 3.38455, qf2_loss: 3.41103, policy_loss: -137.51071, policy_entropy: -0.99160, alpha: 0.16898, time: 33.37623
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   182 ----
[CW] collect: return: 253.50025, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 3.62280, qf2_loss: 3.60173, policy_loss: -138.59957, policy_entropy: -0.98939, alpha: 0.16812, time: 33.50985
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   183 ----
[CW] collect: return: 326.89742, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 3.79991, qf2_loss: 3.77871, policy_loss: -139.37413, policy_entropy: -0.99677, alpha: 0.16752, time: 33.44427
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   184 ----
[CW] collect: return: 353.40223, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 3.10022, qf2_loss: 3.03231, policy_loss: -139.27939, policy_entropy: -1.00477, alpha: 0.16772, time: 33.51797
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   185 ----
[CW] collect: return: 344.68536, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 4.71322, qf2_loss: 4.68483, policy_loss: -139.59884, policy_entropy: -0.98812, alpha: 0.16757, time: 33.71250
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   186 ----
[CW] collect: return: 324.31991, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 3.28427, qf2_loss: 3.27109, policy_loss: -139.96435, policy_entropy: -0.99270, alpha: 0.16645, time: 33.44274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   187 ----
[CW] collect: return: 226.21997, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 3.75168, qf2_loss: 3.74755, policy_loss: -141.57611, policy_entropy: -1.00240, alpha: 0.16627, time: 33.40296
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   188 ----
[CW] collect: return: 293.50124, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 3.15238, qf2_loss: 3.11593, policy_loss: -141.51930, policy_entropy: -0.99660, alpha: 0.16600, time: 33.59440
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   189 ----
[CW] collect: return: 252.46270, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 3.17179, qf2_loss: 3.14005, policy_loss: -142.34198, policy_entropy: -1.00207, alpha: 0.16602, time: 33.30506
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   190 ----
[CW] collect: return: 285.73324, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 3.92067, qf2_loss: 3.92586, policy_loss: -141.62817, policy_entropy: -0.99577, alpha: 0.16580, time: 33.06006
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   191 ----
[CW] collect: return: 326.66301, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 3.46637, qf2_loss: 3.45392, policy_loss: -142.35813, policy_entropy: -0.99578, alpha: 0.16570, time: 33.54188
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   192 ----
[CW] collect: return: 355.25376, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 3.05650, qf2_loss: 3.06006, policy_loss: -144.03673, policy_entropy: -1.00916, alpha: 0.16570, time: 33.43890
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   193 ----
[CW] collect: return: 289.18966, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 3.22208, qf2_loss: 3.16694, policy_loss: -144.45463, policy_entropy: -1.00972, alpha: 0.16679, time: 33.43317
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   194 ----
[CW] collect: return: 337.58003, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 3.15397, qf2_loss: 3.10266, policy_loss: -144.12912, policy_entropy: -1.00177, alpha: 0.16715, time: 33.52823
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   195 ----
[CW] collect: return: 324.63341, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 3.36981, qf2_loss: 3.37095, policy_loss: -144.99191, policy_entropy: -0.99875, alpha: 0.16725, time: 33.24320
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   196 ----
[CW] collect: return: 246.71203, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 3.43211, qf2_loss: 3.41301, policy_loss: -145.78923, policy_entropy: -1.00738, alpha: 0.16723, time: 33.78284
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   197 ----
[CW] collect: return: 272.24355, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 3.14142, qf2_loss: 3.11996, policy_loss: -145.63707, policy_entropy: -1.01643, alpha: 0.16880, time: 33.74023
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   198 ----
[CW] collect: return: 320.63297, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 4.34975, qf2_loss: 4.33553, policy_loss: -146.68775, policy_entropy: -0.99614, alpha: 0.16988, time: 33.45965
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   199 ----
[CW] collect: return: 213.07733, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 4.17162, qf2_loss: 4.16707, policy_loss: -147.04548, policy_entropy: -1.00591, alpha: 0.16964, time: 33.26191
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   200 ----
[CW] collect: return: 292.81139, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 3.15356, qf2_loss: 3.18855, policy_loss: -147.39663, policy_entropy: -0.99978, alpha: 0.16968, time: 33.47996
[CW] eval: return: 311.15891, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   201 ----
[CW] collect: return: 374.85517, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 3.58071, qf2_loss: 3.57171, policy_loss: -147.67492, policy_entropy: -0.99910, alpha: 0.16987, time: 33.22863
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   202 ----
[CW] collect: return: 422.68728, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 5.14115, qf2_loss: 5.03526, policy_loss: -148.66932, policy_entropy: -1.01522, alpha: 0.17039, time: 33.57272
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   203 ----
[CW] collect: return: 313.80739, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 6.16911, qf2_loss: 6.10810, policy_loss: -149.30075, policy_entropy: -0.99504, alpha: 0.17086, time: 32.81451
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   204 ----
[CW] collect: return: 325.86702, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 3.35609, qf2_loss: 3.35945, policy_loss: -149.41167, policy_entropy: -1.01346, alpha: 0.17140, time: 32.82150
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   205 ----
[CW] collect: return: 385.16785, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 3.06671, qf2_loss: 3.05943, policy_loss: -150.34785, policy_entropy: -1.02152, alpha: 0.17316, time: 33.73700
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   206 ----
[CW] collect: return: 308.24152, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 2.97725, qf2_loss: 2.97517, policy_loss: -150.17303, policy_entropy: -1.00614, alpha: 0.17447, time: 33.46540
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   207 ----
[CW] collect: return: 292.40232, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 3.23408, qf2_loss: 3.18658, policy_loss: -150.60321, policy_entropy: -1.01066, alpha: 0.17618, time: 33.19520
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   208 ----
[CW] collect: return: 249.29651, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 3.63140, qf2_loss: 3.63196, policy_loss: -151.34555, policy_entropy: -1.00557, alpha: 0.17701, time: 33.45318
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   209 ----
[CW] collect: return: 350.82329, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 3.23674, qf2_loss: 3.24351, policy_loss: -151.58491, policy_entropy: -1.00857, alpha: 0.17741, time: 33.45037
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   210 ----
[CW] collect: return: 336.15537, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 3.27666, qf2_loss: 3.25688, policy_loss: -152.53115, policy_entropy: -1.00112, alpha: 0.17828, time: 33.45469
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   211 ----
[CW] collect: return: 229.11477, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 3.84156, qf2_loss: 3.76947, policy_loss: -152.17230, policy_entropy: -0.99651, alpha: 0.17819, time: 33.54093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   212 ----
[CW] collect: return: 364.53907, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 3.77988, qf2_loss: 3.73439, policy_loss: -153.00784, policy_entropy: -0.99590, alpha: 0.17789, time: 33.17904
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   213 ----
[CW] collect: return: 254.24155, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 3.19300, qf2_loss: 3.19823, policy_loss: -153.79029, policy_entropy: -1.00949, alpha: 0.17783, time: 33.49828
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   214 ----
[CW] collect: return: 327.74718, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 3.78130, qf2_loss: 3.73599, policy_loss: -154.14927, policy_entropy: -1.00659, alpha: 0.17890, time: 33.50354
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   215 ----
[CW] collect: return: 279.81918, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 3.26971, qf2_loss: 3.22944, policy_loss: -154.95709, policy_entropy: -0.99784, alpha: 0.17899, time: 33.53648
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   216 ----
[CW] collect: return: 340.06965, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 3.48188, qf2_loss: 3.48940, policy_loss: -155.29304, policy_entropy: -1.01285, alpha: 0.17954, time: 34.05290
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   217 ----
[CW] collect: return: 240.15041, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 3.48173, qf2_loss: 3.52993, policy_loss: -155.64783, policy_entropy: -1.00648, alpha: 0.18077, time: 40.22473
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   218 ----
[CW] collect: return: 241.75605, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 3.85797, qf2_loss: 3.78216, policy_loss: -156.06364, policy_entropy: -1.00702, alpha: 0.18159, time: 34.02675
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   219 ----
[CW] collect: return: 286.94149, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 3.87503, qf2_loss: 3.84058, policy_loss: -156.81629, policy_entropy: -1.00457, alpha: 0.18217, time: 33.63192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   220 ----
[CW] collect: return: 305.81575, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 3.19888, qf2_loss: 3.17805, policy_loss: -157.22604, policy_entropy: -1.01048, alpha: 0.18369, time: 33.80854
[CW] eval: return: 275.49484, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   221 ----
[CW] collect: return: 273.11454, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 3.76729, qf2_loss: 3.75665, policy_loss: -157.82262, policy_entropy: -1.00417, alpha: 0.18432, time: 33.46746
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   222 ----
[CW] collect: return: 278.36423, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 3.67775, qf2_loss: 3.70558, policy_loss: -158.71025, policy_entropy: -1.00188, alpha: 0.18487, time: 37.03201
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   223 ----
[CW] collect: return: 330.27766, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 3.57171, qf2_loss: 3.54907, policy_loss: -158.42871, policy_entropy: -1.00602, alpha: 0.18537, time: 34.54255
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   224 ----
[CW] collect: return: 294.58668, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 3.95942, qf2_loss: 3.97763, policy_loss: -159.56957, policy_entropy: -1.01225, alpha: 0.18652, time: 33.66689
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   225 ----
[CW] collect: return: 256.81292, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 3.53617, qf2_loss: 3.53780, policy_loss: -159.09649, policy_entropy: -1.00420, alpha: 0.18742, time: 33.95628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   226 ----
[CW] collect: return: 292.41937, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 3.69219, qf2_loss: 3.66689, policy_loss: -160.25369, policy_entropy: -1.00691, alpha: 0.18871, time: 33.79709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   227 ----
[CW] collect: return: 412.21430, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 3.42216, qf2_loss: 3.45544, policy_loss: -160.27062, policy_entropy: -1.00683, alpha: 0.18924, time: 33.87591
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   228 ----
[CW] collect: return: 381.21304, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 3.73751, qf2_loss: 3.71631, policy_loss: -161.22623, policy_entropy: -1.01001, alpha: 0.19001, time: 34.04205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   229 ----
[CW] collect: return: 338.97150, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 4.95124, qf2_loss: 4.89722, policy_loss: -161.25376, policy_entropy: -1.00020, alpha: 0.19082, time: 33.62751
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   230 ----
[CW] collect: return: 375.39377, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 4.12962, qf2_loss: 4.12141, policy_loss: -161.87905, policy_entropy: -1.00194, alpha: 0.19097, time: 33.55217
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   231 ----
[CW] collect: return: 251.70616, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 3.70039, qf2_loss: 3.72796, policy_loss: -162.67944, policy_entropy: -1.00861, alpha: 0.19140, time: 33.66194
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   232 ----
[CW] collect: return: 249.18469, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 3.71585, qf2_loss: 3.68148, policy_loss: -162.92200, policy_entropy: -0.99819, alpha: 0.19236, time: 33.94766
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   233 ----
[CW] collect: return: 231.56440, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 3.43768, qf2_loss: 3.41143, policy_loss: -162.79014, policy_entropy: -0.99773, alpha: 0.19204, time: 33.74996
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   234 ----
[CW] collect: return: 342.01196, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 3.67602, qf2_loss: 3.67174, policy_loss: -163.25495, policy_entropy: -1.00530, alpha: 0.19249, time: 33.33265
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   235 ----
[CW] collect: return: 322.82698, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 3.25619, qf2_loss: 3.25854, policy_loss: -164.21434, policy_entropy: -1.00387, alpha: 0.19257, time: 33.64279
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   236 ----
[CW] collect: return: 296.45924, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 3.58391, qf2_loss: 3.54270, policy_loss: -164.40880, policy_entropy: -1.00685, alpha: 0.19390, time: 33.61007
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   237 ----
[CW] collect: return: 286.01844, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 3.90122, qf2_loss: 3.88781, policy_loss: -165.09973, policy_entropy: -1.00039, alpha: 0.19385, time: 34.03818
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   238 ----
[CW] collect: return: 327.15576, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 4.22205, qf2_loss: 4.17953, policy_loss: -165.19378, policy_entropy: -0.99691, alpha: 0.19376, time: 34.18794
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   239 ----
[CW] collect: return: 202.96887, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 3.40424, qf2_loss: 3.38321, policy_loss: -166.04114, policy_entropy: -1.00268, alpha: 0.19398, time: 33.80119
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   240 ----
[CW] collect: return: 216.92078, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 3.18549, qf2_loss: 3.18387, policy_loss: -166.15916, policy_entropy: -1.00552, alpha: 0.19447, time: 33.62611
[CW] eval: return: 263.60940, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   241 ----
[CW] collect: return: 344.68404, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 3.14497, qf2_loss: 3.11521, policy_loss: -166.32745, policy_entropy: -1.00922, alpha: 0.19545, time: 33.79160
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   242 ----
[CW] collect: return: 320.80416, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 3.21726, qf2_loss: 3.18647, policy_loss: -167.76787, policy_entropy: -1.01029, alpha: 0.19641, time: 33.84468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   243 ----
[CW] collect: return: 300.25196, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 3.85280, qf2_loss: 3.78270, policy_loss: -167.30700, policy_entropy: -1.00124, alpha: 0.19763, time: 33.64302
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   244 ----
[CW] collect: return: 301.68747, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 4.00345, qf2_loss: 4.00917, policy_loss: -167.58813, policy_entropy: -0.99657, alpha: 0.19738, time: 33.70954
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   245 ----
[CW] collect: return: 332.86122, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 3.28975, qf2_loss: 3.26180, policy_loss: -168.68966, policy_entropy: -1.01137, alpha: 0.19792, time: 34.18666
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   246 ----
[CW] collect: return: 341.41664, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 3.26772, qf2_loss: 3.24067, policy_loss: -169.20654, policy_entropy: -1.01014, alpha: 0.19967, time: 33.84459
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   247 ----
[CW] collect: return: 261.73487, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 3.09290, qf2_loss: 3.09566, policy_loss: -169.39346, policy_entropy: -1.00874, alpha: 0.20056, time: 33.70207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   248 ----
[CW] collect: return: 280.64180, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 3.76659, qf2_loss: 3.72751, policy_loss: -169.67183, policy_entropy: -1.00835, alpha: 0.20206, time: 33.53124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   249 ----
[CW] collect: return: 229.39328, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 3.95628, qf2_loss: 3.93495, policy_loss: -169.88022, policy_entropy: -0.99648, alpha: 0.20299, time: 33.40806
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   250 ----
[CW] collect: return: 319.76810, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 3.58799, qf2_loss: 3.54498, policy_loss: -171.09111, policy_entropy: -1.00845, alpha: 0.20300, time: 33.85287
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   251 ----
[CW] collect: return: 330.53347, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 3.29073, qf2_loss: 3.27917, policy_loss: -170.82191, policy_entropy: -1.00948, alpha: 0.20421, time: 33.94997
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   252 ----
[CW] collect: return: 293.36817, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 3.10997, qf2_loss: 3.08482, policy_loss: -171.31001, policy_entropy: -0.99467, alpha: 0.20441, time: 33.91641
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   253 ----
[CW] collect: return: 266.43871, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 3.44127, qf2_loss: 3.41438, policy_loss: -172.20474, policy_entropy: -1.00287, alpha: 0.20482, time: 33.90958
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   254 ----
[CW] collect: return: 248.22654, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 4.05063, qf2_loss: 4.05824, policy_loss: -172.08295, policy_entropy: -0.99489, alpha: 0.20449, time: 33.77516
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   255 ----
[CW] collect: return: 280.90926, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 3.36168, qf2_loss: 3.33443, policy_loss: -172.30366, policy_entropy: -0.99549, alpha: 0.20347, time: 33.61530
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   256 ----
[CW] collect: return: 255.74411, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 3.03692, qf2_loss: 3.05231, policy_loss: -172.73631, policy_entropy: -0.99306, alpha: 0.20247, time: 33.72460
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   257 ----
[CW] collect: return: 255.57507, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 3.29436, qf2_loss: 3.24185, policy_loss: -173.66842, policy_entropy: -0.99780, alpha: 0.20188, time: 33.88468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   258 ----
[CW] collect: return: 340.03438, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 3.34525, qf2_loss: 3.33801, policy_loss: -173.90902, policy_entropy: -0.99676, alpha: 0.20198, time: 33.75352
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   259 ----
[CW] collect: return: 257.57493, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 3.65164, qf2_loss: 3.62441, policy_loss: -174.21600, policy_entropy: -0.98216, alpha: 0.20008, time: 33.77476
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   260 ----
[CW] collect: return: 303.41777, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 3.52002, qf2_loss: 3.47952, policy_loss: -174.91467, policy_entropy: -0.99657, alpha: 0.19847, time: 33.53610
[CW] eval: return: 308.67454, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   261 ----
[CW] collect: return: 216.88085, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 3.55896, qf2_loss: 3.50515, policy_loss: -174.62738, policy_entropy: -1.00510, alpha: 0.19842, time: 33.92414
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   262 ----
[CW] collect: return: 292.34478, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 3.32283, qf2_loss: 3.35631, policy_loss: -174.84613, policy_entropy: -1.00082, alpha: 0.19855, time: 33.68254
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   263 ----
[CW] collect: return: 327.43299, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 3.26016, qf2_loss: 3.23102, policy_loss: -175.49597, policy_entropy: -1.00207, alpha: 0.19927, time: 33.38631
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   264 ----
[CW] collect: return: 307.54667, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 3.62663, qf2_loss: 3.60976, policy_loss: -176.54495, policy_entropy: -0.99735, alpha: 0.19932, time: 33.89577
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   265 ----
[CW] collect: return: 329.98314, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 3.09976, qf2_loss: 3.08834, policy_loss: -176.71633, policy_entropy: -1.00181, alpha: 0.19908, time: 33.84494
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   266 ----
[CW] collect: return: 326.78129, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 3.32031, qf2_loss: 3.28999, policy_loss: -177.02798, policy_entropy: -1.00287, alpha: 0.19956, time: 33.84050
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   267 ----
[CW] collect: return: 282.13065, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 3.34589, qf2_loss: 3.34316, policy_loss: -177.27013, policy_entropy: -0.99430, alpha: 0.19881, time: 33.50508
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   268 ----
[CW] collect: return: 356.60177, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 3.70925, qf2_loss: 3.68319, policy_loss: -177.61011, policy_entropy: -0.99620, alpha: 0.19817, time: 33.57331
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   269 ----
[CW] collect: return: 357.00661, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 4.15899, qf2_loss: 4.13289, policy_loss: -177.74834, policy_entropy: -1.00110, alpha: 0.19824, time: 34.13592
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   270 ----
[CW] collect: return: 306.28990, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 3.84001, qf2_loss: 3.85654, policy_loss: -178.64388, policy_entropy: -0.99830, alpha: 0.19822, time: 33.63574
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   271 ----
[CW] collect: return: 330.78049, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 3.38853, qf2_loss: 3.37219, policy_loss: -178.79159, policy_entropy: -1.00572, alpha: 0.19806, time: 33.82117
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   272 ----
[CW] collect: return: 261.60598, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 3.32361, qf2_loss: 3.32876, policy_loss: -179.50877, policy_entropy: -1.00872, alpha: 0.19919, time: 33.52852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   273 ----
[CW] collect: return: 219.73278, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 3.28739, qf2_loss: 3.28393, policy_loss: -179.26153, policy_entropy: -0.99224, alpha: 0.19899, time: 33.76811
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   274 ----
[CW] collect: return: 337.83203, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 3.18565, qf2_loss: 3.17897, policy_loss: -180.14374, policy_entropy: -0.99718, alpha: 0.19849, time: 33.79452
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   275 ----
[CW] collect: return: 447.48729, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 3.38922, qf2_loss: 3.38607, policy_loss: -180.88478, policy_entropy: -1.00347, alpha: 0.19880, time: 33.65013
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   276 ----
[CW] collect: return: 293.78033, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 3.89740, qf2_loss: 3.76916, policy_loss: -181.55400, policy_entropy: -0.99432, alpha: 0.19886, time: 33.83846
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   277 ----
[CW] collect: return: 380.10873, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 4.53326, qf2_loss: 4.48852, policy_loss: -181.10940, policy_entropy: -1.00053, alpha: 0.19819, time: 33.66348
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   278 ----
[CW] collect: return: 302.11329, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 4.04341, qf2_loss: 4.04611, policy_loss: -181.67680, policy_entropy: -1.00192, alpha: 0.19821, time: 33.77040
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   279 ----
[CW] collect: return: 285.26492, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 3.29977, qf2_loss: 3.27580, policy_loss: -182.07534, policy_entropy: -1.00272, alpha: 0.19888, time: 33.80407
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   280 ----
[CW] collect: return: 280.53094, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 4.45143, qf2_loss: 4.42586, policy_loss: -182.82926, policy_entropy: -1.00160, alpha: 0.19889, time: 33.76881
[CW] eval: return: 285.64580, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   281 ----
[CW] collect: return: 252.17537, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 3.55745, qf2_loss: 3.55268, policy_loss: -182.83468, policy_entropy: -1.00203, alpha: 0.19939, time: 34.15534
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   282 ----
[CW] collect: return: 329.52940, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 3.09788, qf2_loss: 3.10424, policy_loss: -183.16494, policy_entropy: -1.00825, alpha: 0.20016, time: 33.67947
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   283 ----
[CW] collect: return: 336.57738, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 3.18956, qf2_loss: 3.19071, policy_loss: -182.73699, policy_entropy: -1.00389, alpha: 0.20083, time: 33.56628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   284 ----
[CW] collect: return: 261.84973, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 3.68253, qf2_loss: 3.63716, policy_loss: -183.93688, policy_entropy: -1.00458, alpha: 0.20229, time: 33.88636
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   285 ----
[CW] collect: return: 314.44308, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 3.98088, qf2_loss: 3.93274, policy_loss: -183.96479, policy_entropy: -0.99929, alpha: 0.20236, time: 33.46525
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   286 ----
[CW] collect: return: 266.79585, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 3.29499, qf2_loss: 3.26560, policy_loss: -184.27559, policy_entropy: -0.99694, alpha: 0.20216, time: 34.11496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   287 ----
[CW] collect: return: 220.29997, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 3.16809, qf2_loss: 3.20492, policy_loss: -184.51913, policy_entropy: -1.00643, alpha: 0.20171, time: 33.84779
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   288 ----
[CW] collect: return: 182.48178, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 3.20129, qf2_loss: 3.21368, policy_loss: -185.08317, policy_entropy: -0.99399, alpha: 0.20234, time: 33.81181
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   289 ----
[CW] collect: return: 341.15970, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 3.76923, qf2_loss: 3.74125, policy_loss: -184.97250, policy_entropy: -0.98791, alpha: 0.20087, time: 33.73154
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   290 ----
[CW] collect: return: 347.37337, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 3.28107, qf2_loss: 3.25997, policy_loss: -185.11797, policy_entropy: -0.99845, alpha: 0.19990, time: 33.68743
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   291 ----
[CW] collect: return: 392.82990, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 3.71522, qf2_loss: 3.69384, policy_loss: -186.03676, policy_entropy: -1.00697, alpha: 0.20023, time: 33.71785
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   292 ----
[CW] collect: return: 290.82367, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 3.79816, qf2_loss: 3.76561, policy_loss: -186.73566, policy_entropy: -1.00474, alpha: 0.20072, time: 33.82326
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   293 ----
[CW] collect: return: 345.20885, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 3.41820, qf2_loss: 3.41317, policy_loss: -186.77179, policy_entropy: -0.99941, alpha: 0.20112, time: 33.90061
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   294 ----
[CW] collect: return: 298.54894, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 3.79360, qf2_loss: 3.76485, policy_loss: -187.15196, policy_entropy: -1.00953, alpha: 0.20135, time: 33.82914
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   295 ----
[CW] collect: return: 283.38011, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 4.06904, qf2_loss: 4.04457, policy_loss: -187.43215, policy_entropy: -1.00195, alpha: 0.20245, time: 33.72037
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   296 ----
[CW] collect: return: 265.53907, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 3.51104, qf2_loss: 3.49228, policy_loss: -187.26804, policy_entropy: -1.00716, alpha: 0.20271, time: 33.53277
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   297 ----
[CW] collect: return: 350.89296, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 3.27145, qf2_loss: 3.25098, policy_loss: -188.10787, policy_entropy: -0.99634, alpha: 0.20356, time: 33.74703
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   298 ----
[CW] collect: return: 256.70500, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 3.57394, qf2_loss: 3.51995, policy_loss: -188.53384, policy_entropy: -0.99613, alpha: 0.20316, time: 33.85183
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   299 ----
[CW] collect: return: 281.57098, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 3.52430, qf2_loss: 3.49388, policy_loss: -188.79479, policy_entropy: -0.99925, alpha: 0.20275, time: 33.67017
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   300 ----
[CW] collect: return: 232.43152, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 3.52089, qf2_loss: 3.47071, policy_loss: -189.06281, policy_entropy: -1.00655, alpha: 0.20312, time: 33.79013
[CW] eval: return: 287.34889, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   301 ----
[CW] collect: return: 378.12557, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 3.70605, qf2_loss: 3.69653, policy_loss: -189.59331, policy_entropy: -0.99715, alpha: 0.20379, time: 33.56098
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   302 ----
[CW] collect: return: 362.58555, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 3.24898, qf2_loss: 3.23328, policy_loss: -189.63856, policy_entropy: -0.99797, alpha: 0.20303, time: 33.69657
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   303 ----
[CW] collect: return: 300.59679, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 3.35645, qf2_loss: 3.35047, policy_loss: -189.94587, policy_entropy: -0.99827, alpha: 0.20281, time: 33.93439
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   304 ----
[CW] collect: return: 339.12789, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 3.89055, qf2_loss: 3.86424, policy_loss: -191.02506, policy_entropy: -1.00632, alpha: 0.20353, time: 33.93459
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   305 ----
[CW] collect: return: 277.82871, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 4.04189, qf2_loss: 4.03943, policy_loss: -191.25499, policy_entropy: -0.99699, alpha: 0.20402, time: 33.93981
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   306 ----
[CW] collect: return: 274.60397, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 4.15287, qf2_loss: 4.15420, policy_loss: -191.07857, policy_entropy: -0.98746, alpha: 0.20279, time: 33.82349
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   307 ----
[CW] collect: return: 290.10520, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 3.90531, qf2_loss: 3.86265, policy_loss: -191.13089, policy_entropy: -1.00225, alpha: 0.20141, time: 33.85692
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   308 ----
[CW] collect: return: 343.53515, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 3.62076, qf2_loss: 3.60739, policy_loss: -191.82122, policy_entropy: -1.00980, alpha: 0.20223, time: 33.83090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   309 ----
[CW] collect: return: 296.57544, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 3.18426, qf2_loss: 3.20411, policy_loss: -192.05363, policy_entropy: -0.99927, alpha: 0.20317, time: 33.71743
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   310 ----
[CW] collect: return: 330.42580, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 3.21423, qf2_loss: 3.20707, policy_loss: -191.98370, policy_entropy: -0.99958, alpha: 0.20288, time: 33.79778
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   311 ----
[CW] collect: return: 269.73983, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 3.44852, qf2_loss: 3.43628, policy_loss: -192.52228, policy_entropy: -1.00925, alpha: 0.20395, time: 33.93353
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   312 ----
[CW] collect: return: 272.30964, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 3.53374, qf2_loss: 3.50154, policy_loss: -192.96596, policy_entropy: -1.00060, alpha: 0.20417, time: 33.79095
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   313 ----
[CW] collect: return: 363.21564, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 3.57331, qf2_loss: 3.55720, policy_loss: -192.95114, policy_entropy: -1.00032, alpha: 0.20415, time: 33.74321
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   314 ----
[CW] collect: return: 267.88213, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 3.63428, qf2_loss: 3.58277, policy_loss: -194.07671, policy_entropy: -0.99863, alpha: 0.20451, time: 33.81026
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   315 ----
[CW] collect: return: 383.88888, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 4.55091, qf2_loss: 4.57734, policy_loss: -194.06622, policy_entropy: -1.00374, alpha: 0.20453, time: 33.97498
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   316 ----
[CW] collect: return: 318.76146, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 4.04333, qf2_loss: 3.98224, policy_loss: -194.00638, policy_entropy: -0.99470, alpha: 0.20474, time: 33.86725
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   317 ----
[CW] collect: return: 362.05419, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 3.77156, qf2_loss: 3.74621, policy_loss: -194.91996, policy_entropy: -1.00768, alpha: 0.20430, time: 33.69709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   318 ----
[CW] collect: return: 342.67800, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 4.50684, qf2_loss: 4.46801, policy_loss: -195.54432, policy_entropy: -1.00643, alpha: 0.20590, time: 33.81182
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   319 ----
[CW] collect: return: 293.41334, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 3.37841, qf2_loss: 3.36852, policy_loss: -195.05223, policy_entropy: -1.00436, alpha: 0.20654, time: 33.82185
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   320 ----
[CW] collect: return: 346.93522, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 3.12620, qf2_loss: 3.10042, policy_loss: -195.14710, policy_entropy: -1.00870, alpha: 0.20735, time: 33.85886
[CW] eval: return: 326.07142, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   321 ----
[CW] collect: return: 299.21286, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 3.71405, qf2_loss: 3.66021, policy_loss: -196.16712, policy_entropy: -1.00196, alpha: 0.20850, time: 33.92003
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   322 ----
[CW] collect: return: 344.32743, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 3.58873, qf2_loss: 3.57008, policy_loss: -196.06551, policy_entropy: -1.00259, alpha: 0.20911, time: 33.83196
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   323 ----
[CW] collect: return: 353.61795, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 3.68788, qf2_loss: 3.70953, policy_loss: -196.48704, policy_entropy: -1.00535, alpha: 0.20924, time: 33.86378
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   324 ----
[CW] collect: return: 272.39222, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 3.82878, qf2_loss: 3.84170, policy_loss: -197.10937, policy_entropy: -1.00747, alpha: 0.21041, time: 33.76632
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   325 ----
[CW] collect: return: 314.12694, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 3.67250, qf2_loss: 3.68684, policy_loss: -196.87611, policy_entropy: -1.00463, alpha: 0.21148, time: 33.62701
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   326 ----
[CW] collect: return: 407.17444, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 3.77796, qf2_loss: 3.75997, policy_loss: -197.41697, policy_entropy: -0.99696, alpha: 0.21205, time: 33.74893
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   327 ----
[CW] collect: return: 353.72426, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 3.82706, qf2_loss: 3.79496, policy_loss: -197.53301, policy_entropy: -0.99644, alpha: 0.21139, time: 38.73739
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   328 ----
[CW] collect: return: 348.55178, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 3.65162, qf2_loss: 3.61418, policy_loss: -198.21226, policy_entropy: -1.00575, alpha: 0.21115, time: 33.57500
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   329 ----
[CW] collect: return: 321.10245, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 3.67171, qf2_loss: 3.66207, policy_loss: -198.55162, policy_entropy: -1.00244, alpha: 0.21219, time: 33.87436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   330 ----
[CW] collect: return: 248.09045, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 3.63781, qf2_loss: 3.62037, policy_loss: -198.80881, policy_entropy: -1.00899, alpha: 0.21249, time: 33.74697
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   331 ----
[CW] collect: return: 351.01106, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 3.96491, qf2_loss: 3.96681, policy_loss: -198.91002, policy_entropy: -0.99503, alpha: 0.21331, time: 33.88819
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   332 ----
[CW] collect: return: 294.01681, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 4.14046, qf2_loss: 4.12567, policy_loss: -199.43955, policy_entropy: -0.99809, alpha: 0.21282, time: 33.77477
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   333 ----
[CW] collect: return: 350.82153, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 3.46716, qf2_loss: 3.46127, policy_loss: -199.17180, policy_entropy: -0.99976, alpha: 0.21229, time: 37.20539
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   334 ----
[CW] collect: return: 364.56177, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 4.59883, qf2_loss: 4.56283, policy_loss: -200.43534, policy_entropy: -1.00347, alpha: 0.21270, time: 33.58675
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   335 ----
[CW] collect: return: 315.18003, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 4.30536, qf2_loss: 4.33077, policy_loss: -200.27749, policy_entropy: -1.00197, alpha: 0.21297, time: 33.68844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   336 ----
[CW] collect: return: 294.22060, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 3.56887, qf2_loss: 3.54722, policy_loss: -199.93323, policy_entropy: -1.00059, alpha: 0.21321, time: 33.83804
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   337 ----
[CW] collect: return: 400.07016, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 3.88989, qf2_loss: 3.85766, policy_loss: -200.91377, policy_entropy: -1.00444, alpha: 0.21363, time: 33.46073
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   338 ----
[CW] collect: return: 298.70423, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 3.54307, qf2_loss: 3.56724, policy_loss: -200.91166, policy_entropy: -1.00194, alpha: 0.21434, time: 33.68104
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   339 ----
[CW] collect: return: 332.44935, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 3.80905, qf2_loss: 3.81892, policy_loss: -200.83940, policy_entropy: -0.99791, alpha: 0.21373, time: 33.82042
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   340 ----
[CW] collect: return: 348.17703, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 3.40344, qf2_loss: 3.37480, policy_loss: -201.20851, policy_entropy: -1.01085, alpha: 0.21522, time: 33.79747
[CW] eval: return: 325.48537, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   341 ----
[CW] collect: return: 337.84346, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 3.65982, qf2_loss: 3.62587, policy_loss: -201.55900, policy_entropy: -0.99669, alpha: 0.21541, time: 33.48860
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   342 ----
[CW] collect: return: 307.23457, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 3.99593, qf2_loss: 3.99293, policy_loss: -202.31629, policy_entropy: -1.00423, alpha: 0.21568, time: 33.32233
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   343 ----
[CW] collect: return: 370.50594, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 3.89865, qf2_loss: 3.91366, policy_loss: -203.07164, policy_entropy: -1.00744, alpha: 0.21618, time: 33.66928
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   344 ----
[CW] collect: return: 345.96466, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 4.67238, qf2_loss: 4.61126, policy_loss: -203.23014, policy_entropy: -0.98955, alpha: 0.21684, time: 33.74988
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   345 ----
[CW] collect: return: 314.65520, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 3.94630, qf2_loss: 3.93268, policy_loss: -202.97050, policy_entropy: -1.00972, alpha: 0.21645, time: 33.65673
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   346 ----
[CW] collect: return: 325.72902, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 3.85341, qf2_loss: 3.83310, policy_loss: -203.50885, policy_entropy: -0.99798, alpha: 0.21642, time: 33.90324
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   347 ----
[CW] collect: return: 393.14181, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 3.69320, qf2_loss: 3.63715, policy_loss: -204.77185, policy_entropy: -1.00426, alpha: 0.21678, time: 33.76106
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   348 ----
[CW] collect: return: 325.64016, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 3.60698, qf2_loss: 3.62875, policy_loss: -204.17276, policy_entropy: -1.00560, alpha: 0.21743, time: 33.58232
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   349 ----
[CW] collect: return: 266.82059, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 3.51758, qf2_loss: 3.55139, policy_loss: -204.75364, policy_entropy: -1.00402, alpha: 0.21884, time: 33.65950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   350 ----
[CW] collect: return: 359.05868, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 3.87886, qf2_loss: 3.85181, policy_loss: -204.21216, policy_entropy: -1.00060, alpha: 0.21917, time: 33.81045
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   351 ----
[CW] collect: return: 369.91847, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 3.76273, qf2_loss: 3.77872, policy_loss: -205.10048, policy_entropy: -0.99904, alpha: 0.21892, time: 33.33608
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   352 ----
[CW] collect: return: 307.45191, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 3.83015, qf2_loss: 3.84124, policy_loss: -204.90725, policy_entropy: -0.99842, alpha: 0.21879, time: 33.49251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   353 ----
[CW] collect: return: 414.80620, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 3.97399, qf2_loss: 3.99745, policy_loss: -205.66175, policy_entropy: -1.00543, alpha: 0.21874, time: 33.45409
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   354 ----
[CW] collect: return: 330.94156, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 4.35056, qf2_loss: 4.35634, policy_loss: -205.53096, policy_entropy: -0.99907, alpha: 0.21887, time: 33.61085
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   355 ----
[CW] collect: return: 280.85176, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 5.65287, qf2_loss: 5.61976, policy_loss: -206.05730, policy_entropy: -0.98868, alpha: 0.21789, time: 33.59462
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   356 ----
[CW] collect: return: 450.62250, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 3.47747, qf2_loss: 3.46339, policy_loss: -206.24689, policy_entropy: -1.01176, alpha: 0.21818, time: 33.75968
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   357 ----
[CW] collect: return: 343.51470, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 3.42375, qf2_loss: 3.42979, policy_loss: -207.05541, policy_entropy: -1.00417, alpha: 0.22017, time: 33.93164
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   358 ----
[CW] collect: return: 302.91570, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 3.69674, qf2_loss: 3.67703, policy_loss: -207.42579, policy_entropy: -1.00516, alpha: 0.22073, time: 33.62746
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   359 ----
[CW] collect: return: 362.32935, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 4.09163, qf2_loss: 4.05523, policy_loss: -207.12750, policy_entropy: -0.99940, alpha: 0.22084, time: 33.71380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   360 ----
[CW] collect: return: 263.27627, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 3.36665, qf2_loss: 3.37305, policy_loss: -207.18581, policy_entropy: -1.00503, alpha: 0.22153, time: 33.93023
[CW] eval: return: 340.77896, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   361 ----
[CW] collect: return: 305.98306, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 3.86208, qf2_loss: 3.81479, policy_loss: -207.78930, policy_entropy: -1.00075, alpha: 0.22162, time: 33.25653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   362 ----
[CW] collect: return: 355.65001, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 4.44878, qf2_loss: 4.42408, policy_loss: -207.81326, policy_entropy: -0.99231, alpha: 0.22155, time: 33.60020
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   363 ----
[CW] collect: return: 419.33850, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 4.34057, qf2_loss: 4.30187, policy_loss: -208.65943, policy_entropy: -1.00423, alpha: 0.22032, time: 33.49692
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   364 ----
[CW] collect: return: 215.53876, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 3.63397, qf2_loss: 3.64659, policy_loss: -207.91455, policy_entropy: -1.00547, alpha: 0.22090, time: 33.56610
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   365 ----
[CW] collect: return: 317.42110, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 3.57693, qf2_loss: 3.56398, policy_loss: -208.85982, policy_entropy: -0.99902, alpha: 0.22202, time: 33.70100
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   366 ----
[CW] collect: return: 405.36964, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 4.26302, qf2_loss: 4.25885, policy_loss: -208.89120, policy_entropy: -0.99813, alpha: 0.22191, time: 33.46650
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   367 ----
[CW] collect: return: 372.10512, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 3.57888, qf2_loss: 3.58276, policy_loss: -209.85399, policy_entropy: -1.01395, alpha: 0.22246, time: 33.72271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   368 ----
[CW] collect: return: 378.74354, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 4.31772, qf2_loss: 4.31825, policy_loss: -209.88890, policy_entropy: -0.99848, alpha: 0.22402, time: 33.46246
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   369 ----
[CW] collect: return: 342.40918, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 3.93239, qf2_loss: 3.88863, policy_loss: -209.85427, policy_entropy: -1.00263, alpha: 0.22356, time: 33.67335
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   370 ----
[CW] collect: return: 331.24076, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 3.70258, qf2_loss: 3.72487, policy_loss: -210.61537, policy_entropy: -1.00142, alpha: 0.22387, time: 33.60884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   371 ----
[CW] collect: return: 347.32970, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 4.72391, qf2_loss: 4.74139, policy_loss: -210.11825, policy_entropy: -0.99502, alpha: 0.22416, time: 33.68511
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   372 ----
[CW] collect: return: 338.42354, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 5.12088, qf2_loss: 5.09603, policy_loss: -210.34165, policy_entropy: -0.99433, alpha: 0.22292, time: 33.68997
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   373 ----
[CW] collect: return: 327.59262, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 3.65233, qf2_loss: 3.63861, policy_loss: -210.75269, policy_entropy: -1.00535, alpha: 0.22263, time: 33.36492
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   374 ----
[CW] collect: return: 359.79998, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 3.86031, qf2_loss: 3.83758, policy_loss: -211.48075, policy_entropy: -1.00047, alpha: 0.22329, time: 33.59066
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   375 ----
[CW] collect: return: 364.21130, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 3.83946, qf2_loss: 3.85087, policy_loss: -211.67811, policy_entropy: -1.00846, alpha: 0.22445, time: 33.58557
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   376 ----
[CW] collect: return: 355.24834, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 3.64655, qf2_loss: 3.62419, policy_loss: -212.31887, policy_entropy: -0.99334, alpha: 0.22411, time: 33.72733
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   377 ----
[CW] collect: return: 347.11715, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 3.80992, qf2_loss: 3.81940, policy_loss: -211.35668, policy_entropy: -1.01057, alpha: 0.22460, time: 33.34728
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   378 ----
[CW] collect: return: 370.85634, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 3.82132, qf2_loss: 3.80276, policy_loss: -212.56793, policy_entropy: -1.00360, alpha: 0.22606, time: 33.32228
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   379 ----
[CW] collect: return: 289.43508, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 3.44179, qf2_loss: 3.45436, policy_loss: -211.99545, policy_entropy: -0.98894, alpha: 0.22535, time: 33.43759
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   380 ----
[CW] collect: return: 349.05318, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 3.75234, qf2_loss: 3.73865, policy_loss: -212.61620, policy_entropy: -0.99778, alpha: 0.22419, time: 32.96343
[CW] eval: return: 328.64038, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   381 ----
[CW] collect: return: 307.97604, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 3.95144, qf2_loss: 3.91465, policy_loss: -213.21796, policy_entropy: -1.00056, alpha: 0.22370, time: 33.24361
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   382 ----
[CW] collect: return: 356.42590, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 3.78621, qf2_loss: 3.76784, policy_loss: -212.78750, policy_entropy: -0.99623, alpha: 0.22318, time: 33.29457
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   383 ----
[CW] collect: return: 250.12831, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 3.75687, qf2_loss: 3.79049, policy_loss: -214.25863, policy_entropy: -1.00988, alpha: 0.22396, time: 33.60070
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   384 ----
[CW] collect: return: 375.24843, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 3.62089, qf2_loss: 3.65685, policy_loss: -213.03929, policy_entropy: -1.00146, alpha: 0.22484, time: 33.47027
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   385 ----
[CW] collect: return: 361.04019, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 4.75858, qf2_loss: 4.72114, policy_loss: -214.02857, policy_entropy: -0.99481, alpha: 0.22491, time: 33.58783
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   386 ----
[CW] collect: return: 325.62880, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 3.82781, qf2_loss: 3.82999, policy_loss: -214.01384, policy_entropy: -1.00379, alpha: 0.22486, time: 33.52348
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   387 ----
[CW] collect: return: 246.47677, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 4.21552, qf2_loss: 4.17606, policy_loss: -214.76707, policy_entropy: -0.98815, alpha: 0.22439, time: 35.71054
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   388 ----
[CW] collect: return: 252.82172, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 4.22228, qf2_loss: 4.20460, policy_loss: -214.48572, policy_entropy: -1.00010, alpha: 0.22294, time: 33.52993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   389 ----
[CW] collect: return: 251.31655, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 3.80558, qf2_loss: 3.80337, policy_loss: -215.42970, policy_entropy: -1.00329, alpha: 0.22264, time: 33.44510
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   390 ----
[CW] collect: return: 350.94786, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 3.80466, qf2_loss: 3.82302, policy_loss: -214.61453, policy_entropy: -0.99409, alpha: 0.22305, time: 33.66339
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   391 ----
[CW] collect: return: 318.86322, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 3.67257, qf2_loss: 3.64501, policy_loss: -215.10670, policy_entropy: -1.00330, alpha: 0.22278, time: 33.56815
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   392 ----
[CW] collect: return: 395.30528, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 5.10579, qf2_loss: 5.10299, policy_loss: -215.93146, policy_entropy: -1.00204, alpha: 0.22351, time: 33.55883
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   393 ----
[CW] collect: return: 264.54632, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 3.77870, qf2_loss: 3.79194, policy_loss: -215.56787, policy_entropy: -1.00734, alpha: 0.22411, time: 33.69698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   394 ----
[CW] collect: return: 380.76896, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 3.75591, qf2_loss: 3.75521, policy_loss: -216.32066, policy_entropy: -1.00130, alpha: 0.22519, time: 33.60560
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   395 ----
[CW] collect: return: 285.46983, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 3.56850, qf2_loss: 3.55242, policy_loss: -215.90175, policy_entropy: -1.00489, alpha: 0.22542, time: 33.45146
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   396 ----
[CW] collect: return: 364.92898, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 4.20872, qf2_loss: 4.23065, policy_loss: -217.07019, policy_entropy: -1.00386, alpha: 0.22607, time: 33.58841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   397 ----
[CW] collect: return: 383.42696, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 4.11299, qf2_loss: 4.11657, policy_loss: -216.73982, policy_entropy: -0.99183, alpha: 0.22593, time: 33.62338
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   398 ----
[CW] collect: return: 265.82124, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 3.67758, qf2_loss: 3.65770, policy_loss: -216.94657, policy_entropy: -1.00180, alpha: 0.22498, time: 33.66735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   399 ----
[CW] collect: return: 282.08864, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 3.70271, qf2_loss: 3.69899, policy_loss: -217.11474, policy_entropy: -1.00733, alpha: 0.22593, time: 33.73863
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   400 ----
[CW] collect: return: 321.24732, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 3.89240, qf2_loss: 3.86877, policy_loss: -217.58329, policy_entropy: -1.00931, alpha: 0.22712, time: 33.35071
[CW] eval: return: 340.46573, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   401 ----
[CW] collect: return: 334.63456, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 3.50703, qf2_loss: 3.51145, policy_loss: -217.93379, policy_entropy: -0.99928, alpha: 0.22827, time: 33.31758
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   402 ----
[CW] collect: return: 315.71358, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 3.85215, qf2_loss: 3.87809, policy_loss: -217.51491, policy_entropy: -0.99672, alpha: 0.22790, time: 33.40665
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   403 ----
[CW] collect: return: 295.53915, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 4.58834, qf2_loss: 4.61691, policy_loss: -218.43113, policy_entropy: -1.00348, alpha: 0.22732, time: 33.52799
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   404 ----
[CW] collect: return: 300.15636, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 4.30553, qf2_loss: 4.29370, policy_loss: -217.42882, policy_entropy: -0.98313, alpha: 0.22656, time: 33.54785
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   405 ----
[CW] collect: return: 326.05543, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 3.80621, qf2_loss: 3.77507, policy_loss: -217.95239, policy_entropy: -1.00227, alpha: 0.22522, time: 33.21502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   406 ----
[CW] collect: return: 365.58568, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 4.60097, qf2_loss: 4.55143, policy_loss: -218.16637, policy_entropy: -0.99957, alpha: 0.22538, time: 33.24341
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   407 ----
[CW] collect: return: 342.70426, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 4.04641, qf2_loss: 4.06664, policy_loss: -219.43864, policy_entropy: -0.99949, alpha: 0.22512, time: 33.61130
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   408 ----
[CW] collect: return: 386.99253, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 3.63603, qf2_loss: 3.61717, policy_loss: -219.71074, policy_entropy: -1.00531, alpha: 0.22578, time: 33.58291
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   409 ----
[CW] collect: return: 296.69342, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 3.89785, qf2_loss: 3.88766, policy_loss: -219.66612, policy_entropy: -0.99410, alpha: 0.22581, time: 33.72871
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   410 ----
[CW] collect: return: 345.55421, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 4.03285, qf2_loss: 4.03516, policy_loss: -219.46050, policy_entropy: -0.99243, alpha: 0.22459, time: 33.16790
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   411 ----
[CW] collect: return: 495.23688, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 3.57638, qf2_loss: 3.56931, policy_loss: -220.44275, policy_entropy: -1.00146, alpha: 0.22361, time: 33.25691
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   412 ----
[CW] collect: return: 363.41921, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 4.13305, qf2_loss: 4.10170, policy_loss: -220.55891, policy_entropy: -0.99822, alpha: 0.22435, time: 33.62902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   413 ----
[CW] collect: return: 446.37682, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 4.36525, qf2_loss: 4.33034, policy_loss: -220.10327, policy_entropy: -0.98722, alpha: 0.22304, time: 33.47872
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   414 ----
[CW] collect: return: 391.53404, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 4.28535, qf2_loss: 4.31013, policy_loss: -220.41716, policy_entropy: -0.99975, alpha: 0.22178, time: 33.63491
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   415 ----
[CW] collect: return: 435.86330, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 3.73092, qf2_loss: 3.73054, policy_loss: -221.23527, policy_entropy: -0.99802, alpha: 0.22149, time: 33.56470
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   416 ----
[CW] collect: return: 351.33416, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 3.54067, qf2_loss: 3.54304, policy_loss: -220.59259, policy_entropy: -0.99778, alpha: 0.22129, time: 33.58018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   417 ----
[CW] collect: return: 404.10089, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 3.98325, qf2_loss: 3.95385, policy_loss: -220.71751, policy_entropy: -1.00103, alpha: 0.22105, time: 33.83442
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   418 ----
[CW] collect: return: 295.71016, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 7.45886, qf2_loss: 7.42545, policy_loss: -221.26983, policy_entropy: -0.98739, alpha: 0.22049, time: 33.29738
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   419 ----
[CW] collect: return: 335.11593, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 4.01535, qf2_loss: 4.01033, policy_loss: -221.65783, policy_entropy: -0.99937, alpha: 0.21878, time: 33.59167
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   420 ----
[CW] collect: return: 310.13977, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 3.44317, qf2_loss: 3.43623, policy_loss: -221.65710, policy_entropy: -1.01187, alpha: 0.21974, time: 33.41433
[CW] eval: return: 328.77243, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   421 ----
[CW] collect: return: 342.73785, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 3.36418, qf2_loss: 3.40858, policy_loss: -221.76746, policy_entropy: -1.00441, alpha: 0.22129, time: 33.57577
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   422 ----
[CW] collect: return: 389.94512, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 3.70514, qf2_loss: 3.73254, policy_loss: -222.10157, policy_entropy: -1.00773, alpha: 0.22223, time: 33.51967
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   423 ----
[CW] collect: return: 327.69867, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 3.98749, qf2_loss: 4.00327, policy_loss: -221.78478, policy_entropy: -1.00162, alpha: 0.22341, time: 33.39963
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   424 ----
[CW] collect: return: 371.61335, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 4.30082, qf2_loss: 4.32745, policy_loss: -222.75740, policy_entropy: -1.00898, alpha: 0.22393, time: 33.33090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   425 ----
[CW] collect: return: 297.90648, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 4.09116, qf2_loss: 4.09559, policy_loss: -222.95389, policy_entropy: -1.00404, alpha: 0.22539, time: 33.47325
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   426 ----
[CW] collect: return: 368.02430, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 3.91217, qf2_loss: 3.86809, policy_loss: -223.44299, policy_entropy: -1.00678, alpha: 0.22642, time: 33.59264
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   427 ----
[CW] collect: return: 328.48850, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 4.54828, qf2_loss: 4.53261, policy_loss: -223.66735, policy_entropy: -1.00181, alpha: 0.22697, time: 33.46251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   428 ----
[CW] collect: return: 288.46346, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 4.39975, qf2_loss: 4.37631, policy_loss: -223.23534, policy_entropy: -0.99537, alpha: 0.22695, time: 33.62584
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   429 ----
[CW] collect: return: 277.71239, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 3.91881, qf2_loss: 3.92061, policy_loss: -223.63310, policy_entropy: -1.00177, alpha: 0.22623, time: 33.43286
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   430 ----
[CW] collect: return: 363.67356, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 3.87288, qf2_loss: 3.89320, policy_loss: -223.86202, policy_entropy: -0.99402, alpha: 0.22642, time: 33.37153
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   431 ----
[CW] collect: return: 390.62601, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 4.73446, qf2_loss: 4.70786, policy_loss: -223.90841, policy_entropy: -0.99739, alpha: 0.22526, time: 33.58764
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   432 ----
[CW] collect: return: 320.51549, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 4.20865, qf2_loss: 4.21666, policy_loss: -224.10468, policy_entropy: -1.00018, alpha: 0.22517, time: 33.66516
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   433 ----
[CW] collect: return: 298.37888, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 3.76736, qf2_loss: 3.74593, policy_loss: -224.40788, policy_entropy: -0.99766, alpha: 0.22458, time: 33.75018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   434 ----
[CW] collect: return: 319.40250, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 4.16109, qf2_loss: 4.13217, policy_loss: -224.12661, policy_entropy: -0.99986, alpha: 0.22506, time: 33.49391
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   435 ----
[CW] collect: return: 385.68679, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 3.95959, qf2_loss: 3.91028, policy_loss: -224.26687, policy_entropy: -0.99935, alpha: 0.22489, time: 33.16372
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   436 ----
[CW] collect: return: 379.01995, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 3.90437, qf2_loss: 3.89273, policy_loss: -224.43590, policy_entropy: -1.00799, alpha: 0.22504, time: 33.47429
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   437 ----
[CW] collect: return: 431.63812, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 4.43317, qf2_loss: 4.47451, policy_loss: -225.10390, policy_entropy: -0.99858, alpha: 0.22542, time: 33.50034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   438 ----
[CW] collect: return: 410.28658, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 3.89678, qf2_loss: 3.87254, policy_loss: -226.39713, policy_entropy: -0.99246, alpha: 0.22499, time: 33.28266
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   439 ----
[CW] collect: return: 258.93602, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 4.35439, qf2_loss: 4.40211, policy_loss: -225.51934, policy_entropy: -1.00469, alpha: 0.22519, time: 41.23494
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   440 ----
[CW] collect: return: 336.85686, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 4.08581, qf2_loss: 4.06132, policy_loss: -225.97758, policy_entropy: -0.99941, alpha: 0.22527, time: 33.34889
[CW] eval: return: 327.83635, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   441 ----
[CW] collect: return: 346.35393, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 4.43558, qf2_loss: 4.47157, policy_loss: -225.94452, policy_entropy: -1.00021, alpha: 0.22531, time: 33.66174
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   442 ----
[CW] collect: return: 342.43119, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 4.15682, qf2_loss: 4.16307, policy_loss: -226.04300, policy_entropy: -0.99854, alpha: 0.22470, time: 33.45925
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   443 ----
[CW] collect: return: 394.84660, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 4.43439, qf2_loss: 4.37826, policy_loss: -226.79880, policy_entropy: -1.00831, alpha: 0.22568, time: 33.45613
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   444 ----
[CW] collect: return: 351.02593, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 3.89897, qf2_loss: 3.89367, policy_loss: -226.96874, policy_entropy: -0.99547, alpha: 0.22598, time: 33.41888
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   445 ----
[CW] collect: return: 393.64768, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 4.08186, qf2_loss: 4.11967, policy_loss: -226.90577, policy_entropy: -1.00627, alpha: 0.22609, time: 33.57275
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   446 ----
[CW] collect: return: 321.96888, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 3.98005, qf2_loss: 3.95549, policy_loss: -227.27024, policy_entropy: -0.99419, alpha: 0.22562, time: 33.89145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   447 ----
[CW] collect: return: 285.18844, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 3.93116, qf2_loss: 3.91948, policy_loss: -226.97140, policy_entropy: -1.00126, alpha: 0.22585, time: 33.46574
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   448 ----
[CW] collect: return: 359.26874, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 3.94943, qf2_loss: 3.99774, policy_loss: -228.02163, policy_entropy: -1.00336, alpha: 0.22606, time: 33.66192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   449 ----
[CW] collect: return: 400.06017, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 4.17702, qf2_loss: 4.14152, policy_loss: -227.32173, policy_entropy: -0.99214, alpha: 0.22591, time: 33.68099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   450 ----
[CW] collect: return: 342.51114, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 4.44687, qf2_loss: 4.44599, policy_loss: -226.71467, policy_entropy: -1.00334, alpha: 0.22528, time: 33.59173
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   451 ----
[CW] collect: return: 338.23254, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 3.66797, qf2_loss: 3.61541, policy_loss: -228.04040, policy_entropy: -0.99934, alpha: 0.22586, time: 33.36613
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   452 ----
[CW] collect: return: 281.55349, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 3.75588, qf2_loss: 3.78679, policy_loss: -227.85928, policy_entropy: -0.99918, alpha: 0.22567, time: 33.45099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   453 ----
[CW] collect: return: 319.54332, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 4.20026, qf2_loss: 4.16824, policy_loss: -228.30166, policy_entropy: -1.00234, alpha: 0.22574, time: 33.45847
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   454 ----
[CW] collect: return: 352.38404, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 5.03408, qf2_loss: 5.02428, policy_loss: -228.67500, policy_entropy: -1.00024, alpha: 0.22561, time: 33.03145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   455 ----
[CW] collect: return: 325.45760, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 4.12539, qf2_loss: 4.14557, policy_loss: -228.24148, policy_entropy: -0.99745, alpha: 0.22530, time: 33.61100
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   456 ----
[CW] collect: return: 343.80812, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 4.66004, qf2_loss: 4.66829, policy_loss: -229.31692, policy_entropy: -0.99688, alpha: 0.22554, time: 33.32095
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   457 ----
[CW] collect: return: 326.45788, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 4.82818, qf2_loss: 4.79422, policy_loss: -228.84072, policy_entropy: -0.99773, alpha: 0.22445, time: 33.56597
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   458 ----
[CW] collect: return: 355.87952, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 3.52575, qf2_loss: 3.51278, policy_loss: -228.65907, policy_entropy: -1.00499, alpha: 0.22537, time: 33.14207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   459 ----
[CW] collect: return: 316.18194, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 4.19787, qf2_loss: 4.17596, policy_loss: -229.52883, policy_entropy: -0.99968, alpha: 0.22520, time: 33.30164
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   460 ----
[CW] collect: return: 344.37158, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 3.73848, qf2_loss: 3.74053, policy_loss: -228.61510, policy_entropy: -0.99870, alpha: 0.22536, time: 33.52480
[CW] eval: return: 347.47938, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   461 ----
[CW] collect: return: 365.37611, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 3.68619, qf2_loss: 3.65763, policy_loss: -230.37387, policy_entropy: -1.00183, alpha: 0.22551, time: 33.45331
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   462 ----
[CW] collect: return: 362.55276, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 4.25572, qf2_loss: 4.32443, policy_loss: -230.31824, policy_entropy: -1.00545, alpha: 0.22606, time: 33.58588
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   463 ----
[CW] collect: return: 330.10144, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 4.44042, qf2_loss: 4.40548, policy_loss: -230.21692, policy_entropy: -0.99013, alpha: 0.22589, time: 33.54521
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   464 ----
[CW] collect: return: 318.13562, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 3.58900, qf2_loss: 3.58096, policy_loss: -230.00287, policy_entropy: -0.99799, alpha: 0.22496, time: 33.47524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   465 ----
[CW] collect: return: 370.65585, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 3.45234, qf2_loss: 3.47139, policy_loss: -230.61405, policy_entropy: -0.99901, alpha: 0.22457, time: 33.74042
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   466 ----
[CW] collect: return: 331.20027, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 3.47235, qf2_loss: 3.48235, policy_loss: -230.46609, policy_entropy: -0.99958, alpha: 0.22391, time: 33.50659
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   467 ----
[CW] collect: return: 373.98336, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 3.99646, qf2_loss: 4.01918, policy_loss: -230.98730, policy_entropy: -0.99867, alpha: 0.22413, time: 33.42977
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   468 ----
[CW] collect: return: 328.89238, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 4.22801, qf2_loss: 4.26523, policy_loss: -232.03018, policy_entropy: -1.00221, alpha: 0.22412, time: 33.46669
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   469 ----
[CW] collect: return: 310.72734, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 4.48516, qf2_loss: 4.45440, policy_loss: -231.10533, policy_entropy: -0.99547, alpha: 0.22405, time: 33.71821
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   470 ----
[CW] collect: return: 297.48448, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 3.72796, qf2_loss: 3.68527, policy_loss: -231.45703, policy_entropy: -1.00218, alpha: 0.22328, time: 33.38323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   471 ----
[CW] collect: return: 347.59737, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 3.87740, qf2_loss: 3.89921, policy_loss: -231.70850, policy_entropy: -0.98995, alpha: 0.22320, time: 33.40995
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   472 ----
[CW] collect: return: 273.40243, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 3.94906, qf2_loss: 3.95833, policy_loss: -232.23012, policy_entropy: -1.00283, alpha: 0.22195, time: 33.25872
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   473 ----
[CW] collect: return: 324.45527, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 5.39969, qf2_loss: 5.36951, policy_loss: -232.64298, policy_entropy: -0.98941, alpha: 0.22180, time: 33.66434
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   474 ----
[CW] collect: return: 358.93981, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 3.97042, qf2_loss: 3.98323, policy_loss: -231.40989, policy_entropy: -0.99777, alpha: 0.22029, time: 33.86613
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   475 ----
[CW] collect: return: 265.93576, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 4.39596, qf2_loss: 4.36689, policy_loss: -232.47890, policy_entropy: -0.99646, alpha: 0.21990, time: 33.56265
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   476 ----
[CW] collect: return: 351.30897, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 3.86969, qf2_loss: 3.84726, policy_loss: -232.58751, policy_entropy: -0.99844, alpha: 0.21978, time: 33.44476
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   477 ----
[CW] collect: return: 398.80761, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 3.58850, qf2_loss: 3.56552, policy_loss: -232.10371, policy_entropy: -1.00055, alpha: 0.21947, time: 33.53093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   478 ----
[CW] collect: return: 416.48288, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 3.71901, qf2_loss: 3.69264, policy_loss: -233.13250, policy_entropy: -1.00419, alpha: 0.21994, time: 33.32047
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   479 ----
[CW] collect: return: 384.86046, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 3.88828, qf2_loss: 3.89891, policy_loss: -232.76776, policy_entropy: -1.00105, alpha: 0.22031, time: 33.27077
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   480 ----
[CW] collect: return: 362.33486, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 4.38242, qf2_loss: 4.37941, policy_loss: -233.65661, policy_entropy: -0.99362, alpha: 0.22013, time: 33.77519
[CW] eval: return: 352.30405, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   481 ----
[CW] collect: return: 349.38973, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 4.06014, qf2_loss: 4.05500, policy_loss: -233.46274, policy_entropy: -0.99157, alpha: 0.21921, time: 33.32052
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   482 ----
[CW] collect: return: 352.29058, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 4.11084, qf2_loss: 4.14323, policy_loss: -233.88950, policy_entropy: -1.00627, alpha: 0.21825, time: 33.49496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   483 ----
[CW] collect: return: 357.46320, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 4.20339, qf2_loss: 4.20754, policy_loss: -233.22131, policy_entropy: -1.00072, alpha: 0.21841, time: 33.23988
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   484 ----
[CW] collect: return: 234.71216, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 3.78513, qf2_loss: 3.73412, policy_loss: -233.93563, policy_entropy: -1.00322, alpha: 0.21920, time: 33.71971
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   485 ----
[CW] collect: return: 328.33488, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 5.22651, qf2_loss: 5.16583, policy_loss: -234.47338, policy_entropy: -0.99530, alpha: 0.21935, time: 33.24510
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   486 ----
[CW] collect: return: 297.31625, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 4.39881, qf2_loss: 4.38095, policy_loss: -234.31887, policy_entropy: -0.99734, alpha: 0.21856, time: 33.29402
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   487 ----
[CW] collect: return: 313.40702, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 5.08484, qf2_loss: 5.08487, policy_loss: -234.48880, policy_entropy: -0.99768, alpha: 0.21732, time: 33.51334
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   488 ----
[CW] collect: return: 304.58090, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 4.01067, qf2_loss: 4.00442, policy_loss: -234.42793, policy_entropy: -0.99996, alpha: 0.21787, time: 33.59428
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   489 ----
[CW] collect: return: 395.11150, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 4.76771, qf2_loss: 4.76295, policy_loss: -234.55545, policy_entropy: -1.00942, alpha: 0.21863, time: 33.57269
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   490 ----
[CW] collect: return: 389.31730, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 3.86956, qf2_loss: 3.83479, policy_loss: -234.63922, policy_entropy: -1.00281, alpha: 0.21947, time: 32.97565
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   491 ----
[CW] collect: return: 389.84710, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 3.79133, qf2_loss: 3.80705, policy_loss: -235.03821, policy_entropy: -1.00365, alpha: 0.22015, time: 33.24459
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   492 ----
[CW] collect: return: 359.46679, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 3.92666, qf2_loss: 3.94564, policy_loss: -234.92211, policy_entropy: -0.99198, alpha: 0.22017, time: 33.35491
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   493 ----
[CW] collect: return: 383.48557, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 3.87104, qf2_loss: 3.86790, policy_loss: -235.03537, policy_entropy: -1.00674, alpha: 0.21955, time: 33.29694
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   494 ----
[CW] collect: return: 361.57849, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 4.11680, qf2_loss: 4.11390, policy_loss: -236.35395, policy_entropy: -1.00948, alpha: 0.22076, time: 33.50406
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   495 ----
[CW] collect: return: 254.69022, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 3.72998, qf2_loss: 3.74748, policy_loss: -236.27712, policy_entropy: -1.00218, alpha: 0.22185, time: 33.56687
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   496 ----
[CW] collect: return: 263.69626, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 3.69993, qf2_loss: 3.68955, policy_loss: -236.00396, policy_entropy: -0.99965, alpha: 0.22230, time: 33.21867
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   497 ----
[CW] collect: return: 337.16226, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 4.22341, qf2_loss: 4.18130, policy_loss: -236.13958, policy_entropy: -0.99758, alpha: 0.22180, time: 33.34983
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   498 ----
[CW] collect: return: 369.58159, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 3.78398, qf2_loss: 3.76404, policy_loss: -236.39023, policy_entropy: -0.99257, alpha: 0.22144, time: 33.43765
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   499 ----
[CW] collect: return: 357.36234, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 4.01205, qf2_loss: 4.03633, policy_loss: -236.86417, policy_entropy: -1.00285, alpha: 0.22115, time: 34.75023
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   500 ----
[CW] collect: return: 360.93903, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 3.95122, qf2_loss: 3.94765, policy_loss: -236.41917, policy_entropy: -0.99727, alpha: 0.22080, time: 36.06665
[CW] eval: return: 345.99759, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   501 ----
[CW] collect: return: 385.61149, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 4.31114, qf2_loss: 4.35023, policy_loss: -236.99227, policy_entropy: -0.99482, alpha: 0.22029, time: 33.48894
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   502 ----
[CW] collect: return: 401.08370, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 4.16667, qf2_loss: 4.11673, policy_loss: -236.15696, policy_entropy: -0.99833, alpha: 0.21988, time: 33.41896
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   503 ----
[CW] collect: return: 350.09163, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 3.88957, qf2_loss: 3.89866, policy_loss: -237.06006, policy_entropy: -1.00329, alpha: 0.21980, time: 33.65550
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   504 ----
[CW] collect: return: 288.22560, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 4.75483, qf2_loss: 4.75798, policy_loss: -237.53048, policy_entropy: -0.99599, alpha: 0.21990, time: 33.26395
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   505 ----
[CW] collect: return: 347.44816, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 4.60543, qf2_loss: 4.57237, policy_loss: -238.15537, policy_entropy: -0.99426, alpha: 0.21856, time: 33.28501
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   506 ----
[CW] collect: return: 283.69843, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 4.66541, qf2_loss: 4.65809, policy_loss: -237.37588, policy_entropy: -0.99417, alpha: 0.21800, time: 33.86982
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   507 ----
[CW] collect: return: 371.18607, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 3.95274, qf2_loss: 3.94998, policy_loss: -237.98993, policy_entropy: -1.00877, alpha: 0.21841, time: 33.71332
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   508 ----
[CW] collect: return: 338.71656, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 3.40041, qf2_loss: 3.42595, policy_loss: -238.63512, policy_entropy: -1.00276, alpha: 0.21908, time: 33.47542
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   509 ----
[CW] collect: return: 289.09909, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 3.69265, qf2_loss: 3.70372, policy_loss: -237.82621, policy_entropy: -1.00669, alpha: 0.21949, time: 33.25515
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   510 ----
[CW] collect: return: 321.73431, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 3.66206, qf2_loss: 3.66029, policy_loss: -237.73132, policy_entropy: -1.00134, alpha: 0.22053, time: 33.41642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   511 ----
[CW] collect: return: 396.16872, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 3.36634, qf2_loss: 3.39869, policy_loss: -238.75438, policy_entropy: -1.00374, alpha: 0.22099, time: 33.54514
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   512 ----
[CW] collect: return: 356.16491, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 3.67563, qf2_loss: 3.67500, policy_loss: -238.05716, policy_entropy: -0.99217, alpha: 0.22065, time: 33.37282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   513 ----
[CW] collect: return: 385.74831, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 5.04685, qf2_loss: 5.00907, policy_loss: -238.66781, policy_entropy: -1.00204, alpha: 0.22039, time: 33.43266
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   514 ----
[CW] collect: return: 331.89900, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 3.72152, qf2_loss: 3.72604, policy_loss: -238.93931, policy_entropy: -1.00135, alpha: 0.22023, time: 33.57448
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   515 ----
[CW] collect: return: 361.64267, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 3.59800, qf2_loss: 3.59737, policy_loss: -238.76250, policy_entropy: -1.00681, alpha: 0.22099, time: 33.54465
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   516 ----
[CW] collect: return: 336.11017, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 4.31596, qf2_loss: 4.31158, policy_loss: -238.91394, policy_entropy: -1.00508, alpha: 0.22174, time: 33.57494
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   517 ----
[CW] collect: return: 397.99766, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 4.30562, qf2_loss: 4.38272, policy_loss: -239.63722, policy_entropy: -0.99655, alpha: 0.22256, time: 33.03203
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   518 ----
[CW] collect: return: 313.68097, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 4.22988, qf2_loss: 4.24277, policy_loss: -238.92446, policy_entropy: -0.99689, alpha: 0.22133, time: 33.56347
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   519 ----
[CW] collect: return: 425.57435, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 4.50813, qf2_loss: 4.49668, policy_loss: -239.08755, policy_entropy: -1.00043, alpha: 0.22150, time: 33.42003
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   520 ----
[CW] collect: return: 339.61997, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 3.75278, qf2_loss: 3.77530, policy_loss: -239.40076, policy_entropy: -1.00506, alpha: 0.22132, time: 33.48508
[CW] eval: return: 353.83146, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   521 ----
[CW] collect: return: 363.58441, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 3.96519, qf2_loss: 3.91903, policy_loss: -239.62660, policy_entropy: -0.98931, alpha: 0.22147, time: 33.30282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   522 ----
[CW] collect: return: 351.09336, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 4.32514, qf2_loss: 4.28741, policy_loss: -240.33713, policy_entropy: -0.99950, alpha: 0.22095, time: 33.52190
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   523 ----
[CW] collect: return: 353.96229, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 3.89831, qf2_loss: 3.91367, policy_loss: -239.76180, policy_entropy: -1.00469, alpha: 0.22046, time: 33.54418
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   524 ----
[CW] collect: return: 400.78698, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 3.85516, qf2_loss: 3.90476, policy_loss: -240.83367, policy_entropy: -1.01235, alpha: 0.22213, time: 33.50438
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   525 ----
[CW] collect: return: 333.79654, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 4.50742, qf2_loss: 4.52196, policy_loss: -240.63171, policy_entropy: -0.99470, alpha: 0.22267, time: 33.40711
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   526 ----
[CW] collect: return: 321.53687, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 3.76470, qf2_loss: 3.77568, policy_loss: -240.44928, policy_entropy: -1.00097, alpha: 0.22244, time: 33.20658
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   527 ----
[CW] collect: return: 281.67393, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 4.71446, qf2_loss: 4.73085, policy_loss: -240.01834, policy_entropy: -0.99074, alpha: 0.22182, time: 33.48694
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   528 ----
[CW] collect: return: 385.50880, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 4.63939, qf2_loss: 4.64668, policy_loss: -240.60450, policy_entropy: -1.00117, alpha: 0.22088, time: 33.67603
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   529 ----
[CW] collect: return: 333.11284, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 3.69530, qf2_loss: 3.66102, policy_loss: -240.37575, policy_entropy: -1.00358, alpha: 0.22104, time: 33.14639
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   530 ----
[CW] collect: return: 364.88095, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 3.47072, qf2_loss: 3.49010, policy_loss: -241.69230, policy_entropy: -1.00431, alpha: 0.22209, time: 33.51400
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   531 ----
[CW] collect: return: 297.42276, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 4.15080, qf2_loss: 4.15760, policy_loss: -241.94623, policy_entropy: -1.00278, alpha: 0.22240, time: 33.39669
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   532 ----
[CW] collect: return: 353.29290, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 4.34522, qf2_loss: 4.31725, policy_loss: -241.21395, policy_entropy: -0.99993, alpha: 0.22286, time: 33.50471
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   533 ----
[CW] collect: return: 282.89346, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 4.25028, qf2_loss: 4.25365, policy_loss: -241.76774, policy_entropy: -0.99115, alpha: 0.22236, time: 33.64144
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   534 ----
[CW] collect: return: 344.15222, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 3.74560, qf2_loss: 3.75884, policy_loss: -241.41029, policy_entropy: -0.99327, alpha: 0.22109, time: 33.49082
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   535 ----
[CW] collect: return: 327.75352, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 3.56214, qf2_loss: 3.54543, policy_loss: -241.31861, policy_entropy: -0.99557, alpha: 0.22020, time: 33.35369
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   536 ----
[CW] collect: return: 421.59947, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 3.59056, qf2_loss: 3.59779, policy_loss: -241.73116, policy_entropy: -1.00651, alpha: 0.21958, time: 33.19394
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   537 ----
[CW] collect: return: 390.90557, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 3.68937, qf2_loss: 3.68851, policy_loss: -241.42951, policy_entropy: -0.99678, alpha: 0.21987, time: 33.68196
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   538 ----
[CW] collect: return: 433.91241, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 4.02181, qf2_loss: 4.02271, policy_loss: -241.40737, policy_entropy: -0.98996, alpha: 0.21935, time: 33.16204
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   539 ----
[CW] collect: return: 388.48724, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 4.37216, qf2_loss: 4.37061, policy_loss: -242.66928, policy_entropy: -1.00692, alpha: 0.21878, time: 33.16228
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   540 ----
[CW] collect: return: 294.68805, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 4.04343, qf2_loss: 4.02637, policy_loss: -241.80635, policy_entropy: -0.99725, alpha: 0.21908, time: 33.46090
[CW] eval: return: 358.16383, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   541 ----
[CW] collect: return: 282.45718, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 3.62398, qf2_loss: 3.61079, policy_loss: -242.40910, policy_entropy: -0.99547, alpha: 0.21882, time: 33.40738
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   542 ----
[CW] collect: return: 348.72221, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 3.65684, qf2_loss: 3.67066, policy_loss: -242.36782, policy_entropy: -1.01170, alpha: 0.21906, time: 33.27929
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   543 ----
[CW] collect: return: 379.87411, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 4.13642, qf2_loss: 4.14678, policy_loss: -241.76086, policy_entropy: -0.99189, alpha: 0.21948, time: 33.30749
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   544 ----
[CW] collect: return: 433.14237, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 3.54034, qf2_loss: 3.50693, policy_loss: -242.87889, policy_entropy: -0.99646, alpha: 0.21887, time: 33.58331
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   545 ----
[CW] collect: return: 408.98013, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 4.68449, qf2_loss: 4.70068, policy_loss: -242.71561, policy_entropy: -1.00112, alpha: 0.21896, time: 33.51245
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   546 ----
[CW] collect: return: 354.76895, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 4.21709, qf2_loss: 4.22583, policy_loss: -242.97550, policy_entropy: -1.00446, alpha: 0.21824, time: 33.37902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   547 ----
[CW] collect: return: 329.44877, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 4.48938, qf2_loss: 4.51010, policy_loss: -243.21667, policy_entropy: -0.99862, alpha: 0.21894, time: 33.60615
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   548 ----
[CW] collect: return: 372.35396, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 4.23928, qf2_loss: 4.25847, policy_loss: -243.07953, policy_entropy: -0.99747, alpha: 0.21910, time: 33.56099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   549 ----
[CW] collect: return: 400.08922, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 3.83522, qf2_loss: 3.84933, policy_loss: -243.15679, policy_entropy: -1.00234, alpha: 0.21874, time: 33.69524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   550 ----
[CW] collect: return: 347.72758, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 3.75279, qf2_loss: 3.75286, policy_loss: -244.11416, policy_entropy: -1.00511, alpha: 0.21922, time: 33.55139
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   551 ----
[CW] collect: return: 318.96400, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 3.99396, qf2_loss: 4.00466, policy_loss: -243.74704, policy_entropy: -1.00322, alpha: 0.22016, time: 35.00061
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   552 ----
[CW] collect: return: 335.90776, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 3.99221, qf2_loss: 3.97630, policy_loss: -244.39770, policy_entropy: -1.00499, alpha: 0.22043, time: 33.27809
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   553 ----
[CW] collect: return: 327.09825, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 3.96003, qf2_loss: 3.98026, policy_loss: -243.87136, policy_entropy: -0.99678, alpha: 0.22155, time: 33.62113
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   554 ----
[CW] collect: return: 352.36588, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 4.43089, qf2_loss: 4.44434, policy_loss: -243.46851, policy_entropy: -0.99214, alpha: 0.22021, time: 33.71107
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   555 ----
[CW] collect: return: 384.47626, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 3.98418, qf2_loss: 3.95290, policy_loss: -244.47843, policy_entropy: -0.99977, alpha: 0.21892, time: 33.29944
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   556 ----
[CW] collect: return: 373.73396, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 4.16176, qf2_loss: 4.20850, policy_loss: -244.37675, policy_entropy: -1.00924, alpha: 0.22024, time: 33.09443
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   557 ----
[CW] collect: return: 333.91077, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 3.94349, qf2_loss: 3.96890, policy_loss: -244.45840, policy_entropy: -0.99780, alpha: 0.22054, time: 33.15939
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   558 ----
[CW] collect: return: 373.14581, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 4.47331, qf2_loss: 4.49576, policy_loss: -244.77493, policy_entropy: -1.00436, alpha: 0.22083, time: 33.58777
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   559 ----
[CW] collect: return: 379.79978, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 5.06361, qf2_loss: 5.06890, policy_loss: -244.22742, policy_entropy: -1.00297, alpha: 0.22153, time: 33.38546
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   560 ----
[CW] collect: return: 345.65689, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 4.10114, qf2_loss: 4.10372, policy_loss: -244.18983, policy_entropy: -0.99992, alpha: 0.22150, time: 33.62489
[CW] eval: return: 345.99294, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   561 ----
[CW] collect: return: 439.30948, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 4.11186, qf2_loss: 4.11141, policy_loss: -244.55564, policy_entropy: -1.00036, alpha: 0.22219, time: 33.22080
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   562 ----
[CW] collect: return: 385.78646, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 3.97953, qf2_loss: 3.98284, policy_loss: -245.16218, policy_entropy: -0.99631, alpha: 0.22134, time: 33.38030
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   563 ----
[CW] collect: return: 287.18658, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 3.87102, qf2_loss: 3.85968, policy_loss: -244.44848, policy_entropy: -0.99635, alpha: 0.22113, time: 33.51377
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   564 ----
[CW] collect: return: 331.19580, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 3.89359, qf2_loss: 3.90405, policy_loss: -244.80824, policy_entropy: -1.00268, alpha: 0.22028, time: 33.42926
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   565 ----
[CW] collect: return: 338.96668, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 3.89589, qf2_loss: 3.88535, policy_loss: -245.31427, policy_entropy: -1.00551, alpha: 0.22153, time: 33.60411
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   566 ----
[CW] collect: return: 393.53632, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 3.73966, qf2_loss: 3.79225, policy_loss: -244.67821, policy_entropy: -0.99499, alpha: 0.22132, time: 36.52172
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   567 ----
[CW] collect: return: 351.66039, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 4.00829, qf2_loss: 4.01472, policy_loss: -245.56706, policy_entropy: -1.00244, alpha: 0.22159, time: 33.62314
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   568 ----
[CW] collect: return: 321.98623, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 4.49896, qf2_loss: 4.52409, policy_loss: -245.34070, policy_entropy: -0.99276, alpha: 0.22118, time: 33.48623
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   569 ----
[CW] collect: return: 337.78155, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 4.09648, qf2_loss: 4.11053, policy_loss: -245.67444, policy_entropy: -1.00502, alpha: 0.22082, time: 33.63999
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   570 ----
[CW] collect: return: 303.41241, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 3.74286, qf2_loss: 3.69706, policy_loss: -245.61575, policy_entropy: -1.00208, alpha: 0.22144, time: 33.70703
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   571 ----
[CW] collect: return: 305.60730, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 4.01609, qf2_loss: 4.05912, policy_loss: -245.91308, policy_entropy: -1.00631, alpha: 0.22145, time: 33.28643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   572 ----
[CW] collect: return: 336.58624, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 4.02730, qf2_loss: 4.02171, policy_loss: -245.76162, policy_entropy: -0.99898, alpha: 0.22273, time: 33.63383
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   573 ----
[CW] collect: return: 331.01487, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 4.15009, qf2_loss: 4.15359, policy_loss: -245.67301, policy_entropy: -0.99865, alpha: 0.22199, time: 33.48593
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   574 ----
[CW] collect: return: 287.26328, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 4.22581, qf2_loss: 4.23008, policy_loss: -245.98120, policy_entropy: -0.99936, alpha: 0.22215, time: 33.72883
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   575 ----
[CW] collect: return: 403.41882, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 4.34268, qf2_loss: 4.35397, policy_loss: -246.25008, policy_entropy: -0.99736, alpha: 0.22152, time: 33.07403
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   576 ----
[CW] collect: return: 377.71844, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 4.79723, qf2_loss: 4.80099, policy_loss: -246.60756, policy_entropy: -0.98942, alpha: 0.22082, time: 33.62673
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   577 ----
[CW] collect: return: 368.70400, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 4.87715, qf2_loss: 4.85066, policy_loss: -245.69864, policy_entropy: -0.99601, alpha: 0.21908, time: 33.37417
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   578 ----
[CW] collect: return: 335.46999, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 4.20554, qf2_loss: 4.21697, policy_loss: -246.30897, policy_entropy: -1.00351, alpha: 0.21881, time: 33.56115
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   579 ----
[CW] collect: return: 383.44423, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 3.69838, qf2_loss: 3.67984, policy_loss: -246.91117, policy_entropy: -0.99960, alpha: 0.21903, time: 33.17121
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   580 ----
[CW] collect: return: 341.67000, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 3.42689, qf2_loss: 3.44369, policy_loss: -246.75504, policy_entropy: -1.00132, alpha: 0.21983, time: 33.75222
[CW] eval: return: 308.14345, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   581 ----
[CW] collect: return: 329.38226, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 3.72775, qf2_loss: 3.75344, policy_loss: -247.08921, policy_entropy: -1.01001, alpha: 0.22039, time: 33.32960
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   582 ----
[CW] collect: return: 302.39910, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 3.61190, qf2_loss: 3.64085, policy_loss: -247.14521, policy_entropy: -1.00015, alpha: 0.22180, time: 33.55573
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   583 ----
[CW] collect: return: 322.33608, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 4.17542, qf2_loss: 4.18018, policy_loss: -247.06413, policy_entropy: -1.00215, alpha: 0.22146, time: 33.75092
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   584 ----
[CW] collect: return: 322.63420, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 5.38484, qf2_loss: 5.37343, policy_loss: -246.60517, policy_entropy: -0.99467, alpha: 0.22134, time: 33.33787
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   585 ----
[CW] collect: return: 327.54598, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 4.56864, qf2_loss: 4.60568, policy_loss: -247.46428, policy_entropy: -0.99530, alpha: 0.22062, time: 33.68779
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   586 ----
[CW] collect: return: 307.68074, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 3.87227, qf2_loss: 3.82612, policy_loss: -247.51748, policy_entropy: -0.99438, alpha: 0.21998, time: 33.61303
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   587 ----
[CW] collect: return: 431.02715, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 3.74416, qf2_loss: 3.75159, policy_loss: -247.90742, policy_entropy: -1.00541, alpha: 0.21936, time: 33.74595
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   588 ----
[CW] collect: return: 339.59354, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 3.79587, qf2_loss: 3.78886, policy_loss: -247.58232, policy_entropy: -1.00533, alpha: 0.21993, time: 33.51293
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   589 ----
[CW] collect: return: 336.76509, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 4.32177, qf2_loss: 4.36386, policy_loss: -248.04323, policy_entropy: -0.99027, alpha: 0.22062, time: 33.66242
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   590 ----
[CW] collect: return: 271.95243, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 4.34679, qf2_loss: 4.43305, policy_loss: -248.40524, policy_entropy: -0.99886, alpha: 0.21921, time: 33.30586
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   591 ----
[CW] collect: return: 357.13165, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 4.37189, qf2_loss: 4.34946, policy_loss: -247.46551, policy_entropy: -0.99280, alpha: 0.21859, time: 33.14661
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   592 ----
[CW] collect: return: 308.45833, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 5.07028, qf2_loss: 5.04489, policy_loss: -248.06641, policy_entropy: -0.99359, alpha: 0.21767, time: 33.73200
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   593 ----
[CW] collect: return: 332.70503, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 4.08514, qf2_loss: 4.07477, policy_loss: -249.12529, policy_entropy: -1.01109, alpha: 0.21745, time: 33.48742
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   594 ----
[CW] collect: return: 313.88345, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 4.59310, qf2_loss: 4.60509, policy_loss: -248.29395, policy_entropy: -0.99910, alpha: 0.21891, time: 33.77978
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   595 ----
[CW] collect: return: 485.46761, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 4.07349, qf2_loss: 4.06384, policy_loss: -248.70361, policy_entropy: -0.99899, alpha: 0.21793, time: 33.71452
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   596 ----
[CW] collect: return: 413.61166, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 3.64192, qf2_loss: 3.66465, policy_loss: -248.62481, policy_entropy: -0.99751, alpha: 0.21852, time: 33.41899
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   597 ----
[CW] collect: return: 335.73577, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 3.99510, qf2_loss: 3.99834, policy_loss: -248.58217, policy_entropy: -0.99204, alpha: 0.21706, time: 33.64855
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   598 ----
[CW] collect: return: 378.59503, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 4.21320, qf2_loss: 4.21496, policy_loss: -248.94756, policy_entropy: -1.00976, alpha: 0.21732, time: 33.46581
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   599 ----
[CW] collect: return: 299.02090, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 4.61178, qf2_loss: 4.62872, policy_loss: -249.34677, policy_entropy: -0.99825, alpha: 0.21794, time: 33.72412
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   600 ----
[CW] collect: return: 360.96218, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 4.12919, qf2_loss: 4.12436, policy_loss: -248.30864, policy_entropy: -0.98722, alpha: 0.21675, time: 33.52562
[CW] eval: return: 354.01295, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   601 ----
[CW] collect: return: 438.29447, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 3.93076, qf2_loss: 3.97120, policy_loss: -249.05610, policy_entropy: -1.00466, alpha: 0.21599, time: 33.59641
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   602 ----
[CW] collect: return: 220.54875, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 4.38465, qf2_loss: 4.36652, policy_loss: -249.28551, policy_entropy: -1.00259, alpha: 0.21683, time: 33.49284
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   603 ----
[CW] collect: return: 321.22356, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 4.56256, qf2_loss: 4.51416, policy_loss: -249.26217, policy_entropy: -1.00535, alpha: 0.21704, time: 33.56980
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   604 ----
[CW] collect: return: 320.80423, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 4.14770, qf2_loss: 4.15563, policy_loss: -249.20778, policy_entropy: -1.00381, alpha: 0.21839, time: 33.43307
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   605 ----
[CW] collect: return: 362.28015, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 3.90793, qf2_loss: 3.90934, policy_loss: -249.48138, policy_entropy: -0.99383, alpha: 0.21794, time: 33.67606
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   606 ----
[CW] collect: return: 335.94599, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 4.02699, qf2_loss: 3.99310, policy_loss: -249.39582, policy_entropy: -1.00307, alpha: 0.21743, time: 33.51508
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   607 ----
[CW] collect: return: 306.12027, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 3.79110, qf2_loss: 3.81135, policy_loss: -249.19929, policy_entropy: -0.99782, alpha: 0.21799, time: 33.55292
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   608 ----
[CW] collect: return: 340.84381, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 4.30292, qf2_loss: 4.37580, policy_loss: -249.42842, policy_entropy: -1.00720, alpha: 0.21812, time: 33.49990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   609 ----
[CW] collect: return: 355.85003, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 4.00743, qf2_loss: 4.00964, policy_loss: -249.65838, policy_entropy: -1.00128, alpha: 0.21911, time: 33.64021
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   610 ----
[CW] collect: return: 347.12027, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 3.80181, qf2_loss: 3.84222, policy_loss: -249.52026, policy_entropy: -1.00540, alpha: 0.21905, time: 33.59044
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   611 ----
[CW] collect: return: 365.57883, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 3.72247, qf2_loss: 3.74551, policy_loss: -249.45502, policy_entropy: -0.99145, alpha: 0.21938, time: 33.68721
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   612 ----
[CW] collect: return: 406.99386, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 4.23050, qf2_loss: 4.23595, policy_loss: -250.19456, policy_entropy: -0.98855, alpha: 0.21771, time: 33.06086
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   613 ----
[CW] collect: return: 425.17951, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 4.06472, qf2_loss: 4.08538, policy_loss: -249.78169, policy_entropy: -0.99335, alpha: 0.21599, time: 33.50458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   614 ----
[CW] collect: return: 327.60395, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 4.17814, qf2_loss: 4.19773, policy_loss: -250.59242, policy_entropy: -1.00860, alpha: 0.21597, time: 33.25013
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   615 ----
[CW] collect: return: 352.63906, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 4.06101, qf2_loss: 4.06456, policy_loss: -250.57867, policy_entropy: -0.99475, alpha: 0.21638, time: 33.65887
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   616 ----
[CW] collect: return: 370.55101, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 4.15662, qf2_loss: 4.14579, policy_loss: -249.59475, policy_entropy: -1.00222, alpha: 0.21612, time: 33.53921
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   617 ----
[CW] collect: return: 381.83042, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 4.00285, qf2_loss: 3.99384, policy_loss: -251.10879, policy_entropy: -1.00046, alpha: 0.21648, time: 33.45626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   618 ----
[CW] collect: return: 347.37458, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 4.51832, qf2_loss: 4.60150, policy_loss: -250.71291, policy_entropy: -0.99794, alpha: 0.21636, time: 33.56758
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   619 ----
[CW] collect: return: 320.14996, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 4.31727, qf2_loss: 4.33447, policy_loss: -250.65344, policy_entropy: -0.99006, alpha: 0.21573, time: 33.40702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   620 ----
[CW] collect: return: 336.92243, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 3.88524, qf2_loss: 3.89262, policy_loss: -250.46577, policy_entropy: -1.00840, alpha: 0.21477, time: 33.45039
[CW] eval: return: 388.92764, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   621 ----
[CW] collect: return: 356.63138, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 3.68504, qf2_loss: 3.70178, policy_loss: -250.86017, policy_entropy: -0.99851, alpha: 0.21569, time: 34.70811
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   622 ----
[CW] collect: return: 350.44965, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 4.08997, qf2_loss: 4.05334, policy_loss: -251.10129, policy_entropy: -0.99645, alpha: 0.21550, time: 33.38765
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   623 ----
[CW] collect: return: 340.66885, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 4.57176, qf2_loss: 4.57737, policy_loss: -251.15946, policy_entropy: -0.99580, alpha: 0.21481, time: 33.60641
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   624 ----
[CW] collect: return: 335.57060, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 4.57534, qf2_loss: 4.59298, policy_loss: -251.33601, policy_entropy: -1.00670, alpha: 0.21498, time: 33.30700
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   625 ----
[CW] collect: return: 424.01966, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 3.87919, qf2_loss: 3.85896, policy_loss: -251.98854, policy_entropy: -0.99870, alpha: 0.21522, time: 33.60674
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   626 ----
[CW] collect: return: 352.69496, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 3.73687, qf2_loss: 3.73785, policy_loss: -250.31951, policy_entropy: -1.00252, alpha: 0.21521, time: 33.57171
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   627 ----
[CW] collect: return: 369.89898, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 4.02893, qf2_loss: 4.02871, policy_loss: -251.21557, policy_entropy: -1.00549, alpha: 0.21621, time: 33.33632
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   628 ----
[CW] collect: return: 327.16451, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 4.05158, qf2_loss: 4.02843, policy_loss: -251.93009, policy_entropy: -1.01234, alpha: 0.21699, time: 33.26900
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   629 ----
[CW] collect: return: 336.02783, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 3.86365, qf2_loss: 3.89572, policy_loss: -251.75930, policy_entropy: -1.00529, alpha: 0.21899, time: 33.49901
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   630 ----
[CW] collect: return: 375.54702, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 6.89405, qf2_loss: 6.86908, policy_loss: -251.96998, policy_entropy: -0.98817, alpha: 0.21900, time: 33.57608
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   631 ----
[CW] collect: return: 400.01643, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 5.82485, qf2_loss: 5.80355, policy_loss: -251.93959, policy_entropy: -0.99649, alpha: 0.21694, time: 33.27130
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   632 ----
[CW] collect: return: 312.74550, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 3.93568, qf2_loss: 3.96716, policy_loss: -251.76241, policy_entropy: -0.99793, alpha: 0.21676, time: 33.27281
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   633 ----
[CW] collect: return: 412.11392, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 3.76675, qf2_loss: 3.75028, policy_loss: -251.81206, policy_entropy: -1.00460, alpha: 0.21690, time: 33.38747
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   634 ----
[CW] collect: return: 324.45639, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 3.58995, qf2_loss: 3.61238, policy_loss: -251.17603, policy_entropy: -0.98979, alpha: 0.21653, time: 33.67111
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   635 ----
[CW] collect: return: 369.91019, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 3.47339, qf2_loss: 3.48227, policy_loss: -252.80410, policy_entropy: -0.99746, alpha: 0.21495, time: 33.70162
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   636 ----
[CW] collect: return: 406.94248, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 3.76208, qf2_loss: 3.79834, policy_loss: -252.24163, policy_entropy: -0.99955, alpha: 0.21544, time: 33.33683
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   637 ----
[CW] collect: return: 383.32729, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 4.20356, qf2_loss: 4.17599, policy_loss: -252.26628, policy_entropy: -1.00482, alpha: 0.21572, time: 33.26558
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   638 ----
[CW] collect: return: 369.18887, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 3.84080, qf2_loss: 3.81545, policy_loss: -251.71027, policy_entropy: -1.00772, alpha: 0.21700, time: 33.12670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   639 ----
[CW] collect: return: 278.38017, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 3.97503, qf2_loss: 4.03816, policy_loss: -252.47762, policy_entropy: -0.99826, alpha: 0.21710, time: 33.13116
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   640 ----
[CW] collect: return: 478.53674, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 4.10750, qf2_loss: 4.12081, policy_loss: -252.11860, policy_entropy: -1.00262, alpha: 0.21700, time: 33.09344
[CW] eval: return: 355.58311, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   641 ----
[CW] collect: return: 385.32590, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 4.48964, qf2_loss: 4.46463, policy_loss: -251.70663, policy_entropy: -0.99443, alpha: 0.21715, time: 33.34400
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   642 ----
[CW] collect: return: 381.88816, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 4.78819, qf2_loss: 4.81305, policy_loss: -252.26859, policy_entropy: -0.99969, alpha: 0.21626, time: 33.31732
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   643 ----
[CW] collect: return: 373.69085, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 4.78427, qf2_loss: 4.80952, policy_loss: -253.35936, policy_entropy: -1.00004, alpha: 0.21618, time: 33.26892
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   644 ----
[CW] collect: return: 343.82896, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 5.31366, qf2_loss: 5.33271, policy_loss: -254.01332, policy_entropy: -0.99744, alpha: 0.21611, time: 33.31661
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   645 ----
[CW] collect: return: 277.35782, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 3.78963, qf2_loss: 3.82788, policy_loss: -252.54275, policy_entropy: -1.00035, alpha: 0.21578, time: 33.55359
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   646 ----
[CW] collect: return: 301.51489, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 3.75620, qf2_loss: 3.73947, policy_loss: -253.12468, policy_entropy: -1.00079, alpha: 0.21555, time: 33.61441
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   647 ----
[CW] collect: return: 359.78619, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 4.52318, qf2_loss: 4.53053, policy_loss: -252.43905, policy_entropy: -1.00093, alpha: 0.21573, time: 33.72175
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   648 ----
[CW] collect: return: 375.65936, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 4.58075, qf2_loss: 4.63477, policy_loss: -253.64899, policy_entropy: -0.99852, alpha: 0.21609, time: 33.65218
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   649 ----
[CW] collect: return: 367.57214, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 3.88639, qf2_loss: 3.87266, policy_loss: -253.96294, policy_entropy: -0.99869, alpha: 0.21541, time: 33.30324
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   650 ----
[CW] collect: return: 329.19902, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 4.93615, qf2_loss: 4.92768, policy_loss: -253.35237, policy_entropy: -1.00172, alpha: 0.21602, time: 33.06155
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   651 ----
[CW] collect: return: 251.82386, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 5.06873, qf2_loss: 5.06913, policy_loss: -253.55059, policy_entropy: -1.00369, alpha: 0.21670, time: 33.61137
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   652 ----
[CW] collect: return: 356.33548, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 4.19788, qf2_loss: 4.19282, policy_loss: -253.64836, policy_entropy: -1.00120, alpha: 0.21678, time: 33.52955
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   653 ----
[CW] collect: return: 331.85048, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 3.94914, qf2_loss: 3.95511, policy_loss: -253.44846, policy_entropy: -0.99956, alpha: 0.21647, time: 33.55504
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   654 ----
[CW] collect: return: 347.27847, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 3.82652, qf2_loss: 3.82964, policy_loss: -253.57044, policy_entropy: -0.99704, alpha: 0.21646, time: 33.42997
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   655 ----
[CW] collect: return: 321.22754, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 4.17365, qf2_loss: 4.22479, policy_loss: -253.86513, policy_entropy: -1.00592, alpha: 0.21704, time: 33.51267
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   656 ----
[CW] collect: return: 341.56727, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 6.09925, qf2_loss: 6.11261, policy_loss: -253.98555, policy_entropy: -0.99637, alpha: 0.21713, time: 33.42257
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   657 ----
[CW] collect: return: 323.11019, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 4.00780, qf2_loss: 4.02914, policy_loss: -253.69836, policy_entropy: -0.99401, alpha: 0.21620, time: 33.83271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   658 ----
[CW] collect: return: 435.88996, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 3.88645, qf2_loss: 3.89665, policy_loss: -253.49416, policy_entropy: -1.00174, alpha: 0.21582, time: 33.74187
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   659 ----
[CW] collect: return: 399.80854, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 4.08651, qf2_loss: 4.10899, policy_loss: -253.75721, policy_entropy: -1.00675, alpha: 0.21656, time: 33.56191
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   660 ----
[CW] collect: return: 325.08604, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 4.05530, qf2_loss: 4.06454, policy_loss: -254.10337, policy_entropy: -0.99575, alpha: 0.21673, time: 33.81995
[CW] eval: return: 349.78648, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   661 ----
[CW] collect: return: 377.89397, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 4.16825, qf2_loss: 4.15749, policy_loss: -254.55345, policy_entropy: -1.00980, alpha: 0.21684, time: 33.62686
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   662 ----
[CW] collect: return: 404.42072, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 4.25958, qf2_loss: 4.24934, policy_loss: -254.09439, policy_entropy: -0.99439, alpha: 0.21774, time: 33.50976
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   663 ----
[CW] collect: return: 354.98736, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 4.47388, qf2_loss: 4.50902, policy_loss: -254.59579, policy_entropy: -0.99555, alpha: 0.21703, time: 35.36800
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   664 ----
[CW] collect: return: 342.88668, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 4.24346, qf2_loss: 4.23914, policy_loss: -254.34520, policy_entropy: -0.99888, alpha: 0.21641, time: 33.37409
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   665 ----
[CW] collect: return: 345.91078, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 4.68599, qf2_loss: 4.67908, policy_loss: -254.70406, policy_entropy: -1.00615, alpha: 0.21696, time: 33.78458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   666 ----
[CW] collect: return: 396.96601, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 4.02776, qf2_loss: 4.04106, policy_loss: -254.48772, policy_entropy: -0.99661, alpha: 0.21684, time: 33.28464
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   667 ----
[CW] collect: return: 393.12474, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 3.84989, qf2_loss: 3.82071, policy_loss: -254.28355, policy_entropy: -0.99993, alpha: 0.21622, time: 33.69652
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   668 ----
[CW] collect: return: 370.89230, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 4.43839, qf2_loss: 4.42318, policy_loss: -254.18963, policy_entropy: -0.99888, alpha: 0.21578, time: 33.08895
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   669 ----
[CW] collect: return: 352.95426, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 4.59268, qf2_loss: 4.56019, policy_loss: -254.95497, policy_entropy: -0.99603, alpha: 0.21630, time: 33.73099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   670 ----
[CW] collect: return: 398.88108, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 4.50715, qf2_loss: 4.51694, policy_loss: -254.55357, policy_entropy: -1.00137, alpha: 0.21612, time: 34.18432
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   671 ----
[CW] collect: return: 307.93755, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 5.08688, qf2_loss: 5.06356, policy_loss: -254.92638, policy_entropy: -1.00031, alpha: 0.21592, time: 33.65564
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   672 ----
[CW] collect: return: 374.52003, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 4.61301, qf2_loss: 4.56211, policy_loss: -254.53708, policy_entropy: -0.99418, alpha: 0.21550, time: 33.69464
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   673 ----
[CW] collect: return: 424.06052, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 4.31255, qf2_loss: 4.26196, policy_loss: -255.18697, policy_entropy: -1.01091, alpha: 0.21619, time: 33.56306
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   674 ----
[CW] collect: return: 419.33061, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 4.17355, qf2_loss: 4.15497, policy_loss: -255.13544, policy_entropy: -0.99836, alpha: 0.21628, time: 33.60590
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   675 ----
[CW] collect: return: 390.60056, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 4.17651, qf2_loss: 4.15926, policy_loss: -255.77100, policy_entropy: -1.00782, alpha: 0.21729, time: 33.53533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   676 ----
[CW] collect: return: 380.06947, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 4.30546, qf2_loss: 4.31107, policy_loss: -255.09886, policy_entropy: -0.99953, alpha: 0.21781, time: 33.60477
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   677 ----
[CW] collect: return: 298.81498, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 4.31831, qf2_loss: 4.26770, policy_loss: -256.11000, policy_entropy: -1.00504, alpha: 0.21796, time: 33.31258
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   678 ----
[CW] collect: return: 312.08001, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 4.56122, qf2_loss: 4.55992, policy_loss: -255.51371, policy_entropy: -0.98784, alpha: 0.21766, time: 36.48044
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   679 ----
[CW] collect: return: 459.87259, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 5.01952, qf2_loss: 5.02739, policy_loss: -255.76390, policy_entropy: -1.00082, alpha: 0.21674, time: 33.49317
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   680 ----
[CW] collect: return: 325.55223, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 5.97084, qf2_loss: 5.95192, policy_loss: -255.33938, policy_entropy: -0.99565, alpha: 0.21686, time: 33.68908
[CW] eval: return: 357.76278, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   681 ----
[CW] collect: return: 395.68022, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 4.89855, qf2_loss: 4.91788, policy_loss: -255.52390, policy_entropy: -0.99620, alpha: 0.21567, time: 33.31852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   682 ----
[CW] collect: return: 340.02287, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 4.15889, qf2_loss: 4.15340, policy_loss: -256.04087, policy_entropy: -1.00672, alpha: 0.21554, time: 33.59021
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   683 ----
[CW] collect: return: 374.45964, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 4.72164, qf2_loss: 4.75195, policy_loss: -255.91905, policy_entropy: -0.99672, alpha: 0.21609, time: 33.48051
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   684 ----
[CW] collect: return: 388.47992, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 4.72312, qf2_loss: 4.76769, policy_loss: -256.36425, policy_entropy: -1.00279, alpha: 0.21690, time: 33.64138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   685 ----
[CW] collect: return: 385.72899, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 4.33234, qf2_loss: 4.28340, policy_loss: -255.34345, policy_entropy: -0.99875, alpha: 0.21671, time: 33.36950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   686 ----
[CW] collect: return: 290.84718, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 4.81548, qf2_loss: 4.74563, policy_loss: -255.86704, policy_entropy: -0.99873, alpha: 0.21631, time: 33.57589
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   687 ----
[CW] collect: return: 462.43358, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 4.21706, qf2_loss: 4.28922, policy_loss: -255.75594, policy_entropy: -1.00580, alpha: 0.21645, time: 33.12788
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   688 ----
[CW] collect: return: 347.48615, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 4.22384, qf2_loss: 4.20833, policy_loss: -256.13126, policy_entropy: -1.00113, alpha: 0.21725, time: 33.19554
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   689 ----
[CW] collect: return: 406.16229, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 3.89456, qf2_loss: 3.89510, policy_loss: -256.34935, policy_entropy: -1.00465, alpha: 0.21785, time: 33.24699
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   690 ----
[CW] collect: return: 322.06532, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 3.89716, qf2_loss: 3.91908, policy_loss: -255.90880, policy_entropy: -0.99876, alpha: 0.21807, time: 33.53531
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   691 ----
[CW] collect: return: 373.99829, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 4.66556, qf2_loss: 4.70674, policy_loss: -256.27552, policy_entropy: -1.00597, alpha: 0.21853, time: 33.43811
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   692 ----
[CW] collect: return: 372.41105, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 5.08008, qf2_loss: 5.06424, policy_loss: -255.85650, policy_entropy: -0.99054, alpha: 0.21808, time: 33.78851
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   693 ----
[CW] collect: return: 409.73544, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 4.99525, qf2_loss: 4.98502, policy_loss: -256.08673, policy_entropy: -1.00611, alpha: 0.21792, time: 33.51995
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   694 ----
[CW] collect: return: 333.13378, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 4.47573, qf2_loss: 4.49031, policy_loss: -256.79772, policy_entropy: -0.99449, alpha: 0.21793, time: 33.57135
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   695 ----
[CW] collect: return: 390.26157, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 4.10037, qf2_loss: 4.16370, policy_loss: -257.91122, policy_entropy: -1.00596, alpha: 0.21802, time: 33.47053
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   696 ----
[CW] collect: return: 369.22068, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 4.22909, qf2_loss: 4.20921, policy_loss: -256.15027, policy_entropy: -0.99817, alpha: 0.21816, time: 33.47680
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   697 ----
[CW] collect: return: 292.87329, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 4.21031, qf2_loss: 4.19127, policy_loss: -256.93577, policy_entropy: -1.00244, alpha: 0.21872, time: 33.04602
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   698 ----
[CW] collect: return: 327.85732, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 4.70796, qf2_loss: 4.69418, policy_loss: -256.64642, policy_entropy: -1.00263, alpha: 0.21904, time: 33.06567
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   699 ----
[CW] collect: return: 404.21807, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 7.35168, qf2_loss: 7.38320, policy_loss: -257.14070, policy_entropy: -1.00712, alpha: 0.21946, time: 33.61835
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   700 ----
[CW] collect: return: 400.94057, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 6.85423, qf2_loss: 6.77027, policy_loss: -257.20876, policy_entropy: -0.99015, alpha: 0.21913, time: 33.63774
[CW] eval: return: 371.11581, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   701 ----
[CW] collect: return: 390.35507, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 4.14399, qf2_loss: 4.15616, policy_loss: -256.65419, policy_entropy: -1.00614, alpha: 0.21850, time: 33.30274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   702 ----
[CW] collect: return: 424.34101, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 4.09120, qf2_loss: 4.05136, policy_loss: -257.11750, policy_entropy: -1.00041, alpha: 0.21937, time: 33.49433
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   703 ----
[CW] collect: return: 329.76513, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 4.55480, qf2_loss: 4.52622, policy_loss: -257.14390, policy_entropy: -1.00149, alpha: 0.21979, time: 33.76685
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   704 ----
[CW] collect: return: 365.79498, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 4.32288, qf2_loss: 4.34624, policy_loss: -257.56775, policy_entropy: -1.00454, alpha: 0.22002, time: 34.07458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   705 ----
[CW] collect: return: 317.98603, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 3.79398, qf2_loss: 3.81278, policy_loss: -257.41647, policy_entropy: -0.99242, alpha: 0.22004, time: 33.55068
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   706 ----
[CW] collect: return: 385.64981, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 4.35426, qf2_loss: 4.30926, policy_loss: -257.29578, policy_entropy: -0.99980, alpha: 0.21937, time: 33.83847
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   707 ----
[CW] collect: return: 333.20742, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 4.12946, qf2_loss: 4.11205, policy_loss: -257.68039, policy_entropy: -1.00132, alpha: 0.21937, time: 33.44537
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   708 ----
[CW] collect: return: 399.85301, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 4.23662, qf2_loss: 4.24501, policy_loss: -257.38920, policy_entropy: -1.00173, alpha: 0.21932, time: 33.53909
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   709 ----
[CW] collect: return: 299.69885, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 4.78671, qf2_loss: 4.79599, policy_loss: -258.20325, policy_entropy: -0.99884, alpha: 0.21982, time: 33.38111
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   710 ----
[CW] collect: return: 356.03412, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 4.65130, qf2_loss: 4.67815, policy_loss: -257.73069, policy_entropy: -0.99877, alpha: 0.21934, time: 33.63652
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   711 ----
[CW] collect: return: 438.51172, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 5.31687, qf2_loss: 5.28734, policy_loss: -257.36387, policy_entropy: -0.99971, alpha: 0.21922, time: 33.12857
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   712 ----
[CW] collect: return: 438.12119, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 4.90708, qf2_loss: 4.95600, policy_loss: -257.59192, policy_entropy: -0.99668, alpha: 0.21865, time: 33.02570
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   713 ----
[CW] collect: return: 307.84406, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 4.20969, qf2_loss: 4.17933, policy_loss: -258.43293, policy_entropy: -0.99951, alpha: 0.21866, time: 33.10037
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   714 ----
[CW] collect: return: 366.97291, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 4.61362, qf2_loss: 4.61352, policy_loss: -257.94077, policy_entropy: -0.99366, alpha: 0.21845, time: 33.22425
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   715 ----
[CW] collect: return: 320.97500, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 4.71183, qf2_loss: 4.70293, policy_loss: -258.41571, policy_entropy: -1.00600, alpha: 0.21756, time: 33.49091
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   716 ----
[CW] collect: return: 398.31094, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 5.75958, qf2_loss: 5.70467, policy_loss: -257.62707, policy_entropy: -0.99659, alpha: 0.21851, time: 33.71099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   717 ----
[CW] collect: return: 279.95755, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 4.58732, qf2_loss: 4.58830, policy_loss: -257.14856, policy_entropy: -0.99066, alpha: 0.21762, time: 33.68641
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   718 ----
[CW] collect: return: 388.31093, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 4.56293, qf2_loss: 4.58019, policy_loss: -258.06868, policy_entropy: -0.99633, alpha: 0.21654, time: 33.96872
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   719 ----
[CW] collect: return: 410.26749, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 4.37945, qf2_loss: 4.38392, policy_loss: -258.52803, policy_entropy: -0.99734, alpha: 0.21584, time: 33.51074
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   720 ----
[CW] collect: return: 423.16514, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 4.42414, qf2_loss: 4.45911, policy_loss: -258.41371, policy_entropy: -1.01510, alpha: 0.21676, time: 33.64381
[CW] eval: return: 360.20085, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   721 ----
[CW] collect: return: 366.34616, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 4.47873, qf2_loss: 4.49961, policy_loss: -258.57328, policy_entropy: -0.99543, alpha: 0.21767, time: 33.05360
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   722 ----
[CW] collect: return: 456.87536, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 4.47774, qf2_loss: 4.46753, policy_loss: -258.37602, policy_entropy: -1.00690, alpha: 0.21727, time: 33.45654
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   723 ----
[CW] collect: return: 395.37770, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 4.70017, qf2_loss: 4.65434, policy_loss: -258.24920, policy_entropy: -1.00005, alpha: 0.21856, time: 33.51935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   724 ----
[CW] collect: return: 417.21176, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 4.59461, qf2_loss: 4.63463, policy_loss: -257.79076, policy_entropy: -0.99203, alpha: 0.21767, time: 33.27591
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   725 ----
[CW] collect: return: 405.43806, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 4.70852, qf2_loss: 4.67060, policy_loss: -258.67708, policy_entropy: -1.00683, alpha: 0.21760, time: 35.44753
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   726 ----
[CW] collect: return: 410.23076, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 8.53612, qf2_loss: 8.56370, policy_loss: -258.35681, policy_entropy: -1.00331, alpha: 0.21865, time: 33.02129
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   727 ----
[CW] collect: return: 269.66414, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 6.02541, qf2_loss: 6.03432, policy_loss: -259.15420, policy_entropy: -0.99798, alpha: 0.21774, time: 33.75086
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   728 ----
[CW] collect: return: 325.04884, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 4.37209, qf2_loss: 4.36585, policy_loss: -258.98334, policy_entropy: -0.99616, alpha: 0.21844, time: 33.47628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   729 ----
[CW] collect: return: 423.14639, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 4.14900, qf2_loss: 4.15533, policy_loss: -258.71783, policy_entropy: -1.00324, alpha: 0.21801, time: 33.61004
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   730 ----
[CW] collect: return: 402.05541, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 4.10526, qf2_loss: 4.12920, policy_loss: -258.34494, policy_entropy: -0.99533, alpha: 0.21768, time: 33.36207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   731 ----
[CW] collect: return: 255.49651, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 4.72363, qf2_loss: 4.74111, policy_loss: -258.07494, policy_entropy: -0.99618, alpha: 0.21710, time: 33.42783
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   732 ----
[CW] collect: return: 420.93718, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 5.15554, qf2_loss: 5.18231, policy_loss: -259.55889, policy_entropy: -1.00320, alpha: 0.21684, time: 33.72521
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   733 ----
[CW] collect: return: 360.95967, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 4.67380, qf2_loss: 4.68663, policy_loss: -258.57425, policy_entropy: -1.00588, alpha: 0.21833, time: 33.41969
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   734 ----
[CW] collect: return: 420.26563, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 4.31570, qf2_loss: 4.33533, policy_loss: -259.16220, policy_entropy: -0.99735, alpha: 0.21813, time: 33.70907
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   735 ----
[CW] collect: return: 376.26984, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 4.21537, qf2_loss: 4.24938, policy_loss: -259.35413, policy_entropy: -1.00449, alpha: 0.21812, time: 32.88893
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   736 ----
[CW] collect: return: 338.54984, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 4.82079, qf2_loss: 4.81994, policy_loss: -259.90414, policy_entropy: -1.00526, alpha: 0.21889, time: 33.90338
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   737 ----
[CW] collect: return: 408.25746, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 5.82191, qf2_loss: 5.76630, policy_loss: -259.22290, policy_entropy: -0.99995, alpha: 0.21968, time: 34.23939
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   738 ----
[CW] collect: return: 326.70655, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 7.64139, qf2_loss: 7.62927, policy_loss: -259.75515, policy_entropy: -0.99502, alpha: 0.21898, time: 33.65147
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   739 ----
[CW] collect: return: 407.66031, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 5.34187, qf2_loss: 5.33215, policy_loss: -259.23092, policy_entropy: -0.99280, alpha: 0.21835, time: 33.57673
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   740 ----
[CW] collect: return: 377.82698, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 4.21340, qf2_loss: 4.19065, policy_loss: -260.36623, policy_entropy: -1.00537, alpha: 0.21810, time: 33.43033
[CW] eval: return: 393.19784, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   741 ----
[CW] collect: return: 400.49695, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 4.12786, qf2_loss: 4.11199, policy_loss: -260.05760, policy_entropy: -1.00419, alpha: 0.21888, time: 33.17175
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   742 ----
[CW] collect: return: 357.87430, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 4.16633, qf2_loss: 4.16949, policy_loss: -259.33816, policy_entropy: -0.99952, alpha: 0.21885, time: 33.27472
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   743 ----
[CW] collect: return: 436.16990, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 4.55291, qf2_loss: 4.58864, policy_loss: -260.30111, policy_entropy: -1.00574, alpha: 0.21951, time: 33.59671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   744 ----
[CW] collect: return: 350.43781, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 4.70023, qf2_loss: 4.72300, policy_loss: -259.05762, policy_entropy: -0.99991, alpha: 0.21980, time: 33.60397
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   745 ----
[CW] collect: return: 350.69391, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 5.16693, qf2_loss: 5.14000, policy_loss: -259.54460, policy_entropy: -1.00466, alpha: 0.22037, time: 33.67816
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   746 ----
[CW] collect: return: 359.03082, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 4.65524, qf2_loss: 4.70282, policy_loss: -260.10851, policy_entropy: -0.99534, alpha: 0.22059, time: 33.71986
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   747 ----
[CW] collect: return: 245.58861, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 4.76811, qf2_loss: 4.76278, policy_loss: -260.12584, policy_entropy: -0.98909, alpha: 0.21945, time: 33.47177
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   748 ----
[CW] collect: return: 333.53342, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 4.32686, qf2_loss: 4.33810, policy_loss: -260.33925, policy_entropy: -1.00301, alpha: 0.21823, time: 33.68737
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   749 ----
[CW] collect: return: 357.67674, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 4.74492, qf2_loss: 4.74996, policy_loss: -259.79569, policy_entropy: -1.00411, alpha: 0.21921, time: 33.32270
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   750 ----
[CW] collect: return: 343.97397, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 4.71561, qf2_loss: 4.77421, policy_loss: -259.76983, policy_entropy: -1.00139, alpha: 0.21960, time: 33.00212
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   751 ----
[CW] collect: return: 372.26580, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 4.82012, qf2_loss: 4.84177, policy_loss: -259.73536, policy_entropy: -0.99507, alpha: 0.21917, time: 33.15468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   752 ----
[CW] collect: return: 371.33708, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 4.61188, qf2_loss: 4.65004, policy_loss: -260.22272, policy_entropy: -1.00172, alpha: 0.21890, time: 33.06543
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   753 ----
[CW] collect: return: 273.89232, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 4.34394, qf2_loss: 4.37182, policy_loss: -260.73354, policy_entropy: -1.00549, alpha: 0.21975, time: 33.37847
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   754 ----
[CW] collect: return: 333.47456, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 5.02030, qf2_loss: 5.05385, policy_loss: -260.43689, policy_entropy: -0.99876, alpha: 0.22008, time: 33.00523
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   755 ----
[CW] collect: return: 418.86782, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 4.93164, qf2_loss: 4.91387, policy_loss: -261.10227, policy_entropy: -1.00323, alpha: 0.21919, time: 33.31277
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   756 ----
[CW] collect: return: 329.01158, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 4.99957, qf2_loss: 5.00621, policy_loss: -260.53376, policy_entropy: -0.99729, alpha: 0.22029, time: 33.42770
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   757 ----
[CW] collect: return: 376.37722, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 5.01250, qf2_loss: 5.01642, policy_loss: -260.46064, policy_entropy: -1.00815, alpha: 0.22009, time: 33.47600
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   758 ----
[CW] collect: return: 306.22132, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 5.57067, qf2_loss: 5.53116, policy_loss: -259.79977, policy_entropy: -0.99808, alpha: 0.22086, time: 33.22860
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   759 ----
[CW] collect: return: 352.81597, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 5.42989, qf2_loss: 5.47883, policy_loss: -260.45611, policy_entropy: -0.99187, alpha: 0.22063, time: 33.16278
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   760 ----
[CW] collect: return: 454.49486, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 5.21517, qf2_loss: 5.20741, policy_loss: -260.48140, policy_entropy: -1.00030, alpha: 0.21970, time: 33.16080
[CW] eval: return: 375.63297, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   761 ----
[CW] collect: return: 370.19414, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 4.63877, qf2_loss: 4.63653, policy_loss: -259.88030, policy_entropy: -0.99608, alpha: 0.21909, time: 33.37467
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   762 ----
[CW] collect: return: 344.95264, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 4.83085, qf2_loss: 4.83453, policy_loss: -260.73860, policy_entropy: -1.00402, alpha: 0.21881, time: 33.10920
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   763 ----
[CW] collect: return: 420.20567, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 4.75075, qf2_loss: 4.78102, policy_loss: -260.18904, policy_entropy: -1.00226, alpha: 0.21933, time: 33.08157
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   764 ----
[CW] collect: return: 351.97098, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 4.66602, qf2_loss: 4.67510, policy_loss: -259.97837, policy_entropy: -1.00308, alpha: 0.22014, time: 33.41671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   765 ----
[CW] collect: return: 370.67681, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 5.13005, qf2_loss: 5.17865, policy_loss: -261.23466, policy_entropy: -1.00629, alpha: 0.22086, time: 33.24006
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   766 ----
[CW] collect: return: 272.84862, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 4.82387, qf2_loss: 4.82331, policy_loss: -261.01678, policy_entropy: -0.99396, alpha: 0.22111, time: 33.05921
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   767 ----
[CW] collect: return: 365.59748, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 5.16900, qf2_loss: 5.16271, policy_loss: -260.76010, policy_entropy: -1.00077, alpha: 0.22040, time: 33.39736
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   768 ----
[CW] collect: return: 259.14326, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 5.64378, qf2_loss: 5.74310, policy_loss: -260.74702, policy_entropy: -1.00489, alpha: 0.22141, time: 33.52026
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   769 ----
[CW] collect: return: 307.00146, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 5.41900, qf2_loss: 5.40983, policy_loss: -261.38109, policy_entropy: -1.00439, alpha: 0.22208, time: 33.77892
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   770 ----
[CW] collect: return: 383.61447, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 5.11771, qf2_loss: 5.08562, policy_loss: -261.94214, policy_entropy: -0.99526, alpha: 0.22228, time: 33.54468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   771 ----
[CW] collect: return: 313.58721, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 5.29899, qf2_loss: 5.27874, policy_loss: -261.71714, policy_entropy: -1.00601, alpha: 0.22208, time: 33.63861
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   772 ----
[CW] collect: return: 284.97752, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 4.80380, qf2_loss: 4.81884, policy_loss: -261.02585, policy_entropy: -0.99098, alpha: 0.22192, time: 33.43503
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   773 ----
[CW] collect: return: 430.92080, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 4.58661, qf2_loss: 4.58034, policy_loss: -260.32806, policy_entropy: -0.99671, alpha: 0.22058, time: 33.52654
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   774 ----
[CW] collect: return: 368.73577, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 4.74341, qf2_loss: 4.74629, policy_loss: -261.23346, policy_entropy: -0.99613, alpha: 0.21975, time: 33.61083
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   775 ----
[CW] collect: return: 362.53077, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 4.59964, qf2_loss: 4.59539, policy_loss: -261.80815, policy_entropy: -1.00320, alpha: 0.22008, time: 33.05521
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   776 ----
[CW] collect: return: 312.76097, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 4.74181, qf2_loss: 4.73624, policy_loss: -262.02611, policy_entropy: -1.00845, alpha: 0.22122, time: 33.36160
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   777 ----
[CW] collect: return: 358.86892, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 4.76271, qf2_loss: 4.76126, policy_loss: -261.50969, policy_entropy: -1.00126, alpha: 0.22199, time: 33.39851
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   778 ----
[CW] collect: return: 359.83877, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 5.57258, qf2_loss: 5.58513, policy_loss: -262.49289, policy_entropy: -1.00272, alpha: 0.22212, time: 33.32590
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   779 ----
[CW] collect: return: 415.20325, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 6.48892, qf2_loss: 6.53225, policy_loss: -261.41331, policy_entropy: -0.98950, alpha: 0.22148, time: 33.18109
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   780 ----
[CW] collect: return: 393.33101, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 4.85795, qf2_loss: 4.87538, policy_loss: -262.13788, policy_entropy: -0.99303, alpha: 0.21999, time: 33.32682
[CW] eval: return: 359.90462, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   781 ----
[CW] collect: return: 355.14811, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 5.19879, qf2_loss: 5.23363, policy_loss: -262.50973, policy_entropy: -1.00164, alpha: 0.21918, time: 33.23435
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   782 ----
[CW] collect: return: 370.45987, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 5.00420, qf2_loss: 4.97485, policy_loss: -261.76118, policy_entropy: -0.98964, alpha: 0.21871, time: 35.55212
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   783 ----
[CW] collect: return: 310.39095, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 5.13729, qf2_loss: 5.09292, policy_loss: -261.33815, policy_entropy: -1.00555, alpha: 0.21802, time: 33.58835
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   784 ----
[CW] collect: return: 348.84418, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 5.36115, qf2_loss: 5.38160, policy_loss: -261.53665, policy_entropy: -1.00001, alpha: 0.21877, time: 33.54069
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   785 ----
[CW] collect: return: 329.42772, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 4.73740, qf2_loss: 4.70161, policy_loss: -261.29652, policy_entropy: -0.99346, alpha: 0.21846, time: 33.42532
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   786 ----
[CW] collect: return: 314.43007, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 4.79266, qf2_loss: 4.75853, policy_loss: -261.34199, policy_entropy: -1.00333, alpha: 0.21768, time: 33.52253
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   787 ----
[CW] collect: return: 212.59183, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 5.11314, qf2_loss: 5.13836, policy_loss: -261.62223, policy_entropy: -0.98648, alpha: 0.21705, time: 33.24613
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   788 ----
[CW] collect: return: 307.91360, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 5.14882, qf2_loss: 5.15415, policy_loss: -261.62497, policy_entropy: -1.01199, alpha: 0.21685, time: 33.46381
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   789 ----
[CW] collect: return: 344.33096, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 5.39862, qf2_loss: 5.47809, policy_loss: -262.39919, policy_entropy: -1.00097, alpha: 0.21802, time: 33.35683
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   790 ----
[CW] collect: return: 405.06635, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 5.07645, qf2_loss: 5.11100, policy_loss: -262.30710, policy_entropy: -0.99721, alpha: 0.21822, time: 36.85206
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   791 ----
[CW] collect: return: 412.49088, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 4.94049, qf2_loss: 4.90996, policy_loss: -262.26365, policy_entropy: -1.00259, alpha: 0.21797, time: 33.68573
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   792 ----
[CW] collect: return: 331.96233, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 5.07229, qf2_loss: 5.10167, policy_loss: -262.33061, policy_entropy: -1.01212, alpha: 0.21920, time: 33.29992
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   793 ----
[CW] collect: return: 387.40249, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 5.35590, qf2_loss: 5.38051, policy_loss: -261.87023, policy_entropy: -0.99510, alpha: 0.21969, time: 33.21282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   794 ----
[CW] collect: return: 372.80334, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 6.58329, qf2_loss: 6.55154, policy_loss: -261.95651, policy_entropy: -0.98618, alpha: 0.21868, time: 33.58000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   795 ----
[CW] collect: return: 361.88329, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 6.58377, qf2_loss: 6.62180, policy_loss: -262.71463, policy_entropy: -0.99280, alpha: 0.21664, time: 35.13647
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   796 ----
[CW] collect: return: 366.94896, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 5.52286, qf2_loss: 5.51800, policy_loss: -262.82413, policy_entropy: -0.99968, alpha: 0.21582, time: 33.05749
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   797 ----
[CW] collect: return: 265.77291, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 5.11022, qf2_loss: 5.13092, policy_loss: -262.15837, policy_entropy: -1.00264, alpha: 0.21593, time: 33.35702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   798 ----
[CW] collect: return: 372.61605, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 4.78704, qf2_loss: 4.81889, policy_loss: -261.65041, policy_entropy: -0.99166, alpha: 0.21535, time: 33.58767
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   799 ----
[CW] collect: return: 362.87050, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 4.78978, qf2_loss: 4.79548, policy_loss: -262.36166, policy_entropy: -1.00686, alpha: 0.21515, time: 33.67411
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   800 ----
[CW] collect: return: 394.79702, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 4.86614, qf2_loss: 4.86767, policy_loss: -262.22867, policy_entropy: -0.99394, alpha: 0.21553, time: 33.52424
[CW] eval: return: 361.91525, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   801 ----
[CW] collect: return: 316.79961, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 5.44943, qf2_loss: 5.45951, policy_loss: -263.00441, policy_entropy: -0.99960, alpha: 0.21444, time: 33.05350
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   802 ----
[CW] collect: return: 362.76578, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 5.06383, qf2_loss: 5.05850, policy_loss: -262.97618, policy_entropy: -0.99626, alpha: 0.21424, time: 33.48002
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   803 ----
[CW] collect: return: 431.99747, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 5.85137, qf2_loss: 5.88809, policy_loss: -263.00531, policy_entropy: -1.00148, alpha: 0.21420, time: 33.39087
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   804 ----
[CW] collect: return: 297.17749, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 5.87782, qf2_loss: 5.87117, policy_loss: -262.91025, policy_entropy: -1.00767, alpha: 0.21511, time: 33.59085
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   805 ----
[CW] collect: return: 368.88452, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 5.20740, qf2_loss: 5.19546, policy_loss: -263.14819, policy_entropy: -1.00716, alpha: 0.21617, time: 33.41574
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   806 ----
[CW] collect: return: 403.65217, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 5.10099, qf2_loss: 5.13592, policy_loss: -263.34213, policy_entropy: -1.00242, alpha: 0.21728, time: 33.14002
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   807 ----
[CW] collect: return: 342.77438, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 4.82649, qf2_loss: 4.83131, policy_loss: -263.24949, policy_entropy: -1.01366, alpha: 0.21808, time: 33.28784
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   808 ----
[CW] collect: return: 370.26185, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 4.88641, qf2_loss: 4.87591, policy_loss: -263.12795, policy_entropy: -0.99910, alpha: 0.21981, time: 33.25464
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   809 ----
[CW] collect: return: 283.86613, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 5.25706, qf2_loss: 5.26823, policy_loss: -262.84007, policy_entropy: -1.00388, alpha: 0.21957, time: 33.36929
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   810 ----
[CW] collect: return: 361.43016, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 5.18717, qf2_loss: 5.18324, policy_loss: -262.77269, policy_entropy: -0.99835, alpha: 0.22010, time: 33.58144
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   811 ----
[CW] collect: return: 337.38826, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 5.61396, qf2_loss: 5.67178, policy_loss: -262.47933, policy_entropy: -0.99419, alpha: 0.21999, time: 33.56731
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   812 ----
[CW] collect: return: 309.69135, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 5.24523, qf2_loss: 5.27168, policy_loss: -263.76346, policy_entropy: -0.99021, alpha: 0.21781, time: 33.59702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   813 ----
[CW] collect: return: 364.44229, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 5.66322, qf2_loss: 5.63704, policy_loss: -263.18954, policy_entropy: -1.00201, alpha: 0.21736, time: 33.57460
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   814 ----
[CW] collect: return: 318.35322, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 5.41179, qf2_loss: 5.44448, policy_loss: -263.67272, policy_entropy: -1.00632, alpha: 0.21749, time: 33.72315
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   815 ----
[CW] collect: return: 317.57741, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 5.08697, qf2_loss: 5.09620, policy_loss: -263.63391, policy_entropy: -0.99423, alpha: 0.21789, time: 33.67699
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   816 ----
[CW] collect: return: 331.52505, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 5.54947, qf2_loss: 5.55562, policy_loss: -263.39657, policy_entropy: -0.99897, alpha: 0.21729, time: 33.81669
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   817 ----
[CW] collect: return: 409.24388, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 6.75442, qf2_loss: 6.69938, policy_loss: -262.95941, policy_entropy: -0.99675, alpha: 0.21728, time: 33.49379
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   818 ----
[CW] collect: return: 394.97910, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 6.08404, qf2_loss: 6.15208, policy_loss: -263.16275, policy_entropy: -1.00398, alpha: 0.21665, time: 33.58446
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   819 ----
[CW] collect: return: 261.76169, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 6.04774, qf2_loss: 6.14534, policy_loss: -262.99816, policy_entropy: -0.98561, alpha: 0.21615, time: 33.56731
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   820 ----
[CW] collect: return: 439.13898, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 6.02514, qf2_loss: 6.02611, policy_loss: -263.19629, policy_entropy: -0.99629, alpha: 0.21477, time: 33.75378
[CW] eval: return: 373.79492, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   821 ----
[CW] collect: return: 370.06796, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 5.69439, qf2_loss: 5.60885, policy_loss: -263.38319, policy_entropy: -0.99356, alpha: 0.21394, time: 33.30764
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   822 ----
[CW] collect: return: 377.46012, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 5.20912, qf2_loss: 5.27855, policy_loss: -263.34467, policy_entropy: -1.00159, alpha: 0.21306, time: 33.37420
[CW] ---------------------------

============================= JOB FEEDBACK =============================

NodeName=uc2n904
Job ID: 21893753
Array Job ID: 21893753_0
Cluster: uc2
User/Group: uprnr/stud
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 4
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-08:01:32 core-walltime
Job Wall-clock time: 08:00:23
Memory Utilized: 4.94 GB
Memory Efficiency: 8.43% of 58.59 GB
