{"collect/return": 377.4601174034178, "collect/steps": 1000.0, "collect/total_steps": 828000.0, "train/qf1_loss": 5.2091242837905884, "train/qf2_loss": 5.278546979427338, "train/policy_loss": -263.3446688842773, "train/policy_entropy": -1.001585990190506, "train/alpha": 0.21306088030338288, "train/time": 33.37419772148132, "eval/return": 373.79492360729927, "eval/steps": 1000.0, "_timestamp": 1678399609.2960637, "_runtime": 28770.590604543686, "_step": 822}