{"collect/return": 837.631558264955, "collect/steps": 1000.0, "collect/total_steps": 832000.0, "train/qf1_loss": 41.191381244659425, "train/qf2_loss": 41.567096118927005, "train/policy_loss": -596.9868395996094, "train/policy_entropy": -1.0051804369688033, "train/alpha": 0.4703478363156319, "train/time": 33.380388021469116, "eval/return": 826.8172747853489, "eval/steps": 1000.0, "_timestamp": 1678385756.16184, "_runtime": 28653.824749946594, "_step": 826}