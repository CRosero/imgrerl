[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 61.48155, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 0.71354, qf2_loss: 0.71145, policy_loss: -2.19854, policy_entropy: 0.68258, alpha: 0.98503, time: 38.24791
[CW] eval: return: 139.23021, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 104.25723, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.06083, qf2_loss: 0.06089, policy_loss: -2.62563, policy_entropy: 0.68257, alpha: 0.95626, time: 33.02910
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 143.76953, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.06128, qf2_loss: 0.06173, policy_loss: -3.15670, policy_entropy: 0.68024, alpha: 0.92872, time: 33.62167
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 162.43006, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.06941, qf2_loss: 0.07027, policy_loss: -3.71433, policy_entropy: 0.67542, alpha: 0.90238, time: 33.49026
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 174.76014, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.09526, qf2_loss: 0.09590, policy_loss: -4.41851, policy_entropy: 0.66994, alpha: 0.87715, time: 33.64113
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 217.55957, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.14156, qf2_loss: 0.14217, policy_loss: -5.21325, policy_entropy: 0.66233, alpha: 0.85298, time: 33.73444
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 190.17054, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.13721, qf2_loss: 0.13709, policy_loss: -6.00612, policy_entropy: 0.65284, alpha: 0.82983, time: 33.76901
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 130.42020, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.22741, qf2_loss: 0.22761, policy_loss: -6.62222, policy_entropy: 0.65131, alpha: 0.80759, time: 33.85764
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 33.08794, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.17995, qf2_loss: 0.17997, policy_loss: -7.03544, policy_entropy: 0.64762, alpha: 0.78615, time: 33.77827
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 148.68220, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.24413, qf2_loss: 0.24358, policy_loss: -7.74963, policy_entropy: 0.63790, alpha: 0.76554, time: 33.54992
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 121.48667, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.38059, qf2_loss: 0.37988, policy_loss: -8.40431, policy_entropy: 0.62706, alpha: 0.74574, time: 33.58516
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 51.16282, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.29456, qf2_loss: 0.29371, policy_loss: -8.68906, policy_entropy: 0.61730, alpha: 0.72670, time: 33.86228
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 260.49043, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.72759, qf2_loss: 0.72562, policy_loss: -9.74990, policy_entropy: 0.59734, alpha: 0.70842, time: 33.87542
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 167.24824, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.47149, qf2_loss: 0.47068, policy_loss: -10.53433, policy_entropy: 0.56945, alpha: 0.69090, time: 33.03529
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 90.99426, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.51422, qf2_loss: 0.51498, policy_loss: -11.14740, policy_entropy: 0.53689, alpha: 0.67420, time: 33.65497
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 101.20382, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.65871, qf2_loss: 0.65842, policy_loss: -11.76604, policy_entropy: 0.50270, alpha: 0.65828, time: 33.72587
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 192.78175, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.67808, qf2_loss: 0.68237, policy_loss: -12.68303, policy_entropy: 0.47046, alpha: 0.64303, time: 34.00655
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 189.41910, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.75701, qf2_loss: 0.75931, policy_loss: -13.58266, policy_entropy: 0.43062, alpha: 0.62848, time: 33.84228
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 216.82786, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.92584, qf2_loss: 0.92599, policy_loss: -14.32730, policy_entropy: 0.39993, alpha: 0.61455, time: 33.66117
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 147.14870, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 1.13719, qf2_loss: 1.13965, policy_loss: -15.27729, policy_entropy: 0.35840, alpha: 0.60119, time: 33.84647
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 253.25341, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.83605, qf2_loss: 0.83918, policy_loss: -15.97311, policy_entropy: 0.33180, alpha: 0.58839, time: 33.51277
[CW] eval: return: 232.64708, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 237.19631, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 1.35097, qf2_loss: 1.35187, policy_loss: -17.33726, policy_entropy: 0.28857, alpha: 0.57607, time: 33.52581
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 200.78303, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.96489, qf2_loss: 0.96576, policy_loss: -18.10313, policy_entropy: 0.25125, alpha: 0.56430, time: 33.31983
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 253.60115, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 1.21726, qf2_loss: 1.22572, policy_loss: -19.09543, policy_entropy: 0.21099, alpha: 0.55298, time: 33.34065
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 201.28899, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 1.26772, qf2_loss: 1.27178, policy_loss: -20.05630, policy_entropy: 0.17819, alpha: 0.54211, time: 33.21305
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 262.79476, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 1.13972, qf2_loss: 1.13973, policy_loss: -20.95198, policy_entropy: 0.14678, alpha: 0.53163, time: 33.60512
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 272.25746, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 1.33404, qf2_loss: 1.34340, policy_loss: -22.04904, policy_entropy: 0.12587, alpha: 0.52147, time: 33.72138
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 366.91724, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 1.24285, qf2_loss: 1.25213, policy_loss: -23.33827, policy_entropy: 0.09167, alpha: 0.51161, time: 33.58517
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 242.21644, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 1.35961, qf2_loss: 1.37572, policy_loss: -24.15418, policy_entropy: 0.08442, alpha: 0.50200, time: 33.69327
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 221.51553, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 1.27395, qf2_loss: 1.29037, policy_loss: -25.44411, policy_entropy: 0.06509, alpha: 0.49255, time: 33.38065
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 365.37247, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 1.52230, qf2_loss: 1.53827, policy_loss: -27.04114, policy_entropy: 0.04649, alpha: 0.48334, time: 33.20401
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 297.99045, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 1.52983, qf2_loss: 1.55501, policy_loss: -27.69742, policy_entropy: 0.03002, alpha: 0.47432, time: 32.95405
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 271.37445, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 1.45881, qf2_loss: 1.46996, policy_loss: -28.69145, policy_entropy: -0.00528, alpha: 0.46556, time: 33.74232
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 324.44203, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 1.47154, qf2_loss: 1.48198, policy_loss: -29.89262, policy_entropy: -0.03516, alpha: 0.45713, time: 33.69808
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 281.38487, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 1.49323, qf2_loss: 1.50627, policy_loss: -31.20681, policy_entropy: -0.05850, alpha: 0.44896, time: 33.68053
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 283.89832, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 1.68571, qf2_loss: 1.71441, policy_loss: -31.93000, policy_entropy: -0.08991, alpha: 0.44103, time: 33.71079
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 260.99824, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 1.84242, qf2_loss: 1.84874, policy_loss: -33.60524, policy_entropy: -0.12346, alpha: 0.43337, time: 33.78550
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 257.60905, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 1.99366, qf2_loss: 2.00355, policy_loss: -34.18781, policy_entropy: -0.15688, alpha: 0.42606, time: 33.74685
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 301.07348, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 1.67839, qf2_loss: 1.68311, policy_loss: -35.47889, policy_entropy: -0.19106, alpha: 0.41897, time: 33.59308
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 208.63360, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 1.74314, qf2_loss: 1.75437, policy_loss: -36.74900, policy_entropy: -0.23156, alpha: 0.41220, time: 33.72483
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 285.93702, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 2.07359, qf2_loss: 2.08679, policy_loss: -37.76762, policy_entropy: -0.24835, alpha: 0.40569, time: 33.00974
[CW] eval: return: 291.23331, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 244.78028, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 2.46719, qf2_loss: 2.47508, policy_loss: -38.54584, policy_entropy: -0.27098, alpha: 0.39925, time: 33.38815
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 274.60535, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 2.08784, qf2_loss: 2.09280, policy_loss: -39.99551, policy_entropy: -0.28377, alpha: 0.39298, time: 33.17211
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 363.04567, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 1.97253, qf2_loss: 1.97652, policy_loss: -40.74365, policy_entropy: -0.32428, alpha: 0.38693, time: 33.71078
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 212.47609, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 2.04573, qf2_loss: 2.05809, policy_loss: -42.18443, policy_entropy: -0.34893, alpha: 0.38105, time: 33.51295
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 340.76351, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 2.46291, qf2_loss: 2.49005, policy_loss: -43.47380, policy_entropy: -0.36551, alpha: 0.37538, time: 33.44140
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 291.30941, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 2.88438, qf2_loss: 2.89850, policy_loss: -44.36018, policy_entropy: -0.37223, alpha: 0.36972, time: 33.43802
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 333.08894, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 2.12280, qf2_loss: 2.14357, policy_loss: -45.78257, policy_entropy: -0.39261, alpha: 0.36420, time: 33.70646
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 292.61551, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 2.16018, qf2_loss: 2.17406, policy_loss: -46.94963, policy_entropy: -0.40251, alpha: 0.35873, time: 33.66327
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 272.74205, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 2.14400, qf2_loss: 2.14244, policy_loss: -48.01524, policy_entropy: -0.41380, alpha: 0.35332, time: 33.45258
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 305.12169, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 2.32036, qf2_loss: 2.34080, policy_loss: -49.09602, policy_entropy: -0.42752, alpha: 0.34800, time: 33.67811
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 259.44124, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 2.21410, qf2_loss: 2.23393, policy_loss: -49.91294, policy_entropy: -0.44359, alpha: 0.34278, time: 33.71892
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 252.83772, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 2.38276, qf2_loss: 2.39868, policy_loss: -50.86191, policy_entropy: -0.45751, alpha: 0.33767, time: 33.70353
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 216.56869, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 2.61808, qf2_loss: 2.63721, policy_loss: -52.05098, policy_entropy: -0.45430, alpha: 0.33253, time: 33.75783
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 263.53980, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 2.63215, qf2_loss: 2.64555, policy_loss: -53.40376, policy_entropy: -0.46251, alpha: 0.32744, time: 33.79134
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 202.97610, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 2.47520, qf2_loss: 2.47025, policy_loss: -54.34330, policy_entropy: -0.48510, alpha: 0.32243, time: 33.68929
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 273.64395, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 2.66198, qf2_loss: 2.66532, policy_loss: -55.21227, policy_entropy: -0.49963, alpha: 0.31761, time: 33.56157
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 278.43170, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 2.89521, qf2_loss: 2.90707, policy_loss: -56.86781, policy_entropy: -0.50664, alpha: 0.31281, time: 33.15377
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 252.80596, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 2.61896, qf2_loss: 2.61945, policy_loss: -57.20049, policy_entropy: -0.53002, alpha: 0.30809, time: 33.85652
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 328.56848, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 2.70201, qf2_loss: 2.72180, policy_loss: -58.27963, policy_entropy: -0.53898, alpha: 0.30358, time: 33.83600
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 308.27658, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 2.80017, qf2_loss: 2.79419, policy_loss: -60.02834, policy_entropy: -0.55653, alpha: 0.29910, time: 33.46309
[CW] eval: return: 276.62721, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 265.44927, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 3.24311, qf2_loss: 3.25100, policy_loss: -61.37372, policy_entropy: -0.55870, alpha: 0.29469, time: 33.68717
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 342.10876, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 2.89313, qf2_loss: 2.91617, policy_loss: -62.07696, policy_entropy: -0.58401, alpha: 0.29039, time: 33.79614
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 251.53740, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 2.68180, qf2_loss: 2.66785, policy_loss: -62.66405, policy_entropy: -0.59225, alpha: 0.28624, time: 33.73724
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 300.63335, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 3.09320, qf2_loss: 3.08703, policy_loss: -63.96786, policy_entropy: -0.59043, alpha: 0.28208, time: 33.15885
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 284.21790, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 2.91953, qf2_loss: 2.93211, policy_loss: -65.13738, policy_entropy: -0.62653, alpha: 0.27801, time: 33.74555
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 331.79867, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 2.80186, qf2_loss: 2.80421, policy_loss: -66.13840, policy_entropy: -0.63660, alpha: 0.27424, time: 33.65085
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 356.84831, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 2.89506, qf2_loss: 2.91157, policy_loss: -67.32612, policy_entropy: -0.64209, alpha: 0.27039, time: 33.67499
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 327.11383, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 3.17507, qf2_loss: 3.15738, policy_loss: -68.41694, policy_entropy: -0.65776, alpha: 0.26663, time: 33.61665
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 234.82740, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 3.18454, qf2_loss: 3.19746, policy_loss: -69.71437, policy_entropy: -0.65653, alpha: 0.26302, time: 33.72822
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 293.60804, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 3.05651, qf2_loss: 3.07105, policy_loss: -70.51064, policy_entropy: -0.67869, alpha: 0.25936, time: 33.31410
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 322.14712, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 3.10547, qf2_loss: 3.12894, policy_loss: -71.86906, policy_entropy: -0.68162, alpha: 0.25584, time: 33.73293
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 355.47322, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 3.34245, qf2_loss: 3.36133, policy_loss: -73.35295, policy_entropy: -0.67719, alpha: 0.25228, time: 33.66038
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 203.27836, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 3.44257, qf2_loss: 3.44564, policy_loss: -73.91309, policy_entropy: -0.68907, alpha: 0.24872, time: 33.85721
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 324.03889, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 3.73097, qf2_loss: 3.77023, policy_loss: -74.82200, policy_entropy: -0.68704, alpha: 0.24517, time: 33.53393
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 319.07787, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 4.53690, qf2_loss: 4.54708, policy_loss: -75.65199, policy_entropy: -0.70554, alpha: 0.24171, time: 33.57994
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 277.23051, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 3.65876, qf2_loss: 3.70545, policy_loss: -76.81047, policy_entropy: -0.71368, alpha: 0.23828, time: 33.67337
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 302.77302, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 3.21425, qf2_loss: 3.23068, policy_loss: -78.01943, policy_entropy: -0.73455, alpha: 0.23500, time: 33.74562
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 228.48466, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 3.36939, qf2_loss: 3.37490, policy_loss: -78.89077, policy_entropy: -0.72019, alpha: 0.23188, time: 33.58039
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 261.18183, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 3.30467, qf2_loss: 3.32940, policy_loss: -79.56489, policy_entropy: -0.75030, alpha: 0.22867, time: 33.71339
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 313.76467, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 3.46965, qf2_loss: 3.48242, policy_loss: -80.74547, policy_entropy: -0.76000, alpha: 0.22565, time: 33.35590
[CW] eval: return: 305.32247, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 375.93025, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 3.65879, qf2_loss: 3.69367, policy_loss: -82.11448, policy_entropy: -0.75698, alpha: 0.22272, time: 33.60328
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 299.53173, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 3.61024, qf2_loss: 3.64449, policy_loss: -82.83474, policy_entropy: -0.76041, alpha: 0.21973, time: 33.24070
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 404.00357, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 7.30652, qf2_loss: 7.39041, policy_loss: -83.86403, policy_entropy: -0.78859, alpha: 0.21688, time: 33.47048
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 285.30392, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 7.42074, qf2_loss: 7.38191, policy_loss: -84.32503, policy_entropy: -0.78205, alpha: 0.21405, time: 33.65236
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 339.89031, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 4.68776, qf2_loss: 4.72192, policy_loss: -85.36969, policy_entropy: -0.80597, alpha: 0.21139, time: 33.37279
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 323.50446, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 3.74403, qf2_loss: 3.75580, policy_loss: -86.60239, policy_entropy: -0.82309, alpha: 0.20891, time: 33.45685
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 339.70680, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 3.38863, qf2_loss: 3.40758, policy_loss: -87.59783, policy_entropy: -0.83947, alpha: 0.20668, time: 33.23054
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 345.09508, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 3.49215, qf2_loss: 3.51726, policy_loss: -89.03594, policy_entropy: -0.84499, alpha: 0.20448, time: 33.22391
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 259.10557, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 3.61239, qf2_loss: 3.61340, policy_loss: -89.84268, policy_entropy: -0.86257, alpha: 0.20236, time: 33.73205
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 371.08906, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 3.74221, qf2_loss: 3.79879, policy_loss: -91.21583, policy_entropy: -0.87850, alpha: 0.20049, time: 33.37590
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 346.19349, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 3.72297, qf2_loss: 3.72570, policy_loss: -92.02304, policy_entropy: -0.88953, alpha: 0.19873, time: 33.21233
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 187.04762, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 3.98689, qf2_loss: 4.02166, policy_loss: -92.71753, policy_entropy: -0.89474, alpha: 0.19717, time: 33.69872
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 353.82300, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 4.10646, qf2_loss: 4.15966, policy_loss: -93.72536, policy_entropy: -0.90076, alpha: 0.19552, time: 33.71683
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 307.04112, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 4.29217, qf2_loss: 4.27242, policy_loss: -95.03250, policy_entropy: -0.90582, alpha: 0.19398, time: 33.67581
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 359.05545, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 4.30714, qf2_loss: 4.30477, policy_loss: -95.50779, policy_entropy: -0.93748, alpha: 0.19261, time: 33.79181
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 378.60503, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 4.46675, qf2_loss: 4.47787, policy_loss: -96.79714, policy_entropy: -0.94054, alpha: 0.19161, time: 33.80366
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 311.21624, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 4.26908, qf2_loss: 4.27968, policy_loss: -97.94274, policy_entropy: -0.93108, alpha: 0.19049, time: 33.19753
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 349.35214, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 4.32443, qf2_loss: 4.30977, policy_loss: -98.87889, policy_entropy: -0.95667, alpha: 0.18936, time: 33.33841
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 348.07966, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 4.00947, qf2_loss: 4.03443, policy_loss: -99.97734, policy_entropy: -0.97164, alpha: 0.18867, time: 33.26918
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 288.73857, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 4.33139, qf2_loss: 4.40850, policy_loss: -100.31354, policy_entropy: -0.97797, alpha: 0.18809, time: 33.42997
[CW] eval: return: 321.62144, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 360.34993, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 4.92274, qf2_loss: 4.93103, policy_loss: -102.71281, policy_entropy: -0.97312, alpha: 0.18772, time: 33.48654
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 332.13969, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 4.90548, qf2_loss: 4.90713, policy_loss: -102.70751, policy_entropy: -0.97341, alpha: 0.18707, time: 33.20392
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 334.57034, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 4.84588, qf2_loss: 4.91262, policy_loss: -104.23199, policy_entropy: -0.99024, alpha: 0.18657, time: 33.68323
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 383.86167, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 4.26611, qf2_loss: 4.27193, policy_loss: -105.25256, policy_entropy: -0.99049, alpha: 0.18630, time: 33.53280
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 287.76938, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 4.25010, qf2_loss: 4.27488, policy_loss: -105.89305, policy_entropy: -1.01936, alpha: 0.18648, time: 33.08395
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 245.56462, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 4.56850, qf2_loss: 4.58311, policy_loss: -106.61877, policy_entropy: -1.01276, alpha: 0.18690, time: 33.73319
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 264.43346, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 4.57321, qf2_loss: 4.55854, policy_loss: -107.26734, policy_entropy: -1.01417, alpha: 0.18737, time: 33.77213
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 350.70485, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 5.01732, qf2_loss: 5.02896, policy_loss: -108.01085, policy_entropy: -1.01450, alpha: 0.18771, time: 33.25644
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 335.61706, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 5.14104, qf2_loss: 5.20143, policy_loss: -109.48783, policy_entropy: -1.00536, alpha: 0.18806, time: 33.89015
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 439.22668, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 4.92867, qf2_loss: 4.95433, policy_loss: -110.59710, policy_entropy: -1.02536, alpha: 0.18834, time: 33.83887
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 325.80578, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 4.88336, qf2_loss: 4.89927, policy_loss: -111.04055, policy_entropy: -1.03290, alpha: 0.18931, time: 33.59948
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 359.19199, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 5.35224, qf2_loss: 5.37097, policy_loss: -112.56204, policy_entropy: -1.02148, alpha: 0.19042, time: 33.71850
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 331.44549, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 4.68372, qf2_loss: 4.66244, policy_loss: -113.02372, policy_entropy: -1.02836, alpha: 0.19128, time: 33.90068
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 331.38592, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 5.25413, qf2_loss: 5.23036, policy_loss: -113.93955, policy_entropy: -1.04006, alpha: 0.19252, time: 33.67848
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 299.19509, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 4.70968, qf2_loss: 4.76983, policy_loss: -114.99468, policy_entropy: -1.04649, alpha: 0.19448, time: 33.47558
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 314.52974, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 4.87643, qf2_loss: 4.92078, policy_loss: -116.02299, policy_entropy: -1.03586, alpha: 0.19626, time: 33.28054
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 330.74572, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 5.89069, qf2_loss: 5.90421, policy_loss: -117.18695, policy_entropy: -1.01904, alpha: 0.19744, time: 33.44830
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 382.28926, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 5.28678, qf2_loss: 5.28796, policy_loss: -118.41531, policy_entropy: -1.01572, alpha: 0.19845, time: 33.54082
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 383.49133, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 5.58510, qf2_loss: 5.66674, policy_loss: -119.21023, policy_entropy: -1.02082, alpha: 0.19938, time: 33.53612
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 357.40994, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 10.99802, qf2_loss: 11.04632, policy_loss: -119.75808, policy_entropy: -1.01769, alpha: 0.20050, time: 33.28070
[CW] eval: return: 371.46872, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 340.94981, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 5.91916, qf2_loss: 5.96243, policy_loss: -120.92565, policy_entropy: -1.00720, alpha: 0.20107, time: 33.49106
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 384.97952, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 4.57424, qf2_loss: 4.59325, policy_loss: -122.24655, policy_entropy: -1.01249, alpha: 0.20175, time: 33.11215
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 370.78152, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 4.67172, qf2_loss: 4.69811, policy_loss: -123.27402, policy_entropy: -1.00994, alpha: 0.20235, time: 33.34584
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 277.51717, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 4.56416, qf2_loss: 4.55605, policy_loss: -123.72193, policy_entropy: -1.01351, alpha: 0.20310, time: 33.69198
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 363.59521, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 5.04218, qf2_loss: 5.02800, policy_loss: -125.36874, policy_entropy: -1.01289, alpha: 0.20404, time: 33.67287
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 364.27492, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 4.90384, qf2_loss: 4.90817, policy_loss: -126.83064, policy_entropy: -1.02181, alpha: 0.20504, time: 33.22081
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 353.03096, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 5.30899, qf2_loss: 5.35409, policy_loss: -126.86896, policy_entropy: -1.02537, alpha: 0.20694, time: 33.60014
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 330.21724, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 5.58619, qf2_loss: 5.56235, policy_loss: -128.03132, policy_entropy: -1.00423, alpha: 0.20784, time: 33.21817
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 366.28481, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 6.41030, qf2_loss: 6.43925, policy_loss: -128.82605, policy_entropy: -1.02631, alpha: 0.20905, time: 33.32862
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 365.82848, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 5.56369, qf2_loss: 5.63105, policy_loss: -129.44643, policy_entropy: -1.00394, alpha: 0.21048, time: 33.15733
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 349.32379, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 5.07811, qf2_loss: 5.10138, policy_loss: -130.74084, policy_entropy: -1.01737, alpha: 0.21156, time: 33.83883
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 359.20806, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 5.17704, qf2_loss: 5.16471, policy_loss: -132.22368, policy_entropy: -1.01349, alpha: 0.21289, time: 33.94787
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 350.46575, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 5.40327, qf2_loss: 5.42258, policy_loss: -132.96031, policy_entropy: -1.01159, alpha: 0.21391, time: 33.86774
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 358.68390, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 5.51789, qf2_loss: 5.56172, policy_loss: -133.89162, policy_entropy: -0.99425, alpha: 0.21415, time: 33.69094
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 397.51365, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 5.26454, qf2_loss: 5.21253, policy_loss: -134.71491, policy_entropy: -1.01259, alpha: 0.21487, time: 33.79646
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 437.76530, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 5.31632, qf2_loss: 5.32544, policy_loss: -135.39679, policy_entropy: -1.01344, alpha: 0.21559, time: 33.81983
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 384.38711, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 5.29070, qf2_loss: 5.30435, policy_loss: -137.01806, policy_entropy: -1.00184, alpha: 0.21651, time: 33.62535
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 418.12998, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 5.96597, qf2_loss: 5.99812, policy_loss: -137.83033, policy_entropy: -1.00673, alpha: 0.21709, time: 33.62145
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 390.14817, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 5.84068, qf2_loss: 5.85646, policy_loss: -138.77804, policy_entropy: -1.01806, alpha: 0.21859, time: 33.78676
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 362.42195, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 5.54893, qf2_loss: 5.54287, policy_loss: -139.75855, policy_entropy: -1.01647, alpha: 0.21996, time: 33.60881
[CW] eval: return: 368.81784, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 386.11627, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 4.75193, qf2_loss: 4.78720, policy_loss: -140.09902, policy_entropy: -1.00994, alpha: 0.22142, time: 33.63515
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 407.61169, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 5.07106, qf2_loss: 5.10138, policy_loss: -141.84195, policy_entropy: -1.00584, alpha: 0.22232, time: 33.70800
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 365.66431, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 5.77775, qf2_loss: 5.79474, policy_loss: -142.84471, policy_entropy: -1.00442, alpha: 0.22258, time: 33.46446
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 332.65951, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 4.96047, qf2_loss: 4.91199, policy_loss: -143.10173, policy_entropy: -1.00428, alpha: 0.22345, time: 33.40704
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 373.84144, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 5.23431, qf2_loss: 5.23444, policy_loss: -144.25073, policy_entropy: -1.00150, alpha: 0.22413, time: 33.78771
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 387.87172, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 5.23461, qf2_loss: 5.28567, policy_loss: -145.09368, policy_entropy: -1.00350, alpha: 0.22457, time: 33.97223
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 355.13088, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 5.14730, qf2_loss: 5.18877, policy_loss: -146.18417, policy_entropy: -1.00025, alpha: 0.22435, time: 33.79238
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 302.91050, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 7.06421, qf2_loss: 7.09957, policy_loss: -146.78431, policy_entropy: -0.99417, alpha: 0.22458, time: 33.01110
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 417.58291, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 6.96762, qf2_loss: 6.98965, policy_loss: -147.80009, policy_entropy: -0.99689, alpha: 0.22310, time: 33.06216
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 340.93948, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 6.08191, qf2_loss: 6.08593, policy_loss: -148.66710, policy_entropy: -0.98987, alpha: 0.22311, time: 33.86145
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 369.35177, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 5.06680, qf2_loss: 5.10019, policy_loss: -150.53897, policy_entropy: -1.00079, alpha: 0.22198, time: 33.44969
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 402.21480, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 5.32170, qf2_loss: 5.36461, policy_loss: -151.22056, policy_entropy: -0.99868, alpha: 0.22209, time: 33.35987
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 368.86208, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 5.03682, qf2_loss: 5.08298, policy_loss: -151.29775, policy_entropy: -1.01321, alpha: 0.22269, time: 33.84003
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 391.66459, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 5.89060, qf2_loss: 6.00389, policy_loss: -152.51983, policy_entropy: -0.99500, alpha: 0.22315, time: 33.25123
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 326.58603, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 6.19592, qf2_loss: 6.25095, policy_loss: -153.79271, policy_entropy: -1.00142, alpha: 0.22315, time: 33.38762
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 335.26651, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 7.25766, qf2_loss: 7.24869, policy_loss: -154.48976, policy_entropy: -0.99484, alpha: 0.22293, time: 33.67579
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 363.88423, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 8.25958, qf2_loss: 8.42003, policy_loss: -155.16898, policy_entropy: -1.00048, alpha: 0.22284, time: 33.92594
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 339.00067, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 9.77678, qf2_loss: 9.70255, policy_loss: -156.23228, policy_entropy: -0.98589, alpha: 0.22180, time: 33.66847
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 319.74358, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 5.45314, qf2_loss: 5.45978, policy_loss: -157.92543, policy_entropy: -1.00031, alpha: 0.22074, time: 33.62637
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 385.00326, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 6.29296, qf2_loss: 6.23562, policy_loss: -158.34520, policy_entropy: -1.00244, alpha: 0.22129, time: 33.11903
[CW] eval: return: 356.53293, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 412.11291, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 5.37935, qf2_loss: 5.38652, policy_loss: -159.28769, policy_entropy: -1.00284, alpha: 0.22170, time: 33.27544
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 385.60819, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 5.07567, qf2_loss: 5.08348, policy_loss: -159.66970, policy_entropy: -1.01020, alpha: 0.22206, time: 33.32081
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 350.95848, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 5.57236, qf2_loss: 5.57386, policy_loss: -161.88624, policy_entropy: -1.00242, alpha: 0.22332, time: 33.81145
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 397.66881, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 6.22319, qf2_loss: 6.23043, policy_loss: -161.99362, policy_entropy: -1.01739, alpha: 0.22430, time: 33.34886
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 327.24842, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 5.96170, qf2_loss: 5.97543, policy_loss: -163.06548, policy_entropy: -1.01439, alpha: 0.22652, time: 33.80275
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 377.03780, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 5.57844, qf2_loss: 5.56129, policy_loss: -163.74899, policy_entropy: -1.01293, alpha: 0.22808, time: 33.81137
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 393.22824, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 5.93003, qf2_loss: 5.98820, policy_loss: -164.24360, policy_entropy: -1.01536, alpha: 0.22971, time: 33.42579
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 375.55845, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 6.34065, qf2_loss: 6.35246, policy_loss: -165.53234, policy_entropy: -1.01617, alpha: 0.23237, time: 33.88880
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 382.15126, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 9.83252, qf2_loss: 9.88413, policy_loss: -166.51159, policy_entropy: -0.99060, alpha: 0.23264, time: 33.09105
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 433.73312, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 7.51261, qf2_loss: 7.47429, policy_loss: -167.49298, policy_entropy: -0.99727, alpha: 0.23147, time: 33.73139
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 405.17617, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 6.02540, qf2_loss: 6.02798, policy_loss: -168.22483, policy_entropy: -1.01121, alpha: 0.23174, time: 33.68384
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 316.80678, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 5.70170, qf2_loss: 5.69496, policy_loss: -169.22731, policy_entropy: -1.01149, alpha: 0.23315, time: 33.76062
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 419.06495, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 5.97972, qf2_loss: 5.95635, policy_loss: -170.35994, policy_entropy: -1.01518, alpha: 0.23463, time: 33.22132
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 433.22832, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 6.37963, qf2_loss: 6.42163, policy_loss: -170.62813, policy_entropy: -1.01570, alpha: 0.23685, time: 33.74744
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 325.70848, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 5.52038, qf2_loss: 5.54634, policy_loss: -171.48465, policy_entropy: -1.02240, alpha: 0.23890, time: 33.78172
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 412.89882, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 5.76575, qf2_loss: 5.81078, policy_loss: -172.44831, policy_entropy: -1.01126, alpha: 0.24149, time: 33.85704
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 420.25981, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 6.25051, qf2_loss: 6.31603, policy_loss: -173.64826, policy_entropy: -1.01242, alpha: 0.24291, time: 33.71665
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 338.18786, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 13.47239, qf2_loss: 13.61819, policy_loss: -174.15481, policy_entropy: -0.98023, alpha: 0.24302, time: 33.57471
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 334.18776, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 7.20450, qf2_loss: 7.21386, policy_loss: -174.43215, policy_entropy: -1.01215, alpha: 0.24209, time: 33.51435
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 405.31028, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 5.77271, qf2_loss: 5.79838, policy_loss: -176.01010, policy_entropy: -1.00370, alpha: 0.24313, time: 33.22920
[CW] eval: return: 368.74682, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 361.21640, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 5.73520, qf2_loss: 5.80008, policy_loss: -176.54680, policy_entropy: -1.02718, alpha: 0.24475, time: 33.64717
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 373.47359, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 5.85221, qf2_loss: 5.87988, policy_loss: -177.15300, policy_entropy: -1.00070, alpha: 0.24688, time: 33.49179
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 423.01276, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 5.75715, qf2_loss: 5.75767, policy_loss: -178.42870, policy_entropy: -1.00136, alpha: 0.24664, time: 33.68422
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 469.05703, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 5.98830, qf2_loss: 6.01944, policy_loss: -179.07016, policy_entropy: -1.02048, alpha: 0.24838, time: 33.72281
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 387.46203, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 9.74505, qf2_loss: 9.82274, policy_loss: -179.70082, policy_entropy: -0.99193, alpha: 0.24976, time: 33.66714
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 384.95936, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 8.26814, qf2_loss: 8.33419, policy_loss: -180.35760, policy_entropy: -0.99339, alpha: 0.24878, time: 33.83184
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 438.54244, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 5.96810, qf2_loss: 5.97200, policy_loss: -181.69809, policy_entropy: -1.01707, alpha: 0.24902, time: 33.81776
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 437.14610, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 5.93706, qf2_loss: 5.94658, policy_loss: -182.33909, policy_entropy: -1.01404, alpha: 0.25095, time: 33.71177
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 375.09356, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 5.79931, qf2_loss: 5.78305, policy_loss: -183.07698, policy_entropy: -0.99904, alpha: 0.25168, time: 33.54868
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 391.00099, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 5.61976, qf2_loss: 5.60479, policy_loss: -183.82657, policy_entropy: -1.00102, alpha: 0.25245, time: 33.55314
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 380.71519, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 5.59110, qf2_loss: 5.59015, policy_loss: -184.72685, policy_entropy: -1.00396, alpha: 0.25230, time: 33.62211
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 441.27474, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 5.76876, qf2_loss: 5.73940, policy_loss: -186.02928, policy_entropy: -1.00343, alpha: 0.25317, time: 33.57915
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 442.63312, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 7.40512, qf2_loss: 7.41326, policy_loss: -186.51762, policy_entropy: -1.00983, alpha: 0.25415, time: 33.65416
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 378.37196, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 5.55968, qf2_loss: 5.66643, policy_loss: -187.33936, policy_entropy: -1.00902, alpha: 0.25547, time: 33.58643
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 407.18580, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 5.61083, qf2_loss: 5.63634, policy_loss: -188.77205, policy_entropy: -1.00344, alpha: 0.25593, time: 33.53046
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 398.48654, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 5.61887, qf2_loss: 5.61232, policy_loss: -188.74882, policy_entropy: -0.99874, alpha: 0.25671, time: 33.64542
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 386.41122, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 5.77312, qf2_loss: 5.78003, policy_loss: -190.32755, policy_entropy: -1.00686, alpha: 0.25677, time: 33.55925
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 381.68593, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 5.52854, qf2_loss: 5.55229, policy_loss: -190.72526, policy_entropy: -1.01454, alpha: 0.25831, time: 33.56697
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 382.19919, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 5.35542, qf2_loss: 5.38983, policy_loss: -191.59997, policy_entropy: -0.99657, alpha: 0.25935, time: 33.37144
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 358.93543, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 5.55430, qf2_loss: 5.61254, policy_loss: -191.50489, policy_entropy: -0.99685, alpha: 0.25856, time: 33.43312
[CW] eval: return: 398.86822, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 435.19968, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 6.02306, qf2_loss: 6.04861, policy_loss: -192.35476, policy_entropy: -1.00870, alpha: 0.25900, time: 33.61202
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 389.53892, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 5.82832, qf2_loss: 5.89949, policy_loss: -193.37513, policy_entropy: -0.99416, alpha: 0.25919, time: 33.43837
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 401.94710, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 5.28085, qf2_loss: 5.27565, policy_loss: -193.94985, policy_entropy: -0.99332, alpha: 0.25872, time: 33.20579
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 363.60982, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 5.19427, qf2_loss: 5.24651, policy_loss: -194.51476, policy_entropy: -0.99857, alpha: 0.25757, time: 33.36464
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 420.74892, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 5.89069, qf2_loss: 5.98195, policy_loss: -195.44217, policy_entropy: -0.99206, alpha: 0.25742, time: 33.34114
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 419.43496, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 7.96126, qf2_loss: 7.99545, policy_loss: -195.93544, policy_entropy: -0.98993, alpha: 0.25632, time: 33.30729
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 375.76292, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 6.42746, qf2_loss: 6.41821, policy_loss: -196.74314, policy_entropy: -0.99639, alpha: 0.25413, time: 33.48852
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 395.74818, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 5.04125, qf2_loss: 5.00234, policy_loss: -197.75718, policy_entropy: -1.00747, alpha: 0.25433, time: 33.50230
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 373.56262, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 5.01068, qf2_loss: 5.00882, policy_loss: -198.29576, policy_entropy: -1.00680, alpha: 0.25609, time: 33.34675
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 375.34246, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 5.14091, qf2_loss: 5.12307, policy_loss: -199.01124, policy_entropy: -1.00555, alpha: 0.25736, time: 33.34675
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 402.75662, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 5.05670, qf2_loss: 5.03865, policy_loss: -199.84430, policy_entropy: -1.00410, alpha: 0.25732, time: 33.47902
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 431.99280, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 5.03811, qf2_loss: 5.04833, policy_loss: -201.10826, policy_entropy: -1.00310, alpha: 0.25816, time: 33.39705
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 360.96094, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 4.81253, qf2_loss: 4.87106, policy_loss: -200.44291, policy_entropy: -0.98451, alpha: 0.25721, time: 33.35064
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 429.86685, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 5.17663, qf2_loss: 5.17007, policy_loss: -202.14383, policy_entropy: -0.99178, alpha: 0.25565, time: 33.35875
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 393.01243, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 5.95892, qf2_loss: 6.03826, policy_loss: -202.56376, policy_entropy: -0.99303, alpha: 0.25404, time: 33.57541
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 401.56584, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 5.90559, qf2_loss: 5.92417, policy_loss: -202.98094, policy_entropy: -0.99737, alpha: 0.25354, time: 33.27199
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 350.92343, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 5.63593, qf2_loss: 5.66749, policy_loss: -204.01939, policy_entropy: -0.98366, alpha: 0.25203, time: 33.18356
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 385.44644, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 6.24290, qf2_loss: 6.29509, policy_loss: -204.37537, policy_entropy: -0.99579, alpha: 0.25015, time: 33.44651
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 406.15871, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 4.84013, qf2_loss: 4.89507, policy_loss: -204.63140, policy_entropy: -1.00291, alpha: 0.25024, time: 33.23586
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 388.15803, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 5.31866, qf2_loss: 5.32885, policy_loss: -205.42090, policy_entropy: -1.00490, alpha: 0.25101, time: 35.69645
[CW] eval: return: 400.59509, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 386.72636, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 5.83711, qf2_loss: 5.85517, policy_loss: -206.36491, policy_entropy: -0.98736, alpha: 0.24997, time: 33.33577
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 421.00272, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 4.83398, qf2_loss: 4.85069, policy_loss: -207.00972, policy_entropy: -0.99284, alpha: 0.24833, time: 35.15010
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 396.94283, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 5.48521, qf2_loss: 5.54017, policy_loss: -207.51284, policy_entropy: -1.00395, alpha: 0.24798, time: 33.13816
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 459.24026, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 7.10079, qf2_loss: 7.09620, policy_loss: -208.20919, policy_entropy: -0.99801, alpha: 0.24847, time: 33.52409
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 448.35549, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 6.71316, qf2_loss: 6.80795, policy_loss: -209.15537, policy_entropy: -0.99208, alpha: 0.24759, time: 33.59783
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 365.73020, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 5.09004, qf2_loss: 5.11681, policy_loss: -209.36392, policy_entropy: -1.00399, alpha: 0.24718, time: 33.69878
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 447.22082, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 4.93499, qf2_loss: 4.96601, policy_loss: -210.29161, policy_entropy: -1.00006, alpha: 0.24798, time: 33.63778
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 412.45945, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 5.71043, qf2_loss: 5.73581, policy_loss: -210.99724, policy_entropy: -1.01280, alpha: 0.24855, time: 34.63143
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 449.29027, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 5.23133, qf2_loss: 5.25214, policy_loss: -211.23667, policy_entropy: -1.00168, alpha: 0.24929, time: 33.60392
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 467.26809, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 4.92439, qf2_loss: 5.01154, policy_loss: -212.55205, policy_entropy: -1.00804, alpha: 0.25007, time: 33.76243
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 379.46902, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 6.12669, qf2_loss: 6.15084, policy_loss: -213.18634, policy_entropy: -1.00188, alpha: 0.25071, time: 33.69876
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 446.17820, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 5.52300, qf2_loss: 5.58224, policy_loss: -213.14213, policy_entropy: -0.99854, alpha: 0.25104, time: 33.68065
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 443.54249, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 4.85410, qf2_loss: 4.83545, policy_loss: -213.90415, policy_entropy: -0.99471, alpha: 0.25105, time: 33.27567
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 355.66801, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 5.11026, qf2_loss: 5.15620, policy_loss: -215.03877, policy_entropy: -1.00084, alpha: 0.25084, time: 33.32421
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 386.76045, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 5.15828, qf2_loss: 5.17963, policy_loss: -215.50568, policy_entropy: -0.99813, alpha: 0.25002, time: 33.35012
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 434.37804, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 5.63493, qf2_loss: 5.65535, policy_loss: -215.27869, policy_entropy: -1.01215, alpha: 0.25102, time: 33.28235
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 428.65042, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 5.44267, qf2_loss: 5.45187, policy_loss: -216.13396, policy_entropy: -1.00875, alpha: 0.25318, time: 33.33147
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 473.78462, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 5.98614, qf2_loss: 6.03994, policy_loss: -217.38537, policy_entropy: -0.99867, alpha: 0.25365, time: 33.29558
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 341.00107, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 5.34709, qf2_loss: 5.35543, policy_loss: -218.06163, policy_entropy: -0.99713, alpha: 0.25294, time: 33.34311
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 413.94136, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 5.45459, qf2_loss: 5.47415, policy_loss: -218.30546, policy_entropy: -1.02421, alpha: 0.25445, time: 33.47409
[CW] eval: return: 413.59666, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 420.51080, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 5.35204, qf2_loss: 5.33223, policy_loss: -218.96504, policy_entropy: -1.00082, alpha: 0.25661, time: 33.66926
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 424.76068, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 6.10845, qf2_loss: 6.14596, policy_loss: -219.20632, policy_entropy: -0.99617, alpha: 0.25721, time: 33.49008
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 464.63363, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 5.63573, qf2_loss: 5.65264, policy_loss: -220.62625, policy_entropy: -1.01178, alpha: 0.25744, time: 33.45462
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 436.17663, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 5.41501, qf2_loss: 5.41745, policy_loss: -221.02168, policy_entropy: -0.99770, alpha: 0.25736, time: 33.61823
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 436.84976, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 6.29946, qf2_loss: 6.46299, policy_loss: -221.21132, policy_entropy: -0.99720, alpha: 0.25773, time: 33.21773
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 409.14518, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 6.95422, qf2_loss: 6.83019, policy_loss: -221.82321, policy_entropy: -1.00564, alpha: 0.25749, time: 33.32628
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 415.10587, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 5.47449, qf2_loss: 5.48579, policy_loss: -222.78479, policy_entropy: -1.00444, alpha: 0.25872, time: 33.39179
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 456.64104, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 5.35124, qf2_loss: 5.44994, policy_loss: -223.20475, policy_entropy: -1.00347, alpha: 0.25912, time: 33.59391
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 453.62153, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 5.81441, qf2_loss: 5.80370, policy_loss: -224.14915, policy_entropy: -0.99446, alpha: 0.25870, time: 33.42303
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 430.71237, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 5.32205, qf2_loss: 5.34138, policy_loss: -224.80024, policy_entropy: -1.00545, alpha: 0.25952, time: 33.45461
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 365.76499, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 5.76682, qf2_loss: 5.78392, policy_loss: -225.03839, policy_entropy: -1.00684, alpha: 0.26009, time: 33.49612
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 417.68143, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 5.90203, qf2_loss: 5.89933, policy_loss: -225.84104, policy_entropy: -1.00154, alpha: 0.26058, time: 33.34976
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 474.17036, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 6.85871, qf2_loss: 6.91554, policy_loss: -226.29373, policy_entropy: -0.99495, alpha: 0.26016, time: 33.38331
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 412.60979, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 5.99491, qf2_loss: 6.03149, policy_loss: -226.39842, policy_entropy: -1.01324, alpha: 0.26051, time: 33.39342
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 348.16420, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 5.49877, qf2_loss: 5.50029, policy_loss: -227.37645, policy_entropy: -1.01119, alpha: 0.26319, time: 33.21892
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 439.67261, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 6.22880, qf2_loss: 6.27100, policy_loss: -228.29636, policy_entropy: -1.00091, alpha: 0.26378, time: 33.46264
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 416.29777, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 5.71139, qf2_loss: 5.67639, policy_loss: -229.23859, policy_entropy: -1.00588, alpha: 0.26475, time: 33.35851
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 434.75873, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 6.44003, qf2_loss: 6.51162, policy_loss: -229.06742, policy_entropy: -1.01181, alpha: 0.26555, time: 33.08326
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 431.40236, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 6.17652, qf2_loss: 6.23942, policy_loss: -230.05637, policy_entropy: -1.01197, alpha: 0.26754, time: 33.03116
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 425.57552, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 7.81571, qf2_loss: 7.75904, policy_loss: -230.29692, policy_entropy: -1.00726, alpha: 0.26862, time: 33.50923
[CW] eval: return: 427.19109, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 399.36226, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 5.73071, qf2_loss: 5.78995, policy_loss: -231.30617, policy_entropy: -1.00756, alpha: 0.27017, time: 33.54003
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 476.43676, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 5.11096, qf2_loss: 5.12209, policy_loss: -231.64351, policy_entropy: -1.01225, alpha: 0.27177, time: 33.59381
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 428.98087, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 5.40375, qf2_loss: 5.40922, policy_loss: -232.40579, policy_entropy: -1.00248, alpha: 0.27345, time: 33.27023
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 516.51895, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 5.61922, qf2_loss: 5.63377, policy_loss: -233.07601, policy_entropy: -1.00157, alpha: 0.27323, time: 33.27832
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 361.88130, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 5.43008, qf2_loss: 5.50067, policy_loss: -233.05991, policy_entropy: -1.01140, alpha: 0.27436, time: 33.11610
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 480.23455, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 5.74496, qf2_loss: 5.73633, policy_loss: -233.46297, policy_entropy: -1.00297, alpha: 0.27555, time: 33.15041
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 425.41045, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 5.32799, qf2_loss: 5.30777, policy_loss: -234.93132, policy_entropy: -1.00703, alpha: 0.27632, time: 33.54833
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 514.67205, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 9.83483, qf2_loss: 9.85461, policy_loss: -235.40223, policy_entropy: -1.00402, alpha: 0.27742, time: 33.96374
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 418.78186, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 7.67376, qf2_loss: 7.80781, policy_loss: -236.58962, policy_entropy: -0.99561, alpha: 0.27717, time: 33.48892
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 465.13336, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 5.41213, qf2_loss: 5.41761, policy_loss: -236.88986, policy_entropy: -1.01299, alpha: 0.27800, time: 33.04968
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 445.35887, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 5.08655, qf2_loss: 5.09874, policy_loss: -237.76949, policy_entropy: -1.00863, alpha: 0.28029, time: 33.53766
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 394.79254, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 5.52810, qf2_loss: 5.51022, policy_loss: -237.94508, policy_entropy: -1.01241, alpha: 0.28100, time: 33.38468
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 490.14650, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 5.27104, qf2_loss: 5.32601, policy_loss: -238.62161, policy_entropy: -1.00481, alpha: 0.28249, time: 33.38073
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 426.06028, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 6.23622, qf2_loss: 6.25452, policy_loss: -239.64887, policy_entropy: -1.01441, alpha: 0.28471, time: 33.31845
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 435.86953, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 8.41167, qf2_loss: 8.51869, policy_loss: -239.80723, policy_entropy: -0.99709, alpha: 0.28579, time: 33.08153
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 492.61301, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 5.72259, qf2_loss: 5.74831, policy_loss: -240.55015, policy_entropy: -1.00378, alpha: 0.28524, time: 32.93319
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 497.18118, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 5.14097, qf2_loss: 5.17324, policy_loss: -240.64965, policy_entropy: -1.01995, alpha: 0.28700, time: 33.45486
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 443.25422, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 5.90361, qf2_loss: 5.87776, policy_loss: -242.06170, policy_entropy: -1.01089, alpha: 0.29014, time: 32.98161
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 439.14043, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 5.66824, qf2_loss: 5.65189, policy_loss: -242.61539, policy_entropy: -0.99897, alpha: 0.29131, time: 33.17478
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 502.12277, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 5.93971, qf2_loss: 5.99012, policy_loss: -243.19252, policy_entropy: -0.99697, alpha: 0.29006, time: 33.29909
[CW] eval: return: 466.91368, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 531.05439, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 6.83751, qf2_loss: 6.88274, policy_loss: -243.82607, policy_entropy: -0.99462, alpha: 0.28977, time: 33.34031
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 461.27621, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 6.22097, qf2_loss: 6.26497, policy_loss: -244.75296, policy_entropy: -1.01555, alpha: 0.29006, time: 33.04237
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 454.02692, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 6.40196, qf2_loss: 6.44874, policy_loss: -244.99706, policy_entropy: -0.99523, alpha: 0.29131, time: 33.39464
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 498.24260, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 5.42105, qf2_loss: 5.40715, policy_loss: -245.72956, policy_entropy: -0.99831, alpha: 0.29066, time: 33.41157
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 506.64802, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 6.72185, qf2_loss: 6.76621, policy_loss: -246.07714, policy_entropy: -0.99115, alpha: 0.29057, time: 33.54230
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 475.28069, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 5.81685, qf2_loss: 5.78353, policy_loss: -246.88916, policy_entropy: -0.99922, alpha: 0.28913, time: 32.93854
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 473.67394, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 5.94793, qf2_loss: 5.91661, policy_loss: -247.17529, policy_entropy: -1.02134, alpha: 0.29047, time: 33.58095
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 508.99306, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 6.21224, qf2_loss: 6.27084, policy_loss: -247.95733, policy_entropy: -1.00020, alpha: 0.29234, time: 33.35443
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 516.65034, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 5.90944, qf2_loss: 5.94166, policy_loss: -248.38733, policy_entropy: -1.02045, alpha: 0.29407, time: 33.36362
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 509.56642, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 5.61858, qf2_loss: 5.57711, policy_loss: -249.60076, policy_entropy: -0.99653, alpha: 0.29554, time: 32.96266
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 425.60801, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 5.87723, qf2_loss: 5.87502, policy_loss: -250.57841, policy_entropy: -1.00102, alpha: 0.29536, time: 32.90630
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 480.18698, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 5.93800, qf2_loss: 5.99784, policy_loss: -250.48057, policy_entropy: -1.01018, alpha: 0.29691, time: 33.34002
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 414.66778, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 6.62557, qf2_loss: 6.66220, policy_loss: -250.98116, policy_entropy: -1.01883, alpha: 0.29855, time: 33.00181
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 505.76806, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 6.40845, qf2_loss: 6.42276, policy_loss: -251.94952, policy_entropy: -1.00030, alpha: 0.30020, time: 33.60514
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 455.07938, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 5.42503, qf2_loss: 5.40034, policy_loss: -252.76316, policy_entropy: -1.00361, alpha: 0.30039, time: 33.06434
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 485.72156, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 5.82884, qf2_loss: 5.85794, policy_loss: -253.09836, policy_entropy: -1.01042, alpha: 0.30133, time: 33.00273
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 430.74004, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 6.88938, qf2_loss: 6.82965, policy_loss: -253.68830, policy_entropy: -1.00618, alpha: 0.30327, time: 33.31314
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 465.90832, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 8.32468, qf2_loss: 8.44402, policy_loss: -254.35478, policy_entropy: -1.00551, alpha: 0.30428, time: 33.14021
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 434.01522, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 5.73347, qf2_loss: 5.76275, policy_loss: -255.19930, policy_entropy: -0.99809, alpha: 0.30390, time: 32.92768
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 429.67243, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 5.19679, qf2_loss: 5.23716, policy_loss: -255.71924, policy_entropy: -0.99478, alpha: 0.30354, time: 32.91692
[CW] eval: return: 470.03300, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 463.77665, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 5.59606, qf2_loss: 5.59719, policy_loss: -256.16226, policy_entropy: -0.99792, alpha: 0.30273, time: 33.39446
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 438.82526, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 5.58305, qf2_loss: 5.62026, policy_loss: -256.53090, policy_entropy: -1.00814, alpha: 0.30330, time: 33.26314
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 412.09099, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 5.93273, qf2_loss: 6.00696, policy_loss: -256.89411, policy_entropy: -1.02100, alpha: 0.30560, time: 32.98436
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 519.69038, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 6.17911, qf2_loss: 6.22992, policy_loss: -258.37304, policy_entropy: -0.99010, alpha: 0.30676, time: 33.22765
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 522.17756, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 5.49431, qf2_loss: 5.50541, policy_loss: -258.55958, policy_entropy: -1.00801, alpha: 0.30629, time: 33.30013
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 511.29565, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 5.63689, qf2_loss: 5.63404, policy_loss: -258.96493, policy_entropy: -1.00726, alpha: 0.30776, time: 33.60524
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 665.84058, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 5.73593, qf2_loss: 5.72503, policy_loss: -260.13083, policy_entropy: -1.01769, alpha: 0.31016, time: 33.09045
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 448.68523, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 5.83231, qf2_loss: 5.78684, policy_loss: -260.95139, policy_entropy: -1.01105, alpha: 0.31194, time: 32.92517
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 458.95836, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 6.18523, qf2_loss: 6.21523, policy_loss: -260.96685, policy_entropy: -0.99819, alpha: 0.31277, time: 33.67053
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 447.67592, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 5.64561, qf2_loss: 5.71266, policy_loss: -261.86337, policy_entropy: -1.01478, alpha: 0.31476, time: 33.08566
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 473.56434, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 5.87861, qf2_loss: 5.91602, policy_loss: -263.21414, policy_entropy: -0.98297, alpha: 0.31419, time: 33.26335
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 465.49643, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 5.99109, qf2_loss: 6.02133, policy_loss: -263.11491, policy_entropy: -1.00476, alpha: 0.31316, time: 33.02139
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 503.87362, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 6.00445, qf2_loss: 6.00248, policy_loss: -263.61800, policy_entropy: -1.00422, alpha: 0.31434, time: 33.27978
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 440.10416, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 7.03305, qf2_loss: 7.07630, policy_loss: -264.25821, policy_entropy: -1.00501, alpha: 0.31463, time: 33.36704
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 537.83526, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 6.32534, qf2_loss: 6.32267, policy_loss: -264.44985, policy_entropy: -1.01240, alpha: 0.31614, time: 33.43110
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 379.89762, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 6.31944, qf2_loss: 6.40849, policy_loss: -265.20167, policy_entropy: -0.99762, alpha: 0.31663, time: 33.07510
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 432.77186, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 5.21772, qf2_loss: 5.28938, policy_loss: -266.42618, policy_entropy: -1.00178, alpha: 0.31641, time: 32.98587
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 479.67977, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 5.45714, qf2_loss: 5.43811, policy_loss: -266.95634, policy_entropy: -1.00153, alpha: 0.31754, time: 33.24270
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 511.18480, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 5.77179, qf2_loss: 5.81582, policy_loss: -267.39984, policy_entropy: -1.00289, alpha: 0.31774, time: 32.75513
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 408.38559, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 5.57689, qf2_loss: 5.57936, policy_loss: -267.79577, policy_entropy: -1.00627, alpha: 0.31834, time: 33.09207
[CW] eval: return: 474.88799, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 467.57694, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 5.87405, qf2_loss: 5.81388, policy_loss: -268.48727, policy_entropy: -0.98936, alpha: 0.31806, time: 32.79806
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 483.13402, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 14.20016, qf2_loss: 14.37268, policy_loss: -268.79850, policy_entropy: -0.99975, alpha: 0.31800, time: 33.02026
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 451.81770, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 10.91081, qf2_loss: 10.94552, policy_loss: -269.99967, policy_entropy: -0.97355, alpha: 0.31425, time: 33.23823
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 431.62247, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 5.86611, qf2_loss: 5.85236, policy_loss: -270.14930, policy_entropy: -1.01283, alpha: 0.31326, time: 33.02046
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 522.44806, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 5.18669, qf2_loss: 5.19631, policy_loss: -271.08764, policy_entropy: -1.00754, alpha: 0.31512, time: 33.25197
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 502.11504, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 5.17711, qf2_loss: 5.22564, policy_loss: -271.42014, policy_entropy: -1.02382, alpha: 0.31760, time: 32.77353
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 465.25561, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 4.95019, qf2_loss: 4.93814, policy_loss: -272.84053, policy_entropy: -1.01081, alpha: 0.32113, time: 32.99254
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 455.12128, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 5.04096, qf2_loss: 5.00280, policy_loss: -272.84995, policy_entropy: -1.01732, alpha: 0.32348, time: 32.87511
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 435.28098, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 5.70621, qf2_loss: 5.77764, policy_loss: -273.40909, policy_entropy: -1.00553, alpha: 0.32614, time: 33.14187
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 552.74841, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 5.46942, qf2_loss: 5.46797, policy_loss: -274.49933, policy_entropy: -1.01110, alpha: 0.32683, time: 33.35181
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 536.34119, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 5.62004, qf2_loss: 5.58968, policy_loss: -274.36024, policy_entropy: -1.01342, alpha: 0.32918, time: 32.92651
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 503.01072, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 5.66619, qf2_loss: 5.71814, policy_loss: -275.40020, policy_entropy: -1.00347, alpha: 0.33087, time: 36.16607
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 501.43500, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 6.10511, qf2_loss: 6.10479, policy_loss: -275.42343, policy_entropy: -1.00061, alpha: 0.33181, time: 33.45350
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 470.42028, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 6.34923, qf2_loss: 6.40439, policy_loss: -275.86027, policy_entropy: -1.00093, alpha: 0.33135, time: 33.06709
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 378.94469, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 5.71430, qf2_loss: 5.68983, policy_loss: -276.85737, policy_entropy: -1.01321, alpha: 0.33285, time: 33.76125
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 512.03360, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 6.30517, qf2_loss: 6.40053, policy_loss: -277.01616, policy_entropy: -1.00574, alpha: 0.33493, time: 33.11244
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 593.27960, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 6.44701, qf2_loss: 6.46182, policy_loss: -278.27273, policy_entropy: -0.98797, alpha: 0.33408, time: 33.54937
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 509.76935, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 6.77742, qf2_loss: 6.83947, policy_loss: -278.20187, policy_entropy: -0.99884, alpha: 0.33257, time: 33.38249
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 522.41974, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 5.86891, qf2_loss: 5.87860, policy_loss: -278.90890, policy_entropy: -1.01014, alpha: 0.33412, time: 33.15367
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 542.68544, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 6.01464, qf2_loss: 5.96769, policy_loss: -279.82450, policy_entropy: -1.00084, alpha: 0.33404, time: 33.15961
[CW] eval: return: 542.47925, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 535.27355, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 6.09768, qf2_loss: 6.10187, policy_loss: -280.63329, policy_entropy: -0.98945, alpha: 0.33410, time: 32.77741
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 588.43790, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 5.51274, qf2_loss: 5.55132, policy_loss: -281.31680, policy_entropy: -0.99689, alpha: 0.33285, time: 33.28933
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 495.92022, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 5.96717, qf2_loss: 5.98754, policy_loss: -281.41462, policy_entropy: -1.01306, alpha: 0.33361, time: 33.52011
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 521.52845, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 6.43503, qf2_loss: 6.45228, policy_loss: -282.41777, policy_entropy: -0.99442, alpha: 0.33444, time: 33.06417
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 474.88676, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 6.01108, qf2_loss: 6.04568, policy_loss: -282.04604, policy_entropy: -1.01399, alpha: 0.33469, time: 33.45860
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 451.43150, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 5.92339, qf2_loss: 5.95469, policy_loss: -282.50708, policy_entropy: -0.99974, alpha: 0.33560, time: 33.05707
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 550.59414, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 6.07891, qf2_loss: 6.11676, policy_loss: -283.61572, policy_entropy: -0.99676, alpha: 0.33635, time: 32.97334
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 619.72694, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 6.42521, qf2_loss: 6.49313, policy_loss: -284.18913, policy_entropy: -1.01384, alpha: 0.33669, time: 32.93416
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 615.04806, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 6.07768, qf2_loss: 6.11334, policy_loss: -285.03427, policy_entropy: -1.01330, alpha: 0.33995, time: 33.01774
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 481.03716, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 6.70642, qf2_loss: 6.67372, policy_loss: -285.56670, policy_entropy: -0.98887, alpha: 0.33972, time: 33.41463
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 549.41656, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 6.56824, qf2_loss: 6.68288, policy_loss: -285.48189, policy_entropy: -0.99888, alpha: 0.33882, time: 33.45832
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 629.40792, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 5.72402, qf2_loss: 5.78346, policy_loss: -286.67885, policy_entropy: -1.01393, alpha: 0.33924, time: 33.57245
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 591.51927, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 6.95752, qf2_loss: 6.98223, policy_loss: -287.34103, policy_entropy: -1.00089, alpha: 0.34111, time: 33.27220
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 506.50712, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 6.58844, qf2_loss: 6.53021, policy_loss: -287.19198, policy_entropy: -0.99272, alpha: 0.34094, time: 32.97770
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 537.22192, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 6.93282, qf2_loss: 6.97297, policy_loss: -287.89078, policy_entropy: -1.00676, alpha: 0.34047, time: 33.21627
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 540.87749, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 7.23764, qf2_loss: 7.22875, policy_loss: -288.66041, policy_entropy: -0.99381, alpha: 0.34107, time: 32.85330
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 531.16719, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 6.45516, qf2_loss: 6.51725, policy_loss: -288.76955, policy_entropy: -1.00543, alpha: 0.34046, time: 33.38995
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 626.69684, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 7.13465, qf2_loss: 7.14671, policy_loss: -289.57078, policy_entropy: -0.99718, alpha: 0.34097, time: 32.71899
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 617.21618, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 6.63910, qf2_loss: 6.60190, policy_loss: -289.59644, policy_entropy: -1.00077, alpha: 0.34070, time: 32.82500
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 611.46311, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 6.29281, qf2_loss: 6.41347, policy_loss: -290.85851, policy_entropy: -1.01980, alpha: 0.34242, time: 33.07098
[CW] eval: return: 512.72480, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 547.51430, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 6.63344, qf2_loss: 6.69581, policy_loss: -291.65040, policy_entropy: -1.00917, alpha: 0.34560, time: 33.65067
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 551.09424, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 8.14672, qf2_loss: 8.19457, policy_loss: -292.18810, policy_entropy: -1.00281, alpha: 0.34703, time: 32.88727
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 502.15904, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 7.94787, qf2_loss: 8.04239, policy_loss: -292.88945, policy_entropy: -0.99877, alpha: 0.34763, time: 33.15314
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 593.46064, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 6.98205, qf2_loss: 7.07154, policy_loss: -292.93134, policy_entropy: -1.01530, alpha: 0.34858, time: 33.45423
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 685.14898, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 6.08705, qf2_loss: 6.10650, policy_loss: -293.06863, policy_entropy: -1.01003, alpha: 0.35167, time: 32.71457
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 475.20348, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 6.79895, qf2_loss: 6.83616, policy_loss: -294.00696, policy_entropy: -1.00654, alpha: 0.35414, time: 32.95847
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 486.81231, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 8.84484, qf2_loss: 8.91166, policy_loss: -294.89860, policy_entropy: -1.01005, alpha: 0.35608, time: 33.30767
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 539.83954, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 7.76285, qf2_loss: 7.81162, policy_loss: -295.25196, policy_entropy: -1.00451, alpha: 0.35694, time: 33.62483
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 529.63473, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 7.22425, qf2_loss: 7.18203, policy_loss: -295.92975, policy_entropy: -0.99638, alpha: 0.35638, time: 33.50124
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 529.54976, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 6.85872, qf2_loss: 6.89215, policy_loss: -296.74697, policy_entropy: -1.00093, alpha: 0.35666, time: 33.13516
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 552.81856, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 6.33716, qf2_loss: 6.40457, policy_loss: -296.65208, policy_entropy: -1.00926, alpha: 0.35746, time: 33.21390
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 619.24317, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 7.07758, qf2_loss: 7.07073, policy_loss: -297.81486, policy_entropy: -1.00124, alpha: 0.35888, time: 32.97253
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 525.13637, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 7.55178, qf2_loss: 7.55106, policy_loss: -298.49197, policy_entropy: -1.00843, alpha: 0.36068, time: 33.29950
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 545.26637, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 7.94315, qf2_loss: 8.10767, policy_loss: -298.36116, policy_entropy: -1.00708, alpha: 0.36235, time: 33.00791
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 534.56480, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 7.75960, qf2_loss: 7.75400, policy_loss: -299.94230, policy_entropy: -0.99499, alpha: 0.36207, time: 32.76704
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 564.01286, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 7.93984, qf2_loss: 8.00818, policy_loss: -300.86637, policy_entropy: -1.00886, alpha: 0.36305, time: 33.00122
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 541.19853, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 7.25675, qf2_loss: 7.30319, policy_loss: -300.41070, policy_entropy: -1.01652, alpha: 0.36533, time: 32.94821
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 585.14526, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 7.63232, qf2_loss: 7.66224, policy_loss: -300.96893, policy_entropy: -0.99956, alpha: 0.36810, time: 33.02301
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 602.53986, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 7.29233, qf2_loss: 7.22867, policy_loss: -301.77874, policy_entropy: -0.99529, alpha: 0.36722, time: 33.33948
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 582.10995, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 7.38296, qf2_loss: 7.33463, policy_loss: -302.61701, policy_entropy: -1.00115, alpha: 0.36754, time: 33.82025
[CW] eval: return: 574.90354, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 533.82057, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 8.16256, qf2_loss: 8.18751, policy_loss: -302.56215, policy_entropy: -1.00400, alpha: 0.36757, time: 33.01132
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 532.70112, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 9.24321, qf2_loss: 9.11641, policy_loss: -302.96089, policy_entropy: -0.98979, alpha: 0.36679, time: 32.84250
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 536.72770, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 8.20356, qf2_loss: 8.35008, policy_loss: -303.27406, policy_entropy: -1.01142, alpha: 0.36617, time: 33.34469
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 612.97446, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 7.97124, qf2_loss: 7.90546, policy_loss: -304.34532, policy_entropy: -1.00598, alpha: 0.36800, time: 33.25668
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 605.84100, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 8.97791, qf2_loss: 8.99516, policy_loss: -304.40418, policy_entropy: -0.99495, alpha: 0.36986, time: 33.19397
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 677.64973, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 10.24600, qf2_loss: 10.38761, policy_loss: -306.14170, policy_entropy: -0.99676, alpha: 0.36878, time: 33.50206
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 570.24460, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 8.17934, qf2_loss: 8.18923, policy_loss: -306.14189, policy_entropy: -1.01295, alpha: 0.36907, time: 33.88013
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 580.99492, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 7.57181, qf2_loss: 7.64916, policy_loss: -306.33267, policy_entropy: -0.99436, alpha: 0.36992, time: 33.08090
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 538.30323, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 8.19868, qf2_loss: 8.21869, policy_loss: -307.31464, policy_entropy: -1.00708, alpha: 0.36939, time: 33.53303
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 635.79764, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 8.25483, qf2_loss: 8.32949, policy_loss: -308.14889, policy_entropy: -0.98615, alpha: 0.36946, time: 33.37796
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 620.35477, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 8.03178, qf2_loss: 8.08052, policy_loss: -308.43208, policy_entropy: -1.00664, alpha: 0.36783, time: 32.83178
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 596.51821, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 7.91140, qf2_loss: 8.03445, policy_loss: -308.52353, policy_entropy: -1.00566, alpha: 0.36935, time: 33.23974
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 572.23080, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 9.10578, qf2_loss: 9.18471, policy_loss: -309.51110, policy_entropy: -1.00133, alpha: 0.37085, time: 33.20198
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 623.45592, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 8.40195, qf2_loss: 8.48541, policy_loss: -310.31492, policy_entropy: -0.98972, alpha: 0.37000, time: 33.14941
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 610.11845, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 7.88502, qf2_loss: 7.80830, policy_loss: -310.77278, policy_entropy: -1.00639, alpha: 0.36924, time: 33.44613
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 659.52048, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 8.77834, qf2_loss: 8.92290, policy_loss: -311.27622, policy_entropy: -1.00711, alpha: 0.37003, time: 33.47852
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 610.96117, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 8.22228, qf2_loss: 8.26850, policy_loss: -311.87392, policy_entropy: -1.00611, alpha: 0.37220, time: 33.07074
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 599.42059, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 9.37437, qf2_loss: 9.41654, policy_loss: -313.32233, policy_entropy: -0.99828, alpha: 0.37326, time: 33.00200
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 572.52412, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 9.20839, qf2_loss: 9.20552, policy_loss: -313.66048, policy_entropy: -0.99335, alpha: 0.37164, time: 32.74742
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 585.11659, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 9.51741, qf2_loss: 9.62948, policy_loss: -313.81093, policy_entropy: -0.99889, alpha: 0.37109, time: 33.08118
[CW] eval: return: 617.32720, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 589.63301, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 8.74243, qf2_loss: 8.77421, policy_loss: -313.91556, policy_entropy: -1.01151, alpha: 0.37277, time: 32.86957
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 593.79139, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 8.34512, qf2_loss: 8.39694, policy_loss: -314.72604, policy_entropy: -1.01153, alpha: 0.37495, time: 33.01471
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 603.24924, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 9.40837, qf2_loss: 9.52622, policy_loss: -315.00458, policy_entropy: -1.00983, alpha: 0.37713, time: 35.61646
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 615.43548, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 11.50957, qf2_loss: 11.63856, policy_loss: -316.22481, policy_entropy: -1.00071, alpha: 0.37861, time: 32.95300
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 649.28192, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 8.43127, qf2_loss: 8.50067, policy_loss: -316.35442, policy_entropy: -1.00819, alpha: 0.38016, time: 32.90919
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 586.53993, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 8.63605, qf2_loss: 8.67786, policy_loss: -317.29437, policy_entropy: -1.00746, alpha: 0.38217, time: 33.25305
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 556.16917, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 9.86782, qf2_loss: 9.79021, policy_loss: -317.45628, policy_entropy: -1.00364, alpha: 0.38313, time: 33.23494
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 536.23945, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 9.43788, qf2_loss: 9.58113, policy_loss: -318.64641, policy_entropy: -0.99519, alpha: 0.38311, time: 32.86536
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 841.61291, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 8.38959, qf2_loss: 8.46706, policy_loss: -319.15457, policy_entropy: -1.00804, alpha: 0.38245, time: 33.49161
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 692.78765, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 8.57857, qf2_loss: 8.59142, policy_loss: -319.25785, policy_entropy: -1.00601, alpha: 0.38523, time: 33.41121
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 539.75120, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 9.48576, qf2_loss: 9.59357, policy_loss: -320.64383, policy_entropy: -1.00358, alpha: 0.38642, time: 32.90978
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 541.10899, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 10.03526, qf2_loss: 10.14002, policy_loss: -320.30738, policy_entropy: -1.00895, alpha: 0.38916, time: 32.97346
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 680.53343, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 9.68973, qf2_loss: 9.60969, policy_loss: -321.23156, policy_entropy: -1.00294, alpha: 0.38950, time: 33.27509
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 533.14531, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 9.18608, qf2_loss: 9.14751, policy_loss: -322.13729, policy_entropy: -1.00884, alpha: 0.39165, time: 33.32134
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 665.67736, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 10.44978, qf2_loss: 10.44767, policy_loss: -322.20787, policy_entropy: -0.99220, alpha: 0.39128, time: 32.99777
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 627.49187, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 8.98432, qf2_loss: 9.12898, policy_loss: -322.55209, policy_entropy: -1.00485, alpha: 0.39106, time: 33.34596
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 709.93813, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 11.67358, qf2_loss: 11.77231, policy_loss: -324.04377, policy_entropy: -1.00881, alpha: 0.39294, time: 33.20980
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 652.03335, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 11.45421, qf2_loss: 11.49385, policy_loss: -324.57353, policy_entropy: -0.99346, alpha: 0.39384, time: 33.07802
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 612.32258, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 9.62624, qf2_loss: 9.63225, policy_loss: -324.81388, policy_entropy: -1.00905, alpha: 0.39257, time: 33.10982
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 611.79792, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 11.13019, qf2_loss: 11.29048, policy_loss: -325.40855, policy_entropy: -1.00414, alpha: 0.39415, time: 33.41507
[CW] eval: return: 621.04872, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 536.46639, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 10.01476, qf2_loss: 10.11156, policy_loss: -325.71032, policy_entropy: -1.00345, alpha: 0.39616, time: 33.19828
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 715.70404, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 9.68220, qf2_loss: 9.67174, policy_loss: -326.68679, policy_entropy: -1.00731, alpha: 0.39708, time: 33.00833
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 689.20117, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 9.68555, qf2_loss: 9.78069, policy_loss: -327.75386, policy_entropy: -1.00660, alpha: 0.39882, time: 33.10197
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 653.92043, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 8.92013, qf2_loss: 8.88632, policy_loss: -328.61751, policy_entropy: -1.01047, alpha: 0.40124, time: 33.04157
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 600.09789, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 10.36013, qf2_loss: 10.37347, policy_loss: -327.50819, policy_entropy: -1.00781, alpha: 0.40420, time: 33.47437
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 687.39409, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 11.74730, qf2_loss: 11.96028, policy_loss: -329.11321, policy_entropy: -0.99716, alpha: 0.40539, time: 33.39310
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 688.76989, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 10.56695, qf2_loss: 10.55057, policy_loss: -329.55296, policy_entropy: -1.00624, alpha: 0.40538, time: 32.93362
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 694.15452, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 10.43744, qf2_loss: 10.44726, policy_loss: -330.14571, policy_entropy: -0.99424, alpha: 0.40577, time: 33.39607
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 591.30710, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 10.14000, qf2_loss: 10.14755, policy_loss: -330.53342, policy_entropy: -1.01542, alpha: 0.40613, time: 32.97086
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 687.62514, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 10.92736, qf2_loss: 10.98848, policy_loss: -331.31114, policy_entropy: -1.00816, alpha: 0.41017, time: 33.15199
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 667.27399, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 10.45657, qf2_loss: 10.51256, policy_loss: -332.28849, policy_entropy: -0.99613, alpha: 0.41098, time: 33.30501
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 821.81581, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 13.10066, qf2_loss: 13.17054, policy_loss: -332.46116, policy_entropy: -0.99048, alpha: 0.41001, time: 32.95473
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 748.63615, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 10.44545, qf2_loss: 10.41372, policy_loss: -333.23864, policy_entropy: -1.01766, alpha: 0.41034, time: 33.47760
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 694.54176, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 10.21548, qf2_loss: 10.20796, policy_loss: -332.85704, policy_entropy: -1.02214, alpha: 0.41525, time: 33.10489
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 851.08817, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 10.68668, qf2_loss: 10.70839, policy_loss: -335.01360, policy_entropy: -1.00336, alpha: 0.41977, time: 33.34312
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 757.57718, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 10.72508, qf2_loss: 10.87553, policy_loss: -335.12539, policy_entropy: -0.99964, alpha: 0.41971, time: 33.02383
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 682.27371, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 14.76263, qf2_loss: 14.74502, policy_loss: -336.31309, policy_entropy: -0.99562, alpha: 0.41935, time: 33.59847
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 612.99598, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 10.68226, qf2_loss: 10.75300, policy_loss: -336.93337, policy_entropy: -1.00387, alpha: 0.41801, time: 33.27372
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 677.36647, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 10.58934, qf2_loss: 10.60874, policy_loss: -338.42917, policy_entropy: -1.01767, alpha: 0.42207, time: 33.44058
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 685.32684, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 11.11784, qf2_loss: 11.14145, policy_loss: -337.93739, policy_entropy: -1.00626, alpha: 0.42494, time: 33.40717
[CW] eval: return: 726.48148, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 620.80583, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 10.89931, qf2_loss: 10.98965, policy_loss: -338.35997, policy_entropy: -0.98979, alpha: 0.42557, time: 33.09020
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 830.91249, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 11.45096, qf2_loss: 11.48084, policy_loss: -339.25040, policy_entropy: -1.00626, alpha: 0.42510, time: 32.88269
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 840.25468, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 11.09471, qf2_loss: 11.13379, policy_loss: -340.67948, policy_entropy: -1.01941, alpha: 0.42725, time: 33.08226
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 827.37073, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 11.17894, qf2_loss: 11.24492, policy_loss: -341.30311, policy_entropy: -1.01545, alpha: 0.43410, time: 33.52917
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 850.37683, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 13.23520, qf2_loss: 13.14402, policy_loss: -341.30919, policy_entropy: -0.99489, alpha: 0.43529, time: 35.79606
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 763.88523, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 13.63507, qf2_loss: 13.69488, policy_loss: -342.79791, policy_entropy: -0.99354, alpha: 0.43333, time: 33.33904
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 677.42508, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 12.52610, qf2_loss: 12.61509, policy_loss: -342.50408, policy_entropy: -0.99658, alpha: 0.43202, time: 33.51748
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 844.65863, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 11.58817, qf2_loss: 11.68733, policy_loss: -343.13599, policy_entropy: -1.00129, alpha: 0.43230, time: 33.94204
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 809.98332, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 13.19296, qf2_loss: 13.24020, policy_loss: -343.93988, policy_entropy: -1.00634, alpha: 0.43252, time: 32.89507
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 751.61511, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 12.65496, qf2_loss: 12.66203, policy_loss: -345.03841, policy_entropy: -1.01210, alpha: 0.43475, time: 32.90723
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 760.52240, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 11.31932, qf2_loss: 11.33620, policy_loss: -345.28006, policy_entropy: -1.00671, alpha: 0.43770, time: 32.88253
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 702.36658, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 11.24851, qf2_loss: 11.35817, policy_loss: -346.01851, policy_entropy: -1.00704, alpha: 0.44059, time: 33.38050
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 850.15924, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 12.86652, qf2_loss: 12.84597, policy_loss: -347.14937, policy_entropy: -1.01706, alpha: 0.44405, time: 33.24144
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 827.59223, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 18.75501, qf2_loss: 19.19985, policy_loss: -347.22463, policy_entropy: -0.98829, alpha: 0.44577, time: 33.48392
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 661.48572, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 13.11831, qf2_loss: 13.14484, policy_loss: -348.31523, policy_entropy: -1.00963, alpha: 0.44395, time: 32.92821
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 657.37861, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 12.19728, qf2_loss: 12.18725, policy_loss: -348.38041, policy_entropy: -1.00180, alpha: 0.44705, time: 32.76400
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 827.80069, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 12.68210, qf2_loss: 12.62238, policy_loss: -349.16190, policy_entropy: -1.00521, alpha: 0.44794, time: 33.34551
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 837.58580, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 14.44804, qf2_loss: 14.52060, policy_loss: -349.49546, policy_entropy: -1.00608, alpha: 0.44943, time: 32.67907
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 833.70383, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 12.74614, qf2_loss: 12.75066, policy_loss: -351.02383, policy_entropy: -0.99409, alpha: 0.44998, time: 33.17255
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 825.44503, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 12.72599, qf2_loss: 12.77273, policy_loss: -350.97373, policy_entropy: -1.01181, alpha: 0.45007, time: 33.17694
[CW] eval: return: 824.20565, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 833.47838, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 13.22630, qf2_loss: 13.03804, policy_loss: -354.14401, policy_entropy: -1.00394, alpha: 0.45252, time: 32.94438
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 829.35774, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 13.67320, qf2_loss: 13.65879, policy_loss: -354.35330, policy_entropy: -1.01190, alpha: 0.45575, time: 33.14637
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 837.64881, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 13.52601, qf2_loss: 13.61261, policy_loss: -354.07959, policy_entropy: -1.00907, alpha: 0.45912, time: 33.01562
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 833.93595, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 15.74979, qf2_loss: 15.85378, policy_loss: -355.38185, policy_entropy: -1.00489, alpha: 0.46066, time: 33.30140
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 837.98143, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 13.82799, qf2_loss: 13.92188, policy_loss: -355.65658, policy_entropy: -0.99618, alpha: 0.46066, time: 33.11481
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 588.39826, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 12.68106, qf2_loss: 12.65996, policy_loss: -355.81988, policy_entropy: -1.01339, alpha: 0.46338, time: 33.42536
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 600.08568, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 14.61037, qf2_loss: 14.71201, policy_loss: -356.84438, policy_entropy: -1.00296, alpha: 0.46687, time: 33.25946
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 848.68614, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 15.79110, qf2_loss: 15.94206, policy_loss: -357.65917, policy_entropy: -1.00195, alpha: 0.46731, time: 33.37849
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 838.85288, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 15.15154, qf2_loss: 15.15465, policy_loss: -358.41591, policy_entropy: -0.99674, alpha: 0.46672, time: 33.11010
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 840.12707, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 16.39205, qf2_loss: 16.63309, policy_loss: -360.99623, policy_entropy: -1.00697, alpha: 0.46807, time: 33.17572
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 838.16758, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 14.13499, qf2_loss: 14.19837, policy_loss: -359.87847, policy_entropy: -1.01272, alpha: 0.47103, time: 32.90363
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 841.67085, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 15.86600, qf2_loss: 15.74789, policy_loss: -361.11169, policy_entropy: -0.99819, alpha: 0.47386, time: 32.79362
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 843.99107, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 17.73378, qf2_loss: 17.63177, policy_loss: -362.70907, policy_entropy: -1.00271, alpha: 0.47248, time: 33.40128
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 838.02574, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 15.31449, qf2_loss: 15.56241, policy_loss: -361.76988, policy_entropy: -1.00921, alpha: 0.47542, time: 32.97130
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 833.07778, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 14.10918, qf2_loss: 14.12148, policy_loss: -361.61430, policy_entropy: -1.00042, alpha: 0.47785, time: 32.76744
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 846.04891, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 14.63537, qf2_loss: 14.66615, policy_loss: -364.49670, policy_entropy: -1.00756, alpha: 0.47861, time: 32.94304
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 738.60588, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 14.73892, qf2_loss: 14.82830, policy_loss: -364.80995, policy_entropy: -1.01209, alpha: 0.48124, time: 33.21571
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 843.65713, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 17.63404, qf2_loss: 17.67399, policy_loss: -365.89138, policy_entropy: -0.99795, alpha: 0.48394, time: 33.36096
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 846.55185, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 15.68022, qf2_loss: 15.80786, policy_loss: -366.41581, policy_entropy: -1.00247, alpha: 0.48341, time: 32.88375
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 846.29368, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 22.56549, qf2_loss: 22.73955, policy_loss: -366.19931, policy_entropy: -1.00486, alpha: 0.48468, time: 33.35582
[CW] eval: return: 844.10347, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 845.89092, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 20.48782, qf2_loss: 20.56266, policy_loss: -369.87528, policy_entropy: -0.98797, alpha: 0.48317, time: 33.08934
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 578.14191, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 17.25358, qf2_loss: 17.26651, policy_loss: -369.71928, policy_entropy: -0.99509, alpha: 0.48116, time: 32.95335
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 578.96333, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 16.36811, qf2_loss: 16.34719, policy_loss: -369.74309, policy_entropy: -1.01125, alpha: 0.48103, time: 33.16784
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 838.44973, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 16.07329, qf2_loss: 16.09998, policy_loss: -370.43439, policy_entropy: -1.01302, alpha: 0.48576, time: 33.35512
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 838.80729, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 17.58565, qf2_loss: 17.52221, policy_loss: -372.77479, policy_entropy: -0.99815, alpha: 0.48886, time: 32.91907
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 817.52424, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 23.25327, qf2_loss: 23.48219, policy_loss: -373.89136, policy_entropy: -0.99719, alpha: 0.48848, time: 33.17969
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 851.35828, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 17.29526, qf2_loss: 17.28034, policy_loss: -373.20791, policy_entropy: -1.00408, alpha: 0.48766, time: 33.01900
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 738.10764, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 16.35830, qf2_loss: 16.42780, policy_loss: -373.07373, policy_entropy: -1.01467, alpha: 0.49042, time: 33.12082
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 663.25617, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 16.40823, qf2_loss: 16.51101, policy_loss: -375.82679, policy_entropy: -1.01334, alpha: 0.49597, time: 33.48565
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 681.39396, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 16.66441, qf2_loss: 16.68603, policy_loss: -375.25851, policy_entropy: -1.00693, alpha: 0.50005, time: 33.16288
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 831.15860, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 17.57831, qf2_loss: 17.65672, policy_loss: -377.32597, policy_entropy: -1.00370, alpha: 0.50341, time: 32.96986
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 817.24803, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 17.88470, qf2_loss: 17.91253, policy_loss: -376.33913, policy_entropy: -1.00404, alpha: 0.50336, time: 33.39435
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 674.66010, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 21.43444, qf2_loss: 21.72432, policy_loss: -377.30973, policy_entropy: -0.99240, alpha: 0.50386, time: 32.85717
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 762.47033, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 19.97741, qf2_loss: 20.11431, policy_loss: -379.36536, policy_entropy: -0.99048, alpha: 0.49921, time: 33.37807
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 849.70477, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 24.24274, qf2_loss: 24.39393, policy_loss: -380.93558, policy_entropy: -0.99611, alpha: 0.49802, time: 33.14774
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 762.25775, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 21.20734, qf2_loss: 21.28404, policy_loss: -381.14972, policy_entropy: -1.01250, alpha: 0.49806, time: 33.26995
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 847.89288, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 22.00225, qf2_loss: 22.15966, policy_loss: -380.66690, policy_entropy: -1.00356, alpha: 0.50343, time: 33.00015
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 826.60486, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 21.00733, qf2_loss: 21.07646, policy_loss: -384.07628, policy_entropy: -0.99852, alpha: 0.50252, time: 32.89487
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 844.18263, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 18.80585, qf2_loss: 18.76976, policy_loss: -382.00571, policy_entropy: -1.00948, alpha: 0.50402, time: 33.23138
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 843.01565, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 18.11449, qf2_loss: 18.42694, policy_loss: -383.54715, policy_entropy: -1.00953, alpha: 0.50783, time: 32.97432
[CW] eval: return: 830.09442, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 839.28214, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 20.49356, qf2_loss: 20.56834, policy_loss: -384.35773, policy_entropy: -1.01070, alpha: 0.51228, time: 33.38398
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 844.59228, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 22.96510, qf2_loss: 23.20077, policy_loss: -385.05566, policy_entropy: -1.01260, alpha: 0.51587, time: 33.17090
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 842.03801, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 26.18418, qf2_loss: 26.20034, policy_loss: -387.69748, policy_entropy: -0.99207, alpha: 0.51733, time: 33.25629
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 626.91924, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 23.40100, qf2_loss: 23.53930, policy_loss: -386.84107, policy_entropy: -1.00601, alpha: 0.51724, time: 33.09571
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 817.67714, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 21.42969, qf2_loss: 21.54801, policy_loss: -386.81454, policy_entropy: -1.00694, alpha: 0.52018, time: 33.18944
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 838.31355, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 19.30298, qf2_loss: 19.69831, policy_loss: -390.97911, policy_entropy: -0.99508, alpha: 0.52130, time: 33.32756
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 850.00768, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 23.57680, qf2_loss: 23.67384, policy_loss: -390.61117, policy_entropy: -0.99515, alpha: 0.51892, time: 33.14430
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 846.21124, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 25.36981, qf2_loss: 25.58224, policy_loss: -391.83034, policy_entropy: -0.99633, alpha: 0.51696, time: 33.32054
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 845.09290, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 20.20962, qf2_loss: 20.12098, policy_loss: -391.37045, policy_entropy: -1.00917, alpha: 0.51825, time: 33.29410
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 844.46707, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 19.00051, qf2_loss: 19.19563, policy_loss: -394.01691, policy_entropy: -1.01516, alpha: 0.52346, time: 33.05826
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 827.14960, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 20.63881, qf2_loss: 20.58450, policy_loss: -394.02922, policy_entropy: -1.00897, alpha: 0.52856, time: 33.55033
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 837.03100, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 21.14960, qf2_loss: 21.33104, policy_loss: -394.26239, policy_entropy: -1.00172, alpha: 0.53033, time: 33.44115
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 831.51653, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 20.50765, qf2_loss: 20.67846, policy_loss: -395.48137, policy_entropy: -1.00748, alpha: 0.53230, time: 33.49619
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 844.26700, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 24.50557, qf2_loss: 24.66023, policy_loss: -395.91319, policy_entropy: -1.00473, alpha: 0.53540, time: 33.22525
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 833.18152, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 25.94402, qf2_loss: 25.77818, policy_loss: -395.49389, policy_entropy: -0.99918, alpha: 0.53521, time: 32.72947
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 674.26731, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 46.65245, qf2_loss: 47.02147, policy_loss: -399.32089, policy_entropy: -0.97958, alpha: 0.53122, time: 33.43944
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 675.74705, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 28.71432, qf2_loss: 28.70296, policy_loss: -398.31859, policy_entropy: -0.99757, alpha: 0.52566, time: 33.01788
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 843.98679, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 19.85643, qf2_loss: 19.90087, policy_loss: -400.54575, policy_entropy: -1.00924, alpha: 0.52788, time: 32.98726
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 847.53501, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 19.79235, qf2_loss: 19.94005, policy_loss: -400.45537, policy_entropy: -1.00962, alpha: 0.53124, time: 33.50191
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 845.51678, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 21.83193, qf2_loss: 22.04394, policy_loss: -401.53175, policy_entropy: -1.00795, alpha: 0.53592, time: 33.59240
[CW] eval: return: 821.45130, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 758.65378, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 23.16686, qf2_loss: 23.34430, policy_loss: -403.03789, policy_entropy: -1.00890, alpha: 0.53892, time: 33.59978
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 840.64703, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 23.44261, qf2_loss: 23.53472, policy_loss: -403.74800, policy_entropy: -0.99640, alpha: 0.54048, time: 33.17398
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 846.17171, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 24.44707, qf2_loss: 24.68268, policy_loss: -405.67399, policy_entropy: -1.00491, alpha: 0.54010, time: 33.45761
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 829.81860, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 22.43731, qf2_loss: 22.63082, policy_loss: -406.65832, policy_entropy: -1.00185, alpha: 0.54084, time: 33.60850
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 844.20034, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 21.90989, qf2_loss: 22.20414, policy_loss: -405.92036, policy_entropy: -1.00909, alpha: 0.54483, time: 33.06569
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 835.35692, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 24.00894, qf2_loss: 24.11394, policy_loss: -407.17896, policy_entropy: -1.00944, alpha: 0.54800, time: 32.99397
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 829.15990, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 27.26031, qf2_loss: 27.61218, policy_loss: -408.45925, policy_entropy: -1.00393, alpha: 0.54966, time: 33.28822
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 843.13261, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 23.83756, qf2_loss: 23.89797, policy_loss: -407.08057, policy_entropy: -0.99943, alpha: 0.55083, time: 33.41342
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 843.57158, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 25.06466, qf2_loss: 25.54959, policy_loss: -409.53713, policy_entropy: -1.00367, alpha: 0.55139, time: 33.32068
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 842.42506, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 27.76594, qf2_loss: 27.55277, policy_loss: -409.94843, policy_entropy: -0.99665, alpha: 0.55269, time: 33.19970
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 841.27299, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 29.88637, qf2_loss: 30.14025, policy_loss: -410.14768, policy_entropy: -0.99962, alpha: 0.55203, time: 32.89143
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 836.49686, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 23.81731, qf2_loss: 23.86043, policy_loss: -413.09856, policy_entropy: -0.99524, alpha: 0.55290, time: 33.23250
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 842.46293, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 28.79827, qf2_loss: 29.12020, policy_loss: -414.28868, policy_entropy: -1.00935, alpha: 0.55170, time: 33.16252
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 841.92521, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 24.14834, qf2_loss: 24.25042, policy_loss: -413.29616, policy_entropy: -0.99825, alpha: 0.55277, time: 33.31257
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 847.93024, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 26.17553, qf2_loss: 26.45510, policy_loss: -416.21215, policy_entropy: -1.00564, alpha: 0.55404, time: 32.78647
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 840.79590, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 25.96618, qf2_loss: 26.03819, policy_loss: -415.69743, policy_entropy: -1.01379, alpha: 0.55760, time: 33.43164
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 845.44268, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 25.07001, qf2_loss: 25.13040, policy_loss: -417.63854, policy_entropy: -0.99509, alpha: 0.55959, time: 33.23151
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 834.95517, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 31.66095, qf2_loss: 32.02818, policy_loss: -416.18422, policy_entropy: -0.99554, alpha: 0.55937, time: 33.00590
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 838.37574, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 25.95716, qf2_loss: 25.71190, policy_loss: -419.07004, policy_entropy: -0.99247, alpha: 0.55554, time: 33.08053
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 839.54967, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 25.41635, qf2_loss: 25.73017, policy_loss: -418.59852, policy_entropy: -1.01258, alpha: 0.55669, time: 32.86934
[CW] eval: return: 842.58352, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 843.00652, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 25.25967, qf2_loss: 25.38730, policy_loss: -418.85515, policy_entropy: -1.00275, alpha: 0.55981, time: 33.52851
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 839.29579, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 31.59824, qf2_loss: 31.55515, policy_loss: -421.64444, policy_entropy: -1.00233, alpha: 0.56174, time: 33.37406
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 840.02939, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 28.95567, qf2_loss: 29.26857, policy_loss: -420.76643, policy_entropy: -0.99149, alpha: 0.56117, time: 33.55748
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 847.66200, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 27.73027, qf2_loss: 27.90222, policy_loss: -424.73451, policy_entropy: -1.00329, alpha: 0.55889, time: 33.53786
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 819.28501, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 30.63053, qf2_loss: 30.71461, policy_loss: -424.68728, policy_entropy: -0.99479, alpha: 0.55886, time: 32.97704
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 845.70734, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 29.50591, qf2_loss: 30.19027, policy_loss: -422.72039, policy_entropy: -1.00370, alpha: 0.55788, time: 33.15983
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 821.94690, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 27.32445, qf2_loss: 27.44695, policy_loss: -425.71067, policy_entropy: -1.00929, alpha: 0.56002, time: 33.63886
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 839.26701, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 27.44023, qf2_loss: 27.65583, policy_loss: -425.66274, policy_entropy: -0.98912, alpha: 0.55956, time: 33.62042
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 826.00133, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 25.99231, qf2_loss: 25.88364, policy_loss: -428.78485, policy_entropy: -1.00683, alpha: 0.55969, time: 33.56614
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 839.58301, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 28.01406, qf2_loss: 28.04621, policy_loss: -428.61899, policy_entropy: -1.00798, alpha: 0.56192, time: 33.07199
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 828.38398, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 27.94541, qf2_loss: 28.06364, policy_loss: -430.53469, policy_entropy: -1.00788, alpha: 0.56556, time: 33.01969
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 835.54628, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 26.46914, qf2_loss: 26.92512, policy_loss: -433.05267, policy_entropy: -0.99237, alpha: 0.56541, time: 33.00016
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 839.58002, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 27.60020, qf2_loss: 27.23200, policy_loss: -430.44465, policy_entropy: -1.00025, alpha: 0.56640, time: 33.44350
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 832.83728, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 31.89934, qf2_loss: 32.27900, policy_loss: -433.08039, policy_entropy: -0.97645, alpha: 0.56022, time: 32.92370
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 657.27417, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 32.13915, qf2_loss: 32.16651, policy_loss: -432.25882, policy_entropy: -1.01491, alpha: 0.55816, time: 33.46000
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 831.92327, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 39.90428, qf2_loss: 39.76555, policy_loss: -432.79609, policy_entropy: -0.99813, alpha: 0.56049, time: 33.49710
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 834.57169, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 32.53287, qf2_loss: 32.74759, policy_loss: -432.58326, policy_entropy: -1.00303, alpha: 0.56056, time: 33.48184
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 449.48551, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 34.52724, qf2_loss: 34.72647, policy_loss: -435.09131, policy_entropy: -1.00181, alpha: 0.56154, time: 33.96409
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 815.30420, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 26.69695, qf2_loss: 26.74725, policy_loss: -433.85504, policy_entropy: -1.00150, alpha: 0.56266, time: 40.83712
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 839.92364, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 26.22170, qf2_loss: 26.32794, policy_loss: -435.57580, policy_entropy: -1.00755, alpha: 0.56412, time: 33.35236
[CW] eval: return: 840.42357, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 837.86472, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 27.45308, qf2_loss: 27.45368, policy_loss: -438.48225, policy_entropy: -0.99574, alpha: 0.56512, time: 34.59963
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 844.14672, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 28.69566, qf2_loss: 28.80123, policy_loss: -436.16982, policy_entropy: -1.00711, alpha: 0.56462, time: 33.00146
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 841.82906, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 26.42105, qf2_loss: 26.59069, policy_loss: -439.79072, policy_entropy: -1.00270, alpha: 0.56652, time: 33.08915
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 826.64127, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 27.40056, qf2_loss: 27.45594, policy_loss: -440.08229, policy_entropy: -1.00315, alpha: 0.56867, time: 33.42210
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 496.26275, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 37.66665, qf2_loss: 37.47470, policy_loss: -440.69118, policy_entropy: -0.99277, alpha: 0.56770, time: 33.54638
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 559.14492, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 38.97183, qf2_loss: 38.94085, policy_loss: -440.52839, policy_entropy: -0.99207, alpha: 0.56417, time: 33.40850
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 833.15621, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 33.25241, qf2_loss: 33.63772, policy_loss: -441.71315, policy_entropy: -1.00114, alpha: 0.56294, time: 33.45216
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 828.57510, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 41.87821, qf2_loss: 41.95776, policy_loss: -442.46122, policy_entropy: -0.98556, alpha: 0.56304, time: 33.31237
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 830.57280, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 27.79630, qf2_loss: 27.62859, policy_loss: -445.22591, policy_entropy: -0.98750, alpha: 0.55711, time: 33.00597
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 841.57439, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 26.59824, qf2_loss: 26.53863, policy_loss: -444.66342, policy_entropy: -1.00609, alpha: 0.55417, time: 33.44422
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 823.28921, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 27.17358, qf2_loss: 27.05416, policy_loss: -447.73364, policy_entropy: -1.01272, alpha: 0.55772, time: 33.31684
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 843.62526, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 27.77781, qf2_loss: 27.88606, policy_loss: -445.24835, policy_entropy: -1.01634, alpha: 0.56412, time: 33.10653
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 720.12743, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 31.58787, qf2_loss: 32.13034, policy_loss: -449.36908, policy_entropy: -1.00217, alpha: 0.56638, time: 33.50695
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 823.84645, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 39.41068, qf2_loss: 39.20944, policy_loss: -448.21138, policy_entropy: -1.00106, alpha: 0.56872, time: 32.97536
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 759.60642, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 41.26396, qf2_loss: 42.01328, policy_loss: -447.95468, policy_entropy: -1.00415, alpha: 0.56789, time: 33.27228
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 816.84290, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 38.14251, qf2_loss: 38.08108, policy_loss: -451.22184, policy_entropy: -0.98860, alpha: 0.56770, time: 33.27620
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 750.42591, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 34.14039, qf2_loss: 34.39692, policy_loss: -451.18587, policy_entropy: -0.99680, alpha: 0.56556, time: 33.12695
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 827.41773, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 32.55967, qf2_loss: 32.21738, policy_loss: -452.45634, policy_entropy: -0.99665, alpha: 0.56362, time: 33.23390
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 822.91421, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 27.91195, qf2_loss: 28.05685, policy_loss: -450.96749, policy_entropy: -1.00936, alpha: 0.56261, time: 33.01330
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 818.89238, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 28.94054, qf2_loss: 29.05529, policy_loss: -455.34795, policy_entropy: -1.00341, alpha: 0.56623, time: 33.19715
[CW] eval: return: 831.50130, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 835.64096, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 29.35709, qf2_loss: 29.44352, policy_loss: -452.48993, policy_entropy: -0.99552, alpha: 0.56699, time: 33.19942
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 843.88408, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 31.08533, qf2_loss: 31.20485, policy_loss: -455.19972, policy_entropy: -1.00677, alpha: 0.56696, time: 33.52086
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 832.66119, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 30.91925, qf2_loss: 31.12775, policy_loss: -453.24907, policy_entropy: -1.00085, alpha: 0.56796, time: 32.78975
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 842.73112, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 35.27026, qf2_loss: 35.43674, policy_loss: -455.61312, policy_entropy: -1.00188, alpha: 0.56889, time: 33.29927
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 686.38429, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 35.64548, qf2_loss: 36.04085, policy_loss: -457.00408, policy_entropy: -0.98632, alpha: 0.56636, time: 32.99824
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 821.72756, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 32.59830, qf2_loss: 33.45727, policy_loss: -456.66280, policy_entropy: -1.00408, alpha: 0.56557, time: 33.06966
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 816.30677, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 38.49641, qf2_loss: 38.67441, policy_loss: -458.49718, policy_entropy: -1.00381, alpha: 0.56565, time: 33.42374
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 824.76620, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 32.95193, qf2_loss: 32.88508, policy_loss: -459.85003, policy_entropy: -0.99015, alpha: 0.56440, time: 33.22708
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 824.77617, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 31.70693, qf2_loss: 31.82107, policy_loss: -461.43609, policy_entropy: -0.99512, alpha: 0.56280, time: 33.38157
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 812.47005, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 30.17495, qf2_loss: 30.09840, policy_loss: -461.41508, policy_entropy: -1.01432, alpha: 0.56452, time: 33.34386
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 577.42130, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 30.81801, qf2_loss: 30.79895, policy_loss: -460.52434, policy_entropy: -1.01573, alpha: 0.56874, time: 33.28912
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 839.18659, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 31.20306, qf2_loss: 31.55321, policy_loss: -461.68490, policy_entropy: -1.00400, alpha: 0.57308, time: 32.92495
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 824.66483, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 34.23234, qf2_loss: 34.84930, policy_loss: -465.10625, policy_entropy: -1.00317, alpha: 0.57361, time: 33.37878
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 831.84392, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 33.82649, qf2_loss: 33.77159, policy_loss: -462.63416, policy_entropy: -1.00789, alpha: 0.57445, time: 33.47399
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 831.03371, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 32.58548, qf2_loss: 32.41947, policy_loss: -465.67684, policy_entropy: -0.99509, alpha: 0.57573, time: 33.45211
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 834.26063, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 31.27777, qf2_loss: 31.88336, policy_loss: -467.46558, policy_entropy: -0.98820, alpha: 0.57422, time: 33.34262
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 828.61680, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 32.55329, qf2_loss: 32.67774, policy_loss: -466.72844, policy_entropy: -1.00014, alpha: 0.57163, time: 33.16766
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 840.80516, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 37.16660, qf2_loss: 37.42745, policy_loss: -467.24693, policy_entropy: -1.00464, alpha: 0.57241, time: 33.36025
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 842.50645, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 36.07360, qf2_loss: 35.72082, policy_loss: -469.42107, policy_entropy: -0.98796, alpha: 0.57247, time: 33.49953
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 834.94461, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 40.61217, qf2_loss: 40.78607, policy_loss: -470.57135, policy_entropy: -1.00925, alpha: 0.57003, time: 33.36974
[CW] eval: return: 835.56956, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 836.49545, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 50.21134, qf2_loss: 50.63499, policy_loss: -469.14417, policy_entropy: -0.99984, alpha: 0.57407, time: 33.28838
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 826.98917, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 37.41507, qf2_loss: 37.47903, policy_loss: -471.15889, policy_entropy: -0.98196, alpha: 0.56966, time: 33.01448
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 575.23155, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 32.12309, qf2_loss: 32.20128, policy_loss: -472.14953, policy_entropy: -0.99640, alpha: 0.56520, time: 32.84003
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 843.47480, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 29.61028, qf2_loss: 29.89747, policy_loss: -472.81774, policy_entropy: -0.98687, alpha: 0.56283, time: 33.15090
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 820.19327, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 30.51147, qf2_loss: 30.80959, policy_loss: -474.66633, policy_entropy: -0.99657, alpha: 0.56025, time: 32.97564
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 810.71607, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 32.66436, qf2_loss: 33.04937, policy_loss: -474.42642, policy_entropy: -1.01390, alpha: 0.56144, time: 33.36337
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 837.78300, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 35.44138, qf2_loss: 35.77344, policy_loss: -476.50339, policy_entropy: -1.01574, alpha: 0.56685, time: 33.54342
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 838.37567, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 43.87896, qf2_loss: 44.31859, policy_loss: -476.68430, policy_entropy: -0.99650, alpha: 0.56807, time: 33.37926
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 835.18763, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 40.12949, qf2_loss: 40.55267, policy_loss: -476.63152, policy_entropy: -0.99763, alpha: 0.56850, time: 33.24163
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 835.11581, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 33.12689, qf2_loss: 33.49678, policy_loss: -478.62021, policy_entropy: -0.98773, alpha: 0.56465, time: 32.67374
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 835.70494, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 33.67678, qf2_loss: 34.15901, policy_loss: -477.82086, policy_entropy: -0.99791, alpha: 0.56357, time: 32.99997
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 833.87425, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 34.88394, qf2_loss: 34.84758, policy_loss: -476.52845, policy_entropy: -0.99955, alpha: 0.56282, time: 33.28543
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 836.53757, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 32.97390, qf2_loss: 32.86453, policy_loss: -477.45579, policy_entropy: -0.99655, alpha: 0.56150, time: 33.45980
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 835.77455, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 50.63969, qf2_loss: 50.77913, policy_loss: -479.67017, policy_entropy: -0.99647, alpha: 0.56119, time: 33.14083
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 839.59780, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 40.23353, qf2_loss: 40.74977, policy_loss: -480.38628, policy_entropy: -0.99456, alpha: 0.55990, time: 32.79817
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 742.88394, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 34.43616, qf2_loss: 34.27123, policy_loss: -480.76346, policy_entropy: -0.98828, alpha: 0.55811, time: 33.28165
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 837.28340, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 31.96869, qf2_loss: 32.37023, policy_loss: -482.23998, policy_entropy: -0.99716, alpha: 0.55511, time: 32.48574
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 839.13052, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 32.93469, qf2_loss: 33.16974, policy_loss: -481.24654, policy_entropy: -0.99329, alpha: 0.55339, time: 33.15075
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 812.68005, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 36.22941, qf2_loss: 36.63963, policy_loss: -484.53112, policy_entropy: -0.99620, alpha: 0.55174, time: 33.14615
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 840.00868, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 34.45029, qf2_loss: 34.23051, policy_loss: -483.36050, policy_entropy: -1.00145, alpha: 0.55245, time: 33.28562
[CW] eval: return: 837.34775, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 837.17304, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 33.72469, qf2_loss: 33.98706, policy_loss: -488.89179, policy_entropy: -0.99232, alpha: 0.55007, time: 33.42414
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 837.56697, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 34.76561, qf2_loss: 34.81289, policy_loss: -484.86232, policy_entropy: -1.00807, alpha: 0.55024, time: 33.44951
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 823.27799, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 34.52430, qf2_loss: 35.13154, policy_loss: -488.16821, policy_entropy: -1.00550, alpha: 0.55366, time: 33.52772
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 842.43620, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 33.31217, qf2_loss: 33.65795, policy_loss: -485.39874, policy_entropy: -1.00348, alpha: 0.55433, time: 33.41494
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 833.12229, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 32.08774, qf2_loss: 32.12089, policy_loss: -488.77568, policy_entropy: -0.99859, alpha: 0.55506, time: 33.37765
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 839.25303, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 36.94359, qf2_loss: 37.07521, policy_loss: -490.88070, policy_entropy: -0.99680, alpha: 0.55441, time: 32.77043
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 837.41644, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 39.58241, qf2_loss: 39.67330, policy_loss: -488.12852, policy_entropy: -1.00362, alpha: 0.55320, time: 33.00192
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 821.51697, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 34.70957, qf2_loss: 34.61791, policy_loss: -492.23366, policy_entropy: -0.98424, alpha: 0.55243, time: 32.71331
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 820.55162, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 34.60011, qf2_loss: 34.69152, policy_loss: -495.34799, policy_entropy: -0.98458, alpha: 0.54740, time: 33.17293
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 821.15921, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 34.78891, qf2_loss: 34.94937, policy_loss: -494.54160, policy_entropy: -0.99621, alpha: 0.54362, time: 36.40884
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 824.47960, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 37.00593, qf2_loss: 37.42971, policy_loss: -495.01885, policy_entropy: -0.98555, alpha: 0.54179, time: 32.92055
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 837.26694, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 35.48163, qf2_loss: 35.33121, policy_loss: -495.14438, policy_entropy: -1.00467, alpha: 0.54063, time: 33.31013
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 837.98194, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 34.46578, qf2_loss: 34.81657, policy_loss: -493.90488, policy_entropy: -1.00529, alpha: 0.54223, time: 33.46202
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 828.74788, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 37.19911, qf2_loss: 37.58074, policy_loss: -495.74802, policy_entropy: -0.99917, alpha: 0.54211, time: 33.32518
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 842.20299, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 33.35861, qf2_loss: 33.48102, policy_loss: -498.52529, policy_entropy: -1.00120, alpha: 0.54261, time: 32.82773
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 839.89305, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 34.84374, qf2_loss: 34.90094, policy_loss: -496.99808, policy_entropy: -1.00249, alpha: 0.54347, time: 32.71299
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 839.13506, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 35.27386, qf2_loss: 35.54034, policy_loss: -498.90560, policy_entropy: -0.99648, alpha: 0.54287, time: 33.03834
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 822.60590, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 36.98952, qf2_loss: 37.19644, policy_loss: -500.15897, policy_entropy: -0.98927, alpha: 0.54169, time: 33.14899
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 838.63453, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 36.33683, qf2_loss: 36.43576, policy_loss: -499.73274, policy_entropy: -0.99229, alpha: 0.53920, time: 33.28579
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 817.28197, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 36.59017, qf2_loss: 36.70573, policy_loss: -501.69196, policy_entropy: -0.99457, alpha: 0.53683, time: 33.42410
[CW] eval: return: 833.20102, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 835.84029, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 41.80808, qf2_loss: 41.73270, policy_loss: -502.25049, policy_entropy: -0.99268, alpha: 0.53577, time: 33.06423
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 835.43774, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 38.69066, qf2_loss: 38.78021, policy_loss: -499.58449, policy_entropy: -1.00793, alpha: 0.53460, time: 33.06289
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 834.00319, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 35.08238, qf2_loss: 35.29767, policy_loss: -502.02683, policy_entropy: -1.00133, alpha: 0.53630, time: 33.13718
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 834.52321, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 35.17697, qf2_loss: 35.66085, policy_loss: -502.91491, policy_entropy: -0.98770, alpha: 0.53605, time: 33.36307
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 837.13159, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 34.42902, qf2_loss: 34.56831, policy_loss: -501.78618, policy_entropy: -1.00750, alpha: 0.53397, time: 33.38512
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 819.93727, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 45.87788, qf2_loss: 46.05285, policy_loss: -503.65011, policy_entropy: -1.00975, alpha: 0.53662, time: 33.12122
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 485.89439, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 40.38474, qf2_loss: 40.89224, policy_loss: -503.79479, policy_entropy: -0.98804, alpha: 0.53542, time: 32.81403
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 832.88452, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 37.47170, qf2_loss: 37.39047, policy_loss: -505.50555, policy_entropy: -0.99275, alpha: 0.53414, time: 33.30064
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 836.66733, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 36.80873, qf2_loss: 36.54341, policy_loss: -505.91654, policy_entropy: -0.98939, alpha: 0.53150, time: 32.76677
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 807.71769, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 44.49398, qf2_loss: 45.39364, policy_loss: -508.52805, policy_entropy: -0.99476, alpha: 0.52938, time: 33.22923
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 590.08309, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 47.42244, qf2_loss: 47.16507, policy_loss: -506.31289, policy_entropy: -0.99002, alpha: 0.52817, time: 32.90394
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 818.91819, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 36.44613, qf2_loss: 36.94598, policy_loss: -509.11532, policy_entropy: -0.99860, alpha: 0.52630, time: 33.29931
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 832.12382, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 35.39177, qf2_loss: 35.47826, policy_loss: -510.92766, policy_entropy: -0.99876, alpha: 0.52669, time: 33.47188
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 824.50193, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 34.14927, qf2_loss: 34.45756, policy_loss: -511.78710, policy_entropy: -1.00450, alpha: 0.52618, time: 33.09205
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 835.96300, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 31.56717, qf2_loss: 31.82500, policy_loss: -511.84992, policy_entropy: -0.99211, alpha: 0.52597, time: 33.10981
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 838.74722, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 36.64361, qf2_loss: 36.80571, policy_loss: -515.16030, policy_entropy: -0.98731, alpha: 0.52330, time: 32.86015
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 821.71724, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 39.83614, qf2_loss: 39.85019, policy_loss: -512.08839, policy_entropy: -1.00216, alpha: 0.52152, time: 32.94959
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 825.47706, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 43.98088, qf2_loss: 44.73516, policy_loss: -512.20389, policy_entropy: -0.99996, alpha: 0.52268, time: 33.13218
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 835.73885, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 35.50072, qf2_loss: 35.75922, policy_loss: -515.00111, policy_entropy: -0.99237, alpha: 0.52190, time: 33.15280
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 833.54193, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 34.30208, qf2_loss: 34.49404, policy_loss: -516.09871, policy_entropy: -1.00228, alpha: 0.52004, time: 33.27881
[CW] eval: return: 759.76708, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 541.14080, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 38.86703, qf2_loss: 38.94973, policy_loss: -517.84462, policy_entropy: -1.00112, alpha: 0.52058, time: 32.97360
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 728.27189, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 38.27071, qf2_loss: 38.52475, policy_loss: -516.30534, policy_entropy: -1.01214, alpha: 0.52225, time: 33.40941
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 831.39664, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 37.99795, qf2_loss: 37.67464, policy_loss: -515.75694, policy_entropy: -0.98915, alpha: 0.52201, time: 33.37335
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 830.14100, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 49.64231, qf2_loss: 49.95244, policy_loss: -517.43373, policy_entropy: -1.01703, alpha: 0.52277, time: 33.03343
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 546.33262, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 43.92514, qf2_loss: 44.84217, policy_loss: -517.27965, policy_entropy: -1.00774, alpha: 0.52662, time: 32.97642
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 838.32863, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 36.75187, qf2_loss: 37.05536, policy_loss: -520.95545, policy_entropy: -0.98864, alpha: 0.52651, time: 32.91963
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 836.74948, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 34.45753, qf2_loss: 34.59480, policy_loss: -518.79223, policy_entropy: -0.99761, alpha: 0.52386, time: 33.09415
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 836.96375, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 38.81158, qf2_loss: 39.02058, policy_loss: -521.65657, policy_entropy: -1.00911, alpha: 0.52438, time: 33.09116
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 825.99092, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 34.05715, qf2_loss: 34.13038, policy_loss: -518.99692, policy_entropy: -0.99729, alpha: 0.52476, time: 33.27702
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 836.57349, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 39.48408, qf2_loss: 39.47023, policy_loss: -521.64564, policy_entropy: -1.00407, alpha: 0.52637, time: 33.02644
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 823.74525, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 48.87024, qf2_loss: 48.65582, policy_loss: -521.58469, policy_entropy: -0.99143, alpha: 0.52547, time: 35.89385
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 720.23755, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 42.05345, qf2_loss: 42.04870, policy_loss: -523.02017, policy_entropy: -0.99527, alpha: 0.52358, time: 33.40682
[CW] ---------------------------
[CW] ---- Iteration:   673 ----
[CW] collect: return: 838.96375, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 37.78587, qf2_loss: 38.19049, policy_loss: -523.25167, policy_entropy: -0.97451, alpha: 0.52123, time: 33.13114
[CW] ---------------------------
[CW] ---- Iteration:   674 ----
[CW] collect: return: 838.36571, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 35.77945, qf2_loss: 35.91390, policy_loss: -522.46223, policy_entropy: -0.99859, alpha: 0.51706, time: 33.59808
[CW] ---------------------------
[CW] ---- Iteration:   675 ----
[CW] collect: return: 834.12094, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 34.01067, qf2_loss: 34.05004, policy_loss: -522.42526, policy_entropy: -1.01100, alpha: 0.51767, time: 32.94842
[CW] ---------------------------
[CW] ---- Iteration:   676 ----
[CW] collect: return: 840.63427, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 37.13402, qf2_loss: 37.36936, policy_loss: -525.76445, policy_entropy: -1.00717, alpha: 0.52007, time: 33.19953
[CW] ---------------------------
[CW] ---- Iteration:   677 ----
[CW] collect: return: 671.75672, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 36.35665, qf2_loss: 36.77687, policy_loss: -525.75828, policy_entropy: -0.98453, alpha: 0.51951, time: 33.08132
[CW] ---------------------------
[CW] ---- Iteration:   678 ----
[CW] collect: return: 826.98717, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 33.67465, qf2_loss: 33.75709, policy_loss: -525.29088, policy_entropy: -1.00448, alpha: 0.51868, time: 32.95585
[CW] ---------------------------
[CW] ---- Iteration:   679 ----
[CW] collect: return: 834.57059, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 36.68874, qf2_loss: 37.09884, policy_loss: -526.50747, policy_entropy: -1.00478, alpha: 0.51861, time: 33.27234
[CW] ---------------------------
[CW] ---- Iteration:   680 ----
[CW] collect: return: 759.80242, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 38.11215, qf2_loss: 38.24553, policy_loss: -525.94108, policy_entropy: -1.01213, alpha: 0.52058, time: 33.40684
[CW] eval: return: 825.50870, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   681 ----
[CW] collect: return: 837.54432, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 39.55277, qf2_loss: 39.24863, policy_loss: -528.55151, policy_entropy: -1.00123, alpha: 0.52114, time: 33.30562
[CW] ---------------------------
[CW] ---- Iteration:   682 ----
[CW] collect: return: 821.90206, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 35.21479, qf2_loss: 35.67457, policy_loss: -528.74183, policy_entropy: -1.01316, alpha: 0.52432, time: 32.94309
[CW] ---------------------------
[CW] ---- Iteration:   683 ----
[CW] collect: return: 836.45787, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 42.34747, qf2_loss: 42.42760, policy_loss: -528.68016, policy_entropy: -0.99771, alpha: 0.52564, time: 32.95725
[CW] ---------------------------
[CW] ---- Iteration:   684 ----
[CW] collect: return: 707.50198, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 45.52026, qf2_loss: 45.70912, policy_loss: -530.06520, policy_entropy: -0.98738, alpha: 0.52324, time: 32.89782
[CW] ---------------------------
[CW] ---- Iteration:   685 ----
[CW] collect: return: 841.67088, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 43.01089, qf2_loss: 42.74523, policy_loss: -529.55233, policy_entropy: -0.99848, alpha: 0.52252, time: 33.30390
[CW] ---------------------------
[CW] ---- Iteration:   686 ----
[CW] collect: return: 839.67147, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 36.90510, qf2_loss: 37.00535, policy_loss: -528.93723, policy_entropy: -0.98931, alpha: 0.52112, time: 33.51598
[CW] ---------------------------
[CW] ---- Iteration:   687 ----
[CW] collect: return: 832.86756, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 36.46283, qf2_loss: 36.50518, policy_loss: -531.42683, policy_entropy: -0.98994, alpha: 0.51839, time: 33.20767
[CW] ---------------------------
[CW] ---- Iteration:   688 ----
[CW] collect: return: 833.68738, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 40.30959, qf2_loss: 40.71654, policy_loss: -530.99081, policy_entropy: -0.99977, alpha: 0.51778, time: 32.97284
[CW] ---------------------------
[CW] ---- Iteration:   689 ----
[CW] collect: return: 824.71625, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 40.39903, qf2_loss: 40.29417, policy_loss: -532.30418, policy_entropy: -0.99860, alpha: 0.51637, time: 33.04885
[CW] ---------------------------
[CW] ---- Iteration:   690 ----
[CW] collect: return: 575.06278, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 44.37533, qf2_loss: 44.54655, policy_loss: -534.85547, policy_entropy: -0.98927, alpha: 0.51462, time: 32.91752
[CW] ---------------------------
[CW] ---- Iteration:   691 ----
[CW] collect: return: 836.48722, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 39.46632, qf2_loss: 39.75890, policy_loss: -538.35776, policy_entropy: -0.99579, alpha: 0.51322, time: 32.99173
[CW] ---------------------------
[CW] ---- Iteration:   692 ----
[CW] collect: return: 831.35014, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 40.83273, qf2_loss: 41.23314, policy_loss: -534.47596, policy_entropy: -1.01104, alpha: 0.51287, time: 33.39865
[CW] ---------------------------
[CW] ---- Iteration:   693 ----
[CW] collect: return: 798.26202, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 33.21866, qf2_loss: 33.53573, policy_loss: -535.57060, policy_entropy: -1.00213, alpha: 0.51661, time: 32.59383
[CW] ---------------------------
[CW] ---- Iteration:   694 ----
[CW] collect: return: 827.86724, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 37.31761, qf2_loss: 37.65116, policy_loss: -534.94333, policy_entropy: -1.01185, alpha: 0.51642, time: 32.95734
[CW] ---------------------------
[CW] ---- Iteration:   695 ----
[CW] collect: return: 824.84661, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 54.07228, qf2_loss: 54.47554, policy_loss: -536.76002, policy_entropy: -1.00150, alpha: 0.51855, time: 33.08035
[CW] ---------------------------
[CW] ---- Iteration:   696 ----
[CW] collect: return: 678.69866, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 41.44980, qf2_loss: 42.17396, policy_loss: -537.98590, policy_entropy: -0.99696, alpha: 0.51867, time: 33.27913
[CW] ---------------------------
[CW] ---- Iteration:   697 ----
[CW] collect: return: 830.07832, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 37.16551, qf2_loss: 37.16561, policy_loss: -536.70234, policy_entropy: -1.00261, alpha: 0.51858, time: 33.24123
[CW] ---------------------------
[CW] ---- Iteration:   698 ----
[CW] collect: return: 809.01317, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 36.55591, qf2_loss: 36.56695, policy_loss: -536.67181, policy_entropy: -1.00729, alpha: 0.52022, time: 32.83889
[CW] ---------------------------
[CW] ---- Iteration:   699 ----
[CW] collect: return: 523.27246, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 35.52464, qf2_loss: 35.55755, policy_loss: -540.45327, policy_entropy: -0.98640, alpha: 0.51843, time: 32.83992
[CW] ---------------------------
[CW] ---- Iteration:   700 ----
[CW] collect: return: 842.60907, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 44.81753, qf2_loss: 44.68339, policy_loss: -540.24684, policy_entropy: -0.99660, alpha: 0.51695, time: 32.78749
[CW] eval: return: 685.06349, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   701 ----
[CW] collect: return: 683.73420, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 42.97102, qf2_loss: 43.54036, policy_loss: -539.01140, policy_entropy: -1.00017, alpha: 0.51611, time: 32.90131
[CW] ---------------------------
[CW] ---- Iteration:   702 ----
[CW] collect: return: 831.05832, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 40.57855, qf2_loss: 40.90033, policy_loss: -543.22268, policy_entropy: -1.00689, alpha: 0.51621, time: 33.20619
[CW] ---------------------------
[CW] ---- Iteration:   703 ----
[CW] collect: return: 544.90676, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 39.42066, qf2_loss: 39.42780, policy_loss: -538.85589, policy_entropy: -0.99768, alpha: 0.51834, time: 32.72533
[CW] ---------------------------
[CW] ---- Iteration:   704 ----
[CW] collect: return: 824.25779, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 37.12845, qf2_loss: 37.38840, policy_loss: -541.18441, policy_entropy: -0.99777, alpha: 0.51778, time: 33.20582
[CW] ---------------------------
[CW] ---- Iteration:   705 ----
[CW] collect: return: 834.90345, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 40.29620, qf2_loss: 40.16495, policy_loss: -543.06537, policy_entropy: -0.98418, alpha: 0.51622, time: 33.01409
[CW] ---------------------------
[CW] ---- Iteration:   706 ----
[CW] collect: return: 695.89230, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 41.76569, qf2_loss: 42.54895, policy_loss: -542.75671, policy_entropy: -0.99923, alpha: 0.51405, time: 33.38418
[CW] ---------------------------
[CW] ---- Iteration:   707 ----
[CW] collect: return: 818.63313, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 36.63354, qf2_loss: 36.98125, policy_loss: -544.19288, policy_entropy: -0.99463, alpha: 0.51368, time: 33.41937
[CW] ---------------------------
[CW] ---- Iteration:   708 ----
[CW] collect: return: 823.12192, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 38.25493, qf2_loss: 38.21647, policy_loss: -542.31430, policy_entropy: -1.01753, alpha: 0.51451, time: 32.93865
[CW] ---------------------------
[CW] ---- Iteration:   709 ----
[CW] collect: return: 833.81756, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 39.35603, qf2_loss: 39.58560, policy_loss: -543.60269, policy_entropy: -0.99922, alpha: 0.51645, time: 33.09393
[CW] ---------------------------
[CW] ---- Iteration:   710 ----
[CW] collect: return: 816.86229, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 42.91649, qf2_loss: 42.72445, policy_loss: -541.24713, policy_entropy: -1.00662, alpha: 0.51650, time: 32.80212
[CW] ---------------------------
[CW] ---- Iteration:   711 ----
[CW] collect: return: 831.60087, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 46.22591, qf2_loss: 46.55218, policy_loss: -545.32541, policy_entropy: -0.99101, alpha: 0.51652, time: 33.09211
[CW] ---------------------------
[CW] ---- Iteration:   712 ----
[CW] collect: return: 800.33538, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 53.23288, qf2_loss: 53.16077, policy_loss: -544.54420, policy_entropy: -0.98904, alpha: 0.51409, time: 32.96606
[CW] ---------------------------
[CW] ---- Iteration:   713 ----
[CW] collect: return: 819.85947, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 39.91752, qf2_loss: 40.28362, policy_loss: -546.23407, policy_entropy: -0.98787, alpha: 0.51231, time: 32.91015
[CW] ---------------------------
[CW] ---- Iteration:   714 ----
[CW] collect: return: 839.35422, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 36.92061, qf2_loss: 36.91113, policy_loss: -545.01761, policy_entropy: -0.99997, alpha: 0.51090, time: 33.34622
[CW] ---------------------------
[CW] ---- Iteration:   715 ----
[CW] collect: return: 826.61339, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 34.84399, qf2_loss: 34.88567, policy_loss: -547.21512, policy_entropy: -0.98273, alpha: 0.50914, time: 33.01389
[CW] ---------------------------
[CW] ---- Iteration:   716 ----
[CW] collect: return: 823.83171, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 36.87668, qf2_loss: 37.33896, policy_loss: -551.05310, policy_entropy: -0.98897, alpha: 0.50588, time: 32.93728
[CW] ---------------------------
[CW] ---- Iteration:   717 ----
[CW] collect: return: 833.67621, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 38.79134, qf2_loss: 38.72815, policy_loss: -550.36289, policy_entropy: -0.98504, alpha: 0.50257, time: 33.12060
[CW] ---------------------------
[CW] ---- Iteration:   718 ----
[CW] collect: return: 836.53448, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 37.00177, qf2_loss: 37.18623, policy_loss: -549.65312, policy_entropy: -1.01036, alpha: 0.50216, time: 32.91608
[CW] ---------------------------
[CW] ---- Iteration:   719 ----
[CW] collect: return: 835.98417, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 40.43921, qf2_loss: 40.47956, policy_loss: -546.70678, policy_entropy: -1.01845, alpha: 0.50466, time: 33.31602
[CW] ---------------------------
[CW] ---- Iteration:   720 ----
[CW] collect: return: 682.94051, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 39.33990, qf2_loss: 39.63705, policy_loss: -550.80047, policy_entropy: -1.00199, alpha: 0.50660, time: 33.26144
[CW] eval: return: 808.45068, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   721 ----
[CW] collect: return: 506.92273, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 41.25409, qf2_loss: 42.14497, policy_loss: -547.98640, policy_entropy: -1.02734, alpha: 0.51117, time: 32.99379
[CW] ---------------------------
[CW] ---- Iteration:   722 ----
[CW] collect: return: 830.57159, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 37.57673, qf2_loss: 37.75149, policy_loss: -550.02065, policy_entropy: -0.98930, alpha: 0.51305, time: 32.64228
[CW] ---------------------------
[CW] ---- Iteration:   723 ----
[CW] collect: return: 837.98652, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 39.29496, qf2_loss: 39.26336, policy_loss: -551.68931, policy_entropy: -0.99151, alpha: 0.51011, time: 33.54274
[CW] ---------------------------
[CW] ---- Iteration:   724 ----
[CW] collect: return: 806.05303, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 38.54452, qf2_loss: 39.21651, policy_loss: -549.66260, policy_entropy: -0.99750, alpha: 0.50912, time: 32.84054
[CW] ---------------------------
[CW] ---- Iteration:   725 ----
[CW] collect: return: 804.38571, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 37.00884, qf2_loss: 37.39281, policy_loss: -552.94287, policy_entropy: -0.97958, alpha: 0.50644, time: 33.32420
[CW] ---------------------------
[CW] ---- Iteration:   726 ----
[CW] collect: return: 833.69402, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 42.56262, qf2_loss: 43.06361, policy_loss: -553.48766, policy_entropy: -0.99431, alpha: 0.50307, time: 32.65575
[CW] ---------------------------
[CW] ---- Iteration:   727 ----
[CW] collect: return: 821.29471, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 45.85074, qf2_loss: 46.33752, policy_loss: -552.67172, policy_entropy: -1.00069, alpha: 0.50334, time: 33.11856
[CW] ---------------------------
[CW] ---- Iteration:   728 ----
[CW] collect: return: 829.94865, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 40.06894, qf2_loss: 40.51712, policy_loss: -554.57703, policy_entropy: -0.97742, alpha: 0.50064, time: 33.27954
[CW] ---------------------------
[CW] ---- Iteration:   729 ----
[CW] collect: return: 834.13764, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 48.36386, qf2_loss: 48.42330, policy_loss: -554.52383, policy_entropy: -0.99702, alpha: 0.49870, time: 33.09777
[CW] ---------------------------
[CW] ---- Iteration:   730 ----
[CW] collect: return: 837.57631, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 42.16059, qf2_loss: 42.74058, policy_loss: -555.70499, policy_entropy: -0.99014, alpha: 0.49618, time: 33.10815
[CW] ---------------------------
[CW] ---- Iteration:   731 ----
[CW] collect: return: 830.88824, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 36.22939, qf2_loss: 36.71273, policy_loss: -559.18383, policy_entropy: -0.98025, alpha: 0.49310, time: 33.12989
[CW] ---------------------------
[CW] ---- Iteration:   732 ----
[CW] collect: return: 835.52958, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 35.18494, qf2_loss: 35.50694, policy_loss: -556.36495, policy_entropy: -0.99660, alpha: 0.49163, time: 32.70563
[CW] ---------------------------
[CW] ---- Iteration:   733 ----
[CW] collect: return: 825.07862, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 47.08931, qf2_loss: 47.12556, policy_loss: -557.03662, policy_entropy: -1.00314, alpha: 0.49012, time: 32.68549
[CW] ---------------------------
[CW] ---- Iteration:   734 ----
[CW] collect: return: 824.38298, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 42.56912, qf2_loss: 43.02902, policy_loss: -552.68570, policy_entropy: -1.01153, alpha: 0.49237, time: 32.84297
[CW] ---------------------------
[CW] ---- Iteration:   735 ----
[CW] collect: return: 821.32820, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 40.48780, qf2_loss: 40.77646, policy_loss: -555.30569, policy_entropy: -1.01683, alpha: 0.49526, time: 33.05868
[CW] ---------------------------
[CW] ---- Iteration:   736 ----
[CW] collect: return: 818.69338, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 41.13108, qf2_loss: 41.94261, policy_loss: -559.22690, policy_entropy: -1.00858, alpha: 0.49738, time: 32.45722
[CW] ---------------------------
[CW] ---- Iteration:   737 ----
[CW] collect: return: 763.16596, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 44.48199, qf2_loss: 44.95189, policy_loss: -559.06410, policy_entropy: -0.99875, alpha: 0.49885, time: 32.74925
[CW] ---------------------------
[CW] ---- Iteration:   738 ----
[CW] collect: return: 812.52116, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 44.08679, qf2_loss: 44.15929, policy_loss: -562.51534, policy_entropy: -0.98367, alpha: 0.49678, time: 33.36275
[CW] ---------------------------
[CW] ---- Iteration:   739 ----
[CW] collect: return: 821.81522, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 45.08373, qf2_loss: 44.89110, policy_loss: -562.73146, policy_entropy: -0.99174, alpha: 0.49452, time: 32.88232
[CW] ---------------------------
[CW] ---- Iteration:   740 ----
[CW] collect: return: 652.16929, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 50.87166, qf2_loss: 51.10260, policy_loss: -559.09905, policy_entropy: -1.02475, alpha: 0.49691, time: 33.16628
[CW] eval: return: 800.13584, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   741 ----
[CW] collect: return: 811.86962, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 40.92862, qf2_loss: 41.20896, policy_loss: -560.58131, policy_entropy: -1.00384, alpha: 0.49879, time: 32.68576
[CW] ---------------------------
[CW] ---- Iteration:   742 ----
[CW] collect: return: 839.44006, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 37.10226, qf2_loss: 37.24876, policy_loss: -560.54986, policy_entropy: -1.00114, alpha: 0.49977, time: 33.81577
[CW] ---------------------------
[CW] ---- Iteration:   743 ----
[CW] collect: return: 827.94718, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 43.78752, qf2_loss: 44.71591, policy_loss: -561.89869, policy_entropy: -0.98333, alpha: 0.49857, time: 32.99933
[CW] ---------------------------
[CW] ---- Iteration:   744 ----
[CW] collect: return: 533.60372, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 44.47369, qf2_loss: 45.11787, policy_loss: -558.73711, policy_entropy: -1.01494, alpha: 0.49715, time: 32.71836
[CW] ---------------------------
[CW] ---- Iteration:   745 ----
[CW] collect: return: 740.79303, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 48.16803, qf2_loss: 47.68401, policy_loss: -562.85119, policy_entropy: -1.00198, alpha: 0.49904, time: 33.29284
[CW] ---------------------------
[CW] ---- Iteration:   746 ----
[CW] collect: return: 827.49757, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 37.10250, qf2_loss: 37.67340, policy_loss: -563.70042, policy_entropy: -0.98431, alpha: 0.49756, time: 33.49051
[CW] ---------------------------
[CW] ---- Iteration:   747 ----
[CW] collect: return: 824.15924, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 39.71422, qf2_loss: 40.12890, policy_loss: -562.13068, policy_entropy: -0.98842, alpha: 0.49558, time: 32.93176
[CW] ---------------------------
[CW] ---- Iteration:   748 ----
[CW] collect: return: 827.84354, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 36.89574, qf2_loss: 37.32158, policy_loss: -563.45413, policy_entropy: -1.00272, alpha: 0.49390, time: 33.49979
[CW] ---------------------------
[CW] ---- Iteration:   749 ----
[CW] collect: return: 829.37619, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 36.27171, qf2_loss: 36.32135, policy_loss: -566.14158, policy_entropy: -0.98819, alpha: 0.49323, time: 33.27535
[CW] ---------------------------
[CW] ---- Iteration:   750 ----
[CW] collect: return: 827.34790, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 38.70880, qf2_loss: 38.81011, policy_loss: -564.12668, policy_entropy: -0.98820, alpha: 0.49164, time: 33.37030
[CW] ---------------------------
[CW] ---- Iteration:   751 ----
[CW] collect: return: 810.74897, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 38.97831, qf2_loss: 39.47307, policy_loss: -565.82833, policy_entropy: -1.00969, alpha: 0.49047, time: 32.84028
[CW] ---------------------------
[CW] ---- Iteration:   752 ----
[CW] collect: return: 835.32622, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 52.74445, qf2_loss: 52.77776, policy_loss: -566.90267, policy_entropy: -1.00177, alpha: 0.49295, time: 33.25513
[CW] ---------------------------
[CW] ---- Iteration:   753 ----
[CW] collect: return: 820.71783, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 44.23275, qf2_loss: 44.60569, policy_loss: -568.11239, policy_entropy: -0.98095, alpha: 0.49076, time: 32.87254
[CW] ---------------------------
[CW] ---- Iteration:   754 ----
[CW] collect: return: 675.69713, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 48.10087, qf2_loss: 48.29244, policy_loss: -565.08910, policy_entropy: -1.00673, alpha: 0.49013, time: 33.44735
[CW] ---------------------------
[CW] ---- Iteration:   755 ----
[CW] collect: return: 815.87573, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 43.72331, qf2_loss: 43.93440, policy_loss: -566.55372, policy_entropy: -0.98499, alpha: 0.48743, time: 33.22259
[CW] ---------------------------
[CW] ---- Iteration:   756 ----
[CW] collect: return: 832.10490, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 40.87211, qf2_loss: 41.27424, policy_loss: -566.64078, policy_entropy: -1.01053, alpha: 0.48715, time: 32.86865
[CW] ---------------------------
[CW] ---- Iteration:   757 ----
[CW] collect: return: 832.46460, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 39.57486, qf2_loss: 38.90459, policy_loss: -566.16732, policy_entropy: -0.98865, alpha: 0.48853, time: 32.74715
[CW] ---------------------------
[CW] ---- Iteration:   758 ----
[CW] collect: return: 830.26071, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 39.78162, qf2_loss: 41.02686, policy_loss: -568.31574, policy_entropy: -0.99740, alpha: 0.48692, time: 32.66942
[CW] ---------------------------
[CW] ---- Iteration:   759 ----
[CW] collect: return: 835.19344, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 38.39875, qf2_loss: 38.81981, policy_loss: -568.76105, policy_entropy: -0.99696, alpha: 0.48648, time: 33.28592
[CW] ---------------------------
[CW] ---- Iteration:   760 ----
[CW] collect: return: 835.88500, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 37.30631, qf2_loss: 37.70251, policy_loss: -569.81338, policy_entropy: -1.00152, alpha: 0.48628, time: 33.02375
[CW] eval: return: 822.19518, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   761 ----
[CW] collect: return: 813.62282, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 38.22642, qf2_loss: 38.40642, policy_loss: -568.47217, policy_entropy: -0.99687, alpha: 0.48514, time: 33.21257
[CW] ---------------------------
[CW] ---- Iteration:   762 ----
[CW] collect: return: 842.80852, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 39.19372, qf2_loss: 39.67995, policy_loss: -572.42897, policy_entropy: -0.97644, alpha: 0.48359, time: 32.79413
[CW] ---------------------------
[CW] ---- Iteration:   763 ----
[CW] collect: return: 839.56048, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 40.32554, qf2_loss: 40.79432, policy_loss: -569.61317, policy_entropy: -1.01135, alpha: 0.48225, time: 33.32729
[CW] ---------------------------
[CW] ---- Iteration:   764 ----
[CW] collect: return: 821.54439, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 41.67226, qf2_loss: 41.53763, policy_loss: -569.09503, policy_entropy: -1.01007, alpha: 0.48359, time: 33.41899
[CW] ---------------------------
[CW] ---- Iteration:   765 ----
[CW] collect: return: 828.69344, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 43.41409, qf2_loss: 44.17903, policy_loss: -571.41010, policy_entropy: -1.01184, alpha: 0.48736, time: 32.88703
[CW] ---------------------------
[CW] ---- Iteration:   766 ----
[CW] collect: return: 839.38069, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 48.17520, qf2_loss: 48.32339, policy_loss: -571.19751, policy_entropy: -1.00532, alpha: 0.48747, time: 33.39761
[CW] ---------------------------
[CW] ---- Iteration:   767 ----
[CW] collect: return: 831.23136, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 44.70594, qf2_loss: 44.60776, policy_loss: -573.27976, policy_entropy: -1.00229, alpha: 0.48873, time: 32.82818
[CW] ---------------------------
[CW] ---- Iteration:   768 ----
[CW] collect: return: 657.02253, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 44.14534, qf2_loss: 44.61720, policy_loss: -573.38675, policy_entropy: -0.98864, alpha: 0.48789, time: 33.24923
[CW] ---------------------------
[CW] ---- Iteration:   769 ----
[CW] collect: return: 751.42752, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 43.65022, qf2_loss: 43.61617, policy_loss: -575.04436, policy_entropy: -0.99434, alpha: 0.48584, time: 33.48024
[CW] ---------------------------
[CW] ---- Iteration:   770 ----
[CW] collect: return: 828.74205, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 43.51498, qf2_loss: 43.59096, policy_loss: -572.75524, policy_entropy: -1.00505, alpha: 0.48541, time: 32.75524
[CW] ---------------------------
[CW] ---- Iteration:   771 ----
[CW] collect: return: 834.19341, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 47.29511, qf2_loss: 47.44182, policy_loss: -573.82564, policy_entropy: -0.99234, alpha: 0.48505, time: 32.99737
[CW] ---------------------------
[CW] ---- Iteration:   772 ----
[CW] collect: return: 831.88339, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 42.03241, qf2_loss: 42.36580, policy_loss: -574.86940, policy_entropy: -0.99958, alpha: 0.48539, time: 33.18955
[CW] ---------------------------
[CW] ---- Iteration:   773 ----
[CW] collect: return: 830.00109, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 41.86894, qf2_loss: 42.13628, policy_loss: -575.47704, policy_entropy: -1.00551, alpha: 0.48505, time: 32.89111
[CW] ---------------------------
[CW] ---- Iteration:   774 ----
[CW] collect: return: 818.81765, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 41.45412, qf2_loss: 42.24508, policy_loss: -576.87167, policy_entropy: -0.97865, alpha: 0.48404, time: 33.16885
[CW] ---------------------------
[CW] ---- Iteration:   775 ----
[CW] collect: return: 824.51602, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 34.84509, qf2_loss: 35.22406, policy_loss: -577.89579, policy_entropy: -0.99137, alpha: 0.48285, time: 33.23555
[CW] ---------------------------
[CW] ---- Iteration:   776 ----
[CW] collect: return: 799.10804, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 43.82798, qf2_loss: 43.79796, policy_loss: -575.28075, policy_entropy: -1.00454, alpha: 0.48096, time: 33.28574
[CW] ---------------------------
[CW] ---- Iteration:   777 ----
[CW] collect: return: 812.38062, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 38.51796, qf2_loss: 38.54963, policy_loss: -578.17189, policy_entropy: -0.98566, alpha: 0.48036, time: 33.41648
[CW] ---------------------------
[CW] ---- Iteration:   778 ----
[CW] collect: return: 824.61344, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 40.21966, qf2_loss: 40.14218, policy_loss: -576.87317, policy_entropy: -1.01454, alpha: 0.48019, time: 33.28391
[CW] ---------------------------
[CW] ---- Iteration:   779 ----
[CW] collect: return: 830.13549, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 46.81958, qf2_loss: 46.97218, policy_loss: -578.99807, policy_entropy: -0.99592, alpha: 0.48046, time: 32.80111
[CW] ---------------------------
[CW] ---- Iteration:   780 ----
[CW] collect: return: 836.78580, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 38.99851, qf2_loss: 39.46551, policy_loss: -580.19662, policy_entropy: -1.00137, alpha: 0.48002, time: 32.71541
[CW] eval: return: 822.67097, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   781 ----
[CW] collect: return: 831.92159, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 54.93100, qf2_loss: 54.96187, policy_loss: -579.08983, policy_entropy: -1.00544, alpha: 0.48129, time: 33.51436
[CW] ---------------------------
[CW] ---- Iteration:   782 ----
[CW] collect: return: 813.12808, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 51.19034, qf2_loss: 51.79299, policy_loss: -578.47108, policy_entropy: -0.99587, alpha: 0.48141, time: 33.33231
[CW] ---------------------------
[CW] ---- Iteration:   783 ----
[CW] collect: return: 831.68736, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 37.94446, qf2_loss: 38.14732, policy_loss: -580.54916, policy_entropy: -0.97615, alpha: 0.47926, time: 33.44745
[CW] ---------------------------
[CW] ---- Iteration:   784 ----
[CW] collect: return: 831.27113, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 39.53077, qf2_loss: 39.66175, policy_loss: -580.31913, policy_entropy: -0.99510, alpha: 0.47662, time: 32.97535
[CW] ---------------------------
[CW] ---- Iteration:   785 ----
[CW] collect: return: 832.03257, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 38.37567, qf2_loss: 38.55501, policy_loss: -579.12289, policy_entropy: -1.00177, alpha: 0.47501, time: 36.00261
[CW] ---------------------------
[CW] ---- Iteration:   786 ----
[CW] collect: return: 834.99609, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 37.91759, qf2_loss: 38.24330, policy_loss: -581.70556, policy_entropy: -0.99316, alpha: 0.47489, time: 32.88118
[CW] ---------------------------
[CW] ---- Iteration:   787 ----
[CW] collect: return: 837.28714, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 40.49617, qf2_loss: 41.14128, policy_loss: -580.81996, policy_entropy: -1.01780, alpha: 0.47618, time: 33.38405
[CW] ---------------------------
[CW] ---- Iteration:   788 ----
[CW] collect: return: 830.46654, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 39.84819, qf2_loss: 39.55746, policy_loss: -581.15420, policy_entropy: -0.99860, alpha: 0.47753, time: 33.23868
[CW] ---------------------------
[CW] ---- Iteration:   789 ----
[CW] collect: return: 825.83105, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 40.16316, qf2_loss: 40.30309, policy_loss: -582.00513, policy_entropy: -1.00347, alpha: 0.47872, time: 32.95768
[CW] ---------------------------
[CW] ---- Iteration:   790 ----
[CW] collect: return: 829.69127, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 57.47693, qf2_loss: 57.21645, policy_loss: -579.87009, policy_entropy: -1.00704, alpha: 0.47881, time: 33.23181
[CW] ---------------------------
[CW] ---- Iteration:   791 ----
[CW] collect: return: 814.26603, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 42.13759, qf2_loss: 42.39279, policy_loss: -581.04636, policy_entropy: -1.00306, alpha: 0.48029, time: 33.34115
[CW] ---------------------------
[CW] ---- Iteration:   792 ----
[CW] collect: return: 834.64577, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 37.43567, qf2_loss: 37.62929, policy_loss: -582.91371, policy_entropy: -1.00010, alpha: 0.47965, time: 33.24343
[CW] ---------------------------
[CW] ---- Iteration:   793 ----
[CW] collect: return: 832.39455, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 40.65935, qf2_loss: 41.53403, policy_loss: -586.28469, policy_entropy: -0.97697, alpha: 0.47901, time: 33.38687
[CW] ---------------------------
[CW] ---- Iteration:   794 ----
[CW] collect: return: 822.66470, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 35.02537, qf2_loss: 35.20614, policy_loss: -581.90794, policy_entropy: -1.01865, alpha: 0.47672, time: 33.33988
[CW] ---------------------------
[CW] ---- Iteration:   795 ----
[CW] collect: return: 838.94694, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 37.61006, qf2_loss: 37.97182, policy_loss: -584.26536, policy_entropy: -1.01248, alpha: 0.47944, time: 33.19233
[CW] ---------------------------
[CW] ---- Iteration:   796 ----
[CW] collect: return: 825.92268, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 41.27254, qf2_loss: 41.17214, policy_loss: -583.11505, policy_entropy: -1.01085, alpha: 0.48312, time: 33.12917
[CW] ---------------------------
[CW] ---- Iteration:   797 ----
[CW] collect: return: 828.92125, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 53.01256, qf2_loss: 52.90696, policy_loss: -589.48043, policy_entropy: -0.98231, alpha: 0.48254, time: 33.39825
[CW] ---------------------------
[CW] ---- Iteration:   798 ----
[CW] collect: return: 830.71700, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 36.52382, qf2_loss: 36.44297, policy_loss: -588.42820, policy_entropy: -0.97658, alpha: 0.47844, time: 33.46532
[CW] ---------------------------
[CW] ---- Iteration:   799 ----
[CW] collect: return: 832.54542, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 41.58255, qf2_loss: 41.95692, policy_loss: -588.25100, policy_entropy: -0.97839, alpha: 0.47428, time: 33.38310
[CW] ---------------------------
[CW] ---- Iteration:   800 ----
[CW] collect: return: 830.62706, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 35.83122, qf2_loss: 36.10461, policy_loss: -587.86603, policy_entropy: -1.00721, alpha: 0.47278, time: 33.40050
[CW] eval: return: 828.24769, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   801 ----
[CW] collect: return: 839.32720, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 38.02402, qf2_loss: 38.12202, policy_loss: -589.93054, policy_entropy: -1.00230, alpha: 0.47381, time: 33.38073
[CW] ---------------------------
[CW] ---- Iteration:   802 ----
[CW] collect: return: 832.60581, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 40.38323, qf2_loss: 40.44814, policy_loss: -588.46547, policy_entropy: -1.01009, alpha: 0.47474, time: 33.33160
[CW] ---------------------------
[CW] ---- Iteration:   803 ----
[CW] collect: return: 826.80813, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 39.94586, qf2_loss: 40.22706, policy_loss: -588.62364, policy_entropy: -0.99315, alpha: 0.47445, time: 33.10403
[CW] ---------------------------
[CW] ---- Iteration:   804 ----
[CW] collect: return: 812.52410, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 45.90316, qf2_loss: 46.07584, policy_loss: -589.11670, policy_entropy: -0.99686, alpha: 0.47587, time: 32.92563
[CW] ---------------------------
[CW] ---- Iteration:   805 ----
[CW] collect: return: 670.69305, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 44.64337, qf2_loss: 45.05153, policy_loss: -589.78380, policy_entropy: -0.99123, alpha: 0.47342, time: 32.82217
[CW] ---------------------------
[CW] ---- Iteration:   806 ----
[CW] collect: return: 809.42807, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 45.70120, qf2_loss: 45.53662, policy_loss: -590.76721, policy_entropy: -0.99791, alpha: 0.47158, time: 33.01083
[CW] ---------------------------
[CW] ---- Iteration:   807 ----
[CW] collect: return: 832.26958, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 39.02328, qf2_loss: 39.31434, policy_loss: -589.85932, policy_entropy: -0.99650, alpha: 0.47156, time: 33.36931
[CW] ---------------------------
[CW] ---- Iteration:   808 ----
[CW] collect: return: 834.57960, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 37.68431, qf2_loss: 37.70772, policy_loss: -586.44202, policy_entropy: -1.01945, alpha: 0.47316, time: 33.28316
[CW] ---------------------------
[CW] ---- Iteration:   809 ----
[CW] collect: return: 829.55661, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 36.75129, qf2_loss: 36.72425, policy_loss: -593.82329, policy_entropy: -0.99421, alpha: 0.47394, time: 33.13292
[CW] ---------------------------
[CW] ---- Iteration:   810 ----
[CW] collect: return: 830.62334, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 36.38626, qf2_loss: 36.70040, policy_loss: -591.29048, policy_entropy: -0.99918, alpha: 0.47407, time: 32.74533
[CW] ---------------------------
[CW] ---- Iteration:   811 ----
[CW] collect: return: 833.30363, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 38.58749, qf2_loss: 38.87332, policy_loss: -593.38532, policy_entropy: -0.99513, alpha: 0.47352, time: 32.83610
[CW] ---------------------------
[CW] ---- Iteration:   812 ----
[CW] collect: return: 839.76643, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 37.21323, qf2_loss: 37.59350, policy_loss: -593.75085, policy_entropy: -0.99561, alpha: 0.47219, time: 32.76243
[CW] ---------------------------
[CW] ---- Iteration:   813 ----
[CW] collect: return: 837.62684, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 42.42319, qf2_loss: 42.32632, policy_loss: -591.84530, policy_entropy: -0.99428, alpha: 0.47261, time: 32.88892
[CW] ---------------------------
[CW] ---- Iteration:   814 ----
[CW] collect: return: 840.96522, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 45.60712, qf2_loss: 45.37232, policy_loss: -594.22930, policy_entropy: -0.99350, alpha: 0.47096, time: 32.66066
[CW] ---------------------------
[CW] ---- Iteration:   815 ----
[CW] collect: return: 829.90578, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 38.54469, qf2_loss: 38.76844, policy_loss: -593.20929, policy_entropy: -0.99327, alpha: 0.46959, time: 33.31901
[CW] ---------------------------
[CW] ---- Iteration:   816 ----
[CW] collect: return: 840.40668, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 39.04369, qf2_loss: 39.41921, policy_loss: -594.09533, policy_entropy: -0.98803, alpha: 0.46775, time: 33.03590
[CW] ---------------------------
[CW] ---- Iteration:   817 ----
[CW] collect: return: 839.04637, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 36.97372, qf2_loss: 37.31930, policy_loss: -595.04202, policy_entropy: -0.98193, alpha: 0.46562, time: 33.14237
[CW] ---------------------------
[CW] ---- Iteration:   818 ----
[CW] collect: return: 840.24587, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 37.65010, qf2_loss: 37.93848, policy_loss: -594.44561, policy_entropy: -1.00724, alpha: 0.46395, time: 33.38567
[CW] ---------------------------
[CW] ---- Iteration:   819 ----
[CW] collect: return: 837.03539, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 37.39904, qf2_loss: 37.31524, policy_loss: -597.66140, policy_entropy: -0.99820, alpha: 0.46425, time: 32.88542
[CW] ---------------------------
[CW] ---- Iteration:   820 ----
[CW] collect: return: 838.14267, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 37.47211, qf2_loss: 37.69576, policy_loss: -593.51710, policy_entropy: -1.02487, alpha: 0.46645, time: 33.23745
[CW] eval: return: 826.81727, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   821 ----
[CW] collect: return: 828.22030, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 42.81792, qf2_loss: 43.11747, policy_loss: -598.15726, policy_entropy: -0.98082, alpha: 0.46827, time: 33.33583
[CW] ---------------------------
[CW] ---- Iteration:   822 ----
[CW] collect: return: 837.34436, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 39.12916, qf2_loss: 39.58822, policy_loss: -594.21885, policy_entropy: -0.99747, alpha: 0.46466, time: 33.26636
[CW] ---------------------------
[CW] ---- Iteration:   823 ----
[CW] collect: return: 812.63471, steps: 1000.00000, total_steps: 829000.00000
[CW] train: qf1_loss: 36.93054, qf2_loss: 37.35599, policy_loss: -593.89936, policy_entropy: -1.00686, alpha: 0.46539, time: 32.92162
[CW] ---------------------------
[CW] ---- Iteration:   824 ----
[CW] collect: return: 836.17433, steps: 1000.00000, total_steps: 830000.00000
[CW] train: qf1_loss: 41.70259, qf2_loss: 41.65298, policy_loss: -594.72811, policy_entropy: -1.00725, alpha: 0.46709, time: 33.21602
[CW] ---------------------------
[CW] ---- Iteration:   825 ----
[CW] collect: return: 837.27764, steps: 1000.00000, total_steps: 831000.00000
[CW] train: qf1_loss: 55.01231, qf2_loss: 55.05222, policy_loss: -596.24171, policy_entropy: -1.01425, alpha: 0.46881, time: 34.60827
[CW] ---------------------------
[CW] ---- Iteration:   826 ----
[CW] collect: return: 837.63156, steps: 1000.00000, total_steps: 832000.00000
[CW] train: qf1_loss: 41.19138, qf2_loss: 41.56710, policy_loss: -596.98684, policy_entropy: -1.00518, alpha: 0.47035, time: 33.38039
[CW] ---------------------------
