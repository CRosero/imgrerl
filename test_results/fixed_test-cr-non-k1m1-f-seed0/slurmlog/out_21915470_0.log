Hostname: uc2n511.localdomain
Found slurm config: SLURM
Seeting default slurm config
/home/kit/stud/uprnr/imgrerl/test_results/fixed_test-cr-non-k1m1-f-seed0/fixed_test-cr-non-k1m1-f-seed0/fixed_test-cr-non-k1m1-f-seed0__env.echeetah-run/log/rep_00
[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
False
params: 
 {'env': {'env': 'cheetah-run'}} 

additionalVars: 
 {'seed': 0, 'agent': {'image_augmentation_K': 1, 'image_augmentation_M': 1, 'image_augmentation_type': <AugmentationType.NONE: 1>, 'image_augmentation_actor_critic_same_aug': False}}
conf_dict: 
 --------Config-------- 
seed: 0
cuda_id: 0
Subconfig: env
	env: cheetah-run
	obs_type: image
Subconfig: agent
	algo_name: sac
	encoder: lstm
	action_embedding_size: 32
	observ_embedding_size: 0
	reward_embedding_size: 32
	rnn_hidden_size: 512
	dqn_layers: [512, 512, 512]
	policy_layers: [512, 512, 512]
	lr: 0.0003
	gamma: 0.99
	tau: 0.005
	entropy_alpha: 1.0
	image_augmentation_type: AugmentationType.NONE
	image_augmentation_K: 1
	image_augmentation_M: 1
	image_augmentation_actor_critic_same_aug: False
Subconfig: rl
	sampled_seq_len: 64
	buffer_size: 1000000.0
	batch_size: 32
	rollouts_per_iter: 1
	num_updates_per_iter: 100
	num_init_rollouts: 5
Subconfig: eval
	interval: 20
	num_rollouts: 20
---------------------- 
 
Init feature extractor:  6 ;  32 ;  <function relu at 0x15404b4967a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x15404b4967a0>
Init feature extractor:  6 ;  164 ;  <function relu at 0x15404b4967a0>
Critic: 
Critic_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (current_shortcut_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=164, bias=True)
  )
  (qf1): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
  (qf2): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
)
Init feature extractor:  6 ;  32 ;  <function relu at 0x15404b4967a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x15404b4967a0>
Actor: 
Actor_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (policy): TanhGaussianPolicy(
    (fc0): Linear(in_features=612, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=6, bias=True)
    (last_fc_log_std): Linear(in_features=512, out_features=6, bias=True)
  )
)
buffer RAM usage: 11.48 GB
Collecting train stats
[CW] ---- Iteration:     0 ----
[CW] collect: return: 18.40938, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.69984, qf2_loss: 1.70444, policy_loss: -7.85158, policy_entropy: 4.09763, alpha: 0.98504, time: 56.08230
[CW] eval: return: 14.24477, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     1 ----
[CW] collect: return: 4.82978, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.08907, qf2_loss: 0.08925, policy_loss: -8.52407, policy_entropy: 4.10130, alpha: 0.95626, time: 51.58349
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     2 ----
[CW] collect: return: 9.14981, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.08015, qf2_loss: 0.08024, policy_loss: -9.22178, policy_entropy: 4.10064, alpha: 0.92871, time: 51.70390
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     3 ----
[CW] collect: return: 15.07526, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.07396, qf2_loss: 0.07395, policy_loss: -10.16844, policy_entropy: 4.10068, alpha: 0.90231, time: 51.47295
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     4 ----
[CW] collect: return: 8.42504, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06921, qf2_loss: 0.06921, policy_loss: -11.23452, policy_entropy: 4.10177, alpha: 0.87698, time: 51.60563
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     5 ----
[CW] collect: return: 10.28303, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06677, qf2_loss: 0.06678, policy_loss: -12.35646, policy_entropy: 4.10029, alpha: 0.85267, time: 52.26455
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     6 ----
[CW] collect: return: 18.28147, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.07170, qf2_loss: 0.07195, policy_loss: -13.52357, policy_entropy: 4.10035, alpha: 0.82930, time: 51.92259
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     7 ----
[CW] collect: return: 15.78379, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.08025, qf2_loss: 0.08032, policy_loss: -14.72533, policy_entropy: 4.10159, alpha: 0.80683, time: 51.53788
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     8 ----
[CW] collect: return: 5.28940, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.08621, qf2_loss: 0.08632, policy_loss: -15.92654, policy_entropy: 4.10144, alpha: 0.78520, time: 51.72670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     9 ----
[CW] collect: return: 8.02992, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.08014, qf2_loss: 0.08017, policy_loss: -17.13277, policy_entropy: 4.10187, alpha: 0.76436, time: 51.76242
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    10 ----
[CW] collect: return: 10.47865, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.09167, qf2_loss: 0.09175, policy_loss: -18.31689, policy_entropy: 4.10105, alpha: 0.74426, time: 51.86096
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    11 ----
[CW] collect: return: 12.87360, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.07995, qf2_loss: 0.08000, policy_loss: -19.48569, policy_entropy: 4.10029, alpha: 0.72488, time: 51.91535
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    12 ----
[CW] collect: return: 16.75865, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.08665, qf2_loss: 0.08667, policy_loss: -20.63180, policy_entropy: 4.10069, alpha: 0.70617, time: 51.96498
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    13 ----
[CW] collect: return: 6.78229, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.08935, qf2_loss: 0.08944, policy_loss: -21.74586, policy_entropy: 4.10108, alpha: 0.68809, time: 52.12406
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    14 ----
[CW] collect: return: 8.32847, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.07983, qf2_loss: 0.07984, policy_loss: -22.83458, policy_entropy: 4.10042, alpha: 0.67062, time: 51.68928
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    15 ----
[CW] collect: return: 12.43206, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.10968, qf2_loss: 0.10979, policy_loss: -23.89133, policy_entropy: 4.10044, alpha: 0.65372, time: 51.89973
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    16 ----
[CW] collect: return: 19.28651, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.08031, qf2_loss: 0.08034, policy_loss: -24.92872, policy_entropy: 4.10140, alpha: 0.63737, time: 51.69710
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    17 ----
[CW] collect: return: 10.47710, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.11258, qf2_loss: 0.11269, policy_loss: -25.93431, policy_entropy: 4.09998, alpha: 0.62153, time: 52.26625
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    18 ----
[CW] collect: return: 13.09205, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.09755, qf2_loss: 0.09756, policy_loss: -26.91172, policy_entropy: 4.10006, alpha: 0.60619, time: 51.71044
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    19 ----
[CW] collect: return: 17.21403, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.08941, qf2_loss: 0.08938, policy_loss: -27.86798, policy_entropy: 4.09977, alpha: 0.59132, time: 52.17994
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    20 ----
[CW] collect: return: 8.46051, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.09647, qf2_loss: 0.09642, policy_loss: -28.78966, policy_entropy: 4.09967, alpha: 0.57690, time: 52.08028
[CW] eval: return: 14.11848, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    21 ----
[CW] collect: return: 17.84054, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.10352, qf2_loss: 0.10363, policy_loss: -29.69891, policy_entropy: 4.09806, alpha: 0.56292, time: 51.86552
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    22 ----
[CW] collect: return: 7.44532, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.10998, qf2_loss: 0.10998, policy_loss: -30.56082, policy_entropy: 4.09927, alpha: 0.54934, time: 51.83117
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    23 ----
[CW] collect: return: 19.35487, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.14822, qf2_loss: 0.14825, policy_loss: -31.41792, policy_entropy: 4.09846, alpha: 0.53616, time: 51.82977
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    24 ----
[CW] collect: return: 11.87841, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.07218, qf2_loss: 0.07300, policy_loss: -32.24311, policy_entropy: 4.09813, alpha: 0.52336, time: 51.76201
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    25 ----
[CW] collect: return: 13.44424, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.11803, qf2_loss: 0.11836, policy_loss: -33.03331, policy_entropy: 4.09889, alpha: 0.51092, time: 51.78164
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    26 ----
[CW] collect: return: 6.72853, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.15647, qf2_loss: 0.15649, policy_loss: -33.79443, policy_entropy: 4.09703, alpha: 0.49883, time: 51.63074
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    27 ----
[CW] collect: return: 10.89085, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.07557, qf2_loss: 0.07537, policy_loss: -34.54144, policy_entropy: 4.09839, alpha: 0.48707, time: 51.57792
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    28 ----
[CW] collect: return: 26.63383, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.11330, qf2_loss: 0.11302, policy_loss: -35.28953, policy_entropy: 4.09899, alpha: 0.47563, time: 51.83138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    29 ----
[CW] collect: return: 16.75415, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.12452, qf2_loss: 0.12436, policy_loss: -35.99972, policy_entropy: 4.09682, alpha: 0.46450, time: 51.71109
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    30 ----
[CW] collect: return: 11.83507, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.12676, qf2_loss: 0.12670, policy_loss: -36.67041, policy_entropy: 4.09704, alpha: 0.45367, time: 51.93532
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    31 ----
[CW] collect: return: 9.73195, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.11637, qf2_loss: 0.11638, policy_loss: -37.34070, policy_entropy: 4.09682, alpha: 0.44313, time: 52.05380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    32 ----
[CW] collect: return: 16.72559, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.12099, qf2_loss: 0.12094, policy_loss: -37.97292, policy_entropy: 4.09592, alpha: 0.43286, time: 52.39669
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    33 ----
[CW] collect: return: 19.49009, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.14180, qf2_loss: 0.14191, policy_loss: -38.59249, policy_entropy: 4.09645, alpha: 0.42286, time: 52.11711
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    34 ----
[CW] collect: return: 27.83488, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.14465, qf2_loss: 0.14455, policy_loss: -39.20320, policy_entropy: 4.09674, alpha: 0.41312, time: 51.95878
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    35 ----
[CW] collect: return: 12.33020, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.12338, qf2_loss: 0.12310, policy_loss: -39.77557, policy_entropy: 4.09502, alpha: 0.40363, time: 52.12507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    36 ----
[CW] collect: return: 9.21108, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.17242, qf2_loss: 0.17232, policy_loss: -40.34275, policy_entropy: 4.09532, alpha: 0.39438, time: 51.96611
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    37 ----
[CW] collect: return: 10.49293, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.12039, qf2_loss: 0.12019, policy_loss: -40.88168, policy_entropy: 4.09597, alpha: 0.38536, time: 51.92059
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    38 ----
[CW] collect: return: 11.22513, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.16735, qf2_loss: 0.16715, policy_loss: -41.41001, policy_entropy: 4.09399, alpha: 0.37656, time: 52.18070
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    39 ----
[CW] collect: return: 11.71665, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.16016, qf2_loss: 0.15989, policy_loss: -41.89748, policy_entropy: 4.09464, alpha: 0.36799, time: 52.01788
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    40 ----
[CW] collect: return: 17.13683, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.11504, qf2_loss: 0.11485, policy_loss: -42.37542, policy_entropy: 4.09397, alpha: 0.35962, time: 51.92047
[CW] eval: return: 12.84191, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    41 ----
[CW] collect: return: 11.76496, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.14634, qf2_loss: 0.14569, policy_loss: -42.84035, policy_entropy: 4.09214, alpha: 0.35146, time: 51.93961
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    42 ----
[CW] collect: return: 10.79724, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.15796, qf2_loss: 0.15766, policy_loss: -43.29304, policy_entropy: 4.09446, alpha: 0.34350, time: 52.06830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    43 ----
[CW] collect: return: 19.94256, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.12147, qf2_loss: 0.12108, policy_loss: -43.72284, policy_entropy: 4.09284, alpha: 0.33573, time: 51.56126
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    44 ----
[CW] collect: return: 16.63019, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.14125, qf2_loss: 0.14115, policy_loss: -44.15306, policy_entropy: 4.09226, alpha: 0.32815, time: 51.89570
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    45 ----
[CW] collect: return: 22.06884, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.18017, qf2_loss: 0.18015, policy_loss: -44.55879, policy_entropy: 4.09115, alpha: 0.32075, time: 52.09408
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    46 ----
[CW] collect: return: 11.98244, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.11980, qf2_loss: 0.11981, policy_loss: -44.92479, policy_entropy: 4.09230, alpha: 0.31353, time: 52.08205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    47 ----
[CW] collect: return: 18.22301, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.18739, qf2_loss: 0.18767, policy_loss: -45.31932, policy_entropy: 4.09253, alpha: 0.30648, time: 51.66482
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    48 ----
[CW] collect: return: 6.04987, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.13744, qf2_loss: 0.13758, policy_loss: -45.65743, policy_entropy: 4.08789, alpha: 0.29959, time: 51.95930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    49 ----
[CW] collect: return: 22.23272, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.13211, qf2_loss: 0.13316, policy_loss: -46.01130, policy_entropy: 4.08491, alpha: 0.29287, time: 51.89876
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    50 ----
[CW] collect: return: 18.25382, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.15352, qf2_loss: 0.15464, policy_loss: -46.34145, policy_entropy: 4.08292, alpha: 0.28631, time: 52.01852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    51 ----
[CW] collect: return: 30.91887, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.16391, qf2_loss: 0.16504, policy_loss: -46.67233, policy_entropy: 4.08146, alpha: 0.27990, time: 52.06089
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    52 ----
[CW] collect: return: 19.74859, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.16757, qf2_loss: 0.16932, policy_loss: -46.97879, policy_entropy: 4.08007, alpha: 0.27364, time: 51.90103
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    53 ----
[CW] collect: return: 19.23487, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.14078, qf2_loss: 0.14261, policy_loss: -47.25633, policy_entropy: 4.07753, alpha: 0.26753, time: 51.79446
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    54 ----
[CW] collect: return: 15.20430, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.16387, qf2_loss: 0.16549, policy_loss: -47.54827, policy_entropy: 4.07647, alpha: 0.26156, time: 51.67329
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    55 ----
[CW] collect: return: 14.17436, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.14347, qf2_loss: 0.14551, policy_loss: -47.79739, policy_entropy: 4.07291, alpha: 0.25572, time: 51.68483
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    56 ----
[CW] collect: return: 15.77610, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.16706, qf2_loss: 0.16938, policy_loss: -48.06420, policy_entropy: 4.06867, alpha: 0.25002, time: 51.78808
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    57 ----
[CW] collect: return: 23.88350, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.15535, qf2_loss: 0.15752, policy_loss: -48.31466, policy_entropy: 4.06399, alpha: 0.24446, time: 52.00775
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    58 ----
[CW] collect: return: 22.60002, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.16263, qf2_loss: 0.16490, policy_loss: -48.54873, policy_entropy: 4.05723, alpha: 0.23902, time: 51.88778
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    59 ----
[CW] collect: return: 14.86472, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.18136, qf2_loss: 0.18331, policy_loss: -48.75965, policy_entropy: 4.05465, alpha: 0.23371, time: 51.82554
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    60 ----
[CW] collect: return: 15.58416, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.12761, qf2_loss: 0.13002, policy_loss: -48.97609, policy_entropy: 4.04444, alpha: 0.22852, time: 51.90922
[CW] eval: return: 24.27739, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    61 ----
[CW] collect: return: 18.59901, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.16336, qf2_loss: 0.16521, policy_loss: -49.17206, policy_entropy: 4.03655, alpha: 0.22345, time: 52.29807
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    62 ----
[CW] collect: return: 26.27702, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.16953, qf2_loss: 0.17205, policy_loss: -49.36973, policy_entropy: 4.02665, alpha: 0.21850, time: 51.89107
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    63 ----
[CW] collect: return: 13.81573, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.16100, qf2_loss: 0.16303, policy_loss: -49.54910, policy_entropy: 4.01758, alpha: 0.21367, time: 51.77043
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    64 ----
[CW] collect: return: 11.44485, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.18269, qf2_loss: 0.18478, policy_loss: -49.72280, policy_entropy: 4.00596, alpha: 0.20894, time: 51.81700
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    65 ----
[CW] collect: return: 31.62721, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.13705, qf2_loss: 0.13879, policy_loss: -49.88262, policy_entropy: 3.99068, alpha: 0.20433, time: 52.51036
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    66 ----
[CW] collect: return: 42.87594, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.20302, qf2_loss: 0.20437, policy_loss: -50.05552, policy_entropy: 3.97132, alpha: 0.19982, time: 52.46159
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    67 ----
[CW] collect: return: 31.15889, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.14952, qf2_loss: 0.15228, policy_loss: -50.22043, policy_entropy: 3.94214, alpha: 0.19542, time: 51.83253
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    68 ----
[CW] collect: return: 40.01057, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.17091, qf2_loss: 0.17234, policy_loss: -50.35934, policy_entropy: 3.91370, alpha: 0.19113, time: 52.75329
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    69 ----
[CW] collect: return: 56.30376, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.20500, qf2_loss: 0.20680, policy_loss: -50.52139, policy_entropy: 3.88394, alpha: 0.18695, time: 52.74976
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    70 ----
[CW] collect: return: 47.58242, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.21058, qf2_loss: 0.21151, policy_loss: -50.65066, policy_entropy: 3.83137, alpha: 0.18287, time: 56.48755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    71 ----
[CW] collect: return: 69.61879, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.16567, qf2_loss: 0.16751, policy_loss: -50.80097, policy_entropy: 3.76318, alpha: 0.17889, time: 52.88412
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    72 ----
[CW] collect: return: 51.71222, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.24149, qf2_loss: 0.24237, policy_loss: -50.96412, policy_entropy: 3.67986, alpha: 0.17502, time: 52.71053
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    73 ----
[CW] collect: return: 59.51783, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.17606, qf2_loss: 0.17760, policy_loss: -51.08449, policy_entropy: 3.58651, alpha: 0.17127, time: 52.76570
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    74 ----
[CW] collect: return: 65.33215, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.22713, qf2_loss: 0.22789, policy_loss: -51.23566, policy_entropy: 3.50632, alpha: 0.16762, time: 52.78947
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    75 ----
[CW] collect: return: 70.86302, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.24698, qf2_loss: 0.24708, policy_loss: -51.40460, policy_entropy: 3.36336, alpha: 0.16407, time: 52.69486
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    76 ----
[CW] collect: return: 37.97339, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.27209, qf2_loss: 0.27151, policy_loss: -51.50671, policy_entropy: 3.29697, alpha: 0.16063, time: 52.74360
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    77 ----
[CW] collect: return: 74.34983, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.23865, qf2_loss: 0.23908, policy_loss: -51.69501, policy_entropy: 3.15600, alpha: 0.15729, time: 52.94391
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    78 ----
[CW] collect: return: 61.38670, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.24005, qf2_loss: 0.24008, policy_loss: -51.84150, policy_entropy: 3.02218, alpha: 0.15404, time: 52.76266
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    79 ----
[CW] collect: return: 63.93559, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.33433, qf2_loss: 0.33410, policy_loss: -51.98039, policy_entropy: 2.94005, alpha: 0.15088, time: 52.99522
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    80 ----
[CW] collect: return: 37.08234, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.30846, qf2_loss: 0.30667, policy_loss: -52.16452, policy_entropy: 2.80493, alpha: 0.14781, time: 52.47517
[CW] eval: return: 63.30163, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    81 ----
[CW] collect: return: 68.75908, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.28832, qf2_loss: 0.28717, policy_loss: -52.30910, policy_entropy: 2.68127, alpha: 0.14481, time: 52.74021
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    82 ----
[CW] collect: return: 81.83068, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.31952, qf2_loss: 0.31722, policy_loss: -52.56917, policy_entropy: 2.44936, alpha: 0.14193, time: 52.62363
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    83 ----
[CW] collect: return: 49.91510, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.32093, qf2_loss: 0.31900, policy_loss: -52.74145, policy_entropy: 2.20250, alpha: 0.13914, time: 52.58974
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    84 ----
[CW] collect: return: 92.07549, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.34355, qf2_loss: 0.34069, policy_loss: -52.94368, policy_entropy: 1.99924, alpha: 0.13648, time: 52.13769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    85 ----
[CW] collect: return: 55.63390, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.33291, qf2_loss: 0.33067, policy_loss: -53.09647, policy_entropy: 1.80728, alpha: 0.13389, time: 52.99950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    86 ----
[CW] collect: return: 71.16841, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.36334, qf2_loss: 0.36026, policy_loss: -53.30044, policy_entropy: 1.60104, alpha: 0.13139, time: 52.40702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    87 ----
[CW] collect: return: 40.95703, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.31259, qf2_loss: 0.31131, policy_loss: -53.49753, policy_entropy: 1.40284, alpha: 0.12897, time: 52.49637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    88 ----
[CW] collect: return: 58.29384, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.32173, qf2_loss: 0.31931, policy_loss: -53.68455, policy_entropy: 1.24631, alpha: 0.12663, time: 52.48284
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    89 ----
[CW] collect: return: 64.05329, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.31138, qf2_loss: 0.30900, policy_loss: -53.89950, policy_entropy: 1.10455, alpha: 0.12434, time: 52.57102
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    90 ----
[CW] collect: return: 93.29559, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.42098, qf2_loss: 0.41818, policy_loss: -54.14547, policy_entropy: 0.97762, alpha: 0.12211, time: 52.45334
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    91 ----
[CW] collect: return: 72.25117, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.35606, qf2_loss: 0.35299, policy_loss: -54.36985, policy_entropy: 0.87533, alpha: 0.11992, time: 52.40523
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    92 ----
[CW] collect: return: 114.77106, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.36336, qf2_loss: 0.36160, policy_loss: -54.58951, policy_entropy: 0.82731, alpha: 0.11777, time: 52.60855
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    93 ----
[CW] collect: return: 60.38257, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.35329, qf2_loss: 0.35097, policy_loss: -54.81842, policy_entropy: 0.68731, alpha: 0.11565, time: 52.30699
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    94 ----
[CW] collect: return: 105.04869, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.42040, qf2_loss: 0.41508, policy_loss: -55.03489, policy_entropy: 0.64368, alpha: 0.11358, time: 52.24245
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    95 ----
[CW] collect: return: 78.26196, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.39234, qf2_loss: 0.39048, policy_loss: -55.30264, policy_entropy: 0.45211, alpha: 0.11154, time: 52.69016
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    96 ----
[CW] collect: return: 119.93761, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.45462, qf2_loss: 0.45054, policy_loss: -55.51297, policy_entropy: 0.38736, alpha: 0.10956, time: 52.27989
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    97 ----
[CW] collect: return: 68.48083, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.44764, qf2_loss: 0.44505, policy_loss: -55.66720, policy_entropy: 0.39998, alpha: 0.10759, time: 52.44999
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    98 ----
[CW] collect: return: 51.67212, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.38205, qf2_loss: 0.38024, policy_loss: -55.84972, policy_entropy: 0.39810, alpha: 0.10563, time: 52.18410
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    99 ----
[CW] collect: return: 71.28049, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.41482, qf2_loss: 0.41308, policy_loss: -56.11574, policy_entropy: 0.36250, alpha: 0.10369, time: 52.24801
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   100 ----
[CW] collect: return: 126.75424, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.64703, qf2_loss: 0.64371, policy_loss: -56.30670, policy_entropy: 0.34096, alpha: 0.10177, time: 52.50944
[CW] eval: return: 76.11597, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   101 ----
[CW] collect: return: 79.02925, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.38386, qf2_loss: 0.38301, policy_loss: -56.42131, policy_entropy: 0.26237, alpha: 0.09988, time: 52.55480
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   102 ----
[CW] collect: return: 46.89903, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.42812, qf2_loss: 0.42795, policy_loss: -56.65150, policy_entropy: 0.17726, alpha: 0.09802, time: 52.72346
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   103 ----
[CW] collect: return: 61.11289, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.44703, qf2_loss: 0.44818, policy_loss: -56.80834, policy_entropy: 0.16382, alpha: 0.09620, time: 52.36078
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   104 ----
[CW] collect: return: 148.16565, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.48129, qf2_loss: 0.48347, policy_loss: -56.96644, policy_entropy: 0.25251, alpha: 0.09437, time: 52.51110
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   105 ----
[CW] collect: return: 63.75982, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.50441, qf2_loss: 0.50597, policy_loss: -57.20663, policy_entropy: 0.16664, alpha: 0.09256, time: 52.55034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   106 ----
[CW] collect: return: 134.68088, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.45888, qf2_loss: 0.46175, policy_loss: -57.47105, policy_entropy: 0.11355, alpha: 0.09079, time: 52.45116
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   107 ----
[CW] collect: return: 71.94292, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.54045, qf2_loss: 0.54010, policy_loss: -57.67023, policy_entropy: 0.20651, alpha: 0.08903, time: 52.16271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   108 ----
[CW] collect: return: 44.81412, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.67298, qf2_loss: 0.67377, policy_loss: -57.74632, policy_entropy: 0.16538, alpha: 0.08729, time: 52.22845
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   109 ----
[CW] collect: return: 126.68444, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.47077, qf2_loss: 0.47334, policy_loss: -57.93113, policy_entropy: 0.17228, alpha: 0.08556, time: 52.27777
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   110 ----
[CW] collect: return: 60.07831, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.54422, qf2_loss: 0.54295, policy_loss: -58.01446, policy_entropy: 0.24897, alpha: 0.08385, time: 52.35529
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   111 ----
[CW] collect: return: 132.34791, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.48903, qf2_loss: 0.48965, policy_loss: -58.17716, policy_entropy: 0.16676, alpha: 0.08216, time: 52.26403
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   112 ----
[CW] collect: return: 168.35915, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.48926, qf2_loss: 0.48680, policy_loss: -58.34104, policy_entropy: 0.19338, alpha: 0.08050, time: 52.28463
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   113 ----
[CW] collect: return: 106.01229, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.48485, qf2_loss: 0.48305, policy_loss: -58.41656, policy_entropy: 0.21023, alpha: 0.07886, time: 52.25536
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   114 ----
[CW] collect: return: 47.71572, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.48949, qf2_loss: 0.48800, policy_loss: -58.43897, policy_entropy: 0.14451, alpha: 0.07725, time: 52.61269
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   115 ----
[CW] collect: return: 52.89360, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.47350, qf2_loss: 0.47147, policy_loss: -58.33317, policy_entropy: 0.21197, alpha: 0.07566, time: 52.45539
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   116 ----
[CW] collect: return: 41.79018, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.36790, qf2_loss: 0.36721, policy_loss: -58.43339, policy_entropy: 0.20825, alpha: 0.07410, time: 52.65972
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   117 ----
[CW] collect: return: 94.14554, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.43419, qf2_loss: 0.43166, policy_loss: -58.56790, policy_entropy: 0.17902, alpha: 0.07255, time: 52.50730
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   118 ----
[CW] collect: return: 84.81189, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.43417, qf2_loss: 0.43133, policy_loss: -58.59390, policy_entropy: 0.07273, alpha: 0.07105, time: 52.55213
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   119 ----
[CW] collect: return: 63.40481, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.58487, qf2_loss: 0.58624, policy_loss: -58.78200, policy_entropy: -0.08293, alpha: 0.06960, time: 52.30427
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   120 ----
[CW] collect: return: 54.56059, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.48815, qf2_loss: 0.48469, policy_loss: -58.87642, policy_entropy: -0.22756, alpha: 0.06821, time: 52.50161
[CW] eval: return: 88.63719, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   121 ----
[CW] collect: return: 42.18590, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.34255, qf2_loss: 0.33851, policy_loss: -58.77259, policy_entropy: -0.14865, alpha: 0.06685, time: 52.43536
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   122 ----
[CW] collect: return: 60.18968, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.34259, qf2_loss: 0.34034, policy_loss: -58.81337, policy_entropy: -0.25120, alpha: 0.06551, time: 52.07085
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   123 ----
[CW] collect: return: 75.08158, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.46016, qf2_loss: 0.45519, policy_loss: -58.79201, policy_entropy: -0.22197, alpha: 0.06419, time: 52.16274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   124 ----
[CW] collect: return: 33.28203, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.38796, qf2_loss: 0.38389, policy_loss: -58.80853, policy_entropy: -0.38845, alpha: 0.06290, time: 52.03201
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   125 ----
[CW] collect: return: 43.35792, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.39965, qf2_loss: 0.39708, policy_loss: -58.90638, policy_entropy: -0.41222, alpha: 0.06166, time: 51.70418
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   126 ----
[CW] collect: return: 61.02616, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.43944, qf2_loss: 0.43627, policy_loss: -58.93632, policy_entropy: -0.49418, alpha: 0.06043, time: 51.40552
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   127 ----
[CW] collect: return: 60.08745, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.42491, qf2_loss: 0.42216, policy_loss: -58.88268, policy_entropy: -0.57631, alpha: 0.05924, time: 51.80653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   128 ----
[CW] collect: return: 123.49998, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.41450, qf2_loss: 0.41227, policy_loss: -58.96500, policy_entropy: -0.76735, alpha: 0.05810, time: 51.57392
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   129 ----
[CW] collect: return: 55.97067, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.41729, qf2_loss: 0.41547, policy_loss: -59.10044, policy_entropy: -0.84059, alpha: 0.05700, time: 51.64256
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   130 ----
[CW] collect: return: 67.82930, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.45301, qf2_loss: 0.45001, policy_loss: -59.11379, policy_entropy: -0.92467, alpha: 0.05591, time: 51.99084
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   131 ----
[CW] collect: return: 85.13622, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.38865, qf2_loss: 0.38442, policy_loss: -59.28012, policy_entropy: -1.18631, alpha: 0.05487, time: 51.96476
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   132 ----
[CW] collect: return: 75.66822, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.55446, qf2_loss: 0.55011, policy_loss: -59.23676, policy_entropy: -1.22582, alpha: 0.05387, time: 51.98524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   133 ----
[CW] collect: return: 62.73205, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.49088, qf2_loss: 0.49115, policy_loss: -59.29038, policy_entropy: -1.18960, alpha: 0.05288, time: 51.61553
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   134 ----
[CW] collect: return: 58.52985, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.40219, qf2_loss: 0.39727, policy_loss: -59.36165, policy_entropy: -1.36741, alpha: 0.05191, time: 51.24482
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   135 ----
[CW] collect: return: 51.46677, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.43186, qf2_loss: 0.42819, policy_loss: -59.38055, policy_entropy: -1.51332, alpha: 0.05097, time: 51.20889
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   136 ----
[CW] collect: return: 40.09599, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.45298, qf2_loss: 0.45026, policy_loss: -59.53521, policy_entropy: -1.58756, alpha: 0.05007, time: 51.08309
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   137 ----
[CW] collect: return: 207.33280, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.55758, qf2_loss: 0.55775, policy_loss: -59.68038, policy_entropy: -1.72599, alpha: 0.04919, time: 51.47892
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   138 ----
[CW] collect: return: 48.40971, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.53180, qf2_loss: 0.52904, policy_loss: -59.61062, policy_entropy: -1.74512, alpha: 0.04832, time: 51.76391
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   139 ----
[CW] collect: return: 88.63675, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.56632, qf2_loss: 0.56130, policy_loss: -59.68182, policy_entropy: -1.73938, alpha: 0.04746, time: 51.78511
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   140 ----
[CW] collect: return: 41.71338, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.49305, qf2_loss: 0.49196, policy_loss: -59.82872, policy_entropy: -1.80558, alpha: 0.04662, time: 52.52683
[CW] eval: return: 83.75628, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   141 ----
[CW] collect: return: 42.87587, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.45359, qf2_loss: 0.45248, policy_loss: -59.93018, policy_entropy: -1.93433, alpha: 0.04578, time: 52.16654
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   142 ----
[CW] collect: return: 87.28170, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.51092, qf2_loss: 0.50733, policy_loss: -60.04067, policy_entropy: -2.02348, alpha: 0.04498, time: 52.00740
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   143 ----
[CW] collect: return: 49.16436, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.54295, qf2_loss: 0.53786, policy_loss: -60.01579, policy_entropy: -1.94526, alpha: 0.04419, time: 51.67175
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   144 ----
[CW] collect: return: 197.55552, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.58387, qf2_loss: 0.58314, policy_loss: -60.13233, policy_entropy: -1.91688, alpha: 0.04339, time: 51.64448
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   145 ----
[CW] collect: return: 188.25743, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.55083, qf2_loss: 0.55286, policy_loss: -60.60440, policy_entropy: -2.23911, alpha: 0.04261, time: 51.91166
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   146 ----
[CW] collect: return: 168.62899, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.54837, qf2_loss: 0.54521, policy_loss: -60.56194, policy_entropy: -2.23090, alpha: 0.04187, time: 51.81687
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   147 ----
[CW] collect: return: 105.92276, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.80380, qf2_loss: 0.79725, policy_loss: -60.72082, policy_entropy: -2.17427, alpha: 0.04113, time: 51.68959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   148 ----
[CW] collect: return: 131.71082, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.64173, qf2_loss: 0.64095, policy_loss: -60.61156, policy_entropy: -2.16298, alpha: 0.04038, time: 51.25533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   149 ----
[CW] collect: return: 40.67633, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.50481, qf2_loss: 0.50319, policy_loss: -60.82677, policy_entropy: -2.09165, alpha: 0.03964, time: 51.73906
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   150 ----
[CW] collect: return: 56.70330, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.59013, qf2_loss: 0.58960, policy_loss: -61.04953, policy_entropy: -2.21938, alpha: 0.03890, time: 51.39750
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   151 ----
[CW] collect: return: 81.39452, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.49421, qf2_loss: 0.49332, policy_loss: -61.05620, policy_entropy: -2.20111, alpha: 0.03817, time: 51.47155
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   152 ----
[CW] collect: return: 150.17123, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.54777, qf2_loss: 0.54811, policy_loss: -61.17014, policy_entropy: -2.33069, alpha: 0.03746, time: 51.19727
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   153 ----
[CW] collect: return: 68.30926, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.57981, qf2_loss: 0.58193, policy_loss: -61.12781, policy_entropy: -2.37158, alpha: 0.03677, time: 51.48880
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   154 ----
[CW] collect: return: 139.40285, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.58718, qf2_loss: 0.58748, policy_loss: -61.49455, policy_entropy: -2.54779, alpha: 0.03611, time: 51.55337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   155 ----
[CW] collect: return: 56.96608, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.63089, qf2_loss: 0.62805, policy_loss: -61.32570, policy_entropy: -2.62128, alpha: 0.03548, time: 51.65165
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   156 ----
[CW] collect: return: 47.57848, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.52282, qf2_loss: 0.52186, policy_loss: -61.35677, policy_entropy: -2.72609, alpha: 0.03486, time: 52.14870
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   157 ----
[CW] collect: return: 59.13199, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.78962, qf2_loss: 0.79246, policy_loss: -61.48353, policy_entropy: -2.76702, alpha: 0.03425, time: 51.67768
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   158 ----
[CW] collect: return: 64.92012, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.47717, qf2_loss: 0.47787, policy_loss: -61.57254, policy_entropy: -2.72854, alpha: 0.03366, time: 52.09506
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   159 ----
[CW] collect: return: 169.03988, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.56750, qf2_loss: 0.56539, policy_loss: -61.91059, policy_entropy: -2.85903, alpha: 0.03307, time: 51.72572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   160 ----
[CW] collect: return: 125.36955, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.65614, qf2_loss: 0.65411, policy_loss: -62.07448, policy_entropy: -2.92621, alpha: 0.03250, time: 51.86705
[CW] eval: return: 108.95793, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   161 ----
[CW] collect: return: 42.54866, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.55403, qf2_loss: 0.55728, policy_loss: -62.07838, policy_entropy: -2.91747, alpha: 0.03193, time: 51.85463
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   162 ----
[CW] collect: return: 79.39423, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.71289, qf2_loss: 0.71409, policy_loss: -62.21313, policy_entropy: -2.92863, alpha: 0.03137, time: 51.41434
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   163 ----
[CW] collect: return: 47.72543, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.56905, qf2_loss: 0.57076, policy_loss: -62.11562, policy_entropy: -2.90234, alpha: 0.03080, time: 51.59286
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   164 ----
[CW] collect: return: 44.63591, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.49646, qf2_loss: 0.49860, policy_loss: -61.97541, policy_entropy: -3.01121, alpha: 0.03025, time: 51.75382
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   165 ----
[CW] collect: return: 72.87810, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.54836, qf2_loss: 0.54941, policy_loss: -62.13035, policy_entropy: -3.07224, alpha: 0.02973, time: 51.47574
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   166 ----
[CW] collect: return: 120.06303, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.67959, qf2_loss: 0.67624, policy_loss: -62.41414, policy_entropy: -3.18302, alpha: 0.02921, time: 51.40163
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   167 ----
[CW] collect: return: 115.53470, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.49337, qf2_loss: 0.49352, policy_loss: -62.12944, policy_entropy: -3.04236, alpha: 0.02869, time: 51.61554
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   168 ----
[CW] collect: return: 86.29531, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.53229, qf2_loss: 0.53273, policy_loss: -61.97046, policy_entropy: -3.14727, alpha: 0.02818, time: 51.93223
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   169 ----
[CW] collect: return: 113.37214, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.54104, qf2_loss: 0.53912, policy_loss: -62.34986, policy_entropy: -3.19668, alpha: 0.02767, time: 51.64042
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   170 ----
[CW] collect: return: 76.30072, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.68682, qf2_loss: 0.68465, policy_loss: -62.41332, policy_entropy: -3.25612, alpha: 0.02718, time: 51.94264
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   171 ----
[CW] collect: return: 142.18425, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.67629, qf2_loss: 0.66979, policy_loss: -62.52044, policy_entropy: -3.23820, alpha: 0.02670, time: 51.92482
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   172 ----
[CW] collect: return: 63.79422, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.50332, qf2_loss: 0.50406, policy_loss: -62.56713, policy_entropy: -3.38322, alpha: 0.02622, time: 51.81262
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   173 ----
[CW] collect: return: 57.71277, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.56279, qf2_loss: 0.56223, policy_loss: -62.27645, policy_entropy: -3.29184, alpha: 0.02576, time: 51.67792
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   174 ----
[CW] collect: return: 112.53192, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.62714, qf2_loss: 0.62760, policy_loss: -62.46305, policy_entropy: -3.45837, alpha: 0.02531, time: 51.63046
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   175 ----
[CW] collect: return: 54.08306, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.55876, qf2_loss: 0.56203, policy_loss: -62.28660, policy_entropy: -3.45348, alpha: 0.02487, time: 51.22232
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   176 ----
[CW] collect: return: 65.23271, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.46802, qf2_loss: 0.46857, policy_loss: -62.61284, policy_entropy: -3.67583, alpha: 0.02445, time: 51.26456
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   177 ----
[CW] collect: return: 61.95473, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.74606, qf2_loss: 0.74575, policy_loss: -62.15587, policy_entropy: -3.49796, alpha: 0.02403, time: 51.50301
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   178 ----
[CW] collect: return: 42.84272, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.55595, qf2_loss: 0.55572, policy_loss: -62.83516, policy_entropy: -3.79039, alpha: 0.02363, time: 51.67181
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   179 ----
[CW] collect: return: 195.48597, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.61690, qf2_loss: 0.61960, policy_loss: -62.57272, policy_entropy: -3.80986, alpha: 0.02324, time: 51.41053
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   180 ----
[CW] collect: return: 135.27889, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.50379, qf2_loss: 0.50087, policy_loss: -62.92654, policy_entropy: -4.04047, alpha: 0.02289, time: 51.40465
[CW] eval: return: 113.32716, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   181 ----
[CW] collect: return: 79.20290, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.48208, qf2_loss: 0.48209, policy_loss: -62.77755, policy_entropy: -3.96967, alpha: 0.02253, time: 51.98504
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   182 ----
[CW] collect: return: 213.91839, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.85930, qf2_loss: 0.85651, policy_loss: -62.75854, policy_entropy: -4.13859, alpha: 0.02219, time: 52.23955
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   183 ----
[CW] collect: return: 56.14681, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.68188, qf2_loss: 0.68121, policy_loss: -62.69704, policy_entropy: -4.13722, alpha: 0.02187, time: 51.94795
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   184 ----
[CW] collect: return: 78.06533, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.53953, qf2_loss: 0.54177, policy_loss: -63.00519, policy_entropy: -4.41580, alpha: 0.02156, time: 51.88626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   185 ----
[CW] collect: return: 36.65447, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.55376, qf2_loss: 0.55359, policy_loss: -63.11718, policy_entropy: -4.48977, alpha: 0.02129, time: 51.77345
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   186 ----
[CW] collect: return: 118.30791, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.96639, qf2_loss: 0.97322, policy_loss: -62.90038, policy_entropy: -4.67211, alpha: 0.02103, time: 51.81629
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   187 ----
[CW] collect: return: 76.66296, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.69422, qf2_loss: 0.69034, policy_loss: -63.39623, policy_entropy: -5.61097, alpha: 0.02083, time: 51.77532
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   188 ----
[CW] collect: return: 162.66686, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.56693, qf2_loss: 0.56845, policy_loss: -63.48938, policy_entropy: -6.46982, alpha: 0.02086, time: 51.25311
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   189 ----
[CW] collect: return: 128.06736, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.60924, qf2_loss: 0.61168, policy_loss: -63.65986, policy_entropy: -6.35865, alpha: 0.02095, time: 51.51669
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   190 ----
[CW] collect: return: 24.86346, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.68599, qf2_loss: 0.68317, policy_loss: -63.19004, policy_entropy: -6.20045, alpha: 0.02100, time: 51.37420
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   191 ----
[CW] collect: return: 79.15465, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.71251, qf2_loss: 0.71492, policy_loss: -63.19976, policy_entropy: -4.80368, alpha: 0.02094, time: 51.31690
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   192 ----
[CW] collect: return: 169.73649, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.70776, qf2_loss: 0.70718, policy_loss: -64.03563, policy_entropy: -4.61123, alpha: 0.02062, time: 51.36229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   193 ----
[CW] collect: return: 44.24031, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.99555, qf2_loss: 0.99475, policy_loss: -63.42124, policy_entropy: -4.80356, alpha: 0.02031, time: 51.49328
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   194 ----
[CW] collect: return: 69.17577, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.61272, qf2_loss: 0.61532, policy_loss: -63.39992, policy_entropy: -5.22326, alpha: 0.02009, time: 51.65831
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   195 ----
[CW] collect: return: 93.21448, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 1.01882, qf2_loss: 1.02446, policy_loss: -63.80098, policy_entropy: -5.40697, alpha: 0.01991, time: 51.64670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   196 ----
[CW] collect: return: 263.79306, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.66296, qf2_loss: 0.66780, policy_loss: -63.47431, policy_entropy: -5.61386, alpha: 0.01978, time: 51.85783
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   197 ----
[CW] collect: return: 86.84665, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.88653, qf2_loss: 0.88765, policy_loss: -63.78146, policy_entropy: -6.56416, alpha: 0.01981, time: 52.21376
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   198 ----
[CW] collect: return: 50.72097, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.55933, qf2_loss: 0.56119, policy_loss: -63.26638, policy_entropy: -6.23768, alpha: 0.01991, time: 52.45372
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   199 ----
[CW] collect: return: 206.93286, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.63590, qf2_loss: 0.64057, policy_loss: -64.10274, policy_entropy: -6.16216, alpha: 0.01999, time: 52.15439
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   200 ----
[CW] collect: return: 109.50006, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.79805, qf2_loss: 0.80083, policy_loss: -63.69251, policy_entropy: -6.01402, alpha: 0.02001, time: 52.13162
[CW] eval: return: 134.72342, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   201 ----
[CW] collect: return: 79.30041, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.80428, qf2_loss: 0.80295, policy_loss: -63.99024, policy_entropy: -5.76684, alpha: 0.02000, time: 53.01787
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   202 ----
[CW] collect: return: 200.59688, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.89561, qf2_loss: 0.89724, policy_loss: -64.27797, policy_entropy: -5.82046, alpha: 0.01990, time: 51.81160
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   203 ----
[CW] collect: return: 237.07516, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 1.05529, qf2_loss: 1.05542, policy_loss: -64.30711, policy_entropy: -4.99689, alpha: 0.01977, time: 51.69261
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   204 ----
[CW] collect: return: 84.63237, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.70893, qf2_loss: 0.71056, policy_loss: -64.93562, policy_entropy: -4.46052, alpha: 0.01932, time: 51.62526
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   205 ----
[CW] collect: return: 84.87892, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.80766, qf2_loss: 0.80333, policy_loss: -64.45499, policy_entropy: -4.53005, alpha: 0.01884, time: 51.81561
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   206 ----
[CW] collect: return: 102.10837, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 1.16811, qf2_loss: 1.16944, policy_loss: -64.60000, policy_entropy: -4.62674, alpha: 0.01841, time: 51.50651
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   207 ----
[CW] collect: return: 145.80778, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 1.29557, qf2_loss: 1.30003, policy_loss: -64.51434, policy_entropy: -5.50076, alpha: 0.01807, time: 51.56385
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   208 ----
[CW] collect: return: 67.99674, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.66478, qf2_loss: 0.66618, policy_loss: -64.53338, policy_entropy: -6.18066, alpha: 0.01805, time: 51.72014
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   209 ----
[CW] collect: return: 159.67638, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.74423, qf2_loss: 0.74539, policy_loss: -65.45123, policy_entropy: -6.15327, alpha: 0.01811, time: 52.07758
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   210 ----
[CW] collect: return: 60.19822, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.71678, qf2_loss: 0.71755, policy_loss: -64.79008, policy_entropy: -5.58530, alpha: 0.01809, time: 51.83021
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   211 ----
[CW] collect: return: 79.38227, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 1.19708, qf2_loss: 1.19507, policy_loss: -64.73198, policy_entropy: -4.91857, alpha: 0.01784, time: 51.75285
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   212 ----
[CW] collect: return: 295.21818, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.96792, qf2_loss: 0.96465, policy_loss: -64.74500, policy_entropy: -4.94424, alpha: 0.01746, time: 52.57727
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   213 ----
[CW] collect: return: 204.98626, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.76251, qf2_loss: 0.76100, policy_loss: -64.56709, policy_entropy: -5.17338, alpha: 0.01717, time: 51.81178
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   214 ----
[CW] collect: return: 226.84137, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 1.10570, qf2_loss: 1.10260, policy_loss: -65.11210, policy_entropy: -5.50844, alpha: 0.01695, time: 51.57948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   215 ----
[CW] collect: return: 128.40914, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 1.29440, qf2_loss: 1.29922, policy_loss: -66.23443, policy_entropy: -5.82126, alpha: 0.01684, time: 51.42733
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   216 ----
[CW] collect: return: 82.86874, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.83925, qf2_loss: 0.83908, policy_loss: -65.65194, policy_entropy: -5.74792, alpha: 0.01676, time: 51.81363
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   217 ----
[CW] collect: return: 56.87275, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.78062, qf2_loss: 0.77891, policy_loss: -65.59243, policy_entropy: -6.06908, alpha: 0.01669, time: 51.47840
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   218 ----
[CW] collect: return: 269.00791, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 1.15979, qf2_loss: 1.15903, policy_loss: -65.15659, policy_entropy: -6.35161, alpha: 0.01682, time: 51.55835
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   219 ----
[CW] collect: return: 119.00083, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.94402, qf2_loss: 0.94900, policy_loss: -65.74766, policy_entropy: -4.71373, alpha: 0.01669, time: 51.50832
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   220 ----
[CW] collect: return: 124.79257, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.80225, qf2_loss: 0.80031, policy_loss: -65.48063, policy_entropy: -4.83151, alpha: 0.01621, time: 51.12447
[CW] eval: return: 150.09917, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   221 ----
[CW] collect: return: 96.75997, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 1.22716, qf2_loss: 1.22272, policy_loss: -65.51888, policy_entropy: -5.06824, alpha: 0.01586, time: 51.59999
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   222 ----
[CW] collect: return: 195.62432, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 1.17162, qf2_loss: 1.16781, policy_loss: -65.42264, policy_entropy: -5.53434, alpha: 0.01561, time: 51.95414
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   223 ----
[CW] collect: return: 61.68559, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.93254, qf2_loss: 0.92993, policy_loss: -65.88414, policy_entropy: -5.69292, alpha: 0.01549, time: 51.56920
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   224 ----
[CW] collect: return: 212.41465, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.90422, qf2_loss: 0.90479, policy_loss: -65.94351, policy_entropy: -5.84379, alpha: 0.01540, time: 51.52006
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   225 ----
[CW] collect: return: 227.42584, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 1.22451, qf2_loss: 1.22083, policy_loss: -65.68222, policy_entropy: -6.86553, alpha: 0.01547, time: 51.77604
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   226 ----
[CW] collect: return: 259.93082, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.94206, qf2_loss: 0.94424, policy_loss: -67.27419, policy_entropy: -7.16118, alpha: 0.01587, time: 51.98492
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   227 ----
[CW] collect: return: 279.03064, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 3.32861, qf2_loss: 3.32138, policy_loss: -67.11236, policy_entropy: -7.09397, alpha: 0.01631, time: 51.75528
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   228 ----
[CW] collect: return: 164.83816, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 1.00031, qf2_loss: 1.00373, policy_loss: -66.94239, policy_entropy: -6.80496, alpha: 0.01671, time: 51.48300
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   229 ----
[CW] collect: return: 192.67193, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.85124, qf2_loss: 0.85106, policy_loss: -66.97759, policy_entropy: -6.76587, alpha: 0.01706, time: 51.63833
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   230 ----
[CW] collect: return: 229.61219, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.92650, qf2_loss: 0.92133, policy_loss: -67.58594, policy_entropy: -6.75804, alpha: 0.01743, time: 51.58435
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   231 ----
[CW] collect: return: 77.25058, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 1.31580, qf2_loss: 1.31251, policy_loss: -67.57613, policy_entropy: -6.53959, alpha: 0.01777, time: 51.23717
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   232 ----
[CW] collect: return: 198.78056, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.95780, qf2_loss: 0.95638, policy_loss: -67.26951, policy_entropy: -6.41749, alpha: 0.01803, time: 51.47909
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   233 ----
[CW] collect: return: 217.37508, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 1.12703, qf2_loss: 1.12785, policy_loss: -66.95394, policy_entropy: -6.32238, alpha: 0.01823, time: 51.19950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   234 ----
[CW] collect: return: 131.30826, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.97413, qf2_loss: 0.97439, policy_loss: -67.44438, policy_entropy: -6.29502, alpha: 0.01843, time: 51.34467
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   235 ----
[CW] collect: return: 242.52274, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.98279, qf2_loss: 0.98433, policy_loss: -67.47927, policy_entropy: -6.16209, alpha: 0.01858, time: 51.47867
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   236 ----
[CW] collect: return: 104.81441, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.98888, qf2_loss: 0.99250, policy_loss: -67.66219, policy_entropy: -5.96899, alpha: 0.01864, time: 51.84013
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   237 ----
[CW] collect: return: 233.11413, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 1.80633, qf2_loss: 1.80588, policy_loss: -67.56712, policy_entropy: -5.71622, alpha: 0.01860, time: 51.78859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   238 ----
[CW] collect: return: 192.81366, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 1.55327, qf2_loss: 1.55345, policy_loss: -66.75354, policy_entropy: -5.58064, alpha: 0.01822, time: 51.73217
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   239 ----
[CW] collect: return: 200.34969, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 1.01879, qf2_loss: 1.02539, policy_loss: -67.92919, policy_entropy: -5.80067, alpha: 0.01809, time: 51.71003
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   240 ----
[CW] collect: return: 216.87185, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 1.12543, qf2_loss: 1.12301, policy_loss: -68.76705, policy_entropy: -5.81694, alpha: 0.01800, time: 51.66376
[CW] eval: return: 148.30092, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   241 ----
[CW] collect: return: 47.00707, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 1.33360, qf2_loss: 1.33197, policy_loss: -68.35793, policy_entropy: -5.73442, alpha: 0.01779, time: 51.78108
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   242 ----
[CW] collect: return: 51.95034, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 1.05682, qf2_loss: 1.05469, policy_loss: -67.50349, policy_entropy: -5.23733, alpha: 0.01753, time: 51.28289
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   243 ----
[CW] collect: return: 87.69373, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 1.11450, qf2_loss: 1.11696, policy_loss: -68.20937, policy_entropy: -7.14059, alpha: 0.01749, time: 51.20009
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   244 ----
[CW] collect: return: 220.36482, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 1.74567, qf2_loss: 1.73836, policy_loss: -68.77732, policy_entropy: -7.21873, alpha: 0.01824, time: 51.29801
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   245 ----
[CW] collect: return: 129.39004, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 1.24733, qf2_loss: 1.24205, policy_loss: -69.85817, policy_entropy: -6.97801, alpha: 0.01895, time: 51.54263
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   246 ----
[CW] collect: return: 54.52380, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 1.19047, qf2_loss: 1.19308, policy_loss: -68.73102, policy_entropy: -6.53267, alpha: 0.01946, time: 51.50725
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   247 ----
[CW] collect: return: 110.89925, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 0.96331, qf2_loss: 0.95993, policy_loss: -69.24497, policy_entropy: -6.36857, alpha: 0.01978, time: 51.40637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   248 ----
[CW] collect: return: 61.18711, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 1.17423, qf2_loss: 1.17112, policy_loss: -68.86745, policy_entropy: -5.23452, alpha: 0.01980, time: 51.42086
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   249 ----
[CW] collect: return: 177.87244, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 1.40508, qf2_loss: 1.40666, policy_loss: -69.00991, policy_entropy: -6.24915, alpha: 0.01934, time: 51.65886
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   250 ----
[CW] collect: return: 148.03179, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 1.42767, qf2_loss: 1.42930, policy_loss: -69.15240, policy_entropy: -6.84235, alpha: 0.01977, time: 52.01374
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   251 ----
[CW] collect: return: 264.46243, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 1.62444, qf2_loss: 1.63021, policy_loss: -69.61489, policy_entropy: -6.77350, alpha: 0.02039, time: 51.81104
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   252 ----
[CW] collect: return: 270.84803, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 1.01138, qf2_loss: 1.01093, policy_loss: -69.92795, policy_entropy: -6.66413, alpha: 0.02091, time: 51.53174
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   253 ----
[CW] collect: return: 80.75030, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 1.23892, qf2_loss: 1.22920, policy_loss: -69.25038, policy_entropy: -6.38373, alpha: 0.02136, time: 51.92368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   254 ----
[CW] collect: return: 290.42841, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 1.26516, qf2_loss: 1.26730, policy_loss: -69.40162, policy_entropy: -5.89169, alpha: 0.02149, time: 51.24222
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   255 ----
[CW] collect: return: 254.73688, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 2.43429, qf2_loss: 2.42076, policy_loss: -69.14490, policy_entropy: -5.80256, alpha: 0.02136, time: 50.98961
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   256 ----
[CW] collect: return: 264.82579, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 1.41446, qf2_loss: 1.42202, policy_loss: -69.25100, policy_entropy: -5.82221, alpha: 0.02122, time: 50.78230
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   257 ----
[CW] collect: return: 265.21712, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 1.25016, qf2_loss: 1.25794, policy_loss: -70.08524, policy_entropy: -5.48727, alpha: 0.02097, time: 50.76821
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   258 ----
[CW] collect: return: 246.85672, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 1.13988, qf2_loss: 1.13940, policy_loss: -70.46225, policy_entropy: -5.16577, alpha: 0.02036, time: 50.86003
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   259 ----
[CW] collect: return: 282.20061, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 1.10784, qf2_loss: 1.10791, policy_loss: -70.52321, policy_entropy: -5.20916, alpha: 0.01976, time: 51.03570
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   260 ----
[CW] collect: return: 238.46873, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 1.27650, qf2_loss: 1.27639, policy_loss: -70.53229, policy_entropy: -5.54466, alpha: 0.01931, time: 51.37372
[CW] eval: return: 174.79734, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   261 ----
[CW] collect: return: 192.71736, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 1.81635, qf2_loss: 1.82373, policy_loss: -70.70137, policy_entropy: -5.27160, alpha: 0.01891, time: 51.07564
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   262 ----
[CW] collect: return: 141.29415, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 1.23638, qf2_loss: 1.23617, policy_loss: -70.87365, policy_entropy: -5.25218, alpha: 0.01848, time: 51.21904
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   263 ----
[CW] collect: return: 60.88042, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 1.66835, qf2_loss: 1.66637, policy_loss: -71.37226, policy_entropy: -5.28284, alpha: 0.01800, time: 51.68874
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   264 ----
[CW] collect: return: 116.68228, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 1.23083, qf2_loss: 1.22302, policy_loss: -70.04252, policy_entropy: -5.23841, alpha: 0.01761, time: 51.60734
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   265 ----
[CW] collect: return: 293.06987, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 1.30415, qf2_loss: 1.30883, policy_loss: -71.53764, policy_entropy: -5.32876, alpha: 0.01722, time: 51.49127
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   266 ----
[CW] collect: return: 241.75087, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 1.97018, qf2_loss: 1.97457, policy_loss: -71.47550, policy_entropy: -5.44187, alpha: 0.01689, time: 51.40697
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   267 ----
[CW] collect: return: 221.22263, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 1.51600, qf2_loss: 1.51902, policy_loss: -72.39804, policy_entropy: -5.62421, alpha: 0.01665, time: 51.27090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   268 ----
[CW] collect: return: 200.40989, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 1.65464, qf2_loss: 1.66334, policy_loss: -71.50731, policy_entropy: -5.59988, alpha: 0.01648, time: 51.43401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   269 ----
[CW] collect: return: 107.01973, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 1.37424, qf2_loss: 1.37386, policy_loss: -71.45008, policy_entropy: -5.75722, alpha: 0.01628, time: 51.18115
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   270 ----
[CW] collect: return: 269.11941, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 1.33021, qf2_loss: 1.32770, policy_loss: -71.11692, policy_entropy: -5.75065, alpha: 0.01618, time: 51.29870
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   271 ----
[CW] collect: return: 41.28144, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 1.48512, qf2_loss: 1.48796, policy_loss: -71.50288, policy_entropy: -5.85494, alpha: 0.01610, time: 53.26419
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   272 ----
[CW] collect: return: 113.74602, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 2.47868, qf2_loss: 2.47278, policy_loss: -71.66355, policy_entropy: -5.39598, alpha: 0.01591, time: 51.64718
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   273 ----
[CW] collect: return: 135.64220, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 1.31213, qf2_loss: 1.30339, policy_loss: -71.59509, policy_entropy: -6.23486, alpha: 0.01581, time: 51.43779
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   274 ----
[CW] collect: return: 184.34797, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 1.25525, qf2_loss: 1.25377, policy_loss: -71.06635, policy_entropy: -5.85548, alpha: 0.01585, time: 50.90707
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   275 ----
[CW] collect: return: 93.34605, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 1.17404, qf2_loss: 1.17256, policy_loss: -71.54123, policy_entropy: -6.00523, alpha: 0.01580, time: 50.95104
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   276 ----
[CW] collect: return: 159.56764, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 1.42344, qf2_loss: 1.42226, policy_loss: -71.50713, policy_entropy: -5.71334, alpha: 0.01577, time: 51.75105
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   277 ----
[CW] collect: return: 90.48650, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 1.38521, qf2_loss: 1.37311, policy_loss: -72.50705, policy_entropy: -5.54421, alpha: 0.01556, time: 51.59224
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   278 ----
[CW] collect: return: 222.39564, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 1.52555, qf2_loss: 1.52431, policy_loss: -72.38594, policy_entropy: -5.56567, alpha: 0.01530, time: 51.70943
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   279 ----
[CW] collect: return: 214.24638, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 1.74951, qf2_loss: 1.75262, policy_loss: -72.32451, policy_entropy: -5.35655, alpha: 0.01505, time: 51.56266
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   280 ----
[CW] collect: return: 76.77687, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 1.55388, qf2_loss: 1.55163, policy_loss: -71.91582, policy_entropy: -5.52093, alpha: 0.01478, time: 51.55403
[CW] eval: return: 153.88185, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   281 ----
[CW] collect: return: 80.52139, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 1.60233, qf2_loss: 1.60913, policy_loss: -72.94853, policy_entropy: -5.77805, alpha: 0.01460, time: 51.78339
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   282 ----
[CW] collect: return: 259.37915, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 1.67012, qf2_loss: 1.66916, policy_loss: -72.42649, policy_entropy: -5.65589, alpha: 0.01446, time: 51.08948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   283 ----
[CW] collect: return: 230.87276, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 1.59380, qf2_loss: 1.58134, policy_loss: -71.32814, policy_entropy: -5.47429, alpha: 0.01430, time: 51.70902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   284 ----
[CW] collect: return: 253.79868, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 1.97417, qf2_loss: 1.97338, policy_loss: -72.67070, policy_entropy: -5.59455, alpha: 0.01409, time: 51.02957
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   285 ----
[CW] collect: return: 251.58877, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 1.94910, qf2_loss: 1.95253, policy_loss: -72.97809, policy_entropy: -5.56823, alpha: 0.01390, time: 50.97720
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   286 ----
[CW] collect: return: 159.02031, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 1.66495, qf2_loss: 1.65818, policy_loss: -73.19752, policy_entropy: -5.66846, alpha: 0.01373, time: 51.11229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   287 ----
[CW] collect: return: 163.93472, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 1.42427, qf2_loss: 1.42278, policy_loss: -72.84657, policy_entropy: -5.96617, alpha: 0.01366, time: 51.28267
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   288 ----
[CW] collect: return: 171.38649, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 1.44625, qf2_loss: 1.44174, policy_loss: -72.96485, policy_entropy: -6.28168, alpha: 0.01370, time: 51.00802
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   289 ----
[CW] collect: return: 64.97314, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 1.54223, qf2_loss: 1.53996, policy_loss: -73.58268, policy_entropy: -6.12478, alpha: 0.01379, time: 51.27747
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   290 ----
[CW] collect: return: 211.54678, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 1.63506, qf2_loss: 1.61745, policy_loss: -72.61113, policy_entropy: -6.20372, alpha: 0.01385, time: 52.08317
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   291 ----
[CW] collect: return: 173.65372, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 2.44608, qf2_loss: 2.44822, policy_loss: -73.61843, policy_entropy: -6.30948, alpha: 0.01398, time: 51.78401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   292 ----
[CW] collect: return: 195.95690, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 2.00484, qf2_loss: 2.00445, policy_loss: -73.18690, policy_entropy: -5.96014, alpha: 0.01405, time: 51.76019
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   293 ----
[CW] collect: return: 245.97402, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 1.60964, qf2_loss: 1.61182, policy_loss: -73.86318, policy_entropy: -6.47861, alpha: 0.01413, time: 51.75757
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   294 ----
[CW] collect: return: 218.33917, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 1.53523, qf2_loss: 1.52240, policy_loss: -73.27899, policy_entropy: -6.09679, alpha: 0.01430, time: 51.87812
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   295 ----
[CW] collect: return: 210.42252, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 3.07442, qf2_loss: 3.05755, policy_loss: -75.05921, policy_entropy: -6.14705, alpha: 0.01436, time: 51.87172
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   296 ----
[CW] collect: return: 263.45680, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 1.92747, qf2_loss: 1.92678, policy_loss: -74.17107, policy_entropy: -5.95910, alpha: 0.01435, time: 51.41657
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   297 ----
[CW] collect: return: 187.60860, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 1.72768, qf2_loss: 1.72598, policy_loss: -74.42581, policy_entropy: -6.39063, alpha: 0.01442, time: 51.40559
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   298 ----
[CW] collect: return: 148.72958, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 1.81811, qf2_loss: 1.81118, policy_loss: -74.63684, policy_entropy: -6.48479, alpha: 0.01468, time: 50.99155
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   299 ----
[CW] collect: return: 239.93602, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 1.89888, qf2_loss: 1.89416, policy_loss: -74.66627, policy_entropy: -6.27820, alpha: 0.01490, time: 51.50802
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   300 ----
[CW] collect: return: 204.96021, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 2.14046, qf2_loss: 2.13256, policy_loss: -74.68328, policy_entropy: -6.16915, alpha: 0.01501, time: 51.59021
[CW] eval: return: 201.67973, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   301 ----
[CW] collect: return: 147.49770, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 2.08011, qf2_loss: 2.07283, policy_loss: -74.89176, policy_entropy: -6.22088, alpha: 0.01517, time: 50.97940
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   302 ----
[CW] collect: return: 296.85005, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 1.84995, qf2_loss: 1.83740, policy_loss: -75.08987, policy_entropy: -6.15686, alpha: 0.01524, time: 51.42687
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   303 ----
[CW] collect: return: 241.15299, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 1.94086, qf2_loss: 1.91843, policy_loss: -75.45141, policy_entropy: -5.97134, alpha: 0.01535, time: 51.48919
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   304 ----
[CW] collect: return: 309.47896, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 2.42272, qf2_loss: 2.40484, policy_loss: -75.23494, policy_entropy: -5.95009, alpha: 0.01524, time: 51.47640
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   305 ----
[CW] collect: return: 230.64977, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 1.72934, qf2_loss: 1.71656, policy_loss: -74.97443, policy_entropy: -5.81265, alpha: 0.01521, time: 51.78950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   306 ----
[CW] collect: return: 262.01126, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 1.92846, qf2_loss: 1.91624, policy_loss: -75.47311, policy_entropy: -6.02612, alpha: 0.01513, time: 51.75746
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   307 ----
[CW] collect: return: 303.42128, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 1.95062, qf2_loss: 1.95243, policy_loss: -75.15001, policy_entropy: -6.29606, alpha: 0.01522, time: 51.62351
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   308 ----
[CW] collect: return: 78.85778, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 2.56096, qf2_loss: 2.54260, policy_loss: -76.40441, policy_entropy: -6.10466, alpha: 0.01541, time: 52.11796
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   309 ----
[CW] collect: return: 249.91247, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 2.08558, qf2_loss: 2.06558, policy_loss: -74.43735, policy_entropy: -5.43721, alpha: 0.01526, time: 51.87616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   310 ----
[CW] collect: return: 66.67890, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 2.25891, qf2_loss: 2.24489, policy_loss: -75.59544, policy_entropy: -5.66519, alpha: 0.01499, time: 52.20564
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   311 ----
[CW] collect: return: 187.19900, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 2.13355, qf2_loss: 2.11894, policy_loss: -75.87362, policy_entropy: -5.94158, alpha: 0.01484, time: 52.06910
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   312 ----
[CW] collect: return: 83.45551, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 2.27934, qf2_loss: 2.27275, policy_loss: -75.95282, policy_entropy: -6.11741, alpha: 0.01491, time: 51.83040
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   313 ----
[CW] collect: return: 130.76327, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 2.20374, qf2_loss: 2.19439, policy_loss: -76.73397, policy_entropy: -5.95350, alpha: 0.01488, time: 51.65490
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   314 ----
[CW] collect: return: 169.18407, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 5.49237, qf2_loss: 5.45530, policy_loss: -76.23133, policy_entropy: -6.04362, alpha: 0.01488, time: 52.15758
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   315 ----
[CW] collect: return: 160.16752, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 3.42219, qf2_loss: 3.40484, policy_loss: -75.05294, policy_entropy: -5.82422, alpha: 0.01485, time: 52.29823
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   316 ----
[CW] collect: return: 129.82149, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 2.32806, qf2_loss: 2.31139, policy_loss: -77.37822, policy_entropy: -6.43213, alpha: 0.01488, time: 51.69830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   317 ----
[CW] collect: return: 177.55725, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 1.94874, qf2_loss: 1.94016, policy_loss: -75.58231, policy_entropy: -6.44683, alpha: 0.01512, time: 51.67705
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   318 ----
[CW] collect: return: 240.82155, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 2.09156, qf2_loss: 2.07894, policy_loss: -76.78034, policy_entropy: -6.49002, alpha: 0.01544, time: 51.65178
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   319 ----
[CW] collect: return: 67.85015, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 2.30526, qf2_loss: 2.28597, policy_loss: -77.14293, policy_entropy: -6.39044, alpha: 0.01571, time: 51.52996
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   320 ----
[CW] collect: return: 182.72917, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 2.02845, qf2_loss: 2.01044, policy_loss: -77.19097, policy_entropy: -6.35399, alpha: 0.01593, time: 51.51569
[CW] eval: return: 157.60446, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   321 ----
[CW] collect: return: 205.16377, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 2.23626, qf2_loss: 2.21779, policy_loss: -76.46349, policy_entropy: -5.85785, alpha: 0.01600, time: 51.50708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   322 ----
[CW] collect: return: 153.25271, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 2.50691, qf2_loss: 2.48244, policy_loss: -77.62170, policy_entropy: -6.14860, alpha: 0.01601, time: 51.17325
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   323 ----
[CW] collect: return: 36.50638, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 4.66293, qf2_loss: 4.64530, policy_loss: -75.92247, policy_entropy: -5.76090, alpha: 0.01602, time: 51.10392
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   324 ----
[CW] collect: return: 89.18714, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 2.52637, qf2_loss: 2.50734, policy_loss: -76.06413, policy_entropy: -5.73747, alpha: 0.01582, time: 50.98558
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   325 ----
[CW] collect: return: 76.57545, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 2.28190, qf2_loss: 2.26542, policy_loss: -77.34134, policy_entropy: -5.86244, alpha: 0.01575, time: 51.02081
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   326 ----
[CW] collect: return: 63.63414, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 2.32233, qf2_loss: 2.30819, policy_loss: -77.13974, policy_entropy: -5.66983, alpha: 0.01559, time: 51.11996
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   327 ----
[CW] collect: return: 236.09794, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 2.70926, qf2_loss: 2.69740, policy_loss: -77.13559, policy_entropy: -5.72789, alpha: 0.01538, time: 51.06929
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   328 ----
[CW] collect: return: 101.88535, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 3.19270, qf2_loss: 3.17416, policy_loss: -77.14541, policy_entropy: -5.92588, alpha: 0.01527, time: 51.17576
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   329 ----
[CW] collect: return: 52.17163, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 2.23907, qf2_loss: 2.23100, policy_loss: -77.01757, policy_entropy: -6.30574, alpha: 0.01536, time: 51.09972
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   330 ----
[CW] collect: return: 124.63530, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 2.75846, qf2_loss: 2.74269, policy_loss: -77.22859, policy_entropy: -6.28937, alpha: 0.01553, time: 51.39363
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   331 ----
[CW] collect: return: 228.23864, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 5.19577, qf2_loss: 5.17378, policy_loss: -78.08495, policy_entropy: -6.18885, alpha: 0.01570, time: 51.29223
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   332 ----
[CW] collect: return: 67.52541, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 3.33150, qf2_loss: 3.29839, policy_loss: -77.40283, policy_entropy: -5.76469, alpha: 0.01568, time: 51.14702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   333 ----
[CW] collect: return: 22.18115, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 2.60156, qf2_loss: 2.57167, policy_loss: -76.53320, policy_entropy: -5.68520, alpha: 0.01549, time: 51.08572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   334 ----
[CW] collect: return: 266.47108, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 2.88137, qf2_loss: 2.85014, policy_loss: -77.80145, policy_entropy: -5.93813, alpha: 0.01542, time: 51.06108
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   335 ----
[CW] collect: return: 125.78081, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 3.08606, qf2_loss: 3.07483, policy_loss: -76.95703, policy_entropy: -5.81790, alpha: 0.01534, time: 51.31192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   336 ----
[CW] collect: return: 342.60088, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 2.48260, qf2_loss: 2.46371, policy_loss: -77.79108, policy_entropy: -5.73737, alpha: 0.01520, time: 51.12153
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   337 ----
[CW] collect: return: 40.64168, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 2.58140, qf2_loss: 2.56088, policy_loss: -78.22665, policy_entropy: -5.96952, alpha: 0.01508, time: 50.92343
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   338 ----
[CW] collect: return: 333.23079, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 2.71902, qf2_loss: 2.69531, policy_loss: -77.02045, policy_entropy: -5.81278, alpha: 0.01506, time: 50.93890
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   339 ----
[CW] collect: return: 181.88639, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 2.84504, qf2_loss: 2.81681, policy_loss: -77.96632, policy_entropy: -6.01207, alpha: 0.01500, time: 50.84228
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   340 ----
[CW] collect: return: 179.41153, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 2.96376, qf2_loss: 2.93950, policy_loss: -78.04950, policy_entropy: -6.16047, alpha: 0.01501, time: 51.03321
[CW] eval: return: 195.11482, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   341 ----
[CW] collect: return: 144.27471, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 3.32515, qf2_loss: 3.29486, policy_loss: -77.97342, policy_entropy: -7.71732, alpha: 0.01546, time: 51.06022
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   342 ----
[CW] collect: return: 264.60486, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 3.31806, qf2_loss: 3.29856, policy_loss: -78.80229, policy_entropy: -7.30503, alpha: 0.01629, time: 50.93568
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   343 ----
[CW] collect: return: 109.87230, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 4.66514, qf2_loss: 4.64443, policy_loss: -78.43386, policy_entropy: -7.19753, alpha: 0.01695, time: 51.63160
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   344 ----
[CW] collect: return: 12.55109, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 3.21758, qf2_loss: 3.21676, policy_loss: -77.69960, policy_entropy: -6.91025, alpha: 0.01753, time: 50.90802
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   345 ----
[CW] collect: return: 110.69952, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 2.68805, qf2_loss: 2.66132, policy_loss: -78.58402, policy_entropy: -7.06574, alpha: 0.01810, time: 51.14253
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   346 ----
[CW] collect: return: 274.23529, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 2.85851, qf2_loss: 2.83533, policy_loss: -79.29990, policy_entropy: -7.01173, alpha: 0.01872, time: 51.12812
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   347 ----
[CW] collect: return: 61.78359, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 2.65151, qf2_loss: 2.62612, policy_loss: -79.07394, policy_entropy: -7.00715, alpha: 0.01939, time: 51.13070
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   348 ----
[CW] collect: return: 224.42506, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 3.46784, qf2_loss: 3.43750, policy_loss: -79.26888, policy_entropy: -6.58389, alpha: 0.01994, time: 51.30315
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   349 ----
[CW] collect: return: 302.04771, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 3.81424, qf2_loss: 3.78578, policy_loss: -78.82261, policy_entropy: -6.34922, alpha: 0.02021, time: 51.06787
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   350 ----
[CW] collect: return: 254.44814, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 3.64524, qf2_loss: 3.61178, policy_loss: -79.04760, policy_entropy: -6.03789, alpha: 0.02055, time: 50.90549
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   351 ----
[CW] collect: return: 115.32354, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 2.70461, qf2_loss: 2.66949, policy_loss: -79.28286, policy_entropy: -6.10264, alpha: 0.02043, time: 50.76176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   352 ----
[CW] collect: return: 287.25387, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 3.13824, qf2_loss: 3.11454, policy_loss: -79.60229, policy_entropy: -5.60407, alpha: 0.02043, time: 50.75201
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   353 ----
[CW] collect: return: 246.06784, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 2.91195, qf2_loss: 2.90144, policy_loss: -79.47480, policy_entropy: -5.44389, alpha: 0.02007, time: 50.86919
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   354 ----
[CW] collect: return: 96.85872, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 3.25674, qf2_loss: 3.23220, policy_loss: -79.04406, policy_entropy: -5.48337, alpha: 0.01966, time: 50.73814
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   355 ----
[CW] collect: return: 107.44899, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 5.74542, qf2_loss: 5.71646, policy_loss: -78.52954, policy_entropy: -5.50508, alpha: 0.01935, time: 50.80400
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   356 ----
[CW] collect: return: 132.22711, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 3.46471, qf2_loss: 3.43854, policy_loss: -78.97369, policy_entropy: -5.65733, alpha: 0.01904, time: 51.13017
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   357 ----
[CW] collect: return: 66.34893, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 2.63880, qf2_loss: 2.61071, policy_loss: -77.67304, policy_entropy: -5.30966, alpha: 0.01878, time: 51.53625
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   358 ----
[CW] collect: return: 171.34012, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 2.54764, qf2_loss: 2.53038, policy_loss: -80.28607, policy_entropy: -5.60942, alpha: 0.01839, time: 50.99074
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   359 ----
[CW] collect: return: 143.88113, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 2.61119, qf2_loss: 2.58129, policy_loss: -79.04724, policy_entropy: -5.45763, alpha: 0.01815, time: 51.01718
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   360 ----
[CW] collect: return: 234.61822, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 3.38414, qf2_loss: 3.35811, policy_loss: -78.58232, policy_entropy: -5.33371, alpha: 0.01783, time: 51.07790
[CW] eval: return: 177.45967, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   361 ----
[CW] collect: return: 78.50040, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 4.12603, qf2_loss: 4.09781, policy_loss: -79.30210, policy_entropy: -5.50180, alpha: 0.01753, time: 51.19716
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   362 ----
[CW] collect: return: 300.68761, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 4.01832, qf2_loss: 3.98521, policy_loss: -79.11413, policy_entropy: -5.41170, alpha: 0.01727, time: 51.02436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   363 ----
[CW] collect: return: 242.13725, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 3.62525, qf2_loss: 3.58069, policy_loss: -79.90901, policy_entropy: -5.52064, alpha: 0.01699, time: 50.82714
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   364 ----
[CW] collect: return: 111.71255, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 4.46014, qf2_loss: 4.43319, policy_loss: -80.28120, policy_entropy: -5.80018, alpha: 0.01680, time: 50.82325
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   365 ----
[CW] collect: return: 162.02543, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 4.21706, qf2_loss: 4.16460, policy_loss: -79.40899, policy_entropy: -6.69553, alpha: 0.01689, time: 50.83358
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   366 ----
[CW] collect: return: 108.44373, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 3.13646, qf2_loss: 3.10355, policy_loss: -79.41281, policy_entropy: -6.82816, alpha: 0.01727, time: 50.75186
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   367 ----
[CW] collect: return: 113.75269, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 3.27567, qf2_loss: 3.25349, policy_loss: -78.68241, policy_entropy: -6.56002, alpha: 0.01765, time: 50.64655
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   368 ----
[CW] collect: return: 385.78157, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 5.41035, qf2_loss: 5.39650, policy_loss: -79.68101, policy_entropy: -6.48512, alpha: 0.01795, time: 50.51616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   369 ----
[CW] collect: return: 36.06603, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 3.58962, qf2_loss: 3.54758, policy_loss: -79.35916, policy_entropy: -6.46382, alpha: 0.01821, time: 50.55636
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   370 ----
[CW] collect: return: 314.39485, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 2.90994, qf2_loss: 2.88804, policy_loss: -80.28365, policy_entropy: -6.23958, alpha: 0.01843, time: 50.62308
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   371 ----
[CW] collect: return: 180.89278, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 4.20202, qf2_loss: 4.16299, policy_loss: -80.31892, policy_entropy: -5.95207, alpha: 0.01853, time: 50.71274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   372 ----
[CW] collect: return: 185.11542, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 3.70038, qf2_loss: 3.65698, policy_loss: -80.22449, policy_entropy: -5.43595, alpha: 0.01836, time: 50.85630
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   373 ----
[CW] collect: return: 149.34320, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 3.65269, qf2_loss: 3.60988, policy_loss: -80.83872, policy_entropy: -5.45666, alpha: 0.01799, time: 50.86454
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   374 ----
[CW] collect: return: 200.29905, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 3.58005, qf2_loss: 3.56100, policy_loss: -80.91764, policy_entropy: -5.42695, alpha: 0.01766, time: 50.89723
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   375 ----
[CW] collect: return: 159.46068, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 4.42186, qf2_loss: 4.39065, policy_loss: -79.93644, policy_entropy: -5.65277, alpha: 0.01738, time: 50.95937
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   376 ----
[CW] collect: return: 279.22041, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 4.18424, qf2_loss: 4.14171, policy_loss: -79.66050, policy_entropy: -6.05887, alpha: 0.01729, time: 50.91947
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   377 ----
[CW] collect: return: 63.22734, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 3.42579, qf2_loss: 3.39723, policy_loss: -79.95508, policy_entropy: -5.80534, alpha: 0.01727, time: 50.78391
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   378 ----
[CW] collect: return: 345.04057, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 3.44471, qf2_loss: 3.41338, policy_loss: -79.64106, policy_entropy: -6.05951, alpha: 0.01720, time: 50.40564
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   379 ----
[CW] collect: return: 120.46799, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 3.78819, qf2_loss: 3.74742, policy_loss: -81.46249, policy_entropy: -6.94646, alpha: 0.01747, time: 50.49853
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   380 ----
[CW] collect: return: 278.30897, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 4.78764, qf2_loss: 4.74433, policy_loss: -80.60883, policy_entropy: -6.96252, alpha: 0.01805, time: 50.66418
[CW] eval: return: 197.36944, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   381 ----
[CW] collect: return: 295.11260, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 4.41353, qf2_loss: 4.36610, policy_loss: -80.80817, policy_entropy: -7.16671, alpha: 0.01867, time: 50.55862
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   382 ----
[CW] collect: return: 288.48821, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 4.43794, qf2_loss: 4.42042, policy_loss: -80.43557, policy_entropy: -7.19174, alpha: 0.01941, time: 50.68621
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   383 ----
[CW] collect: return: 90.96737, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 4.57851, qf2_loss: 4.54968, policy_loss: -80.87360, policy_entropy: -6.96987, alpha: 0.02015, time: 50.58269
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   384 ----
[CW] collect: return: 315.98774, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 4.33906, qf2_loss: 4.28090, policy_loss: -80.22514, policy_entropy: -6.74681, alpha: 0.02075, time: 51.60074
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   385 ----
[CW] collect: return: 237.37345, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 3.77226, qf2_loss: 3.74760, policy_loss: -81.80440, policy_entropy: -6.82663, alpha: 0.02125, time: 50.86565
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   386 ----
[CW] collect: return: 246.19597, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 7.93593, qf2_loss: 7.93278, policy_loss: -80.57854, policy_entropy: -5.85812, alpha: 0.02167, time: 50.68256
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   387 ----
[CW] collect: return: 136.50725, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 7.14204, qf2_loss: 7.12116, policy_loss: -80.55771, policy_entropy: -5.34770, alpha: 0.02132, time: 50.93342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   388 ----
[CW] collect: return: 301.07718, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 4.38170, qf2_loss: 4.35674, policy_loss: -81.84564, policy_entropy: -5.51497, alpha: 0.02092, time: 50.76255
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   389 ----
[CW] collect: return: 113.63505, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 3.60644, qf2_loss: 3.57331, policy_loss: -80.79757, policy_entropy: -5.37188, alpha: 0.02055, time: 50.76721
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   390 ----
[CW] collect: return: 90.52665, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 3.74950, qf2_loss: 3.73414, policy_loss: -81.93204, policy_entropy: -5.48182, alpha: 0.02014, time: 50.98435
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   391 ----
[CW] collect: return: 170.01437, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 3.94775, qf2_loss: 3.91060, policy_loss: -81.94653, policy_entropy: -5.60616, alpha: 0.01988, time: 50.66156
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   392 ----
[CW] collect: return: 251.51815, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 3.85437, qf2_loss: 3.80753, policy_loss: -81.35725, policy_entropy: -5.66746, alpha: 0.01965, time: 50.56699
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   393 ----
[CW] collect: return: 244.63542, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 4.68688, qf2_loss: 4.67038, policy_loss: -81.70781, policy_entropy: -5.96688, alpha: 0.01951, time: 50.61303
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   394 ----
[CW] collect: return: 106.92586, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 6.42467, qf2_loss: 6.34520, policy_loss: -80.88435, policy_entropy: -5.92994, alpha: 0.01948, time: 50.48032
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   395 ----
[CW] collect: return: 314.95093, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 4.46682, qf2_loss: 4.41028, policy_loss: -82.65815, policy_entropy: -5.99085, alpha: 0.01947, time: 50.59732
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   396 ----
[CW] collect: return: 80.96723, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 4.46980, qf2_loss: 4.38428, policy_loss: -81.51994, policy_entropy: -5.79919, alpha: 0.01946, time: 50.55230
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   397 ----
[CW] collect: return: 320.86541, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 3.98771, qf2_loss: 3.95044, policy_loss: -81.32331, policy_entropy: -5.43208, alpha: 0.01918, time: 50.46912
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   398 ----
[CW] collect: return: 131.48731, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 4.09337, qf2_loss: 4.06730, policy_loss: -82.04795, policy_entropy: -5.63039, alpha: 0.01890, time: 50.62245
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   399 ----
[CW] collect: return: 278.16704, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 4.52733, qf2_loss: 4.49100, policy_loss: -81.79852, policy_entropy: -5.70244, alpha: 0.01871, time: 50.79824
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   400 ----
[CW] collect: return: 150.47102, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 4.61313, qf2_loss: 4.57183, policy_loss: -81.71032, policy_entropy: -6.23349, alpha: 0.01864, time: 50.91116
[CW] eval: return: 187.97048, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   401 ----
[CW] collect: return: 307.09940, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 5.12999, qf2_loss: 5.10309, policy_loss: -82.17929, policy_entropy: -6.16253, alpha: 0.01879, time: 51.10491
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   402 ----
[CW] collect: return: 212.85144, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 7.42873, qf2_loss: 7.41049, policy_loss: -82.77456, policy_entropy: -5.76562, alpha: 0.01876, time: 50.67096
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   403 ----
[CW] collect: return: 251.00245, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 5.77491, qf2_loss: 5.75369, policy_loss: -82.58274, policy_entropy: -5.79838, alpha: 0.01865, time: 50.73450
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   404 ----
[CW] collect: return: 303.85336, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 5.75275, qf2_loss: 5.69087, policy_loss: -81.58424, policy_entropy: -5.59770, alpha: 0.01844, time: 50.72280
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   405 ----
[CW] collect: return: 112.26191, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 4.42165, qf2_loss: 4.40534, policy_loss: -81.66570, policy_entropy: -5.65407, alpha: 0.01826, time: 50.46936
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   406 ----
[CW] collect: return: 333.23504, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 4.20460, qf2_loss: 4.17097, policy_loss: -83.67437, policy_entropy: -6.10678, alpha: 0.01816, time: 50.52507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   407 ----
[CW] collect: return: 94.39014, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 4.90816, qf2_loss: 4.87169, policy_loss: -83.37054, policy_entropy: -6.11048, alpha: 0.01821, time: 50.65371
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   408 ----
[CW] collect: return: 156.08325, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 4.97886, qf2_loss: 4.92931, policy_loss: -82.87444, policy_entropy: -6.08404, alpha: 0.01827, time: 50.62043
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   409 ----
[CW] collect: return: 58.67801, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 6.01849, qf2_loss: 6.00099, policy_loss: -81.95528, policy_entropy: -5.74002, alpha: 0.01826, time: 50.69613
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   410 ----
[CW] collect: return: 244.65956, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 5.51444, qf2_loss: 5.46890, policy_loss: -83.91589, policy_entropy: -5.92769, alpha: 0.01815, time: 50.65807
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   411 ----
[CW] collect: return: 168.33114, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 4.90200, qf2_loss: 4.87326, policy_loss: -82.35481, policy_entropy: -5.66967, alpha: 0.01805, time: 50.51959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   412 ----
[CW] collect: return: 100.55136, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 11.35434, qf2_loss: 11.29883, policy_loss: -83.16534, policy_entropy: -5.95051, alpha: 0.01785, time: 50.85737
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   413 ----
[CW] collect: return: 187.85172, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 12.16174, qf2_loss: 12.16927, policy_loss: -83.30452, policy_entropy: -6.11217, alpha: 0.01793, time: 50.83294
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   414 ----
[CW] collect: return: 94.30887, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 7.15409, qf2_loss: 7.12192, policy_loss: -82.53204, policy_entropy: -6.04117, alpha: 0.01794, time: 50.82718
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   415 ----
[CW] collect: return: 81.17561, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 4.83796, qf2_loss: 4.81625, policy_loss: -82.03625, policy_entropy: -5.74440, alpha: 0.01791, time: 50.83730
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   416 ----
[CW] collect: return: 311.79370, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 4.38049, qf2_loss: 4.33573, policy_loss: -83.51285, policy_entropy: -5.74332, alpha: 0.01773, time: 52.34742
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   417 ----
[CW] collect: return: 183.07612, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 4.47693, qf2_loss: 4.46803, policy_loss: -83.57124, policy_entropy: -5.89859, alpha: 0.01765, time: 51.10010
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   418 ----
[CW] collect: return: 228.17805, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 4.20827, qf2_loss: 4.18122, policy_loss: -83.01986, policy_entropy: -5.86717, alpha: 0.01756, time: 52.24273
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   419 ----
[CW] collect: return: 43.45013, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 5.07178, qf2_loss: 5.04363, policy_loss: -83.13855, policy_entropy: -5.80528, alpha: 0.01747, time: 50.82108
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   420 ----
[CW] collect: return: 274.44854, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 4.82199, qf2_loss: 4.80316, policy_loss: -83.41145, policy_entropy: -6.02483, alpha: 0.01741, time: 50.43193
[CW] eval: return: 171.77527, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   421 ----
[CW] collect: return: 110.72009, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 7.69482, qf2_loss: 7.68780, policy_loss: -82.63683, policy_entropy: -5.73404, alpha: 0.01736, time: 50.49141
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   422 ----
[CW] collect: return: 324.38625, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 10.73490, qf2_loss: 10.67605, policy_loss: -83.84936, policy_entropy: -6.44601, alpha: 0.01736, time: 50.48139
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   423 ----
[CW] collect: return: 39.36873, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 7.34756, qf2_loss: 7.30819, policy_loss: -84.19413, policy_entropy: -7.13476, alpha: 0.01774, time: 50.35230
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   424 ----
[CW] collect: return: 98.61251, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 7.01270, qf2_loss: 6.97469, policy_loss: -84.01397, policy_entropy: -6.71520, alpha: 0.01846, time: 50.46007
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   425 ----
[CW] collect: return: 231.40350, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 6.23562, qf2_loss: 6.22858, policy_loss: -84.18708, policy_entropy: -5.92809, alpha: 0.01855, time: 50.85613
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   426 ----
[CW] collect: return: 259.40011, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 5.64871, qf2_loss: 5.62456, policy_loss: -83.83112, policy_entropy: -5.90379, alpha: 0.01857, time: 50.87614
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   427 ----
[CW] collect: return: 167.08031, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 5.04619, qf2_loss: 5.01855, policy_loss: -83.77084, policy_entropy: -5.96089, alpha: 0.01851, time: 50.93926
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   428 ----
[CW] collect: return: 236.23804, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 6.73743, qf2_loss: 6.70830, policy_loss: -83.75942, policy_entropy: -5.76013, alpha: 0.01843, time: 50.96052
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   429 ----
[CW] collect: return: 299.74464, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 6.48228, qf2_loss: 6.49217, policy_loss: -86.67806, policy_entropy: -6.15770, alpha: 0.01840, time: 51.25467
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   430 ----
[CW] collect: return: 301.38929, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 6.89937, qf2_loss: 6.87462, policy_loss: -85.42716, policy_entropy: -6.17556, alpha: 0.01844, time: 50.63306
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   431 ----
[CW] collect: return: 232.57066, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 11.63991, qf2_loss: 11.62941, policy_loss: -85.12586, policy_entropy: -6.25582, alpha: 0.01864, time: 50.71244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   432 ----
[CW] collect: return: 177.24021, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 7.02055, qf2_loss: 6.98056, policy_loss: -84.93939, policy_entropy: -5.98283, alpha: 0.01873, time: 50.68687
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   433 ----
[CW] collect: return: 192.28895, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 5.79817, qf2_loss: 5.80003, policy_loss: -84.30645, policy_entropy: -6.08790, alpha: 0.01871, time: 50.45283
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   434 ----
[CW] collect: return: 285.95360, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 6.55000, qf2_loss: 6.51653, policy_loss: -86.32939, policy_entropy: -6.99819, alpha: 0.01900, time: 50.35869
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   435 ----
[CW] collect: return: 270.32875, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 6.89080, qf2_loss: 6.87934, policy_loss: -85.66165, policy_entropy: -6.87115, alpha: 0.01963, time: 50.48107
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   436 ----
[CW] collect: return: 147.87408, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 8.85604, qf2_loss: 8.80870, policy_loss: -85.91015, policy_entropy: -6.81501, alpha: 0.02023, time: 50.52623
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   437 ----
[CW] collect: return: 109.45848, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 7.59086, qf2_loss: 7.59865, policy_loss: -85.54589, policy_entropy: -6.59502, alpha: 0.02070, time: 50.59162
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   438 ----
[CW] collect: return: 80.99578, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 6.44157, qf2_loss: 6.41374, policy_loss: -85.15194, policy_entropy: -6.56659, alpha: 0.02115, time: 50.40034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   439 ----
[CW] collect: return: 88.02143, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 7.67802, qf2_loss: 7.61685, policy_loss: -86.22901, policy_entropy: -5.48052, alpha: 0.02128, time: 50.32265
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   440 ----
[CW] collect: return: 271.62716, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 7.26838, qf2_loss: 7.22991, policy_loss: -85.61459, policy_entropy: -5.31709, alpha: 0.02080, time: 50.72902
[CW] eval: return: 225.52685, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   441 ----
[CW] collect: return: 98.59343, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 10.05550, qf2_loss: 10.00242, policy_loss: -85.58268, policy_entropy: -5.48536, alpha: 0.02035, time: 50.77982
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   442 ----
[CW] collect: return: 282.49955, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 9.09134, qf2_loss: 9.04866, policy_loss: -85.23106, policy_entropy: -6.47180, alpha: 0.02024, time: 50.89729
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   443 ----
[CW] collect: return: 131.87912, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 13.49353, qf2_loss: 13.53899, policy_loss: -85.86551, policy_entropy: -6.01078, alpha: 0.02059, time: 51.01867
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   444 ----
[CW] collect: return: 288.90725, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 10.73189, qf2_loss: 10.65395, policy_loss: -85.57124, policy_entropy: -5.67296, alpha: 0.02041, time: 51.09159
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   445 ----
[CW] collect: return: 100.81131, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 7.65552, qf2_loss: 7.57166, policy_loss: -87.24517, policy_entropy: -6.02642, alpha: 0.02029, time: 50.60097
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   446 ----
[CW] collect: return: 36.77150, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 11.27855, qf2_loss: 11.28255, policy_loss: -85.86195, policy_entropy: -6.11449, alpha: 0.02030, time: 50.39679
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   447 ----
[CW] collect: return: 100.26570, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 11.23807, qf2_loss: 11.22012, policy_loss: -86.56510, policy_entropy: -8.02438, alpha: 0.02093, time: 50.43689
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   448 ----
[CW] collect: return: 332.62055, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 14.12448, qf2_loss: 14.17147, policy_loss: -85.80993, policy_entropy: -7.77914, alpha: 0.02203, time: 50.25952
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   449 ----
[CW] collect: return: 246.73426, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 8.45085, qf2_loss: 8.40663, policy_loss: -85.67184, policy_entropy: -7.42010, alpha: 0.02305, time: 50.35499
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   450 ----
[CW] collect: return: 164.37104, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 7.66398, qf2_loss: 7.65752, policy_loss: -87.20403, policy_entropy: -7.06407, alpha: 0.02388, time: 50.44322
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   451 ----
[CW] collect: return: 141.88362, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 7.96322, qf2_loss: 7.98452, policy_loss: -86.72854, policy_entropy: -6.48718, alpha: 0.02439, time: 50.32519
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   452 ----
[CW] collect: return: 85.83165, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 7.46201, qf2_loss: 7.39569, policy_loss: -87.62233, policy_entropy: -6.41975, alpha: 0.02466, time: 50.51251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   453 ----
[CW] collect: return: 217.00468, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 8.42310, qf2_loss: 8.43275, policy_loss: -87.19858, policy_entropy: -6.13207, alpha: 0.02496, time: 50.53274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   454 ----
[CW] collect: return: 146.80935, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 9.51035, qf2_loss: 9.42216, policy_loss: -87.28977, policy_entropy: -5.98319, alpha: 0.02495, time: 50.56977
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   455 ----
[CW] collect: return: 299.24225, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 11.06303, qf2_loss: 11.11775, policy_loss: -87.72512, policy_entropy: -6.12176, alpha: 0.02499, time: 50.79359
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   456 ----
[CW] collect: return: 325.32857, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 11.87444, qf2_loss: 11.81432, policy_loss: -87.05906, policy_entropy: -6.06045, alpha: 0.02507, time: 50.70090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   457 ----
[CW] collect: return: 161.14201, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 10.05349, qf2_loss: 10.02129, policy_loss: -88.58851, policy_entropy: -5.66621, alpha: 0.02496, time: 50.84884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   458 ----
[CW] collect: return: 102.88361, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 9.43241, qf2_loss: 9.35285, policy_loss: -87.45762, policy_entropy: -5.65622, alpha: 0.02467, time: 52.00140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   459 ----
[CW] collect: return: 171.84311, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 8.67562, qf2_loss: 8.62676, policy_loss: -85.63893, policy_entropy: -5.37239, alpha: 0.02436, time: 50.60420
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   460 ----
[CW] collect: return: 44.90946, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 9.91639, qf2_loss: 9.84119, policy_loss: -87.26622, policy_entropy: -5.61889, alpha: 0.02398, time: 50.52962
[CW] eval: return: 212.71038, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   461 ----
[CW] collect: return: 94.52756, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 12.75401, qf2_loss: 12.77132, policy_loss: -88.30506, policy_entropy: -6.17268, alpha: 0.02384, time: 50.46973
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   462 ----
[CW] collect: return: 257.83628, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 11.64744, qf2_loss: 11.61340, policy_loss: -87.85937, policy_entropy: -5.96494, alpha: 0.02394, time: 50.35117
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   463 ----
[CW] collect: return: 13.96554, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 18.13152, qf2_loss: 18.02832, policy_loss: -88.72766, policy_entropy: -6.03780, alpha: 0.02383, time: 50.45134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   464 ----
[CW] collect: return: 330.21921, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 13.67744, qf2_loss: 13.67290, policy_loss: -88.27060, policy_entropy: -6.23749, alpha: 0.02415, time: 50.50820
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   465 ----
[CW] collect: return: 159.05862, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 10.89349, qf2_loss: 10.78700, policy_loss: -87.30175, policy_entropy: -6.21338, alpha: 0.02418, time: 50.32820
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   466 ----
[CW] collect: return: 219.63665, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 11.27774, qf2_loss: 11.25393, policy_loss: -87.27547, policy_entropy: -5.53983, alpha: 0.02408, time: 50.48108
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   467 ----
[CW] collect: return: 221.03126, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 13.75074, qf2_loss: 13.77147, policy_loss: -87.34609, policy_entropy: -5.90010, alpha: 0.02389, time: 50.60124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   468 ----
[CW] collect: return: 54.40013, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 12.36100, qf2_loss: 12.28397, policy_loss: -88.45255, policy_entropy: -5.81944, alpha: 0.02377, time: 50.55847
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   469 ----
[CW] collect: return: 280.20276, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 11.03878, qf2_loss: 11.02069, policy_loss: -89.02418, policy_entropy: -5.87776, alpha: 0.02367, time: 50.74249
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   470 ----
[CW] collect: return: 186.56013, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 11.86135, qf2_loss: 11.79002, policy_loss: -87.78022, policy_entropy: -5.82679, alpha: 0.02355, time: 50.79896
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   471 ----
[CW] collect: return: 171.77336, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 10.79727, qf2_loss: 10.70488, policy_loss: -88.40724, policy_entropy: -5.58399, alpha: 0.02335, time: 50.65522
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   472 ----
[CW] collect: return: 53.57553, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 16.46532, qf2_loss: 16.37372, policy_loss: -89.38623, policy_entropy: -5.86480, alpha: 0.02311, time: 50.57561
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   473 ----
[CW] collect: return: 266.04021, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 19.95886, qf2_loss: 19.85213, policy_loss: -88.92567, policy_entropy: -5.77187, alpha: 0.02295, time: 50.59907
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   474 ----
[CW] collect: return: 261.75398, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 14.99631, qf2_loss: 14.94702, policy_loss: -88.89910, policy_entropy: -5.60644, alpha: 0.02275, time: 50.32799
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   475 ----
[CW] collect: return: 160.88394, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 11.15984, qf2_loss: 11.10901, policy_loss: -90.74634, policy_entropy: -5.85155, alpha: 0.02250, time: 50.48993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   476 ----
[CW] collect: return: 308.89555, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 11.54778, qf2_loss: 11.49019, policy_loss: -88.26848, policy_entropy: -5.43697, alpha: 0.02226, time: 50.27377
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   477 ----
[CW] collect: return: 271.89221, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 14.25578, qf2_loss: 14.23622, policy_loss: -90.19483, policy_entropy: -5.61882, alpha: 0.02193, time: 50.31978
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   478 ----
[CW] collect: return: 299.48745, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 15.04158, qf2_loss: 15.02171, policy_loss: -88.97989, policy_entropy: -5.44604, alpha: 0.02159, time: 50.59473
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   479 ----
[CW] collect: return: 309.22144, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 13.54794, qf2_loss: 13.47581, policy_loss: -88.49974, policy_entropy: -5.73977, alpha: 0.02129, time: 50.59447
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   480 ----
[CW] collect: return: 102.33816, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 14.46400, qf2_loss: 14.40348, policy_loss: -89.51386, policy_entropy: -5.86050, alpha: 0.02121, time: 50.75457
[CW] eval: return: 191.01949, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   481 ----
[CW] collect: return: 241.41424, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 15.33021, qf2_loss: 15.29787, policy_loss: -90.09576, policy_entropy: -5.70749, alpha: 0.02098, time: 50.75494
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   482 ----
[CW] collect: return: 319.39778, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 16.67779, qf2_loss: 16.59389, policy_loss: -89.97603, policy_entropy: -5.74345, alpha: 0.02086, time: 50.63173
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   483 ----
[CW] collect: return: 213.75452, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 15.79651, qf2_loss: 15.76341, policy_loss: -91.07242, policy_entropy: -5.81680, alpha: 0.02069, time: 50.84952
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   484 ----
[CW] collect: return: 93.10908, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 21.15076, qf2_loss: 21.02321, policy_loss: -90.59796, policy_entropy: -6.15934, alpha: 0.02063, time: 50.87965
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   485 ----
[CW] collect: return: 25.22832, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 33.32551, qf2_loss: 33.41900, policy_loss: -90.14700, policy_entropy: -7.30873, alpha: 0.02108, time: 50.76739
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   486 ----
[CW] collect: return: 207.59548, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 17.24619, qf2_loss: 17.11704, policy_loss: -90.68668, policy_entropy: -6.95370, alpha: 0.02191, time: 50.60163
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   487 ----
[CW] collect: return: 254.79919, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 12.72532, qf2_loss: 12.62262, policy_loss: -89.89676, policy_entropy: -6.72083, alpha: 0.02246, time: 50.45804
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   488 ----
[CW] collect: return: 174.55197, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 12.14502, qf2_loss: 12.06193, policy_loss: -89.87538, policy_entropy: -6.51512, alpha: 0.02294, time: 50.49718
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   489 ----
[CW] collect: return: 89.22494, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 14.10518, qf2_loss: 14.05777, policy_loss: -88.47356, policy_entropy: -6.48119, alpha: 0.02336, time: 52.14954
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   490 ----
[CW] collect: return: 73.49490, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 17.34675, qf2_loss: 17.27607, policy_loss: -90.72448, policy_entropy: -6.46300, alpha: 0.02374, time: 50.59241
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   491 ----
[CW] collect: return: 266.40275, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 23.10182, qf2_loss: 23.02183, policy_loss: -91.69664, policy_entropy: -6.40530, alpha: 0.02411, time: 50.26764
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   492 ----
[CW] collect: return: 267.44428, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 22.63172, qf2_loss: 22.72558, policy_loss: -90.80197, policy_entropy: -6.16496, alpha: 0.02438, time: 50.39185
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   493 ----
[CW] collect: return: 24.15155, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 25.64107, qf2_loss: 25.61744, policy_loss: -91.06212, policy_entropy: -6.09210, alpha: 0.02453, time: 50.25762
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   494 ----
[CW] collect: return: 225.22230, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 37.90177, qf2_loss: 37.70479, policy_loss: -90.72892, policy_entropy: -5.98537, alpha: 0.02460, time: 50.59581
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   495 ----
[CW] collect: return: 118.46806, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 28.02631, qf2_loss: 27.94990, policy_loss: -90.97484, policy_entropy: -5.82246, alpha: 0.02446, time: 50.61734
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   496 ----
[CW] collect: return: 21.90626, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 21.57199, qf2_loss: 21.45284, policy_loss: -90.93277, policy_entropy: -5.89512, alpha: 0.02430, time: 50.89102
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   497 ----
[CW] collect: return: 21.95477, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 18.37691, qf2_loss: 18.26626, policy_loss: -91.82784, policy_entropy: -5.91713, alpha: 0.02422, time: 50.90290
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   498 ----
[CW] collect: return: 22.96204, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 19.26702, qf2_loss: 19.12132, policy_loss: -89.32071, policy_entropy: -5.80413, alpha: 0.02412, time: 50.96773
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   499 ----
[CW] collect: return: 83.89160, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 15.23261, qf2_loss: 15.04508, policy_loss: -89.71686, policy_entropy: -5.84193, alpha: 0.02394, time: 51.08844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   500 ----
[CW] collect: return: 75.81240, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 18.67529, qf2_loss: 18.64874, policy_loss: -90.06135, policy_entropy: -5.55582, alpha: 0.02374, time: 50.96221
[CW] eval: return: 158.16232, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   501 ----
[CW] collect: return: 152.30867, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 18.22504, qf2_loss: 18.14284, policy_loss: -89.79016, policy_entropy: -5.19115, alpha: 0.02309, time: 50.40146
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   502 ----
[CW] collect: return: 86.29413, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 19.58681, qf2_loss: 19.56301, policy_loss: -90.45325, policy_entropy: -5.36255, alpha: 0.02241, time: 50.55604
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   503 ----
[CW] collect: return: 49.02866, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 19.43696, qf2_loss: 19.33401, policy_loss: -89.50135, policy_entropy: -5.67976, alpha: 0.02204, time: 50.68824
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   504 ----
[CW] collect: return: 19.37813, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 25.93045, qf2_loss: 25.92506, policy_loss: -90.08518, policy_entropy: -5.82216, alpha: 0.02184, time: 50.35129
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   505 ----
[CW] collect: return: 178.62424, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 33.95175, qf2_loss: 33.93969, policy_loss: -90.02551, policy_entropy: -5.86745, alpha: 0.02170, time: 50.35542
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   506 ----
[CW] collect: return: 169.04944, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 24.52616, qf2_loss: 24.39275, policy_loss: -90.47219, policy_entropy: -5.86240, alpha: 0.02158, time: 50.38560
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   507 ----
[CW] collect: return: 74.18102, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 21.32519, qf2_loss: 21.24904, policy_loss: -90.01117, policy_entropy: -5.76243, alpha: 0.02147, time: 50.62556
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   508 ----
[CW] collect: return: 23.16691, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 19.97673, qf2_loss: 19.97644, policy_loss: -91.47607, policy_entropy: -5.92577, alpha: 0.02139, time: 51.12111
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   509 ----
[CW] collect: return: 136.00472, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 24.45046, qf2_loss: 24.48119, policy_loss: -88.86026, policy_entropy: -5.49808, alpha: 0.02119, time: 51.05682
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   510 ----
[CW] collect: return: 238.17685, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 27.53828, qf2_loss: 27.35671, policy_loss: -90.79071, policy_entropy: -5.79564, alpha: 0.02091, time: 51.13170
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   511 ----
[CW] collect: return: 21.26848, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 27.70431, qf2_loss: 27.58888, policy_loss: -91.38963, policy_entropy: -5.75120, alpha: 0.02074, time: 50.50434
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   512 ----
[CW] collect: return: 141.06923, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 26.90196, qf2_loss: 26.90897, policy_loss: -90.96100, policy_entropy: -6.05579, alpha: 0.02065, time: 50.74659
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   513 ----
[CW] collect: return: 26.51723, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 36.60898, qf2_loss: 36.55459, policy_loss: -90.11717, policy_entropy: -6.23299, alpha: 0.02074, time: 50.53147
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   514 ----
[CW] collect: return: 173.74603, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 31.01108, qf2_loss: 30.97179, policy_loss: -89.61173, policy_entropy: -6.50868, alpha: 0.02098, time: 50.32412
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   515 ----
[CW] collect: return: 163.98183, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 32.53447, qf2_loss: 32.51523, policy_loss: -92.43231, policy_entropy: -6.40385, alpha: 0.02136, time: 50.35199
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   516 ----
[CW] collect: return: 81.35650, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 25.76669, qf2_loss: 25.70877, policy_loss: -91.10654, policy_entropy: -6.16783, alpha: 0.02160, time: 50.41293
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   517 ----
[CW] collect: return: 27.38734, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 28.22785, qf2_loss: 28.22961, policy_loss: -91.05112, policy_entropy: -5.87360, alpha: 0.02153, time: 50.60557
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   518 ----
[CW] collect: return: 29.62191, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 28.64070, qf2_loss: 28.73179, policy_loss: -90.47806, policy_entropy: -6.01092, alpha: 0.02149, time: 50.49403
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   519 ----
[CW] collect: return: 24.89339, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 26.35341, qf2_loss: 26.28128, policy_loss: -90.11858, policy_entropy: -6.24908, alpha: 0.02161, time: 50.41276
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   520 ----
[CW] collect: return: 185.49942, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 32.49684, qf2_loss: 32.54405, policy_loss: -89.89789, policy_entropy: -6.14629, alpha: 0.02175, time: 50.63127
[CW] eval: return: 72.11641, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   521 ----
[CW] collect: return: 207.26263, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 31.11331, qf2_loss: 30.90577, policy_loss: -90.01477, policy_entropy: -6.13772, alpha: 0.02191, time: 50.68040
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   522 ----
[CW] collect: return: 96.05221, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 25.61440, qf2_loss: 25.67901, policy_loss: -88.72482, policy_entropy: -5.70501, alpha: 0.02188, time: 50.58895
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   523 ----
[CW] collect: return: 144.06133, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 32.11232, qf2_loss: 32.21564, policy_loss: -90.78774, policy_entropy: -5.90945, alpha: 0.02171, time: 50.68303
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   524 ----
[CW] collect: return: 25.31922, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 41.86640, qf2_loss: 41.57841, policy_loss: -90.23632, policy_entropy: -5.87920, alpha: 0.02163, time: 50.93369
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   525 ----
[CW] collect: return: 217.57502, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 36.24111, qf2_loss: 36.35530, policy_loss: -89.43709, policy_entropy: -5.66907, alpha: 0.02145, time: 50.93202
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   526 ----
[CW] collect: return: 25.26517, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 31.99425, qf2_loss: 31.98745, policy_loss: -90.70894, policy_entropy: -6.06486, alpha: 0.02131, time: 50.84384
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   527 ----
[CW] collect: return: 23.54175, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 31.59837, qf2_loss: 31.38574, policy_loss: -89.75801, policy_entropy: -5.66180, alpha: 0.02121, time: 50.86687
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   528 ----
[CW] collect: return: 25.73763, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 34.04282, qf2_loss: 33.95547, policy_loss: -90.29406, policy_entropy: -5.99952, alpha: 0.02107, time: 50.92586
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   529 ----
[CW] collect: return: 23.52276, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 39.53503, qf2_loss: 39.32071, policy_loss: -89.72181, policy_entropy: -6.12052, alpha: 0.02110, time: 50.36614
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   530 ----
[CW] collect: return: 185.55429, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 48.00894, qf2_loss: 47.85585, policy_loss: -88.47357, policy_entropy: -6.30035, alpha: 0.02128, time: 50.44162
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   531 ----
[CW] collect: return: 33.85342, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 43.64448, qf2_loss: 43.77568, policy_loss: -91.52034, policy_entropy: -6.36980, alpha: 0.02155, time: 50.63061
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   532 ----
[CW] collect: return: 67.03150, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 41.37651, qf2_loss: 41.26618, policy_loss: -89.07161, policy_entropy: -5.64278, alpha: 0.02157, time: 50.28003
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   533 ----
[CW] collect: return: 289.06110, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 48.87537, qf2_loss: 48.67574, policy_loss: -90.28246, policy_entropy: -5.80021, alpha: 0.02142, time: 50.66771
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   534 ----
[CW] collect: return: 170.20166, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 33.29188, qf2_loss: 33.36698, policy_loss: -89.36156, policy_entropy: -5.57027, alpha: 0.02111, time: 50.59193
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   535 ----
[CW] collect: return: 265.85197, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 37.22529, qf2_loss: 37.14741, policy_loss: -87.91510, policy_entropy: -6.31610, alpha: 0.02102, time: 50.72484
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   536 ----
[CW] collect: return: 101.47722, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 42.42415, qf2_loss: 42.36330, policy_loss: -89.92965, policy_entropy: -6.67729, alpha: 0.02139, time: 50.79597
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   537 ----
[CW] collect: return: 183.15661, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 40.68063, qf2_loss: 40.31132, policy_loss: -90.88947, policy_entropy: -6.56383, alpha: 0.02192, time: 50.57922
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   538 ----
[CW] collect: return: 135.51608, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 41.09431, qf2_loss: 40.74261, policy_loss: -91.82157, policy_entropy: -6.49760, alpha: 0.02228, time: 50.60475
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   539 ----
[CW] collect: return: 153.73168, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 46.46418, qf2_loss: 46.38510, policy_loss: -90.53273, policy_entropy: -6.37512, alpha: 0.02266, time: 50.93071
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   540 ----
[CW] collect: return: 47.10142, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 69.29997, qf2_loss: 69.96287, policy_loss: -91.89123, policy_entropy: -6.19312, alpha: 0.02295, time: 50.90668
[CW] eval: return: 98.70935, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   541 ----
[CW] collect: return: 87.41040, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 82.11488, qf2_loss: 82.19346, policy_loss: -92.57629, policy_entropy: -6.06390, alpha: 0.02303, time: 50.90571
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   542 ----
[CW] collect: return: 29.85605, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 50.10233, qf2_loss: 50.00817, policy_loss: -90.72869, policy_entropy: -5.86754, alpha: 0.02303, time: 51.31485
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   543 ----
[CW] collect: return: 239.49838, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 43.36161, qf2_loss: 43.09855, policy_loss: -89.54221, policy_entropy: -5.83981, alpha: 0.02292, time: 50.67813
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   544 ----
[CW] collect: return: 82.73836, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 45.38297, qf2_loss: 45.18359, policy_loss: -88.84681, policy_entropy: -5.83479, alpha: 0.02281, time: 50.40948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   545 ----
[CW] collect: return: 160.33298, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 49.74576, qf2_loss: 49.89684, policy_loss: -91.70975, policy_entropy: -5.90667, alpha: 0.02267, time: 50.35986
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   546 ----
[CW] collect: return: 221.14213, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 43.10085, qf2_loss: 42.83499, policy_loss: -90.88377, policy_entropy: -5.89379, alpha: 0.02262, time: 50.40100
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   547 ----
[CW] collect: return: 190.30349, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 49.06155, qf2_loss: 49.03279, policy_loss: -90.71929, policy_entropy: -5.85336, alpha: 0.02250, time: 50.83054
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   548 ----
[CW] collect: return: 249.95229, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 75.86290, qf2_loss: 76.25346, policy_loss: -89.87308, policy_entropy: -5.74247, alpha: 0.02236, time: 50.41509
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   549 ----
[CW] collect: return: 65.87100, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 82.73231, qf2_loss: 82.62203, policy_loss: -92.22034, policy_entropy: -5.57410, alpha: 0.02210, time: 50.71284
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   550 ----
[CW] collect: return: 85.09411, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 54.76292, qf2_loss: 54.36540, policy_loss: -90.08175, policy_entropy: -5.24991, alpha: 0.02162, time: 50.66261
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   551 ----
[CW] collect: return: 257.87236, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 48.94213, qf2_loss: 48.86223, policy_loss: -90.41597, policy_entropy: -5.51405, alpha: 0.02112, time: 50.61917
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   552 ----
[CW] collect: return: 12.15838, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 56.51795, qf2_loss: 56.27540, policy_loss: -90.63385, policy_entropy: -5.57052, alpha: 0.02085, time: 50.64144
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   553 ----
[CW] collect: return: 66.39186, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 59.69656, qf2_loss: 59.68615, policy_loss: -91.71066, policy_entropy: -5.74860, alpha: 0.02063, time: 50.44248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   554 ----
[CW] collect: return: 29.84507, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 53.71938, qf2_loss: 53.53946, policy_loss: -91.19449, policy_entropy: -6.20945, alpha: 0.02054, time: 50.62282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   555 ----
[CW] collect: return: 93.67604, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 63.87816, qf2_loss: 63.53718, policy_loss: -90.43678, policy_entropy: -5.80974, alpha: 0.02058, time: 50.42685
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   556 ----
[CW] collect: return: 39.53952, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 76.05423, qf2_loss: 75.89916, policy_loss: -91.84439, policy_entropy: -6.22856, alpha: 0.02058, time: 50.29200
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   557 ----
[CW] collect: return: 24.46791, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 74.27504, qf2_loss: 73.91188, policy_loss: -92.00042, policy_entropy: -6.46712, alpha: 0.02081, time: 50.40010
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   558 ----
[CW] collect: return: 223.79208, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 71.82745, qf2_loss: 71.53508, policy_loss: -91.39151, policy_entropy: -6.30604, alpha: 0.02113, time: 50.38394
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   559 ----
[CW] collect: return: 30.60329, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 74.64900, qf2_loss: 74.19069, policy_loss: -91.11397, policy_entropy: -6.47246, alpha: 0.02134, time: 50.60183
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   560 ----
[CW] collect: return: 185.00226, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 106.79910, qf2_loss: 106.63834, policy_loss: -90.58848, policy_entropy: -6.65617, alpha: 0.02179, time: 50.37847
[CW] eval: return: 134.29031, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   561 ----
[CW] collect: return: 224.21347, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 96.97953, qf2_loss: 96.93380, policy_loss: -89.97355, policy_entropy: -6.86778, alpha: 0.02250, time: 50.53374
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   562 ----
[CW] collect: return: 198.12027, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 119.23990, qf2_loss: 119.15354, policy_loss: -91.88484, policy_entropy: -7.25214, alpha: 0.02319, time: 51.79901
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   563 ----
[CW] collect: return: 210.34937, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 77.57623, qf2_loss: 77.40862, policy_loss: -92.52018, policy_entropy: -7.12761, alpha: 0.02412, time: 51.15309
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   564 ----
[CW] collect: return: 117.64920, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 79.76093, qf2_loss: 79.50136, policy_loss: -94.02431, policy_entropy: -6.85404, alpha: 0.02507, time: 50.82920
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   565 ----
[CW] collect: return: 151.54893, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 74.80753, qf2_loss: 74.71617, policy_loss: -91.61230, policy_entropy: -6.85993, alpha: 0.02564, time: 50.67642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   566 ----
[CW] collect: return: 230.89091, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 83.14767, qf2_loss: 82.54977, policy_loss: -93.92896, policy_entropy: -6.96079, alpha: 0.02649, time: 52.46260
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   567 ----
[CW] collect: return: 214.07851, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 78.95450, qf2_loss: 79.32019, policy_loss: -93.98410, policy_entropy: -6.71591, alpha: 0.02718, time: 50.83369
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   568 ----
[CW] collect: return: 53.64062, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 72.77747, qf2_loss: 72.47364, policy_loss: -92.41800, policy_entropy: -6.28813, alpha: 0.02765, time: 50.75426
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   569 ----
[CW] collect: return: 107.15244, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 118.80000, qf2_loss: 119.24736, policy_loss: -92.46962, policy_entropy: -6.33727, alpha: 0.02785, time: 50.45922
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   570 ----
[CW] collect: return: 60.15521, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 126.55489, qf2_loss: 125.99432, policy_loss: -94.32272, policy_entropy: -6.14239, alpha: 0.02810, time: 50.36431
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   571 ----
[CW] collect: return: 37.27255, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 116.37293, qf2_loss: 116.39043, policy_loss: -93.18974, policy_entropy: -5.88875, alpha: 0.02812, time: 50.25359
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   572 ----
[CW] collect: return: 152.08984, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 108.19277, qf2_loss: 107.47525, policy_loss: -92.81800, policy_entropy: -6.33554, alpha: 0.02818, time: 50.36406
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   573 ----
[CW] collect: return: 91.57929, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 97.68350, qf2_loss: 97.51629, policy_loss: -91.95465, policy_entropy: -6.42117, alpha: 0.02857, time: 50.32584
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   574 ----
[CW] collect: return: 147.62577, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 98.66442, qf2_loss: 99.15540, policy_loss: -95.16462, policy_entropy: -6.51674, alpha: 0.02896, time: 50.23249
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   575 ----
[CW] collect: return: 42.33892, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 109.85935, qf2_loss: 110.14721, policy_loss: -91.95566, policy_entropy: -6.69766, alpha: 0.02949, time: 50.42775
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   576 ----
[CW] collect: return: 77.08829, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 133.50489, qf2_loss: 134.05078, policy_loss: -95.29275, policy_entropy: -7.12168, alpha: 0.03022, time: 50.55669
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   577 ----
[CW] collect: return: 38.60118, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 164.67833, qf2_loss: 164.94851, policy_loss: -93.84968, policy_entropy: -7.36562, alpha: 0.03152, time: 51.09857
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   578 ----
[CW] collect: return: 41.92827, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 131.71606, qf2_loss: 132.57126, policy_loss: -93.83815, policy_entropy: -6.57051, alpha: 0.03247, time: 51.06275
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   579 ----
[CW] collect: return: 14.82447, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 102.75024, qf2_loss: 102.59099, policy_loss: -93.67863, policy_entropy: -6.31671, alpha: 0.03286, time: 50.96250
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   580 ----
[CW] collect: return: 84.78307, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 110.32066, qf2_loss: 110.19251, policy_loss: -95.37403, policy_entropy: -6.20611, alpha: 0.03304, time: 51.05478
[CW] eval: return: 173.37005, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   581 ----
[CW] collect: return: 132.91739, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 149.43959, qf2_loss: 150.78410, policy_loss: -95.25312, policy_entropy: -6.09110, alpha: 0.03316, time: 50.87019
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   582 ----
[CW] collect: return: 171.47563, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 119.43426, qf2_loss: 119.86388, policy_loss: -96.10946, policy_entropy: -6.09531, alpha: 0.03337, time: 50.64734
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   583 ----
[CW] collect: return: 169.57648, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 118.10290, qf2_loss: 118.73978, policy_loss: -92.96268, policy_entropy: -5.54792, alpha: 0.03316, time: 50.45267
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   584 ----
[CW] collect: return: 194.13199, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 124.55550, qf2_loss: 124.53328, policy_loss: -94.42480, policy_entropy: -5.01221, alpha: 0.03251, time: 50.66285
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   585 ----
[CW] collect: return: 171.23522, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 132.51960, qf2_loss: 133.35458, policy_loss: -95.50925, policy_entropy: -5.29163, alpha: 0.03174, time: 50.38248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   586 ----
[CW] collect: return: 81.62019, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 152.71127, qf2_loss: 153.49049, policy_loss: -93.55029, policy_entropy: -4.83019, alpha: 0.03102, time: 50.34538
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   587 ----
[CW] collect: return: 125.45207, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 166.33109, qf2_loss: 166.40384, policy_loss: -93.50680, policy_entropy: -4.68690, alpha: 0.03004, time: 50.36093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   588 ----
[CW] collect: return: 174.54232, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 161.71598, qf2_loss: 162.55520, policy_loss: -93.93282, policy_entropy: -5.08409, alpha: 0.02915, time: 50.27911
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   589 ----
[CW] collect: return: 168.21565, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 162.58865, qf2_loss: 163.34713, policy_loss: -95.84437, policy_entropy: -4.97027, alpha: 0.02850, time: 50.50023
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   590 ----
[CW] collect: return: 118.42792, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 186.82349, qf2_loss: 186.81995, policy_loss: -96.48514, policy_entropy: -5.84654, alpha: 0.02807, time: 51.06138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   591 ----
[CW] collect: return: 41.71382, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 198.02209, qf2_loss: 199.85543, policy_loss: -96.02593, policy_entropy: -6.03098, alpha: 0.02810, time: 50.99937
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   592 ----
[CW] collect: return: 158.81499, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 181.50267, qf2_loss: 183.11783, policy_loss: -96.16802, policy_entropy: -5.11584, alpha: 0.02782, time: 50.48913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   593 ----
[CW] collect: return: 58.84094, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 176.39285, qf2_loss: 177.43390, policy_loss: -94.78780, policy_entropy: -5.34707, alpha: 0.02727, time: 50.62043
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   594 ----
[CW] collect: return: 144.29119, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 209.00303, qf2_loss: 209.99176, policy_loss: -96.89484, policy_entropy: -6.00904, alpha: 0.02706, time: 50.57648
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   595 ----
[CW] collect: return: 19.89956, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 267.71497, qf2_loss: 270.71127, policy_loss: -95.89059, policy_entropy: -5.93749, alpha: 0.02706, time: 50.49456
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   596 ----
[CW] collect: return: 33.36793, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 500.10317, qf2_loss: 504.80103, policy_loss: -93.13594, policy_entropy: -6.75164, alpha: 0.02721, time: 50.50474
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   597 ----
[CW] collect: return: 3.18023, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 204.81646, qf2_loss: 206.30917, policy_loss: -96.08349, policy_entropy: -5.88996, alpha: 0.02755, time: 51.34184
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   598 ----
[CW] collect: return: 4.04228, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 173.50462, qf2_loss: 174.71213, policy_loss: -96.90485, policy_entropy: -5.84672, alpha: 0.02740, time: 50.40113
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   599 ----
[CW] collect: return: 22.11706, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 178.12930, qf2_loss: 179.71119, policy_loss: -94.40939, policy_entropy: -6.10073, alpha: 0.02739, time: 50.44097
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   600 ----
[CW] collect: return: 25.79903, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 157.89783, qf2_loss: 159.20801, policy_loss: -94.87495, policy_entropy: -6.75079, alpha: 0.02766, time: 50.46391
[CW] eval: return: 87.48502, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   601 ----
[CW] collect: return: 264.79347, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 178.41515, qf2_loss: 178.71103, policy_loss: -95.32045, policy_entropy: -6.63275, alpha: 0.02818, time: 50.94525
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   602 ----
[CW] collect: return: 82.56523, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 161.22552, qf2_loss: 162.83245, policy_loss: -92.16599, policy_entropy: -6.35024, alpha: 0.02857, time: 52.07091
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   603 ----
[CW] collect: return: 27.96322, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 202.76301, qf2_loss: 202.20216, policy_loss: -95.77850, policy_entropy: -6.35307, alpha: 0.02889, time: 52.14984
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   604 ----
[CW] collect: return: 203.20078, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 244.98110, qf2_loss: 247.00228, policy_loss: -97.97157, policy_entropy: -6.43158, alpha: 0.02917, time: 52.09087
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   605 ----
[CW] collect: return: 26.39018, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 263.16256, qf2_loss: 264.81373, policy_loss: -94.53334, policy_entropy: -6.30109, alpha: 0.02949, time: 51.93599
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   606 ----
[CW] collect: return: 27.53309, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 215.48680, qf2_loss: 215.68827, policy_loss: -96.14961, policy_entropy: -6.86568, alpha: 0.03003, time: 51.83340
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   607 ----
[CW] collect: return: 25.37451, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 222.70415, qf2_loss: 224.06829, policy_loss: -96.49990, policy_entropy: -6.58094, alpha: 0.03066, time: 51.74123
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   608 ----
[CW] collect: return: 36.11463, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 200.07726, qf2_loss: 202.43812, policy_loss: -95.27801, policy_entropy: -6.14893, alpha: 0.03103, time: 51.90546
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   609 ----
[CW] collect: return: 23.07760, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 193.98704, qf2_loss: 194.63778, policy_loss: -96.37162, policy_entropy: -6.01489, alpha: 0.03112, time: 51.83306
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   610 ----
[CW] collect: return: 22.46283, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 267.00871, qf2_loss: 268.56362, policy_loss: -97.46810, policy_entropy: -5.89529, alpha: 0.03109, time: 52.68123
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   611 ----
[CW] collect: return: 28.01946, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 202.66312, qf2_loss: 203.17727, policy_loss: -92.95898, policy_entropy: -6.51961, alpha: 0.03125, time: 51.64961
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   612 ----
[CW] collect: return: 25.39878, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 199.98033, qf2_loss: 201.74331, policy_loss: -95.89865, policy_entropy: -7.01445, alpha: 0.03205, time: 51.70600
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   613 ----
[CW] collect: return: 28.42585, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 241.69645, qf2_loss: 241.09866, policy_loss: -100.52399, policy_entropy: -6.71345, alpha: 0.03281, time: 51.68935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   614 ----
[CW] collect: return: 28.04871, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 222.43008, qf2_loss: 222.62283, policy_loss: -95.46081, policy_entropy: -6.38224, alpha: 0.03349, time: 51.69124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   615 ----
[CW] collect: return: 27.44813, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 188.24338, qf2_loss: 189.60848, policy_loss: -90.93550, policy_entropy: -5.82108, alpha: 0.03358, time: 51.67657
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   616 ----
[CW] collect: return: 31.67241, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 237.25267, qf2_loss: 239.70918, policy_loss: -95.84661, policy_entropy: -6.14591, alpha: 0.03368, time: 51.66808
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   617 ----
[CW] collect: return: 49.81687, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 241.16995, qf2_loss: 242.50012, policy_loss: -94.22638, policy_entropy: -5.79003, alpha: 0.03366, time: 51.91096
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   618 ----
[CW] collect: return: 49.20128, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 255.14489, qf2_loss: 257.15201, policy_loss: -95.12416, policy_entropy: -5.76578, alpha: 0.03345, time: 51.81761
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   619 ----
[CW] collect: return: 120.00440, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 409.48178, qf2_loss: 414.78630, policy_loss: -98.24887, policy_entropy: -4.64417, alpha: 0.03279, time: 51.83184
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   620 ----
[CW] collect: return: 117.45640, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 357.77317, qf2_loss: 360.50226, policy_loss: -93.84239, policy_entropy: -4.19134, alpha: 0.03137, time: 51.81995
[CW] eval: return: 62.78161, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   621 ----
[CW] collect: return: 42.69559, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 312.40025, qf2_loss: 316.58679, policy_loss: -95.31341, policy_entropy: -3.89528, alpha: 0.02997, time: 51.94476
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   622 ----
[CW] collect: return: 29.66136, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 306.87148, qf2_loss: 309.35628, policy_loss: -96.34361, policy_entropy: -4.57879, alpha: 0.02883, time: 51.83441
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   623 ----
[CW] collect: return: 28.53112, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 528.63566, qf2_loss: 526.95530, policy_loss: -95.92685, policy_entropy: -5.00347, alpha: 0.02819, time: 51.71250
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   624 ----
[CW] collect: return: 7.21740, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 313.38366, qf2_loss: 316.01849, policy_loss: -94.58554, policy_entropy: -6.31073, alpha: 0.02790, time: 51.67285
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   625 ----
[CW] collect: return: 82.05736, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 279.01318, qf2_loss: 279.73661, policy_loss: -96.73226, policy_entropy: -6.22460, alpha: 0.02811, time: 51.68619
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   626 ----
[CW] collect: return: 129.67223, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 223.26401, qf2_loss: 225.82137, policy_loss: -96.46096, policy_entropy: -6.54902, alpha: 0.02830, time: 51.74662
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   627 ----
[CW] collect: return: 128.86138, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 263.78755, qf2_loss: 264.45872, policy_loss: -96.37324, policy_entropy: -6.37725, alpha: 0.02862, time: 51.69540
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   628 ----
[CW] collect: return: 59.18454, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 302.73311, qf2_loss: 302.33057, policy_loss: -98.57713, policy_entropy: -6.42742, alpha: 0.02885, time: 52.76930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   629 ----
[CW] collect: return: 24.99333, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 272.26729, qf2_loss: 275.67072, policy_loss: -94.56980, policy_entropy: -5.93354, alpha: 0.02901, time: 51.69030
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   630 ----
[CW] collect: return: 154.08945, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 294.30167, qf2_loss: 294.44256, policy_loss: -97.07958, policy_entropy: -5.84090, alpha: 0.02893, time: 51.84336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   631 ----
[CW] collect: return: 66.92521, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 267.56058, qf2_loss: 270.56411, policy_loss: -95.37797, policy_entropy: -6.13818, alpha: 0.02891, time: 52.06841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   632 ----
[CW] collect: return: 102.68725, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 272.25140, qf2_loss: 275.00716, policy_loss: -94.54239, policy_entropy: -6.48973, alpha: 0.02906, time: 51.96552
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   633 ----
[CW] collect: return: 102.92102, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 292.32117, qf2_loss: 294.32984, policy_loss: -95.10317, policy_entropy: -6.68047, alpha: 0.02942, time: 51.53509
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   634 ----
[CW] collect: return: 37.83412, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 282.55382, qf2_loss: 283.86397, policy_loss: -97.90549, policy_entropy: -7.11439, alpha: 0.03016, time: 51.82990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   635 ----
[CW] collect: return: 40.64885, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 279.52611, qf2_loss: 280.55338, policy_loss: -97.47694, policy_entropy: -6.64077, alpha: 0.03085, time: 51.62150
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   636 ----
[CW] collect: return: 70.25824, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 304.99723, qf2_loss: 305.48651, policy_loss: -97.65818, policy_entropy: -6.79254, alpha: 0.03139, time: 51.40467
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   637 ----
[CW] collect: return: 56.02856, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 327.66047, qf2_loss: 331.49572, policy_loss: -98.56801, policy_entropy: -6.82230, alpha: 0.03204, time: 51.12407
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   638 ----
[CW] collect: return: 51.40328, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 341.93245, qf2_loss: 342.00735, policy_loss: -99.18412, policy_entropy: -7.10649, alpha: 0.03279, time: 51.17192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   639 ----
[CW] collect: return: 47.17281, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 336.63340, qf2_loss: 339.92004, policy_loss: -98.04456, policy_entropy: -7.18325, alpha: 0.03376, time: 51.17389
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   640 ----
[CW] collect: return: 23.59257, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 346.31409, qf2_loss: 348.74340, policy_loss: -97.31070, policy_entropy: -7.01059, alpha: 0.03473, time: 51.04575
[CW] eval: return: 75.72656, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   641 ----
[CW] collect: return: 36.36614, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 379.55264, qf2_loss: 379.12212, policy_loss: -97.67432, policy_entropy: -7.22086, alpha: 0.03576, time: 50.68392
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   642 ----
[CW] collect: return: 53.98240, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 379.04152, qf2_loss: 379.94594, policy_loss: -96.05158, policy_entropy: -6.80999, alpha: 0.03661, time: 50.81628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   643 ----
[CW] collect: return: 52.58491, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 495.32709, qf2_loss: 497.84579, policy_loss: -101.81553, policy_entropy: -6.81397, alpha: 0.03734, time: 50.70133
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   644 ----
[CW] collect: return: 64.65696, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 516.76206, qf2_loss: 516.98925, policy_loss: -101.98543, policy_entropy: -7.25465, alpha: 0.03831, time: 51.01918
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   645 ----
[CW] collect: return: 76.52097, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 378.13838, qf2_loss: 379.50725, policy_loss: -98.00557, policy_entropy: -6.58469, alpha: 0.03923, time: 51.09951
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   646 ----
[CW] collect: return: 26.24901, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 409.69824, qf2_loss: 414.50026, policy_loss: -96.49998, policy_entropy: -6.71971, alpha: 0.03981, time: 51.08257
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   647 ----
[CW] collect: return: 107.25721, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 348.24272, qf2_loss: 351.27460, policy_loss: -99.17259, policy_entropy: -6.79444, alpha: 0.04058, time: 50.97218
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   648 ----
[CW] collect: return: 89.35827, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 362.16624, qf2_loss: 364.66407, policy_loss: -98.99298, policy_entropy: -6.18565, alpha: 0.04118, time: 50.98735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   649 ----
[CW] collect: return: 28.13930, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 419.34220, qf2_loss: 422.14222, policy_loss: -98.59919, policy_entropy: -5.91298, alpha: 0.04116, time: 50.88740
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   650 ----
[CW] collect: return: 54.18428, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 538.43802, qf2_loss: 537.96786, policy_loss: -97.71443, policy_entropy: -5.88578, alpha: 0.04115, time: 50.75559
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   651 ----
[CW] collect: return: 164.60850, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 547.89132, qf2_loss: 549.67662, policy_loss: -102.19067, policy_entropy: -6.32513, alpha: 0.04113, time: 50.66464
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   652 ----
[CW] collect: return: 22.92142, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 617.36154, qf2_loss: 617.97361, policy_loss: -102.36246, policy_entropy: -6.70326, alpha: 0.04169, time: 50.60760
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   653 ----
[CW] collect: return: 48.51100, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 603.10744, qf2_loss: 609.64135, policy_loss: -99.95171, policy_entropy: -6.63893, alpha: 0.04244, time: 50.71432
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   654 ----
[CW] collect: return: 140.39950, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 514.22862, qf2_loss: 518.75287, policy_loss: -100.68399, policy_entropy: -6.50757, alpha: 0.04331, time: 50.75484
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   655 ----
[CW] collect: return: 66.14231, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 463.26887, qf2_loss: 466.75643, policy_loss: -100.27957, policy_entropy: -6.06823, alpha: 0.04359, time: 50.68732
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   656 ----
[CW] collect: return: 137.94327, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 475.92409, qf2_loss: 479.90859, policy_loss: -102.94077, policy_entropy: -5.94695, alpha: 0.04372, time: 50.84034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   657 ----
[CW] collect: return: 50.71045, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 487.19766, qf2_loss: 491.80531, policy_loss: -104.48338, policy_entropy: -5.98907, alpha: 0.04355, time: 50.92226
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   658 ----
[CW] collect: return: 61.14855, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 494.47081, qf2_loss: 498.61482, policy_loss: -103.86977, policy_entropy: -6.22255, alpha: 0.04366, time: 50.97282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   659 ----
[CW] collect: return: 80.23566, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 422.27718, qf2_loss: 425.42623, policy_loss: -104.33652, policy_entropy: -6.34371, alpha: 0.04407, time: 51.01748
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   660 ----
[CW] collect: return: 17.19064, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 418.96994, qf2_loss: 425.93183, policy_loss: -102.07929, policy_entropy: -6.07179, alpha: 0.04437, time: 50.97694
[CW] eval: return: 45.83477, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   661 ----
[CW] collect: return: 26.79312, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 534.26990, qf2_loss: 537.12980, policy_loss: -103.17947, policy_entropy: -5.98654, alpha: 0.04436, time: 51.30402
[CW] ---------------------------

============================= JOB FEEDBACK =============================

NodeName=uc2n511
Job ID: 21915470
Array Job ID: 21915470_0
Cluster: uc2
User/Group: uprnr/stud
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 4
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-16:01:00 core-walltime
Job Wall-clock time: 10:00:15
Memory Utilized: 4.77 GB
Memory Efficiency: 8.14% of 58.59 GB
