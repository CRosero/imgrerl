{"collect/return": 26.793123157709488, "collect/steps": 1000.0, "collect/total_steps": 667000.0, "train/qf1_loss": 534.2699017715454, "train/qf2_loss": 537.1297983551026, "train/policy_loss": -103.17947402954101, "train/policy_entropy": -5.986542901992798, "train/alpha": 0.04435596399009228, "train/time": 51.30401563644409, "eval/return": 45.83477008150974, "eval/steps": 1000.0, "_timestamp": 1678754336.504637, "_runtime": 35834.29070305824, "_step": 661}