{"collect/return": 521.8999595441855, "collect/steps": 1000.0, "collect/total_steps": 503000.0, "train/qf1_loss": 8.490416803359984, "train/qf2_loss": 8.393124125003816, "train/policy_loss": -96.31281028747559, "train/policy_entropy": -6.186655626296997, "train/alpha": 0.025610691122710704, "train/time": 69.41342949867249, "eval/return": 381.6366888366174, "eval/steps": 1000.0, "_timestamp": 1678739250.2890713, "_runtime": 35895.141862392426, "_step": 497}