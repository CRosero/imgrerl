Hostname: uc2n511.localdomain
Found slurm config: SLURM
Seeting default slurm config
/home/kit/stud/uprnr/imgrerl/test_results/fixed_test-wr-sot-k2m2-t-seed1/fixed_test-wr-sot-k2m2-t-seed1/fixed_test-wr-sot-k2m2-t-seed1__env.ewalker-run/log/rep_00
[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
True
params: 
 {'env': {'env': 'walker-run'}} 

additionalVars: 
 {'seed': 1, 'agent': {'image_augmentation_K': 2, 'image_augmentation_M': 2, 'image_augmentation_type': <AugmentationType.SAME_OVER_TIME: 2>, 'image_augmentation_actor_critic_same_aug': True}}
conf_dict: 
 --------Config-------- 
seed: 1
cuda_id: 0
Subconfig: env
	env: walker-run
	obs_type: image
Subconfig: agent
	algo_name: sac
	encoder: lstm
	action_embedding_size: 32
	observ_embedding_size: 0
	reward_embedding_size: 32
	rnn_hidden_size: 512
	dqn_layers: [512, 512, 512]
	policy_layers: [512, 512, 512]
	lr: 0.0003
	gamma: 0.99
	tau: 0.005
	entropy_alpha: 1.0
	image_augmentation_type: AugmentationType.SAME_OVER_TIME
	image_augmentation_K: 2
	image_augmentation_M: 2
	image_augmentation_actor_critic_same_aug: True
Subconfig: rl
	sampled_seq_len: 64
	buffer_size: 1000000.0
	batch_size: 32
	rollouts_per_iter: 1
	num_updates_per_iter: 100
	num_init_rollouts: 5
Subconfig: eval
	interval: 20
	num_rollouts: 20
---------------------- 
 
Init feature extractor:  6 ;  32 ;  <function relu at 0x14e1926aa7a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x14e1926aa7a0>
Init feature extractor:  6 ;  164 ;  <function relu at 0x14e1926aa7a0>
Critic: 
Critic_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (current_shortcut_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=164, bias=True)
  )
  (qf1): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
  (qf2): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
)
Init feature extractor:  6 ;  32 ;  <function relu at 0x14e1926aa7a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x14e1926aa7a0>
Actor: 
Actor_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (policy): TanhGaussianPolicy(
    (fc0): Linear(in_features=612, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=6, bias=True)
    (last_fc_log_std): Linear(in_features=512, out_features=6, bias=True)
  )
)
buffer RAM usage: 11.48 GB
Collecting train stats
[CW] ---- Iteration:     0 ----
[CW] collect: return: 34.42918, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 3.28663, qf2_loss: 3.34333, policy_loss: -7.83157, policy_entropy: 4.09756, alpha: 0.98504, time: 76.08339
[CW] eval: return: 25.40235, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     1 ----
[CW] collect: return: 22.79040, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.09114, qf2_loss: 0.09160, policy_loss: -8.50411, policy_entropy: 4.10044, alpha: 0.95626, time: 70.19080
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     2 ----
[CW] collect: return: 22.99172, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.07935, qf2_loss: 0.07966, policy_loss: -9.20421, policy_entropy: 4.10064, alpha: 0.92871, time: 70.08212
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     3 ----
[CW] collect: return: 21.44817, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.07079, qf2_loss: 0.07117, policy_loss: -10.14989, policy_entropy: 4.10219, alpha: 0.90231, time: 69.85629
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     4 ----
[CW] collect: return: 30.80237, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06473, qf2_loss: 0.06492, policy_loss: -11.22985, policy_entropy: 4.10088, alpha: 0.87698, time: 69.72351
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     5 ----
[CW] collect: return: 26.30246, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06149, qf2_loss: 0.06186, policy_loss: -12.36082, policy_entropy: 4.10148, alpha: 0.85267, time: 69.56190
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     6 ----
[CW] collect: return: 23.62970, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.08242, qf2_loss: 0.08465, policy_loss: -13.51971, policy_entropy: 4.10200, alpha: 0.82930, time: 69.52132
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     7 ----
[CW] collect: return: 26.01420, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.09196, qf2_loss: 0.09321, policy_loss: -14.71457, policy_entropy: 4.10133, alpha: 0.80683, time: 69.55227
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     8 ----
[CW] collect: return: 31.16968, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.11333, qf2_loss: 0.11448, policy_loss: -15.92334, policy_entropy: 4.10112, alpha: 0.78519, time: 69.43530
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     9 ----
[CW] collect: return: 26.03596, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.12057, qf2_loss: 0.12167, policy_loss: -17.13598, policy_entropy: 4.10118, alpha: 0.76435, time: 69.35027
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    10 ----
[CW] collect: return: 23.00506, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.09293, qf2_loss: 0.09364, policy_loss: -18.33407, policy_entropy: 4.10171, alpha: 0.74426, time: 69.42403
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    11 ----
[CW] collect: return: 25.74307, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.11915, qf2_loss: 0.12015, policy_loss: -19.50437, policy_entropy: 4.10162, alpha: 0.72488, time: 69.45866
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    12 ----
[CW] collect: return: 28.19937, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.12955, qf2_loss: 0.13063, policy_loss: -20.64884, policy_entropy: 4.10167, alpha: 0.70617, time: 69.31365
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    13 ----
[CW] collect: return: 25.24970, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.10998, qf2_loss: 0.11080, policy_loss: -21.77132, policy_entropy: 4.10062, alpha: 0.68809, time: 69.28096
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    14 ----
[CW] collect: return: 24.02570, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.09370, qf2_loss: 0.09430, policy_loss: -22.86564, policy_entropy: 4.10154, alpha: 0.67062, time: 69.35941
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    15 ----
[CW] collect: return: 25.26939, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.12508, qf2_loss: 0.12588, policy_loss: -23.92548, policy_entropy: 4.10321, alpha: 0.65372, time: 69.30738
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    16 ----
[CW] collect: return: 21.29070, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.13505, qf2_loss: 0.13600, policy_loss: -24.95949, policy_entropy: 4.10124, alpha: 0.63736, time: 69.86704
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    17 ----
[CW] collect: return: 25.90145, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.12726, qf2_loss: 0.12814, policy_loss: -25.96380, policy_entropy: 4.10054, alpha: 0.62153, time: 69.91066
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    18 ----
[CW] collect: return: 22.15333, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.14418, qf2_loss: 0.14518, policy_loss: -26.94518, policy_entropy: 4.10231, alpha: 0.60618, time: 69.71342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    19 ----
[CW] collect: return: 23.10210, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.09927, qf2_loss: 0.09988, policy_loss: -27.89832, policy_entropy: 4.10113, alpha: 0.59131, time: 69.69000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    20 ----
[CW] collect: return: 24.54733, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.17820, qf2_loss: 0.17951, policy_loss: -28.81659, policy_entropy: 4.10094, alpha: 0.57690, time: 69.66856
[CW] eval: return: 24.25216, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    21 ----
[CW] collect: return: 27.70450, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.05885, qf2_loss: 0.05913, policy_loss: -29.70892, policy_entropy: 4.10178, alpha: 0.56291, time: 69.88290
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    22 ----
[CW] collect: return: 26.55881, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.15996, qf2_loss: 0.16104, policy_loss: -30.58355, policy_entropy: 4.10259, alpha: 0.54933, time: 69.71516
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    23 ----
[CW] collect: return: 21.45014, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.07784, qf2_loss: 0.07814, policy_loss: -31.42854, policy_entropy: 4.10047, alpha: 0.53615, time: 69.68976
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    24 ----
[CW] collect: return: 23.22031, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.14294, qf2_loss: 0.14417, policy_loss: -32.24277, policy_entropy: 4.10110, alpha: 0.52334, time: 69.60856
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    25 ----
[CW] collect: return: 25.86647, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.16912, qf2_loss: 0.17014, policy_loss: -33.03250, policy_entropy: 4.10049, alpha: 0.51090, time: 69.34033
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    26 ----
[CW] collect: return: 21.14553, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.10098, qf2_loss: 0.10143, policy_loss: -33.80186, policy_entropy: 4.10238, alpha: 0.49881, time: 69.49043
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    27 ----
[CW] collect: return: 21.85825, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.16426, qf2_loss: 0.16533, policy_loss: -34.55004, policy_entropy: 4.10154, alpha: 0.48705, time: 69.30978
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    28 ----
[CW] collect: return: 25.32989, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.05537, qf2_loss: 0.05571, policy_loss: -35.26815, policy_entropy: 4.10119, alpha: 0.47561, time: 69.43251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    29 ----
[CW] collect: return: 24.33462, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.13676, qf2_loss: 0.13759, policy_loss: -35.96240, policy_entropy: 4.10151, alpha: 0.46448, time: 69.43008
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    30 ----
[CW] collect: return: 23.62520, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.10949, qf2_loss: 0.10988, policy_loss: -36.64222, policy_entropy: 4.10152, alpha: 0.45365, time: 69.48577
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    31 ----
[CW] collect: return: 23.43771, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.13988, qf2_loss: 0.14078, policy_loss: -37.29357, policy_entropy: 4.10059, alpha: 0.44310, time: 69.62579
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    32 ----
[CW] collect: return: 24.67965, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.06738, qf2_loss: 0.06772, policy_loss: -37.93148, policy_entropy: 4.10157, alpha: 0.43284, time: 69.51734
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    33 ----
[CW] collect: return: 23.26909, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.13251, qf2_loss: 0.13317, policy_loss: -38.54299, policy_entropy: 4.10146, alpha: 0.42283, time: 69.58626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    34 ----
[CW] collect: return: 25.43951, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.15343, qf2_loss: 0.15432, policy_loss: -39.13053, policy_entropy: 4.10166, alpha: 0.41309, time: 69.55420
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    35 ----
[CW] collect: return: 24.39777, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.09102, qf2_loss: 0.09152, policy_loss: -39.70563, policy_entropy: 4.10146, alpha: 0.40360, time: 69.34461
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    36 ----
[CW] collect: return: 24.72248, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.12433, qf2_loss: 0.12508, policy_loss: -40.25807, policy_entropy: 4.10165, alpha: 0.39434, time: 69.56886
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    37 ----
[CW] collect: return: 21.74047, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.15581, qf2_loss: 0.15679, policy_loss: -40.78825, policy_entropy: 4.10203, alpha: 0.38532, time: 69.57220
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    38 ----
[CW] collect: return: 28.04114, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.06259, qf2_loss: 0.06295, policy_loss: -41.30748, policy_entropy: 4.10176, alpha: 0.37653, time: 69.56158
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    39 ----
[CW] collect: return: 25.32921, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.11954, qf2_loss: 0.12051, policy_loss: -41.80171, policy_entropy: 4.10172, alpha: 0.36795, time: 69.46091
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    40 ----
[CW] collect: return: 25.59985, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.10137, qf2_loss: 0.10217, policy_loss: -42.27952, policy_entropy: 4.10034, alpha: 0.35958, time: 69.50962
[CW] eval: return: 26.41496, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    41 ----
[CW] collect: return: 24.44457, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.10077, qf2_loss: 0.10134, policy_loss: -42.74618, policy_entropy: 4.10206, alpha: 0.35142, time: 69.68577
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    42 ----
[CW] collect: return: 21.42471, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.15618, qf2_loss: 0.15709, policy_loss: -43.19065, policy_entropy: 4.10244, alpha: 0.34346, time: 69.31631
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    43 ----
[CW] collect: return: 23.61904, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.06782, qf2_loss: 0.06814, policy_loss: -43.62016, policy_entropy: 4.10144, alpha: 0.33569, time: 69.39982
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    44 ----
[CW] collect: return: 24.26167, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.09712, qf2_loss: 0.09765, policy_loss: -44.03360, policy_entropy: 4.10173, alpha: 0.32811, time: 69.28916
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    45 ----
[CW] collect: return: 22.34994, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.12053, qf2_loss: 0.12150, policy_loss: -44.43002, policy_entropy: 4.10152, alpha: 0.32070, time: 69.09336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    46 ----
[CW] collect: return: 23.62800, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.08992, qf2_loss: 0.09039, policy_loss: -44.81064, policy_entropy: 4.10154, alpha: 0.31348, time: 69.09423
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    47 ----
[CW] collect: return: 22.35438, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.08377, qf2_loss: 0.08426, policy_loss: -45.18428, policy_entropy: 4.10142, alpha: 0.30643, time: 69.08458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    48 ----
[CW] collect: return: 25.46597, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.10277, qf2_loss: 0.10318, policy_loss: -45.53380, policy_entropy: 4.10247, alpha: 0.29954, time: 69.08326
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    49 ----
[CW] collect: return: 23.62315, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.13131, qf2_loss: 0.13206, policy_loss: -45.87076, policy_entropy: 4.10203, alpha: 0.29281, time: 69.42393
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    50 ----
[CW] collect: return: 25.57599, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.12288, qf2_loss: 0.12345, policy_loss: -46.19779, policy_entropy: 4.10066, alpha: 0.28625, time: 70.19678
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    51 ----
[CW] collect: return: 20.67794, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.08035, qf2_loss: 0.08086, policy_loss: -46.50778, policy_entropy: 4.10051, alpha: 0.27983, time: 74.77533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    52 ----
[CW] collect: return: 26.95861, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.07927, qf2_loss: 0.07928, policy_loss: -46.80820, policy_entropy: 4.10208, alpha: 0.27357, time: 69.88312
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    53 ----
[CW] collect: return: 26.71108, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.09458, qf2_loss: 0.09555, policy_loss: -47.08696, policy_entropy: 4.10249, alpha: 0.26745, time: 69.78279
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    54 ----
[CW] collect: return: 25.36745, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.10125, qf2_loss: 0.10161, policy_loss: -47.36727, policy_entropy: 4.10163, alpha: 0.26147, time: 69.77563
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    55 ----
[CW] collect: return: 24.33687, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.06533, qf2_loss: 0.06587, policy_loss: -47.62582, policy_entropy: 4.10144, alpha: 0.25563, time: 69.65832
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    56 ----
[CW] collect: return: 27.71609, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.14967, qf2_loss: 0.15031, policy_loss: -47.87173, policy_entropy: 4.10118, alpha: 0.24993, time: 69.57440
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    57 ----
[CW] collect: return: 26.60368, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.08464, qf2_loss: 0.08436, policy_loss: -48.11345, policy_entropy: 4.10288, alpha: 0.24435, time: 69.41959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    58 ----
[CW] collect: return: 25.65504, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.07712, qf2_loss: 0.07763, policy_loss: -48.33741, policy_entropy: 4.10073, alpha: 0.23890, time: 69.54206
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    59 ----
[CW] collect: return: 27.45638, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.12252, qf2_loss: 0.12344, policy_loss: -48.55411, policy_entropy: 4.10356, alpha: 0.23358, time: 69.53858
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    60 ----
[CW] collect: return: 24.93966, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.07221, qf2_loss: 0.07286, policy_loss: -48.75820, policy_entropy: 4.10139, alpha: 0.22838, time: 69.68636
[CW] eval: return: 26.04658, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    61 ----
[CW] collect: return: 28.36076, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.06734, qf2_loss: 0.06749, policy_loss: -48.94932, policy_entropy: 4.10260, alpha: 0.22330, time: 69.94140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    62 ----
[CW] collect: return: 24.11666, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.14150, qf2_loss: 0.14184, policy_loss: -49.13754, policy_entropy: 4.10258, alpha: 0.21833, time: 69.75232
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    63 ----
[CW] collect: return: 26.91229, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.02920, qf2_loss: 0.02939, policy_loss: -49.31061, policy_entropy: 4.10275, alpha: 0.21347, time: 69.74779
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    64 ----
[CW] collect: return: 22.56563, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.09492, qf2_loss: 0.09603, policy_loss: -49.47314, policy_entropy: 4.10122, alpha: 0.20873, time: 69.75659
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    65 ----
[CW] collect: return: 27.68438, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.08769, qf2_loss: 0.08787, policy_loss: -49.62886, policy_entropy: 4.10233, alpha: 0.20409, time: 69.56251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    66 ----
[CW] collect: return: 23.79651, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.11132, qf2_loss: 0.11175, policy_loss: -49.77586, policy_entropy: 4.10135, alpha: 0.19956, time: 69.50380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    67 ----
[CW] collect: return: 29.36690, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.07117, qf2_loss: 0.07197, policy_loss: -49.91089, policy_entropy: 4.10213, alpha: 0.19513, time: 69.54271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    68 ----
[CW] collect: return: 25.48406, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.07387, qf2_loss: 0.07390, policy_loss: -50.03936, policy_entropy: 4.10043, alpha: 0.19079, time: 69.54581
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    69 ----
[CW] collect: return: 24.82452, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.06565, qf2_loss: 0.06623, policy_loss: -50.16298, policy_entropy: 4.10180, alpha: 0.18656, time: 69.51211
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    70 ----
[CW] collect: return: 27.41457, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.09780, qf2_loss: 0.09831, policy_loss: -50.26834, policy_entropy: 4.10202, alpha: 0.18242, time: 69.79154
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    71 ----
[CW] collect: return: 25.49609, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.09969, qf2_loss: 0.10125, policy_loss: -50.37853, policy_entropy: 4.10190, alpha: 0.17837, time: 69.72974
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    72 ----
[CW] collect: return: 29.06411, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.07965, qf2_loss: 0.07943, policy_loss: -50.46011, policy_entropy: 4.10145, alpha: 0.17442, time: 69.60551
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    73 ----
[CW] collect: return: 21.37976, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.05583, qf2_loss: 0.05593, policy_loss: -50.55419, policy_entropy: 4.10190, alpha: 0.17055, time: 69.77188
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    74 ----
[CW] collect: return: 27.48090, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.09364, qf2_loss: 0.09442, policy_loss: -50.63404, policy_entropy: 4.10111, alpha: 0.16677, time: 69.73425
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    75 ----
[CW] collect: return: 25.15412, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.07790, qf2_loss: 0.07775, policy_loss: -50.70439, policy_entropy: 4.10197, alpha: 0.16308, time: 69.53763
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    76 ----
[CW] collect: return: 24.97324, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.08844, qf2_loss: 0.08940, policy_loss: -50.77035, policy_entropy: 4.10057, alpha: 0.15946, time: 69.39796
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    77 ----
[CW] collect: return: 26.53879, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.06983, qf2_loss: 0.06988, policy_loss: -50.83041, policy_entropy: 4.10102, alpha: 0.15593, time: 69.54381
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    78 ----
[CW] collect: return: 25.94599, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.03886, qf2_loss: 0.03942, policy_loss: -50.87999, policy_entropy: 4.10181, alpha: 0.15248, time: 69.52973
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    79 ----
[CW] collect: return: 27.17523, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.07676, qf2_loss: 0.07677, policy_loss: -50.92853, policy_entropy: 4.10210, alpha: 0.14910, time: 69.43548
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    80 ----
[CW] collect: return: 25.41923, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.08805, qf2_loss: 0.08815, policy_loss: -50.96575, policy_entropy: 4.10211, alpha: 0.14580, time: 69.65316
[CW] eval: return: 24.91324, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    81 ----
[CW] collect: return: 30.10335, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.06294, qf2_loss: 0.06353, policy_loss: -50.99717, policy_entropy: 4.10088, alpha: 0.14257, time: 69.75552
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    82 ----
[CW] collect: return: 23.46532, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.08588, qf2_loss: 0.08580, policy_loss: -51.02110, policy_entropy: 4.10108, alpha: 0.13941, time: 69.70207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    83 ----
[CW] collect: return: 26.47022, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.07210, qf2_loss: 0.07313, policy_loss: -51.04395, policy_entropy: 4.10199, alpha: 0.13632, time: 69.63859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    84 ----
[CW] collect: return: 25.07406, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.07997, qf2_loss: 0.08017, policy_loss: -51.05908, policy_entropy: 4.10173, alpha: 0.13331, time: 69.52892
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    85 ----
[CW] collect: return: 27.67276, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.04352, qf2_loss: 0.04425, policy_loss: -51.06693, policy_entropy: 4.10226, alpha: 0.13036, time: 69.45338
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    86 ----
[CW] collect: return: 25.93154, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.10376, qf2_loss: 0.10387, policy_loss: -51.06323, policy_entropy: 4.10066, alpha: 0.12747, time: 69.30235
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    87 ----
[CW] collect: return: 28.72350, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.02607, qf2_loss: 0.02606, policy_loss: -51.06839, policy_entropy: 4.10156, alpha: 0.12465, time: 69.21296
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    88 ----
[CW] collect: return: 23.61298, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.07329, qf2_loss: 0.07406, policy_loss: -51.05847, policy_entropy: 4.10163, alpha: 0.12189, time: 69.33669
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    89 ----
[CW] collect: return: 27.61441, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.08384, qf2_loss: 0.08364, policy_loss: -51.04786, policy_entropy: 4.10172, alpha: 0.11919, time: 69.30842
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    90 ----
[CW] collect: return: 29.39954, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.05747, qf2_loss: 0.05664, policy_loss: -51.02901, policy_entropy: 4.10099, alpha: 0.11655, time: 69.35800
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    91 ----
[CW] collect: return: 24.10700, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.07103, qf2_loss: 0.07080, policy_loss: -51.00742, policy_entropy: 4.10145, alpha: 0.11398, time: 69.30739
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    92 ----
[CW] collect: return: 26.20853, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.09173, qf2_loss: 0.09242, policy_loss: -50.98185, policy_entropy: 4.10079, alpha: 0.11145, time: 69.37934
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    93 ----
[CW] collect: return: 27.56416, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.03777, qf2_loss: 0.03867, policy_loss: -50.95129, policy_entropy: 4.10101, alpha: 0.10899, time: 69.36529
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    94 ----
[CW] collect: return: 28.89033, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.06541, qf2_loss: 0.06621, policy_loss: -50.91961, policy_entropy: 4.10275, alpha: 0.10658, time: 69.72189
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    95 ----
[CW] collect: return: 27.42203, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.05080, qf2_loss: 0.05078, policy_loss: -50.87780, policy_entropy: 4.10088, alpha: 0.10422, time: 69.38510
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    96 ----
[CW] collect: return: 23.09465, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.06121, qf2_loss: 0.06063, policy_loss: -50.83256, policy_entropy: 4.10169, alpha: 0.10192, time: 69.17936
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    97 ----
[CW] collect: return: 26.06455, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.07045, qf2_loss: 0.07018, policy_loss: -50.78261, policy_entropy: 4.10136, alpha: 0.09966, time: 69.08449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    98 ----
[CW] collect: return: 27.43158, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.06994, qf2_loss: 0.07032, policy_loss: -50.73447, policy_entropy: 4.10002, alpha: 0.09746, time: 69.31241
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    99 ----
[CW] collect: return: 23.32450, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.06679, qf2_loss: 0.06747, policy_loss: -50.68081, policy_entropy: 4.10191, alpha: 0.09530, time: 69.21770
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   100 ----
[CW] collect: return: 30.09328, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.05345, qf2_loss: 0.05243, policy_loss: -50.62404, policy_entropy: 4.10181, alpha: 0.09320, time: 69.48795
[CW] eval: return: 24.79100, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   101 ----
[CW] collect: return: 21.13276, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.06466, qf2_loss: 0.06575, policy_loss: -50.56037, policy_entropy: 4.10058, alpha: 0.09113, time: 69.30640
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   102 ----
[CW] collect: return: 21.59355, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.06199, qf2_loss: 0.06150, policy_loss: -50.49414, policy_entropy: 4.10100, alpha: 0.08912, time: 69.48973
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   103 ----
[CW] collect: return: 21.46957, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.05333, qf2_loss: 0.05398, policy_loss: -50.42749, policy_entropy: 4.10237, alpha: 0.08715, time: 69.52667
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   104 ----
[CW] collect: return: 22.88466, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.05084, qf2_loss: 0.05133, policy_loss: -50.35423, policy_entropy: 4.10017, alpha: 0.08522, time: 69.47960
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   105 ----
[CW] collect: return: 23.82430, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.05470, qf2_loss: 0.05430, policy_loss: -50.27673, policy_entropy: 4.10171, alpha: 0.08334, time: 69.31795
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   106 ----
[CW] collect: return: 26.33329, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.08320, qf2_loss: 0.08281, policy_loss: -50.19247, policy_entropy: 4.10032, alpha: 0.08149, time: 69.44570
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   107 ----
[CW] collect: return: 27.79775, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.03468, qf2_loss: 0.03660, policy_loss: -50.11642, policy_entropy: 4.10161, alpha: 0.07969, time: 69.89843
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   108 ----
[CW] collect: return: 24.46950, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.05007, qf2_loss: 0.04933, policy_loss: -50.03577, policy_entropy: 4.10062, alpha: 0.07793, time: 69.96206
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   109 ----
[CW] collect: return: 27.09319, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.06735, qf2_loss: 0.06823, policy_loss: -49.94211, policy_entropy: 4.09975, alpha: 0.07621, time: 69.92985
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   110 ----
[CW] collect: return: 21.81592, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.05665, qf2_loss: 0.05476, policy_loss: -49.85253, policy_entropy: 4.10049, alpha: 0.07452, time: 69.93658
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   111 ----
[CW] collect: return: 23.90365, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.05834, qf2_loss: 0.05791, policy_loss: -49.76657, policy_entropy: 4.10147, alpha: 0.07287, time: 69.83182
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   112 ----
[CW] collect: return: 24.65282, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.04996, qf2_loss: 0.04910, policy_loss: -49.66908, policy_entropy: 4.10111, alpha: 0.07126, time: 69.66645
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   113 ----
[CW] collect: return: 27.85034, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.05688, qf2_loss: 0.05594, policy_loss: -49.57432, policy_entropy: 4.10086, alpha: 0.06968, time: 69.71700
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   114 ----
[CW] collect: return: 28.87746, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.05303, qf2_loss: 0.05461, policy_loss: -49.47121, policy_entropy: 4.10073, alpha: 0.06814, time: 69.70056
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   115 ----
[CW] collect: return: 28.96778, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.06256, qf2_loss: 0.06257, policy_loss: -49.36695, policy_entropy: 4.10103, alpha: 0.06664, time: 69.44640
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   116 ----
[CW] collect: return: 25.54670, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.05413, qf2_loss: 0.05366, policy_loss: -49.26652, policy_entropy: 4.10038, alpha: 0.06516, time: 69.35417
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   117 ----
[CW] collect: return: 24.25116, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.04675, qf2_loss: 0.04713, policy_loss: -49.16125, policy_entropy: 4.10177, alpha: 0.06372, time: 69.25865
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   118 ----
[CW] collect: return: 24.80088, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.05217, qf2_loss: 0.05038, policy_loss: -49.04533, policy_entropy: 4.10068, alpha: 0.06231, time: 69.42284
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   119 ----
[CW] collect: return: 24.29950, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.04887, qf2_loss: 0.05048, policy_loss: -48.91838, policy_entropy: 4.10007, alpha: 0.06093, time: 69.30428
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   120 ----
[CW] collect: return: 26.67854, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.10874, qf2_loss: 0.10861, policy_loss: -48.81988, policy_entropy: 4.10019, alpha: 0.05959, time: 69.34337
[CW] eval: return: 25.04580, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   121 ----
[CW] collect: return: 25.24887, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.01067, qf2_loss: 0.01055, policy_loss: -48.71090, policy_entropy: 4.09978, alpha: 0.05827, time: 69.64549
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   122 ----
[CW] collect: return: 23.87118, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.04421, qf2_loss: 0.04438, policy_loss: -48.59311, policy_entropy: 4.10203, alpha: 0.05698, time: 69.44027
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   123 ----
[CW] collect: return: 23.69180, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.03842, qf2_loss: 0.03812, policy_loss: -48.47368, policy_entropy: 4.09980, alpha: 0.05572, time: 69.42811
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   124 ----
[CW] collect: return: 27.87758, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.05841, qf2_loss: 0.05788, policy_loss: -48.35416, policy_entropy: 4.10125, alpha: 0.05449, time: 69.40362
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   125 ----
[CW] collect: return: 25.34283, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.08056, qf2_loss: 0.07701, policy_loss: -48.22427, policy_entropy: 4.10167, alpha: 0.05328, time: 69.23388
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   126 ----
[CW] collect: return: 19.77850, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.00969, qf2_loss: 0.01367, policy_loss: -48.10294, policy_entropy: 4.10141, alpha: 0.05210, time: 69.15682
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   127 ----
[CW] collect: return: 25.31917, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.04622, qf2_loss: 0.04579, policy_loss: -47.98166, policy_entropy: 4.10067, alpha: 0.05095, time: 69.13637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   128 ----
[CW] collect: return: 22.70504, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.05014, qf2_loss: 0.04927, policy_loss: -47.85080, policy_entropy: 4.10012, alpha: 0.04983, time: 69.16417
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   129 ----
[CW] collect: return: 24.82324, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.07719, qf2_loss: 0.07851, policy_loss: -47.72711, policy_entropy: 4.09959, alpha: 0.04872, time: 69.12265
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   130 ----
[CW] collect: return: 28.61583, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.03293, qf2_loss: 0.02851, policy_loss: -47.59784, policy_entropy: 4.09949, alpha: 0.04765, time: 69.46627
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   131 ----
[CW] collect: return: 25.79474, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.03855, qf2_loss: 0.04185, policy_loss: -47.46463, policy_entropy: 4.10057, alpha: 0.04659, time: 69.25320
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   132 ----
[CW] collect: return: 28.34021, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.05715, qf2_loss: 0.05460, policy_loss: -47.33290, policy_entropy: 4.10040, alpha: 0.04556, time: 69.22973
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   133 ----
[CW] collect: return: 25.98076, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.03077, qf2_loss: 0.03139, policy_loss: -47.20042, policy_entropy: 4.10079, alpha: 0.04455, time: 69.38340
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   134 ----
[CW] collect: return: 22.34171, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.04945, qf2_loss: 0.04997, policy_loss: -47.06756, policy_entropy: 4.10123, alpha: 0.04357, time: 69.24560
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   135 ----
[CW] collect: return: 23.36346, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.05725, qf2_loss: 0.05510, policy_loss: -46.93020, policy_entropy: 4.10001, alpha: 0.04261, time: 69.09192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   136 ----
[CW] collect: return: 22.34764, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.02801, qf2_loss: 0.02886, policy_loss: -46.79468, policy_entropy: 4.10131, alpha: 0.04166, time: 69.01045
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   137 ----
[CW] collect: return: 28.51387, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.05810, qf2_loss: 0.05767, policy_loss: -46.65923, policy_entropy: 4.09984, alpha: 0.04074, time: 69.15904
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   138 ----
[CW] collect: return: 26.24229, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.04279, qf2_loss: 0.04262, policy_loss: -46.52277, policy_entropy: 4.10030, alpha: 0.03984, time: 69.06457
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   139 ----
[CW] collect: return: 25.89125, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.03430, qf2_loss: 0.03597, policy_loss: -46.37634, policy_entropy: 4.09913, alpha: 0.03896, time: 69.11382
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   140 ----
[CW] collect: return: 24.09795, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.05883, qf2_loss: 0.05532, policy_loss: -46.23259, policy_entropy: 4.09908, alpha: 0.03810, time: 69.10060
[CW] eval: return: 24.56090, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   141 ----
[CW] collect: return: 32.93979, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.02474, qf2_loss: 0.02744, policy_loss: -46.10007, policy_entropy: 4.09873, alpha: 0.03726, time: 69.40017
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   142 ----
[CW] collect: return: 28.55560, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.08701, qf2_loss: 0.08553, policy_loss: -45.95261, policy_entropy: 4.09918, alpha: 0.03643, time: 69.14945
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   143 ----
[CW] collect: return: 25.14655, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.01096, qf2_loss: 0.01228, policy_loss: -45.81862, policy_entropy: 4.09923, alpha: 0.03563, time: 69.30632
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   144 ----
[CW] collect: return: 24.02638, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.03757, qf2_loss: 0.03734, policy_loss: -45.67131, policy_entropy: 4.09797, alpha: 0.03484, time: 69.17360
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   145 ----
[CW] collect: return: 29.65075, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.04409, qf2_loss: 0.04405, policy_loss: -45.52598, policy_entropy: 4.09879, alpha: 0.03407, time: 68.93883
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   146 ----
[CW] collect: return: 27.43171, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.04688, qf2_loss: 0.04721, policy_loss: -45.36984, policy_entropy: 4.09873, alpha: 0.03332, time: 69.11702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   147 ----
[CW] collect: return: 29.33237, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.04008, qf2_loss: 0.03819, policy_loss: -45.23652, policy_entropy: 4.09883, alpha: 0.03258, time: 69.52830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   148 ----
[CW] collect: return: 24.42254, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.04048, qf2_loss: 0.04153, policy_loss: -45.09522, policy_entropy: 4.09905, alpha: 0.03186, time: 72.04264
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   149 ----
[CW] collect: return: 23.64377, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.04851, qf2_loss: 0.04856, policy_loss: -44.94576, policy_entropy: 4.09804, alpha: 0.03115, time: 69.12724
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   150 ----
[CW] collect: return: 25.47388, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.05825, qf2_loss: 0.05707, policy_loss: -44.79147, policy_entropy: 4.09762, alpha: 0.03047, time: 69.26843
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   151 ----
[CW] collect: return: 23.83184, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.01478, qf2_loss: 0.01476, policy_loss: -44.65090, policy_entropy: 4.09893, alpha: 0.02979, time: 69.37986
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   152 ----
[CW] collect: return: 31.39074, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.04523, qf2_loss: 0.04529, policy_loss: -44.50706, policy_entropy: 4.09892, alpha: 0.02913, time: 69.30516
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   153 ----
[CW] collect: return: 29.66511, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.04274, qf2_loss: 0.04304, policy_loss: -44.35517, policy_entropy: 4.09941, alpha: 0.02849, time: 69.21582
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   154 ----
[CW] collect: return: 23.89644, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.04197, qf2_loss: 0.04217, policy_loss: -44.20890, policy_entropy: 4.09804, alpha: 0.02786, time: 69.32149
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   155 ----
[CW] collect: return: 21.09617, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.03623, qf2_loss: 0.03535, policy_loss: -44.04047, policy_entropy: 4.09861, alpha: 0.02724, time: 69.46232
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   156 ----
[CW] collect: return: 21.62257, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.06671, qf2_loss: 0.06521, policy_loss: -43.89810, policy_entropy: 4.09735, alpha: 0.02664, time: 69.22870
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   157 ----
[CW] collect: return: 24.15701, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.02449, qf2_loss: 0.02664, policy_loss: -43.75331, policy_entropy: 4.09716, alpha: 0.02605, time: 69.34527
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   158 ----
[CW] collect: return: 21.45847, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.03643, qf2_loss: 0.03667, policy_loss: -43.60621, policy_entropy: 4.09756, alpha: 0.02548, time: 69.33876
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   159 ----
[CW] collect: return: 25.57377, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.04022, qf2_loss: 0.04015, policy_loss: -43.45690, policy_entropy: 4.09540, alpha: 0.02491, time: 69.10903
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   160 ----
[CW] collect: return: 22.55434, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.03721, qf2_loss: 0.03606, policy_loss: -43.30140, policy_entropy: 4.09824, alpha: 0.02436, time: 68.83444
[CW] eval: return: 24.57573, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   161 ----
[CW] collect: return: 28.16402, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.04804, qf2_loss: 0.04902, policy_loss: -43.15372, policy_entropy: 4.09454, alpha: 0.02382, time: 69.22281
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   162 ----
[CW] collect: return: 26.34614, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.03570, qf2_loss: 0.03532, policy_loss: -43.00283, policy_entropy: 4.09217, alpha: 0.02330, time: 69.02717
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   163 ----
[CW] collect: return: 26.50083, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.04142, qf2_loss: 0.04176, policy_loss: -42.85097, policy_entropy: 4.09357, alpha: 0.02278, time: 69.03531
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   164 ----
[CW] collect: return: 27.72552, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.03464, qf2_loss: 0.03352, policy_loss: -42.70090, policy_entropy: 4.09466, alpha: 0.02228, time: 69.12302
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   165 ----
[CW] collect: return: 25.23770, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.04098, qf2_loss: 0.04104, policy_loss: -42.54874, policy_entropy: 4.09151, alpha: 0.02179, time: 68.83864
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   166 ----
[CW] collect: return: 20.55336, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.04358, qf2_loss: 0.04459, policy_loss: -42.39644, policy_entropy: 4.09748, alpha: 0.02130, time: 68.91490
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   167 ----
[CW] collect: return: 24.88114, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.04176, qf2_loss: 0.03842, policy_loss: -42.23837, policy_entropy: 4.09313, alpha: 0.02083, time: 69.18015
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   168 ----
[CW] collect: return: 22.76010, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.05805, qf2_loss: 0.05807, policy_loss: -42.08794, policy_entropy: 4.09046, alpha: 0.02037, time: 69.02832
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   169 ----
[CW] collect: return: 23.88539, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.01426, qf2_loss: 0.01508, policy_loss: -41.94070, policy_entropy: 4.09150, alpha: 0.01992, time: 69.12186
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   170 ----
[CW] collect: return: 26.76691, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.03504, qf2_loss: 0.03455, policy_loss: -41.78466, policy_entropy: 4.09357, alpha: 0.01948, time: 69.12587
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   171 ----
[CW] collect: return: 25.60916, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.03762, qf2_loss: 0.03807, policy_loss: -41.63729, policy_entropy: 4.09421, alpha: 0.01905, time: 69.10656
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   172 ----
[CW] collect: return: 29.02558, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.04525, qf2_loss: 0.04505, policy_loss: -41.48525, policy_entropy: 4.09405, alpha: 0.01863, time: 69.07422
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   173 ----
[CW] collect: return: 26.08904, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.04240, qf2_loss: 0.04082, policy_loss: -41.33567, policy_entropy: 4.09550, alpha: 0.01822, time: 69.14230
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   174 ----
[CW] collect: return: 23.24055, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.03532, qf2_loss: 0.03576, policy_loss: -41.18559, policy_entropy: 4.09384, alpha: 0.01782, time: 69.11702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   175 ----
[CW] collect: return: 26.16172, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.04998, qf2_loss: 0.04958, policy_loss: -41.02867, policy_entropy: 4.09591, alpha: 0.01742, time: 69.04218
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   176 ----
[CW] collect: return: 26.43797, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.03709, qf2_loss: 0.03724, policy_loss: -40.87890, policy_entropy: 4.09392, alpha: 0.01704, time: 68.82337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   177 ----
[CW] collect: return: 23.02919, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.05410, qf2_loss: 0.05304, policy_loss: -40.72697, policy_entropy: 4.09229, alpha: 0.01666, time: 68.97487
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   178 ----
[CW] collect: return: 23.99356, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.02303, qf2_loss: 0.02306, policy_loss: -40.57429, policy_entropy: 4.09033, alpha: 0.01629, time: 69.00570
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   179 ----
[CW] collect: return: 24.11642, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.03680, qf2_loss: 0.03596, policy_loss: -40.42110, policy_entropy: 4.09125, alpha: 0.01593, time: 68.92471
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   180 ----
[CW] collect: return: 26.62286, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.04610, qf2_loss: 0.04645, policy_loss: -40.26346, policy_entropy: 4.09226, alpha: 0.01558, time: 69.08723
[CW] eval: return: 25.39229, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   181 ----
[CW] collect: return: 25.95939, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.03303, qf2_loss: 0.03273, policy_loss: -40.12371, policy_entropy: 4.08860, alpha: 0.01523, time: 69.28989
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   182 ----
[CW] collect: return: 24.57730, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.04495, qf2_loss: 0.04438, policy_loss: -39.96390, policy_entropy: 4.08777, alpha: 0.01490, time: 69.25090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   183 ----
[CW] collect: return: 28.80751, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.03650, qf2_loss: 0.03615, policy_loss: -39.81315, policy_entropy: 4.08796, alpha: 0.01457, time: 69.09333
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   184 ----
[CW] collect: return: 25.34097, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.05084, qf2_loss: 0.04934, policy_loss: -39.66861, policy_entropy: 4.08342, alpha: 0.01425, time: 69.14011
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   185 ----
[CW] collect: return: 24.41062, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.03977, qf2_loss: 0.03892, policy_loss: -39.51517, policy_entropy: 4.08630, alpha: 0.01393, time: 68.96290
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   186 ----
[CW] collect: return: 23.96237, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.03254, qf2_loss: 0.03338, policy_loss: -39.36082, policy_entropy: 4.08542, alpha: 0.01362, time: 68.96376
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   187 ----
[CW] collect: return: 23.19115, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.05488, qf2_loss: 0.05433, policy_loss: -39.20982, policy_entropy: 4.08544, alpha: 0.01332, time: 70.21017
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   188 ----
[CW] collect: return: 25.25236, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.02505, qf2_loss: 0.02408, policy_loss: -39.06137, policy_entropy: 4.08330, alpha: 0.01303, time: 70.16581
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   189 ----
[CW] collect: return: 24.21184, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.04186, qf2_loss: 0.04123, policy_loss: -38.90941, policy_entropy: 4.08444, alpha: 0.01274, time: 69.88512
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   190 ----
[CW] collect: return: 24.86827, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.04527, qf2_loss: 0.04446, policy_loss: -38.76196, policy_entropy: 4.07911, alpha: 0.01246, time: 70.05111
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   191 ----
[CW] collect: return: 25.72392, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.03762, qf2_loss: 0.03703, policy_loss: -38.61047, policy_entropy: 4.07351, alpha: 0.01218, time: 69.95572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   192 ----
[CW] collect: return: 26.09933, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.04061, qf2_loss: 0.03985, policy_loss: -38.46448, policy_entropy: 4.06565, alpha: 0.01192, time: 69.85323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   193 ----
[CW] collect: return: 26.58201, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.05668, qf2_loss: 0.05615, policy_loss: -38.31146, policy_entropy: 4.07296, alpha: 0.01165, time: 69.89861
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   194 ----
[CW] collect: return: 29.09972, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.02327, qf2_loss: 0.02280, policy_loss: -38.16520, policy_entropy: 4.07158, alpha: 0.01140, time: 69.84660
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   195 ----
[CW] collect: return: 24.90116, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.05460, qf2_loss: 0.05367, policy_loss: -38.01790, policy_entropy: 4.06196, alpha: 0.01114, time: 69.61858
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   196 ----
[CW] collect: return: 25.50642, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.02847, qf2_loss: 0.02870, policy_loss: -37.87190, policy_entropy: 4.04686, alpha: 0.01090, time: 69.43276
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   197 ----
[CW] collect: return: 25.77948, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.04853, qf2_loss: 0.04741, policy_loss: -37.71734, policy_entropy: 4.04111, alpha: 0.01066, time: 69.36933
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   198 ----
[CW] collect: return: 27.32368, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.04305, qf2_loss: 0.04249, policy_loss: -37.57029, policy_entropy: 4.06256, alpha: 0.01042, time: 69.26648
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   199 ----
[CW] collect: return: 25.42017, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.04119, qf2_loss: 0.04057, policy_loss: -37.42649, policy_entropy: 4.06136, alpha: 0.01019, time: 69.46190
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   200 ----
[CW] collect: return: 24.85137, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.04259, qf2_loss: 0.04181, policy_loss: -37.27535, policy_entropy: 4.04681, alpha: 0.00997, time: 70.60869
[CW] eval: return: 26.47642, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   201 ----
[CW] collect: return: 25.35217, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.03957, qf2_loss: 0.03909, policy_loss: -37.12619, policy_entropy: 4.04953, alpha: 0.00975, time: 69.71219
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   202 ----
[CW] collect: return: 27.64676, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.05560, qf2_loss: 0.05454, policy_loss: -36.98318, policy_entropy: 3.91546, alpha: 0.00953, time: 69.34132
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   203 ----
[CW] collect: return: 15.97950, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.02788, qf2_loss: 0.02787, policy_loss: -36.84609, policy_entropy: 3.43279, alpha: 0.00933, time: 69.35503
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   204 ----
[CW] collect: return: 26.97409, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.04099, qf2_loss: 0.04038, policy_loss: -36.70561, policy_entropy: 3.27781, alpha: 0.00914, time: 69.33662
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   205 ----
[CW] collect: return: 30.57961, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.05248, qf2_loss: 0.05156, policy_loss: -36.56813, policy_entropy: 3.24756, alpha: 0.00895, time: 69.11769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   206 ----
[CW] collect: return: 30.18678, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.04465, qf2_loss: 0.04360, policy_loss: -36.42496, policy_entropy: 3.25355, alpha: 0.00876, time: 69.18965
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   207 ----
[CW] collect: return: 27.98019, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.06910, qf2_loss: 0.06929, policy_loss: -36.27992, policy_entropy: 3.41264, alpha: 0.00858, time: 69.18161
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   208 ----
[CW] collect: return: 26.74848, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.02737, qf2_loss: 0.02697, policy_loss: -36.14696, policy_entropy: 3.12843, alpha: 0.00840, time: 69.15713
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   209 ----
[CW] collect: return: 31.79100, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.06884, qf2_loss: 0.06704, policy_loss: -35.99882, policy_entropy: 3.10182, alpha: 0.00823, time: 69.07944
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   210 ----
[CW] collect: return: 26.52828, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.03458, qf2_loss: 0.03467, policy_loss: -35.86852, policy_entropy: 2.96819, alpha: 0.00806, time: 69.28738
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   211 ----
[CW] collect: return: 26.41995, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.04450, qf2_loss: 0.04354, policy_loss: -35.72378, policy_entropy: 2.75834, alpha: 0.00790, time: 69.24945
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   212 ----
[CW] collect: return: 13.24877, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.05276, qf2_loss: 0.05190, policy_loss: -35.58928, policy_entropy: 2.67928, alpha: 0.00774, time: 69.18641
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   213 ----
[CW] collect: return: 13.52209, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.05182, qf2_loss: 0.05175, policy_loss: -35.45210, policy_entropy: 2.83317, alpha: 0.00758, time: 69.19024
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   214 ----
[CW] collect: return: 22.63400, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 0.06810, qf2_loss: 0.06682, policy_loss: -35.32478, policy_entropy: 2.63116, alpha: 0.00743, time: 69.10547
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   215 ----
[CW] collect: return: 37.55247, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.04724, qf2_loss: 0.04697, policy_loss: -35.18921, policy_entropy: 2.45872, alpha: 0.00728, time: 68.97859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   216 ----
[CW] collect: return: 14.00608, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.06802, qf2_loss: 0.06640, policy_loss: -35.04151, policy_entropy: 2.53569, alpha: 0.00713, time: 69.07760
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   217 ----
[CW] collect: return: 29.53485, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.06325, qf2_loss: 0.06224, policy_loss: -34.91927, policy_entropy: 1.88762, alpha: 0.00699, time: 69.00827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   218 ----
[CW] collect: return: 47.12906, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.06723, qf2_loss: 0.06711, policy_loss: -34.79078, policy_entropy: 1.29756, alpha: 0.00686, time: 68.94339
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   219 ----
[CW] collect: return: 24.95488, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.06065, qf2_loss: 0.05947, policy_loss: -34.66591, policy_entropy: 0.74420, alpha: 0.00674, time: 68.82277
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   220 ----
[CW] collect: return: 19.62643, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.06142, qf2_loss: 0.06023, policy_loss: -34.55191, policy_entropy: -1.43353, alpha: 0.00664, time: 68.99525
[CW] eval: return: 33.11813, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   221 ----
[CW] collect: return: 45.46896, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.07520, qf2_loss: 0.07383, policy_loss: -34.43621, policy_entropy: -1.61326, alpha: 0.00657, time: 69.30060
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   222 ----
[CW] collect: return: 23.11104, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.09624, qf2_loss: 0.09526, policy_loss: -34.30960, policy_entropy: -1.02409, alpha: 0.00649, time: 69.12639
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   223 ----
[CW] collect: return: 23.17755, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.06957, qf2_loss: 0.06774, policy_loss: -34.19242, policy_entropy: -0.90227, alpha: 0.00640, time: 69.03438
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   224 ----
[CW] collect: return: 42.27699, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.06446, qf2_loss: 0.06380, policy_loss: -34.08480, policy_entropy: -1.69358, alpha: 0.00631, time: 69.01087
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   225 ----
[CW] collect: return: 39.69925, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.07640, qf2_loss: 0.07575, policy_loss: -33.99334, policy_entropy: -2.48652, alpha: 0.00624, time: 68.90914
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   226 ----
[CW] collect: return: 13.59267, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.08023, qf2_loss: 0.07877, policy_loss: -33.87928, policy_entropy: -0.78424, alpha: 0.00616, time: 68.92336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   227 ----
[CW] collect: return: 23.50659, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.06478, qf2_loss: 0.06476, policy_loss: -33.77497, policy_entropy: -1.20533, alpha: 0.00607, time: 69.08212
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   228 ----
[CW] collect: return: 25.10021, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 0.08205, qf2_loss: 0.08214, policy_loss: -33.66786, policy_entropy: -1.39611, alpha: 0.00598, time: 69.01031
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   229 ----
[CW] collect: return: 40.32740, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.10116, qf2_loss: 0.09791, policy_loss: -33.57006, policy_entropy: -1.58928, alpha: 0.00590, time: 68.90891
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   230 ----
[CW] collect: return: 26.68072, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.09057, qf2_loss: 0.09020, policy_loss: -33.46353, policy_entropy: -1.06512, alpha: 0.00581, time: 69.09190
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   231 ----
[CW] collect: return: 24.23445, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 0.06694, qf2_loss: 0.06527, policy_loss: -33.37576, policy_entropy: -1.20983, alpha: 0.00571, time: 69.02527
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   232 ----
[CW] collect: return: 24.58141, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.06676, qf2_loss: 0.06621, policy_loss: -33.28166, policy_entropy: -1.60074, alpha: 0.00562, time: 69.00524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   233 ----
[CW] collect: return: 23.40575, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.08235, qf2_loss: 0.08150, policy_loss: -33.20699, policy_entropy: -2.52792, alpha: 0.00554, time: 69.02274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   234 ----
[CW] collect: return: 25.39493, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.09680, qf2_loss: 0.09245, policy_loss: -33.12561, policy_entropy: -3.04548, alpha: 0.00548, time: 68.84311
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   235 ----
[CW] collect: return: 24.03918, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.06918, qf2_loss: 0.06726, policy_loss: -33.05134, policy_entropy: -3.12260, alpha: 0.00542, time: 68.62476
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   236 ----
[CW] collect: return: 23.02353, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.09330, qf2_loss: 0.09042, policy_loss: -32.99851, policy_entropy: -3.55757, alpha: 0.00537, time: 68.84957
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   237 ----
[CW] collect: return: 23.08799, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 0.09264, qf2_loss: 0.08795, policy_loss: -32.95450, policy_entropy: -3.67566, alpha: 0.00532, time: 68.89303
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   238 ----
[CW] collect: return: 24.03154, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 0.06734, qf2_loss: 0.06495, policy_loss: -32.87589, policy_entropy: -3.52119, alpha: 0.00527, time: 68.77275
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   239 ----
[CW] collect: return: 11.22401, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 0.08785, qf2_loss: 0.08511, policy_loss: -32.81355, policy_entropy: -2.94341, alpha: 0.00521, time: 68.73229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   240 ----
[CW] collect: return: 9.93434, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.09533, qf2_loss: 0.09314, policy_loss: -32.74604, policy_entropy: -3.08634, alpha: 0.00514, time: 69.02752
[CW] eval: return: 24.95022, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   241 ----
[CW] collect: return: 10.86676, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 0.08451, qf2_loss: 0.08320, policy_loss: -32.63157, policy_entropy: -3.19185, alpha: 0.00507, time: 69.12416
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   242 ----
[CW] collect: return: 9.23097, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 0.06536, qf2_loss: 0.06330, policy_loss: -32.53352, policy_entropy: -2.62993, alpha: 0.00500, time: 68.93733
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   243 ----
[CW] collect: return: 22.41778, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 0.06642, qf2_loss: 0.06589, policy_loss: -32.46096, policy_entropy: -2.68137, alpha: 0.00493, time: 68.87851
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   244 ----
[CW] collect: return: 6.03311, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 0.07948, qf2_loss: 0.07841, policy_loss: -32.35058, policy_entropy: -2.91313, alpha: 0.00485, time: 69.92361
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   245 ----
[CW] collect: return: 5.33191, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 0.06083, qf2_loss: 0.05981, policy_loss: -32.25585, policy_entropy: -1.98459, alpha: 0.00477, time: 68.85169
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   246 ----
[CW] collect: return: 10.40483, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 0.07582, qf2_loss: 0.07370, policy_loss: -32.12211, policy_entropy: -1.76175, alpha: 0.00467, time: 68.83106
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   247 ----
[CW] collect: return: 46.49923, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 0.08169, qf2_loss: 0.08024, policy_loss: -32.05888, policy_entropy: -2.29045, alpha: 0.00458, time: 68.85971
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   248 ----
[CW] collect: return: 44.24452, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 0.05897, qf2_loss: 0.05811, policy_loss: -31.93552, policy_entropy: -2.70930, alpha: 0.00450, time: 68.70621
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   249 ----
[CW] collect: return: 44.22739, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 0.09162, qf2_loss: 0.09051, policy_loss: -31.83539, policy_entropy: -2.94761, alpha: 0.00443, time: 68.72148
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   250 ----
[CW] collect: return: 43.34238, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 0.07592, qf2_loss: 0.07456, policy_loss: -31.76629, policy_entropy: -3.67993, alpha: 0.00436, time: 68.87444
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   251 ----
[CW] collect: return: 45.73070, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 0.04603, qf2_loss: 0.04545, policy_loss: -31.65841, policy_entropy: -3.72837, alpha: 0.00431, time: 69.23304
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   252 ----
[CW] collect: return: 19.15804, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 0.05429, qf2_loss: 0.05395, policy_loss: -31.54343, policy_entropy: -3.30132, alpha: 0.00426, time: 69.45866
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   253 ----
[CW] collect: return: 19.24394, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 0.06136, qf2_loss: 0.06003, policy_loss: -31.45940, policy_entropy: -3.54790, alpha: 0.00419, time: 68.95696
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   254 ----
[CW] collect: return: 27.43269, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 0.04688, qf2_loss: 0.04642, policy_loss: -31.34597, policy_entropy: -3.66731, alpha: 0.00413, time: 68.89405
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   255 ----
[CW] collect: return: 18.21859, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 0.05215, qf2_loss: 0.05140, policy_loss: -31.23626, policy_entropy: -1.74121, alpha: 0.00406, time: 68.83138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   256 ----
[CW] collect: return: 17.46643, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 0.06644, qf2_loss: 0.06717, policy_loss: -31.13216, policy_entropy: -1.18893, alpha: 0.00396, time: 69.01330
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   257 ----
[CW] collect: return: 17.64979, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 0.04411, qf2_loss: 0.04312, policy_loss: -31.03686, policy_entropy: -0.65069, alpha: 0.00384, time: 68.96508
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   258 ----
[CW] collect: return: 27.16036, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 0.04701, qf2_loss: 0.04671, policy_loss: -30.95581, policy_entropy: 0.02121, alpha: 0.00372, time: 68.97427
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   259 ----
[CW] collect: return: 18.53275, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 0.04886, qf2_loss: 0.04986, policy_loss: -30.83112, policy_entropy: -0.30330, alpha: 0.00361, time: 68.91662
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   260 ----
[CW] collect: return: 20.77718, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 0.06036, qf2_loss: 0.05943, policy_loss: -30.70669, policy_entropy: -1.44955, alpha: 0.00351, time: 69.15561
[CW] eval: return: 25.53990, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   261 ----
[CW] collect: return: 18.12459, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 0.06262, qf2_loss: 0.06120, policy_loss: -30.62481, policy_entropy: -0.08681, alpha: 0.00342, time: 69.19870
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   262 ----
[CW] collect: return: 27.71661, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 0.04074, qf2_loss: 0.04095, policy_loss: -30.51710, policy_entropy: 0.52311, alpha: 0.00332, time: 69.05843
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   263 ----
[CW] collect: return: 28.60696, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 0.03435, qf2_loss: 0.03354, policy_loss: -30.42892, policy_entropy: -0.24164, alpha: 0.00322, time: 68.91851
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   264 ----
[CW] collect: return: 27.32864, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 0.07262, qf2_loss: 0.07174, policy_loss: -30.29648, policy_entropy: -1.15921, alpha: 0.00315, time: 68.63158
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   265 ----
[CW] collect: return: 27.64378, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 0.03664, qf2_loss: 0.03585, policy_loss: -30.22754, policy_entropy: -0.57481, alpha: 0.00307, time: 68.78309
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   266 ----
[CW] collect: return: 27.63345, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 0.03644, qf2_loss: 0.03685, policy_loss: -30.09562, policy_entropy: -0.15773, alpha: 0.00299, time: 68.78317
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   267 ----
[CW] collect: return: 38.58059, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 0.03906, qf2_loss: 0.03759, policy_loss: -29.97987, policy_entropy: -0.17232, alpha: 0.00291, time: 68.83074
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   268 ----
[CW] collect: return: 27.55467, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 0.04707, qf2_loss: 0.04660, policy_loss: -29.92429, policy_entropy: -0.02845, alpha: 0.00284, time: 68.85564
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   269 ----
[CW] collect: return: 26.18511, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 0.02902, qf2_loss: 0.02851, policy_loss: -29.80356, policy_entropy: -0.23111, alpha: 0.00276, time: 69.00370
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   270 ----
[CW] collect: return: 60.70198, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 0.07652, qf2_loss: 0.07370, policy_loss: -29.70721, policy_entropy: -0.12750, alpha: 0.00269, time: 69.03597
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   271 ----
[CW] collect: return: 27.09774, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 0.03021, qf2_loss: 0.03040, policy_loss: -29.58082, policy_entropy: -0.54695, alpha: 0.00263, time: 68.90666
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   272 ----
[CW] collect: return: 24.79990, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 0.06713, qf2_loss: 0.06675, policy_loss: -29.47241, policy_entropy: -0.42415, alpha: 0.00257, time: 69.03884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   273 ----
[CW] collect: return: 44.05007, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 0.02600, qf2_loss: 0.02602, policy_loss: -29.37009, policy_entropy: 0.01861, alpha: 0.00250, time: 68.94942
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   274 ----
[CW] collect: return: 45.35949, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 0.03362, qf2_loss: 0.03350, policy_loss: -29.26443, policy_entropy: -0.30809, alpha: 0.00244, time: 68.91270
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   275 ----
[CW] collect: return: 30.10679, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 0.04122, qf2_loss: 0.04068, policy_loss: -29.16534, policy_entropy: -0.33175, alpha: 0.00239, time: 68.74930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   276 ----
[CW] collect: return: 58.47039, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 0.03677, qf2_loss: 0.03629, policy_loss: -29.07178, policy_entropy: -0.68663, alpha: 0.00233, time: 68.81870
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   277 ----
[CW] collect: return: 61.59358, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 0.04069, qf2_loss: 0.04013, policy_loss: -28.96507, policy_entropy: -1.24251, alpha: 0.00228, time: 68.79385
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   278 ----
[CW] collect: return: 48.73430, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 0.04308, qf2_loss: 0.04174, policy_loss: -28.85040, policy_entropy: -1.21507, alpha: 0.00224, time: 68.82666
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   279 ----
[CW] collect: return: 60.19052, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 0.03563, qf2_loss: 0.03592, policy_loss: -28.74812, policy_entropy: -0.97411, alpha: 0.00219, time: 68.72291
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   280 ----
[CW] collect: return: 45.33659, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 0.04846, qf2_loss: 0.04770, policy_loss: -28.68907, policy_entropy: -1.23708, alpha: 0.00215, time: 69.06615
[CW] eval: return: 58.89566, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   281 ----
[CW] collect: return: 59.56155, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 0.03322, qf2_loss: 0.03263, policy_loss: -28.57445, policy_entropy: -1.73531, alpha: 0.00210, time: 69.11645
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   282 ----
[CW] collect: return: 43.31055, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 0.04931, qf2_loss: 0.04874, policy_loss: -28.46981, policy_entropy: -1.46991, alpha: 0.00207, time: 69.34500
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   283 ----
[CW] collect: return: 43.92466, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 0.05250, qf2_loss: 0.05150, policy_loss: -28.35066, policy_entropy: -1.62525, alpha: 0.00203, time: 69.36976
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   284 ----
[CW] collect: return: 58.48044, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 0.04357, qf2_loss: 0.04337, policy_loss: -28.27427, policy_entropy: -2.41010, alpha: 0.00199, time: 69.06286
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   285 ----
[CW] collect: return: 43.88882, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 0.02862, qf2_loss: 0.02843, policy_loss: -28.18690, policy_entropy: -1.78469, alpha: 0.00196, time: 69.09390
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   286 ----
[CW] collect: return: 58.69967, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 0.04405, qf2_loss: 0.04375, policy_loss: -28.09923, policy_entropy: -2.14997, alpha: 0.00192, time: 69.01702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   287 ----
[CW] collect: return: 51.40592, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 0.05700, qf2_loss: 0.05510, policy_loss: -27.99208, policy_entropy: -2.37119, alpha: 0.00189, time: 68.90317
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   288 ----
[CW] collect: return: 57.78811, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 0.03920, qf2_loss: 0.03938, policy_loss: -27.91476, policy_entropy: -2.17110, alpha: 0.00185, time: 68.89382
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   289 ----
[CW] collect: return: 60.10642, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 0.03068, qf2_loss: 0.03030, policy_loss: -27.82983, policy_entropy: -2.77587, alpha: 0.00182, time: 68.99526
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   290 ----
[CW] collect: return: 51.00520, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 0.03175, qf2_loss: 0.03174, policy_loss: -27.75133, policy_entropy: -2.86870, alpha: 0.00180, time: 68.91736
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   291 ----
[CW] collect: return: 43.60233, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 0.07835, qf2_loss: 0.07700, policy_loss: -27.61023, policy_entropy: -3.22348, alpha: 0.00177, time: 69.03599
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   292 ----
[CW] collect: return: 61.82932, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 0.03089, qf2_loss: 0.03038, policy_loss: -27.54355, policy_entropy: -3.44029, alpha: 0.00174, time: 69.03754
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   293 ----
[CW] collect: return: 46.82822, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 0.03145, qf2_loss: 0.03114, policy_loss: -27.48031, policy_entropy: -3.98340, alpha: 0.00172, time: 69.01001
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   294 ----
[CW] collect: return: 43.94712, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 0.04951, qf2_loss: 0.04882, policy_loss: -27.37998, policy_entropy: -3.82577, alpha: 0.00171, time: 68.79457
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   295 ----
[CW] collect: return: 56.12453, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 0.03134, qf2_loss: 0.03118, policy_loss: -27.28470, policy_entropy: -3.83697, alpha: 0.00168, time: 70.44906
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   296 ----
[CW] collect: return: 57.27848, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 0.10405, qf2_loss: 0.10311, policy_loss: -27.21529, policy_entropy: -4.22458, alpha: 0.00167, time: 69.14653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   297 ----
[CW] collect: return: 46.15150, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 0.02937, qf2_loss: 0.02895, policy_loss: -27.11454, policy_entropy: -4.88712, alpha: 0.00165, time: 69.46489
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   298 ----
[CW] collect: return: 45.81426, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 0.03201, qf2_loss: 0.03146, policy_loss: -27.02521, policy_entropy: -4.87323, alpha: 0.00164, time: 68.98402
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   299 ----
[CW] collect: return: 49.27594, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 0.03568, qf2_loss: 0.03562, policy_loss: -26.97220, policy_entropy: -5.03460, alpha: 0.00163, time: 69.00848
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   300 ----
[CW] collect: return: 44.29366, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 0.04013, qf2_loss: 0.03875, policy_loss: -26.87191, policy_entropy: -5.80530, alpha: 0.00162, time: 69.03760
[CW] eval: return: 47.38421, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   301 ----
[CW] collect: return: 50.15941, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 0.06852, qf2_loss: 0.06868, policy_loss: -26.80465, policy_entropy: -4.30328, alpha: 0.00161, time: 69.31723
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   302 ----
[CW] collect: return: 44.57873, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 0.03982, qf2_loss: 0.03844, policy_loss: -26.68060, policy_entropy: -4.41010, alpha: 0.00159, time: 69.04804
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   303 ----
[CW] collect: return: 66.30341, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 0.03721, qf2_loss: 0.03670, policy_loss: -26.63004, policy_entropy: -5.06977, alpha: 0.00158, time: 68.89380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   304 ----
[CW] collect: return: 59.15591, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 0.03953, qf2_loss: 0.03945, policy_loss: -26.56012, policy_entropy: -5.27215, alpha: 0.00157, time: 69.59585
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   305 ----
[CW] collect: return: 67.84463, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 0.06394, qf2_loss: 0.06307, policy_loss: -26.52114, policy_entropy: -5.28213, alpha: 0.00156, time: 68.87926
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   306 ----
[CW] collect: return: 44.53582, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 0.03844, qf2_loss: 0.03759, policy_loss: -26.44567, policy_entropy: -6.23687, alpha: 0.00156, time: 68.94942
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   307 ----
[CW] collect: return: 44.26189, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 0.05999, qf2_loss: 0.05930, policy_loss: -26.37807, policy_entropy: -5.81081, alpha: 0.00156, time: 68.92467
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   308 ----
[CW] collect: return: 44.17207, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 0.04994, qf2_loss: 0.04931, policy_loss: -26.33443, policy_entropy: -5.58309, alpha: 0.00155, time: 68.98810
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   309 ----
[CW] collect: return: 42.15977, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 0.03860, qf2_loss: 0.03825, policy_loss: -26.22344, policy_entropy: -5.67366, alpha: 0.00154, time: 69.09258
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   310 ----
[CW] collect: return: 68.59539, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 0.05031, qf2_loss: 0.04931, policy_loss: -26.15752, policy_entropy: -5.70606, alpha: 0.00154, time: 69.04554
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   311 ----
[CW] collect: return: 44.24630, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 0.07872, qf2_loss: 0.07729, policy_loss: -26.07133, policy_entropy: -5.72559, alpha: 0.00154, time: 68.98431
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   312 ----
[CW] collect: return: 62.08975, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 0.05058, qf2_loss: 0.05021, policy_loss: -26.02434, policy_entropy: -6.05821, alpha: 0.00153, time: 68.94397
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   313 ----
[CW] collect: return: 74.71473, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 0.04490, qf2_loss: 0.04454, policy_loss: -25.96372, policy_entropy: -5.99343, alpha: 0.00154, time: 68.97980
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   314 ----
[CW] collect: return: 56.93878, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 0.04390, qf2_loss: 0.04411, policy_loss: -25.87216, policy_entropy: -6.13293, alpha: 0.00154, time: 68.83942
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   315 ----
[CW] collect: return: 50.18683, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 0.04296, qf2_loss: 0.04237, policy_loss: -25.85469, policy_entropy: -6.40981, alpha: 0.00154, time: 68.89720
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   316 ----
[CW] collect: return: 50.28988, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 0.06090, qf2_loss: 0.06114, policy_loss: -25.76193, policy_entropy: -6.10789, alpha: 0.00155, time: 68.87472
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   317 ----
[CW] collect: return: 43.26104, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 0.06237, qf2_loss: 0.06107, policy_loss: -25.70236, policy_entropy: -6.15465, alpha: 0.00155, time: 68.85094
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   318 ----
[CW] collect: return: 45.50573, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 0.04481, qf2_loss: 0.04515, policy_loss: -25.66275, policy_entropy: -5.79983, alpha: 0.00155, time: 68.93721
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   319 ----
[CW] collect: return: 53.01257, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 0.04510, qf2_loss: 0.04394, policy_loss: -25.57448, policy_entropy: -6.42038, alpha: 0.00155, time: 69.04098
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   320 ----
[CW] collect: return: 43.82418, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 0.03853, qf2_loss: 0.03851, policy_loss: -25.52628, policy_entropy: -6.97073, alpha: 0.00156, time: 69.09140
[CW] eval: return: 60.33885, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   321 ----
[CW] collect: return: 65.46357, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 0.05037, qf2_loss: 0.04947, policy_loss: -25.47182, policy_entropy: -7.20311, alpha: 0.00159, time: 69.21476
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   322 ----
[CW] collect: return: 65.50926, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 0.04392, qf2_loss: 0.04347, policy_loss: -25.41065, policy_entropy: -6.97204, alpha: 0.00162, time: 69.19790
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   323 ----
[CW] collect: return: 76.76019, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 0.04785, qf2_loss: 0.04850, policy_loss: -25.32302, policy_entropy: -5.79507, alpha: 0.00163, time: 69.03317
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   324 ----
[CW] collect: return: 56.17223, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 0.04272, qf2_loss: 0.04281, policy_loss: -25.28172, policy_entropy: -7.36052, alpha: 0.00165, time: 68.70703
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   325 ----
[CW] collect: return: 43.21110, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 0.03394, qf2_loss: 0.03352, policy_loss: -25.19257, policy_entropy: -6.54421, alpha: 0.00168, time: 68.70132
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   326 ----
[CW] collect: return: 58.27827, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 0.04634, qf2_loss: 0.04531, policy_loss: -25.11814, policy_entropy: -5.84862, alpha: 0.00169, time: 68.94287
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   327 ----
[CW] collect: return: 72.56237, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 0.04239, qf2_loss: 0.04254, policy_loss: -25.04685, policy_entropy: -5.36278, alpha: 0.00167, time: 69.06626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   328 ----
[CW] collect: return: 66.27458, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 0.04704, qf2_loss: 0.04764, policy_loss: -24.97085, policy_entropy: -6.24732, alpha: 0.00167, time: 68.76496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   329 ----
[CW] collect: return: 44.04032, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 0.04813, qf2_loss: 0.04628, policy_loss: -24.91741, policy_entropy: -5.04928, alpha: 0.00165, time: 68.72140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   330 ----
[CW] collect: return: 74.71479, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 0.02693, qf2_loss: 0.02688, policy_loss: -24.88531, policy_entropy: -5.70175, alpha: 0.00163, time: 68.74212
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   331 ----
[CW] collect: return: 61.31852, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 0.03112, qf2_loss: 0.03078, policy_loss: -24.81534, policy_entropy: -5.76020, alpha: 0.00163, time: 68.68043
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   332 ----
[CW] collect: return: 65.86108, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 0.03706, qf2_loss: 0.03766, policy_loss: -24.74284, policy_entropy: -6.02806, alpha: 0.00161, time: 68.79351
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   333 ----
[CW] collect: return: 45.30372, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 0.03791, qf2_loss: 0.03749, policy_loss: -24.58739, policy_entropy: -5.79897, alpha: 0.00162, time: 68.76141
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   334 ----
[CW] collect: return: 45.09185, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 0.06555, qf2_loss: 0.06507, policy_loss: -24.63574, policy_entropy: -6.32587, alpha: 0.00162, time: 68.51532
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   335 ----
[CW] collect: return: 60.15086, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 0.02769, qf2_loss: 0.02780, policy_loss: -24.55593, policy_entropy: -5.12988, alpha: 0.00161, time: 68.59165
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   336 ----
[CW] collect: return: 46.87830, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 0.02916, qf2_loss: 0.02820, policy_loss: -24.45088, policy_entropy: -5.70752, alpha: 0.00158, time: 68.59257
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   337 ----
[CW] collect: return: 59.32100, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 0.03412, qf2_loss: 0.03442, policy_loss: -24.42342, policy_entropy: -6.73363, alpha: 0.00159, time: 68.55134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   338 ----
[CW] collect: return: 73.03648, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 0.03614, qf2_loss: 0.03565, policy_loss: -24.36975, policy_entropy: -6.59285, alpha: 0.00162, time: 68.70283
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   339 ----
[CW] collect: return: 73.92582, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 0.04140, qf2_loss: 0.04131, policy_loss: -24.29414, policy_entropy: -6.86404, alpha: 0.00165, time: 68.75636
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   340 ----
[CW] collect: return: 76.56658, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 0.02459, qf2_loss: 0.02461, policy_loss: -24.23017, policy_entropy: -7.24504, alpha: 0.00170, time: 68.83962
[CW] eval: return: 75.21993, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   341 ----
[CW] collect: return: 77.08838, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 0.04601, qf2_loss: 0.04611, policy_loss: -24.09276, policy_entropy: -6.16406, alpha: 0.00173, time: 68.87681
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   342 ----
[CW] collect: return: 72.32614, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 0.02589, qf2_loss: 0.02582, policy_loss: -24.08976, policy_entropy: -6.33241, alpha: 0.00174, time: 68.67388
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   343 ----
[CW] collect: return: 77.31155, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 0.03889, qf2_loss: 0.03796, policy_loss: -24.01327, policy_entropy: -6.42465, alpha: 0.00176, time: 68.53504
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   344 ----
[CW] collect: return: 80.07936, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 0.05299, qf2_loss: 0.05225, policy_loss: -23.98629, policy_entropy: -6.42783, alpha: 0.00180, time: 68.55348
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   345 ----
[CW] collect: return: 44.63934, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 0.02827, qf2_loss: 0.02820, policy_loss: -23.88091, policy_entropy: -6.25601, alpha: 0.00181, time: 68.64264
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   346 ----
[CW] collect: return: 83.09890, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 0.03279, qf2_loss: 0.03223, policy_loss: -23.84215, policy_entropy: -6.24074, alpha: 0.00182, time: 68.58382
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   347 ----
[CW] collect: return: 79.02633, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 0.03076, qf2_loss: 0.03107, policy_loss: -23.74158, policy_entropy: -6.07797, alpha: 0.00183, time: 69.41537
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   348 ----
[CW] collect: return: 68.37188, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 0.03050, qf2_loss: 0.03094, policy_loss: -23.75919, policy_entropy: -6.35116, alpha: 0.00184, time: 68.84873
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   349 ----
[CW] collect: return: 57.35883, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 0.03310, qf2_loss: 0.03231, policy_loss: -23.67731, policy_entropy: -6.13288, alpha: 0.00186, time: 68.83902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   350 ----
[CW] collect: return: 57.43455, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 0.03398, qf2_loss: 0.03393, policy_loss: -23.65080, policy_entropy: -6.42060, alpha: 0.00189, time: 68.78549
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   351 ----
[CW] collect: return: 49.20475, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 0.04324, qf2_loss: 0.04265, policy_loss: -23.61700, policy_entropy: -6.12683, alpha: 0.00190, time: 68.67817
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   352 ----
[CW] collect: return: 51.19679, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 0.03036, qf2_loss: 0.03042, policy_loss: -23.48502, policy_entropy: -6.05277, alpha: 0.00191, time: 68.69505
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   353 ----
[CW] collect: return: 56.28223, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 0.02580, qf2_loss: 0.02593, policy_loss: -23.44385, policy_entropy: -6.15427, alpha: 0.00192, time: 68.71349
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   354 ----
[CW] collect: return: 72.80173, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 0.05429, qf2_loss: 0.05406, policy_loss: -23.36848, policy_entropy: -5.62033, alpha: 0.00193, time: 68.61530
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   355 ----
[CW] collect: return: 58.26162, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 0.03143, qf2_loss: 0.03097, policy_loss: -23.37123, policy_entropy: -5.83277, alpha: 0.00189, time: 68.66546
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   356 ----
[CW] collect: return: 78.63741, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 0.02539, qf2_loss: 0.02592, policy_loss: -23.27266, policy_entropy: -5.56773, alpha: 0.00188, time: 68.67305
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   357 ----
[CW] collect: return: 68.42260, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 0.02900, qf2_loss: 0.02900, policy_loss: -23.19208, policy_entropy: -6.17698, alpha: 0.00185, time: 69.46671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   358 ----
[CW] collect: return: 78.58189, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 0.03380, qf2_loss: 0.03286, policy_loss: -23.22325, policy_entropy: -6.83795, alpha: 0.00189, time: 68.84724
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   359 ----
[CW] collect: return: 65.20763, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 0.03334, qf2_loss: 0.03302, policy_loss: -23.07386, policy_entropy: -6.37268, alpha: 0.00193, time: 68.72497
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   360 ----
[CW] collect: return: 65.18856, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 0.04589, qf2_loss: 0.04631, policy_loss: -23.05566, policy_entropy: -6.00456, alpha: 0.00196, time: 68.82759
[CW] eval: return: 60.93314, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   361 ----
[CW] collect: return: 68.52167, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 0.03472, qf2_loss: 0.03458, policy_loss: -22.97793, policy_entropy: -5.76297, alpha: 0.00193, time: 68.96134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   362 ----
[CW] collect: return: 52.27318, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 0.04637, qf2_loss: 0.04555, policy_loss: -22.96757, policy_entropy: -6.22438, alpha: 0.00195, time: 68.70435
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   363 ----
[CW] collect: return: 61.81786, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 0.03734, qf2_loss: 0.03748, policy_loss: -22.90168, policy_entropy: -6.58423, alpha: 0.00197, time: 68.65658
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   364 ----
[CW] collect: return: 49.94930, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 0.03112, qf2_loss: 0.03096, policy_loss: -22.85765, policy_entropy: -6.73467, alpha: 0.00203, time: 68.66340
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   365 ----
[CW] collect: return: 81.53398, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 0.03204, qf2_loss: 0.03240, policy_loss: -22.80947, policy_entropy: -6.19638, alpha: 0.00207, time: 68.60292
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   366 ----
[CW] collect: return: 74.83876, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 0.03241, qf2_loss: 0.03208, policy_loss: -22.77199, policy_entropy: -5.75143, alpha: 0.00206, time: 68.55823
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   367 ----
[CW] collect: return: 45.30412, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 0.03338, qf2_loss: 0.03341, policy_loss: -22.69265, policy_entropy: -6.33119, alpha: 0.00207, time: 68.58533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   368 ----
[CW] collect: return: 45.64220, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 0.05418, qf2_loss: 0.05297, policy_loss: -22.64654, policy_entropy: -6.28884, alpha: 0.00209, time: 68.82724
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   369 ----
[CW] collect: return: 86.23322, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 0.03006, qf2_loss: 0.02999, policy_loss: -22.57061, policy_entropy: -6.32663, alpha: 0.00212, time: 68.93306
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   370 ----
[CW] collect: return: 53.07359, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 0.04127, qf2_loss: 0.04074, policy_loss: -22.59411, policy_entropy: -6.69741, alpha: 0.00216, time: 68.68854
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   371 ----
[CW] collect: return: 61.29957, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 0.05240, qf2_loss: 0.05228, policy_loss: -22.48895, policy_entropy: -6.23265, alpha: 0.00223, time: 68.78589
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   372 ----
[CW] collect: return: 82.38863, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 0.02945, qf2_loss: 0.02983, policy_loss: -22.48421, policy_entropy: -6.26450, alpha: 0.00222, time: 68.73127
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   373 ----
[CW] collect: return: 82.58074, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 0.03193, qf2_loss: 0.03199, policy_loss: -22.39321, policy_entropy: -6.53682, alpha: 0.00227, time: 68.58758
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   374 ----
[CW] collect: return: 84.93219, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 0.05431, qf2_loss: 0.05364, policy_loss: -22.36533, policy_entropy: -5.45745, alpha: 0.00228, time: 68.48165
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   375 ----
[CW] collect: return: 45.97686, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 0.04292, qf2_loss: 0.04174, policy_loss: -22.31270, policy_entropy: -6.84760, alpha: 0.00227, time: 68.59559
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   376 ----
[CW] collect: return: 46.74556, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 0.03735, qf2_loss: 0.03676, policy_loss: -22.29541, policy_entropy: -6.42165, alpha: 0.00235, time: 68.66313
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   377 ----
[CW] collect: return: 42.37025, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 0.04106, qf2_loss: 0.04101, policy_loss: -22.22536, policy_entropy: -5.73884, alpha: 0.00235, time: 68.59888
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   378 ----
[CW] collect: return: 45.32347, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 0.03226, qf2_loss: 0.03304, policy_loss: -22.18142, policy_entropy: -6.55854, alpha: 0.00235, time: 69.55798
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   379 ----
[CW] collect: return: 51.48766, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 0.03394, qf2_loss: 0.03309, policy_loss: -22.14666, policy_entropy: -6.36007, alpha: 0.00241, time: 68.71810
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   380 ----
[CW] collect: return: 73.24727, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 0.04133, qf2_loss: 0.04109, policy_loss: -22.06074, policy_entropy: -6.33972, alpha: 0.00242, time: 68.77335
[CW] eval: return: 106.12340, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   381 ----
[CW] collect: return: 103.43913, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 0.04549, qf2_loss: 0.04491, policy_loss: -22.02447, policy_entropy: -6.27708, alpha: 0.00246, time: 68.88887
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   382 ----
[CW] collect: return: 83.14976, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 0.05423, qf2_loss: 0.05373, policy_loss: -22.06877, policy_entropy: -6.48849, alpha: 0.00251, time: 68.58764
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   383 ----
[CW] collect: return: 59.54081, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 0.03627, qf2_loss: 0.03590, policy_loss: -22.01808, policy_entropy: -6.47366, alpha: 0.00254, time: 68.46226
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   384 ----
[CW] collect: return: 81.96640, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 0.04348, qf2_loss: 0.04356, policy_loss: -21.92922, policy_entropy: -6.95784, alpha: 0.00262, time: 68.56300
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   385 ----
[CW] collect: return: 55.52452, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 0.06358, qf2_loss: 0.06304, policy_loss: -21.91324, policy_entropy: -7.23849, alpha: 0.00274, time: 68.64265
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   386 ----
[CW] collect: return: 87.49029, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 0.02978, qf2_loss: 0.03019, policy_loss: -21.89377, policy_entropy: -6.23373, alpha: 0.00282, time: 68.48180
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   387 ----
[CW] collect: return: 80.52019, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 0.03527, qf2_loss: 0.03457, policy_loss: -21.82110, policy_entropy: -6.12474, alpha: 0.00284, time: 68.74029
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   388 ----
[CW] collect: return: 78.66165, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 0.03409, qf2_loss: 0.03397, policy_loss: -21.79297, policy_entropy: -6.01392, alpha: 0.00285, time: 68.80850
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   389 ----
[CW] collect: return: 76.63169, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 0.03431, qf2_loss: 0.03425, policy_loss: -21.75798, policy_entropy: -6.13469, alpha: 0.00285, time: 68.78631
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   390 ----
[CW] collect: return: 87.26263, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 0.04147, qf2_loss: 0.04076, policy_loss: -21.72810, policy_entropy: -6.31807, alpha: 0.00288, time: 68.75527
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   391 ----
[CW] collect: return: 86.73366, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 0.05096, qf2_loss: 0.05030, policy_loss: -21.67421, policy_entropy: -6.24573, alpha: 0.00292, time: 68.86420
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   392 ----
[CW] collect: return: 62.67377, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 0.04270, qf2_loss: 0.04327, policy_loss: -21.68089, policy_entropy: -6.45575, alpha: 0.00297, time: 68.68155
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   393 ----
[CW] collect: return: 115.63038, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 0.04654, qf2_loss: 0.04606, policy_loss: -21.65160, policy_entropy: -5.90355, alpha: 0.00300, time: 68.61000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   394 ----
[CW] collect: return: 62.68807, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 0.03950, qf2_loss: 0.03936, policy_loss: -21.60145, policy_entropy: -6.11101, alpha: 0.00298, time: 68.56324
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   395 ----
[CW] collect: return: 87.89392, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 0.04029, qf2_loss: 0.04014, policy_loss: -21.58199, policy_entropy: -6.14451, alpha: 0.00301, time: 68.59443
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   396 ----
[CW] collect: return: 85.86399, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 0.06671, qf2_loss: 0.06384, policy_loss: -21.53135, policy_entropy: -6.02898, alpha: 0.00302, time: 68.85879
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   397 ----
[CW] collect: return: 167.51319, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 0.03652, qf2_loss: 0.03707, policy_loss: -21.55932, policy_entropy: -6.25004, alpha: 0.00305, time: 68.68853
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   398 ----
[CW] collect: return: 98.31013, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 0.04404, qf2_loss: 0.04468, policy_loss: -21.51388, policy_entropy: -6.15665, alpha: 0.00307, time: 68.79853
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   399 ----
[CW] collect: return: 89.63808, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 0.04343, qf2_loss: 0.04408, policy_loss: -21.49155, policy_entropy: -6.35454, alpha: 0.00311, time: 68.73148
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   400 ----
[CW] collect: return: 160.59664, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 0.05831, qf2_loss: 0.05640, policy_loss: -21.47198, policy_entropy: -6.51992, alpha: 0.00315, time: 70.34861
[CW] eval: return: 133.77410, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   401 ----
[CW] collect: return: 161.52716, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 0.05152, qf2_loss: 0.05039, policy_loss: -21.43899, policy_entropy: -6.16925, alpha: 0.00323, time: 70.21366
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   402 ----
[CW] collect: return: 85.15582, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 0.05947, qf2_loss: 0.05941, policy_loss: -21.42362, policy_entropy: -5.38515, alpha: 0.00321, time: 68.93219
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   403 ----
[CW] collect: return: 33.46239, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 0.04757, qf2_loss: 0.04595, policy_loss: -21.35903, policy_entropy: -6.15041, alpha: 0.00316, time: 68.84783
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   404 ----
[CW] collect: return: 91.08823, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 0.03913, qf2_loss: 0.03902, policy_loss: -21.34160, policy_entropy: -6.13891, alpha: 0.00319, time: 68.85403
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   405 ----
[CW] collect: return: 103.41722, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 0.04144, qf2_loss: 0.04039, policy_loss: -21.30092, policy_entropy: -6.06465, alpha: 0.00319, time: 68.79582
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   406 ----
[CW] collect: return: 104.32284, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 0.03689, qf2_loss: 0.03712, policy_loss: -21.31789, policy_entropy: -6.46211, alpha: 0.00324, time: 68.78210
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   407 ----
[CW] collect: return: 90.23310, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 0.04057, qf2_loss: 0.04012, policy_loss: -21.30394, policy_entropy: -6.31695, alpha: 0.00329, time: 68.97350
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   408 ----
[CW] collect: return: 143.24574, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 0.04455, qf2_loss: 0.04565, policy_loss: -21.28649, policy_entropy: -6.47001, alpha: 0.00335, time: 68.78363
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   409 ----
[CW] collect: return: 96.69678, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 0.05144, qf2_loss: 0.04998, policy_loss: -21.33229, policy_entropy: -6.64181, alpha: 0.00344, time: 68.93651
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   410 ----
[CW] collect: return: 131.18244, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 0.06362, qf2_loss: 0.06282, policy_loss: -21.25231, policy_entropy: -6.50291, alpha: 0.00354, time: 69.40133
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   411 ----
[CW] collect: return: 105.97047, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 0.05928, qf2_loss: 0.05850, policy_loss: -21.25061, policy_entropy: -6.76906, alpha: 0.00365, time: 68.85037
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   412 ----
[CW] collect: return: 114.23526, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 0.04524, qf2_loss: 0.04473, policy_loss: -21.25099, policy_entropy: -6.68530, alpha: 0.00378, time: 68.75199
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   413 ----
[CW] collect: return: 115.77994, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 0.04752, qf2_loss: 0.04711, policy_loss: -21.19794, policy_entropy: -6.48011, alpha: 0.00387, time: 68.84415
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   414 ----
[CW] collect: return: 132.21687, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 0.05295, qf2_loss: 0.05189, policy_loss: -21.24823, policy_entropy: -6.81972, alpha: 0.00399, time: 68.80778
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   415 ----
[CW] collect: return: 113.45505, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 0.05711, qf2_loss: 0.05569, policy_loss: -21.21693, policy_entropy: -6.70512, alpha: 0.00416, time: 68.93042
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   416 ----
[CW] collect: return: 140.68088, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 0.04898, qf2_loss: 0.04923, policy_loss: -21.19763, policy_entropy: -6.79791, alpha: 0.00429, time: 68.78579
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   417 ----
[CW] collect: return: 131.39344, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 0.05999, qf2_loss: 0.05877, policy_loss: -21.17066, policy_entropy: -6.46150, alpha: 0.00442, time: 68.85621
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   418 ----
[CW] collect: return: 94.34168, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 0.04593, qf2_loss: 0.04524, policy_loss: -21.17288, policy_entropy: -6.72470, alpha: 0.00453, time: 68.72890
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   419 ----
[CW] collect: return: 124.47642, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 0.04815, qf2_loss: 0.04758, policy_loss: -21.16803, policy_entropy: -6.63965, alpha: 0.00469, time: 68.86869
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   420 ----
[CW] collect: return: 117.13842, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 0.06830, qf2_loss: 0.06820, policy_loss: -21.17982, policy_entropy: -6.16828, alpha: 0.00479, time: 69.02391
[CW] eval: return: 99.77911, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   421 ----
[CW] collect: return: 89.47712, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 0.06377, qf2_loss: 0.06512, policy_loss: -21.20349, policy_entropy: -6.96550, alpha: 0.00489, time: 69.00002
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   422 ----
[CW] collect: return: 118.14371, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 0.05994, qf2_loss: 0.05765, policy_loss: -21.13329, policy_entropy: -5.93859, alpha: 0.00505, time: 68.82744
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   423 ----
[CW] collect: return: 102.06125, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 0.05517, qf2_loss: 0.05402, policy_loss: -21.17425, policy_entropy: -6.17237, alpha: 0.00502, time: 68.76409
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   424 ----
[CW] collect: return: 125.91265, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 0.06969, qf2_loss: 0.06793, policy_loss: -21.14977, policy_entropy: -6.09412, alpha: 0.00508, time: 68.78936
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   425 ----
[CW] collect: return: 135.22479, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 0.05765, qf2_loss: 0.05750, policy_loss: -21.16981, policy_entropy: -5.66493, alpha: 0.00505, time: 68.68433
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   426 ----
[CW] collect: return: 106.08265, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 0.05178, qf2_loss: 0.05206, policy_loss: -21.13415, policy_entropy: -5.85786, alpha: 0.00499, time: 68.72993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   427 ----
[CW] collect: return: 88.60146, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 0.07149, qf2_loss: 0.06917, policy_loss: -21.16038, policy_entropy: -6.19699, alpha: 0.00499, time: 68.82990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   428 ----
[CW] collect: return: 111.97463, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 0.05392, qf2_loss: 0.05331, policy_loss: -21.14826, policy_entropy: -5.48406, alpha: 0.00496, time: 68.84913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   429 ----
[CW] collect: return: 114.46642, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 0.07222, qf2_loss: 0.07266, policy_loss: -21.17208, policy_entropy: -5.79669, alpha: 0.00486, time: 68.89536
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   430 ----
[CW] collect: return: 123.29441, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 0.06434, qf2_loss: 0.06297, policy_loss: -21.14430, policy_entropy: -5.94342, alpha: 0.00483, time: 69.85507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   431 ----
[CW] collect: return: 111.65358, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 0.07085, qf2_loss: 0.06954, policy_loss: -21.16068, policy_entropy: -5.97338, alpha: 0.00484, time: 68.78996
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   432 ----
[CW] collect: return: 47.11125, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 0.06806, qf2_loss: 0.06672, policy_loss: -21.13558, policy_entropy: -6.04700, alpha: 0.00482, time: 68.72565
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   433 ----
[CW] collect: return: 98.82321, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 0.06527, qf2_loss: 0.06443, policy_loss: -21.11632, policy_entropy: -5.79672, alpha: 0.00480, time: 68.54198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   434 ----
[CW] collect: return: 121.61436, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 0.06955, qf2_loss: 0.06938, policy_loss: -21.16969, policy_entropy: -5.98707, alpha: 0.00477, time: 68.60318
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   435 ----
[CW] collect: return: 108.19666, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 0.07425, qf2_loss: 0.07279, policy_loss: -21.13433, policy_entropy: -5.77404, alpha: 0.00476, time: 68.64606
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   436 ----
[CW] collect: return: 103.18507, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 0.06788, qf2_loss: 0.06705, policy_loss: -21.14298, policy_entropy: -6.14481, alpha: 0.00474, time: 68.79530
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   437 ----
[CW] collect: return: 99.33592, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 0.08587, qf2_loss: 0.08625, policy_loss: -21.12591, policy_entropy: -5.85661, alpha: 0.00475, time: 68.82198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   438 ----
[CW] collect: return: 108.02001, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 0.12760, qf2_loss: 0.12446, policy_loss: -21.10606, policy_entropy: -5.58901, alpha: 0.00466, time: 68.85480
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   439 ----
[CW] collect: return: 101.11385, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 0.07588, qf2_loss: 0.07653, policy_loss: -21.14930, policy_entropy: -6.13931, alpha: 0.00464, time: 68.85990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   440 ----
[CW] collect: return: 117.05379, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 0.06834, qf2_loss: 0.06742, policy_loss: -21.13451, policy_entropy: -5.93926, alpha: 0.00465, time: 68.81089
[CW] eval: return: 112.09049, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   441 ----
[CW] collect: return: 110.38198, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 0.07164, qf2_loss: 0.07138, policy_loss: -21.13920, policy_entropy: -6.04262, alpha: 0.00464, time: 68.71924
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   442 ----
[CW] collect: return: 117.26110, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 0.07844, qf2_loss: 0.07592, policy_loss: -21.15163, policy_entropy: -5.92760, alpha: 0.00465, time: 68.76228
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   443 ----
[CW] collect: return: 131.47365, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 0.09098, qf2_loss: 0.09127, policy_loss: -21.17624, policy_entropy: -6.26290, alpha: 0.00465, time: 68.55335
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   444 ----
[CW] collect: return: 128.77932, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 0.09131, qf2_loss: 0.08922, policy_loss: -21.16514, policy_entropy: -6.43091, alpha: 0.00476, time: 68.68208
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   445 ----
[CW] collect: return: 130.43187, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 0.09106, qf2_loss: 0.09138, policy_loss: -21.18885, policy_entropy: -6.15173, alpha: 0.00484, time: 68.58216
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   446 ----
[CW] collect: return: 107.03689, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 0.10136, qf2_loss: 0.09962, policy_loss: -21.21383, policy_entropy: -6.43320, alpha: 0.00489, time: 68.78096
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   447 ----
[CW] collect: return: 111.41095, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 0.10218, qf2_loss: 0.10166, policy_loss: -21.20445, policy_entropy: -6.17714, alpha: 0.00496, time: 68.82459
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   448 ----
[CW] collect: return: 111.32565, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 0.09045, qf2_loss: 0.08919, policy_loss: -21.23404, policy_entropy: -6.41169, alpha: 0.00508, time: 68.82322
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   449 ----
[CW] collect: return: 112.22323, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 0.11134, qf2_loss: 0.10932, policy_loss: -21.25173, policy_entropy: -6.04701, alpha: 0.00514, time: 68.84449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   450 ----
[CW] collect: return: 142.32191, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 0.09896, qf2_loss: 0.09860, policy_loss: -21.27902, policy_entropy: -5.92265, alpha: 0.00514, time: 68.75087
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   451 ----
[CW] collect: return: 140.89627, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 0.09306, qf2_loss: 0.09323, policy_loss: -21.27104, policy_entropy: -5.68129, alpha: 0.00508, time: 68.63533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   452 ----
[CW] collect: return: 138.42564, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 0.09894, qf2_loss: 0.09921, policy_loss: -21.33167, policy_entropy: -5.65099, alpha: 0.00499, time: 68.64567
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   453 ----
[CW] collect: return: 133.02049, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 0.10502, qf2_loss: 0.10662, policy_loss: -21.34181, policy_entropy: -5.97340, alpha: 0.00494, time: 69.93379
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   454 ----
[CW] collect: return: 139.82901, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 0.10288, qf2_loss: 0.10187, policy_loss: -21.35780, policy_entropy: -5.81912, alpha: 0.00492, time: 69.05626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   455 ----
[CW] collect: return: 129.16107, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 0.12196, qf2_loss: 0.11866, policy_loss: -21.36279, policy_entropy: -5.89370, alpha: 0.00489, time: 68.79054
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   456 ----
[CW] collect: return: 141.86710, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 0.11482, qf2_loss: 0.11412, policy_loss: -21.38871, policy_entropy: -5.86256, alpha: 0.00483, time: 68.86826
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   457 ----
[CW] collect: return: 154.25680, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 0.11090, qf2_loss: 0.11169, policy_loss: -21.45614, policy_entropy: -6.11215, alpha: 0.00482, time: 68.80122
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   458 ----
[CW] collect: return: 148.30390, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 0.10481, qf2_loss: 0.10388, policy_loss: -21.47969, policy_entropy: -5.96136, alpha: 0.00486, time: 68.93794
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   459 ----
[CW] collect: return: 83.93643, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 0.11888, qf2_loss: 0.11854, policy_loss: -21.45658, policy_entropy: -5.18799, alpha: 0.00477, time: 68.88035
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   460 ----
[CW] collect: return: 120.83159, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 0.10911, qf2_loss: 0.10753, policy_loss: -21.46849, policy_entropy: -5.85378, alpha: 0.00464, time: 68.80436
[CW] eval: return: 127.65805, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   461 ----
[CW] collect: return: 136.05367, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 0.12281, qf2_loss: 0.12111, policy_loss: -21.47971, policy_entropy: -5.89983, alpha: 0.00462, time: 68.75720
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   462 ----
[CW] collect: return: 124.37347, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 0.11908, qf2_loss: 0.12110, policy_loss: -21.54669, policy_entropy: -6.01006, alpha: 0.00462, time: 69.20425
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   463 ----
[CW] collect: return: 155.65165, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 0.12043, qf2_loss: 0.11767, policy_loss: -21.59757, policy_entropy: -5.87004, alpha: 0.00458, time: 68.67440
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   464 ----
[CW] collect: return: 134.43186, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 0.11960, qf2_loss: 0.11878, policy_loss: -21.56070, policy_entropy: -5.93481, alpha: 0.00459, time: 68.70310
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   465 ----
[CW] collect: return: 142.83897, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 0.13546, qf2_loss: 0.13565, policy_loss: -21.57834, policy_entropy: -5.77325, alpha: 0.00454, time: 68.69114
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   466 ----
[CW] collect: return: 133.23548, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 0.11047, qf2_loss: 0.10888, policy_loss: -21.61001, policy_entropy: -5.85836, alpha: 0.00451, time: 68.89150
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   467 ----
[CW] collect: return: 135.75148, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 0.11328, qf2_loss: 0.11234, policy_loss: -21.63345, policy_entropy: -5.62100, alpha: 0.00444, time: 68.97918
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   468 ----
[CW] collect: return: 97.19682, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 0.13043, qf2_loss: 0.12787, policy_loss: -21.69221, policy_entropy: -6.09100, alpha: 0.00442, time: 68.91733
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   469 ----
[CW] collect: return: 124.52509, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 0.12006, qf2_loss: 0.12005, policy_loss: -21.69949, policy_entropy: -5.98350, alpha: 0.00442, time: 69.03718
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   470 ----
[CW] collect: return: 143.30921, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 0.13040, qf2_loss: 0.12874, policy_loss: -21.71936, policy_entropy: -5.72275, alpha: 0.00439, time: 68.83644
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   471 ----
[CW] collect: return: 113.01393, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 0.12765, qf2_loss: 0.12794, policy_loss: -21.74198, policy_entropy: -5.83997, alpha: 0.00434, time: 68.72642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   472 ----
[CW] collect: return: 120.14583, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 0.12388, qf2_loss: 0.12274, policy_loss: -21.76978, policy_entropy: -5.82133, alpha: 0.00431, time: 68.73238
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   473 ----
[CW] collect: return: 152.09541, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 0.11108, qf2_loss: 0.11177, policy_loss: -21.77650, policy_entropy: -6.17979, alpha: 0.00430, time: 68.59663
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   474 ----
[CW] collect: return: 148.65887, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 0.13280, qf2_loss: 0.13025, policy_loss: -21.83452, policy_entropy: -6.35758, alpha: 0.00436, time: 68.66490
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   475 ----
[CW] collect: return: 163.55336, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 0.13543, qf2_loss: 0.13493, policy_loss: -21.86565, policy_entropy: -6.06234, alpha: 0.00441, time: 68.70383
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   476 ----
[CW] collect: return: 140.51530, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 0.15132, qf2_loss: 0.14933, policy_loss: -21.87715, policy_entropy: -5.83475, alpha: 0.00440, time: 68.89053
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   477 ----
[CW] collect: return: 138.76879, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 0.12449, qf2_loss: 0.12545, policy_loss: -21.87550, policy_entropy: -6.36395, alpha: 0.00442, time: 68.94077
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   478 ----
[CW] collect: return: 148.37548, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 0.11902, qf2_loss: 0.11922, policy_loss: -21.95951, policy_entropy: -6.25064, alpha: 0.00448, time: 68.79208
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   479 ----
[CW] collect: return: 123.76976, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 0.13840, qf2_loss: 0.13687, policy_loss: -21.90121, policy_entropy: -6.07284, alpha: 0.00454, time: 68.92222
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   480 ----
[CW] collect: return: 146.10401, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 0.11929, qf2_loss: 0.11963, policy_loss: -21.94805, policy_entropy: -6.06649, alpha: 0.00452, time: 68.78043
[CW] eval: return: 141.12402, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   481 ----
[CW] collect: return: 130.98702, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 0.13244, qf2_loss: 0.13192, policy_loss: -21.98185, policy_entropy: -6.12328, alpha: 0.00456, time: 68.80394
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   482 ----
[CW] collect: return: 140.75718, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 0.12418, qf2_loss: 0.12452, policy_loss: -22.04424, policy_entropy: -6.56211, alpha: 0.00465, time: 68.77240
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   483 ----
[CW] collect: return: 153.49342, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 0.14143, qf2_loss: 0.13993, policy_loss: -22.04725, policy_entropy: -6.66155, alpha: 0.00477, time: 68.64411
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   484 ----
[CW] collect: return: 156.52402, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 0.12545, qf2_loss: 0.12598, policy_loss: -22.12965, policy_entropy: -6.55421, alpha: 0.00493, time: 68.71902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   485 ----
[CW] collect: return: 158.95049, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 0.11619, qf2_loss: 0.11599, policy_loss: -22.07573, policy_entropy: -6.30406, alpha: 0.00506, time: 68.91346
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   486 ----
[CW] collect: return: 55.09221, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 0.13156, qf2_loss: 0.13133, policy_loss: -22.21735, policy_entropy: -6.02073, alpha: 0.00510, time: 68.98983
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   487 ----
[CW] collect: return: 137.07103, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 0.12838, qf2_loss: 0.12757, policy_loss: -22.18704, policy_entropy: -5.80426, alpha: 0.00509, time: 68.89696
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   488 ----
[CW] collect: return: 22.42288, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 0.13371, qf2_loss: 0.13398, policy_loss: -22.18819, policy_entropy: -5.98799, alpha: 0.00506, time: 68.94661
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   489 ----
[CW] collect: return: 146.17594, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 0.12616, qf2_loss: 0.12578, policy_loss: -22.19376, policy_entropy: -5.63364, alpha: 0.00500, time: 68.87550
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   490 ----
[CW] collect: return: 139.87873, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 0.11706, qf2_loss: 0.11537, policy_loss: -22.23956, policy_entropy: -6.00342, alpha: 0.00493, time: 68.69808
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   491 ----
[CW] collect: return: 156.57691, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 0.13420, qf2_loss: 0.13487, policy_loss: -22.30319, policy_entropy: -5.84930, alpha: 0.00495, time: 69.26360
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   492 ----
[CW] collect: return: 154.34544, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 0.12473, qf2_loss: 0.12509, policy_loss: -22.29222, policy_entropy: -5.60733, alpha: 0.00487, time: 68.64554
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   493 ----
[CW] collect: return: 164.68959, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 0.10687, qf2_loss: 0.10600, policy_loss: -22.29871, policy_entropy: -5.29382, alpha: 0.00472, time: 68.61327
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   494 ----
[CW] collect: return: 158.44073, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 0.11378, qf2_loss: 0.11369, policy_loss: -22.37436, policy_entropy: -5.91011, alpha: 0.00464, time: 68.94008
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   495 ----
[CW] collect: return: 160.20318, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 0.11562, qf2_loss: 0.11661, policy_loss: -22.38109, policy_entropy: -5.82985, alpha: 0.00461, time: 68.83472
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   496 ----
[CW] collect: return: 138.37214, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 0.11718, qf2_loss: 0.11623, policy_loss: -22.43959, policy_entropy: -6.01845, alpha: 0.00460, time: 68.93283
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   497 ----
[CW] collect: return: 147.98205, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 0.11782, qf2_loss: 0.11733, policy_loss: -22.44965, policy_entropy: -5.80119, alpha: 0.00456, time: 68.88176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   498 ----
[CW] collect: return: 159.95086, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 0.11509, qf2_loss: 0.11462, policy_loss: -22.38644, policy_entropy: -5.80699, alpha: 0.00452, time: 68.92688
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   499 ----
[CW] collect: return: 95.41118, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 0.12259, qf2_loss: 0.12227, policy_loss: -22.44429, policy_entropy: -6.33359, alpha: 0.00453, time: 68.83073
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   500 ----
[CW] collect: return: 150.16465, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 0.10262, qf2_loss: 0.10330, policy_loss: -22.47554, policy_entropy: -6.21211, alpha: 0.00459, time: 68.76437
[CW] eval: return: 155.67282, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   501 ----
[CW] collect: return: 153.67513, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 0.14301, qf2_loss: 0.14183, policy_loss: -22.46033, policy_entropy: -5.72536, alpha: 0.00461, time: 68.83764
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   502 ----
[CW] collect: return: 155.00570, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 0.10774, qf2_loss: 0.10769, policy_loss: -22.52135, policy_entropy: -5.75158, alpha: 0.00453, time: 68.77926
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   503 ----
[CW] collect: return: 164.58420, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 0.09469, qf2_loss: 0.09477, policy_loss: -22.45192, policy_entropy: -5.51273, alpha: 0.00447, time: 68.70072
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   504 ----
[CW] collect: return: 159.86412, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 0.09079, qf2_loss: 0.09043, policy_loss: -22.50844, policy_entropy: -5.50418, alpha: 0.00437, time: 68.69583
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   505 ----
[CW] collect: return: 67.25539, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 0.11124, qf2_loss: 0.11064, policy_loss: -22.48285, policy_entropy: -5.16424, alpha: 0.00424, time: 70.16203
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   506 ----
[CW] collect: return: 120.73200, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 0.10336, qf2_loss: 0.10345, policy_loss: -22.48676, policy_entropy: -5.77881, alpha: 0.00414, time: 68.93584
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   507 ----
[CW] collect: return: 22.68688, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 0.09465, qf2_loss: 0.09368, policy_loss: -22.51725, policy_entropy: -6.03522, alpha: 0.00411, time: 69.85209
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   508 ----
[CW] collect: return: 165.95382, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 0.11217, qf2_loss: 0.11206, policy_loss: -22.52619, policy_entropy: -6.24854, alpha: 0.00414, time: 68.87099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   509 ----
[CW] collect: return: 169.73764, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 0.10055, qf2_loss: 0.10137, policy_loss: -22.48512, policy_entropy: -5.91758, alpha: 0.00416, time: 68.91759
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   510 ----
[CW] collect: return: 167.19180, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 0.10379, qf2_loss: 0.10445, policy_loss: -22.55222, policy_entropy: -6.50249, alpha: 0.00419, time: 68.81395
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   511 ----
[CW] collect: return: 163.96482, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 0.09522, qf2_loss: 0.09417, policy_loss: -22.57265, policy_entropy: -6.47678, alpha: 0.00427, time: 68.74556
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   512 ----
[CW] collect: return: 135.19626, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 0.10651, qf2_loss: 0.10515, policy_loss: -22.59022, policy_entropy: -6.50350, alpha: 0.00437, time: 68.74768
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   513 ----
[CW] collect: return: 147.89801, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 0.10613, qf2_loss: 0.10473, policy_loss: -22.63471, policy_entropy: -6.39484, alpha: 0.00447, time: 68.78238
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   514 ----
[CW] collect: return: 171.14538, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 0.10050, qf2_loss: 0.10089, policy_loss: -22.61583, policy_entropy: -6.31863, alpha: 0.00452, time: 68.74113
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   515 ----
[CW] collect: return: 169.81587, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 0.15175, qf2_loss: 0.15152, policy_loss: -22.62667, policy_entropy: -6.08631, alpha: 0.00459, time: 69.04155
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   516 ----
[CW] collect: return: 149.58879, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 0.12805, qf2_loss: 0.12209, policy_loss: -22.60086, policy_entropy: -6.20093, alpha: 0.00459, time: 68.94347
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   517 ----
[CW] collect: return: 144.77634, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 0.09749, qf2_loss: 0.09715, policy_loss: -22.66241, policy_entropy: -6.31797, alpha: 0.00466, time: 68.91225
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   518 ----
[CW] collect: return: 158.44207, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 0.10555, qf2_loss: 0.10504, policy_loss: -22.59664, policy_entropy: -6.12639, alpha: 0.00469, time: 68.90791
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   519 ----
[CW] collect: return: 175.71719, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 0.10086, qf2_loss: 0.10139, policy_loss: -22.65293, policy_entropy: -6.61096, alpha: 0.00478, time: 68.95143
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   520 ----
[CW] collect: return: 148.06230, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 0.11440, qf2_loss: 0.11483, policy_loss: -22.68744, policy_entropy: -6.45506, alpha: 0.00490, time: 68.70354
[CW] eval: return: 163.24282, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   521 ----
[CW] collect: return: 156.34217, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 0.14434, qf2_loss: 0.14327, policy_loss: -22.72554, policy_entropy: -6.48984, alpha: 0.00500, time: 68.93790
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   522 ----
[CW] collect: return: 156.49278, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 0.10331, qf2_loss: 0.10425, policy_loss: -22.74608, policy_entropy: -6.46294, alpha: 0.00514, time: 68.70143
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   523 ----
[CW] collect: return: 160.59042, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 0.10082, qf2_loss: 0.10059, policy_loss: -22.70640, policy_entropy: -6.18660, alpha: 0.00523, time: 68.65099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   524 ----
[CW] collect: return: 148.75446, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 0.11244, qf2_loss: 0.11063, policy_loss: -22.69051, policy_entropy: -5.79485, alpha: 0.00524, time: 68.81182
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   525 ----
[CW] collect: return: 138.78567, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 0.11790, qf2_loss: 0.11771, policy_loss: -22.72013, policy_entropy: -6.07920, alpha: 0.00523, time: 69.02044
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   526 ----
[CW] collect: return: 164.67027, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 0.12601, qf2_loss: 0.12364, policy_loss: -22.79358, policy_entropy: -6.16256, alpha: 0.00523, time: 68.94274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   527 ----
[CW] collect: return: 170.64813, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 0.13196, qf2_loss: 0.13176, policy_loss: -22.83574, policy_entropy: -6.50913, alpha: 0.00531, time: 69.00123
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   528 ----
[CW] collect: return: 160.14645, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 0.12497, qf2_loss: 0.12407, policy_loss: -22.79700, policy_entropy: -6.32363, alpha: 0.00544, time: 68.88091
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   529 ----
[CW] collect: return: 151.82356, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 0.11596, qf2_loss: 0.11531, policy_loss: -22.79924, policy_entropy: -6.39241, alpha: 0.00550, time: 68.79089
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   530 ----
[CW] collect: return: 145.59502, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 0.12147, qf2_loss: 0.12090, policy_loss: -22.84715, policy_entropy: -6.30722, alpha: 0.00563, time: 69.96179
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   531 ----
[CW] collect: return: 154.57632, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 0.17155, qf2_loss: 0.17036, policy_loss: -22.79791, policy_entropy: -6.15207, alpha: 0.00572, time: 68.84398
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   532 ----
[CW] collect: return: 157.79827, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 0.12946, qf2_loss: 0.12819, policy_loss: -22.83611, policy_entropy: -6.23809, alpha: 0.00574, time: 68.86260
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   533 ----
[CW] collect: return: 157.75340, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 0.12261, qf2_loss: 0.12207, policy_loss: -22.85185, policy_entropy: -5.91361, alpha: 0.00580, time: 69.13207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   534 ----
[CW] collect: return: 176.04240, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 0.11627, qf2_loss: 0.11747, policy_loss: -22.89602, policy_entropy: -5.73650, alpha: 0.00573, time: 68.86124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   535 ----
[CW] collect: return: 162.79999, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 0.11856, qf2_loss: 0.11927, policy_loss: -22.94178, policy_entropy: -5.66303, alpha: 0.00564, time: 69.32483
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   536 ----
[CW] collect: return: 155.33213, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 0.12794, qf2_loss: 0.12606, policy_loss: -22.89316, policy_entropy: -5.54925, alpha: 0.00554, time: 69.08525
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   537 ----
[CW] collect: return: 161.66715, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 0.13010, qf2_loss: 0.13108, policy_loss: -22.94390, policy_entropy: -5.48280, alpha: 0.00543, time: 69.37172
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   538 ----
[CW] collect: return: 158.21287, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 0.11932, qf2_loss: 0.11878, policy_loss: -22.94810, policy_entropy: -5.41727, alpha: 0.00528, time: 69.19642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   539 ----
[CW] collect: return: 168.45509, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 0.13281, qf2_loss: 0.13138, policy_loss: -23.01211, policy_entropy: -5.96710, alpha: 0.00520, time: 70.44206
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   540 ----
[CW] collect: return: 161.18606, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 0.11638, qf2_loss: 0.11601, policy_loss: -22.99035, policy_entropy: -5.54546, alpha: 0.00516, time: 69.14657
[CW] eval: return: 164.80392, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   541 ----
[CW] collect: return: 151.50494, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 0.11080, qf2_loss: 0.11085, policy_loss: -22.93621, policy_entropy: -5.46981, alpha: 0.00504, time: 69.26144
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   542 ----
[CW] collect: return: 170.21133, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 0.11971, qf2_loss: 0.11950, policy_loss: -23.04546, policy_entropy: -6.11349, alpha: 0.00499, time: 69.12235
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   543 ----
[CW] collect: return: 165.29376, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 0.11753, qf2_loss: 0.11665, policy_loss: -23.05492, policy_entropy: -6.13546, alpha: 0.00502, time: 69.05509
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   544 ----
[CW] collect: return: 170.27542, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 0.11114, qf2_loss: 0.11085, policy_loss: -23.04341, policy_entropy: -5.93076, alpha: 0.00504, time: 69.39383
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   545 ----
[CW] collect: return: 167.98902, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 0.12846, qf2_loss: 0.12690, policy_loss: -23.09587, policy_entropy: -5.99321, alpha: 0.00503, time: 69.27502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   546 ----
[CW] collect: return: 131.57788, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 0.13129, qf2_loss: 0.13123, policy_loss: -23.12246, policy_entropy: -5.90523, alpha: 0.00501, time: 69.23411
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   547 ----
[CW] collect: return: 169.67026, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 0.12523, qf2_loss: 0.12549, policy_loss: -23.10510, policy_entropy: -6.14200, alpha: 0.00502, time: 69.17352
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   548 ----
[CW] collect: return: 167.62400, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 0.13679, qf2_loss: 0.13690, policy_loss: -23.18225, policy_entropy: -6.07285, alpha: 0.00505, time: 69.20581
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   549 ----
[CW] collect: return: 162.19498, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 0.14249, qf2_loss: 0.14071, policy_loss: -23.12720, policy_entropy: -6.21074, alpha: 0.00507, time: 69.01181
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   550 ----
[CW] collect: return: 150.52694, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 0.12683, qf2_loss: 0.12693, policy_loss: -23.15274, policy_entropy: -6.17648, alpha: 0.00512, time: 69.16790
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   551 ----
[CW] collect: return: 165.69134, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 0.12463, qf2_loss: 0.12459, policy_loss: -23.24843, policy_entropy: -6.20192, alpha: 0.00518, time: 69.20333
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   552 ----
[CW] collect: return: 181.18370, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 0.15374, qf2_loss: 0.15312, policy_loss: -23.22432, policy_entropy: -6.04823, alpha: 0.00521, time: 70.09378
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   553 ----
[CW] collect: return: 166.11914, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 0.13151, qf2_loss: 0.13196, policy_loss: -23.27903, policy_entropy: -6.44035, alpha: 0.00525, time: 71.06912
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   554 ----
[CW] collect: return: 152.81533, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 0.14333, qf2_loss: 0.14177, policy_loss: -23.25410, policy_entropy: -6.08230, alpha: 0.00534, time: 70.38842
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   555 ----
[CW] collect: return: 167.13201, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 0.14678, qf2_loss: 0.14686, policy_loss: -23.32803, policy_entropy: -5.94755, alpha: 0.00534, time: 70.52666
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   556 ----
[CW] collect: return: 163.58832, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 0.11655, qf2_loss: 0.11583, policy_loss: -23.30425, policy_entropy: -6.20439, alpha: 0.00536, time: 70.20524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   557 ----
[CW] collect: return: 172.43755, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 0.12481, qf2_loss: 0.12347, policy_loss: -23.35621, policy_entropy: -6.20764, alpha: 0.00543, time: 70.48186
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   558 ----
[CW] collect: return: 168.53274, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 0.14007, qf2_loss: 0.14002, policy_loss: -23.33411, policy_entropy: -6.10175, alpha: 0.00548, time: 70.51694
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   559 ----
[CW] collect: return: 158.30397, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 0.14143, qf2_loss: 0.14135, policy_loss: -23.38918, policy_entropy: -5.99083, alpha: 0.00548, time: 70.16108
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   560 ----
[CW] collect: return: 177.17968, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 0.12789, qf2_loss: 0.12622, policy_loss: -23.41655, policy_entropy: -6.03409, alpha: 0.00548, time: 70.30545
[CW] eval: return: 169.24445, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   561 ----
[CW] collect: return: 164.45884, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 0.17144, qf2_loss: 0.16949, policy_loss: -23.45247, policy_entropy: -5.95281, alpha: 0.00550, time: 70.18470
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   562 ----
[CW] collect: return: 154.63369, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 0.14650, qf2_loss: 0.14797, policy_loss: -23.46684, policy_entropy: -6.02528, alpha: 0.00547, time: 70.25787
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   563 ----
[CW] collect: return: 163.55416, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 0.13127, qf2_loss: 0.13109, policy_loss: -23.49682, policy_entropy: -5.84956, alpha: 0.00546, time: 70.20273
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   564 ----
[CW] collect: return: 165.94468, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 0.13199, qf2_loss: 0.13092, policy_loss: -23.46507, policy_entropy: -5.66032, alpha: 0.00541, time: 70.35303
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   565 ----
[CW] collect: return: 22.44806, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 0.12676, qf2_loss: 0.12671, policy_loss: -23.50169, policy_entropy: -5.53711, alpha: 0.00527, time: 70.40950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   566 ----
[CW] collect: return: 26.53941, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 0.15776, qf2_loss: 0.15543, policy_loss: -23.58128, policy_entropy: -5.69924, alpha: 0.00518, time: 70.46783
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   567 ----
[CW] collect: return: 173.03056, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 0.12337, qf2_loss: 0.12451, policy_loss: -23.55710, policy_entropy: -5.73591, alpha: 0.00508, time: 70.55406
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   568 ----
[CW] collect: return: 159.05719, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 0.13640, qf2_loss: 0.13683, policy_loss: -23.56756, policy_entropy: -5.99165, alpha: 0.00504, time: 70.27914
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   569 ----
[CW] collect: return: 168.21078, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 0.11335, qf2_loss: 0.11268, policy_loss: -23.57148, policy_entropy: -5.98529, alpha: 0.00504, time: 70.24647
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   570 ----
[CW] collect: return: 161.64031, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 0.12032, qf2_loss: 0.11992, policy_loss: -23.63028, policy_entropy: -6.15253, alpha: 0.00505, time: 70.28496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   571 ----
[CW] collect: return: 181.52017, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 0.12523, qf2_loss: 0.12563, policy_loss: -23.69631, policy_entropy: -6.24894, alpha: 0.00512, time: 70.37134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   572 ----
[CW] collect: return: 174.02958, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 0.13208, qf2_loss: 0.13077, policy_loss: -23.72110, policy_entropy: -6.19296, alpha: 0.00518, time: 70.28779
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   573 ----
[CW] collect: return: 168.50416, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 0.14280, qf2_loss: 0.14243, policy_loss: -23.74375, policy_entropy: -6.07344, alpha: 0.00523, time: 70.25514
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   574 ----
[CW] collect: return: 162.46139, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 0.13159, qf2_loss: 0.13056, policy_loss: -23.72371, policy_entropy: -6.01659, alpha: 0.00524, time: 70.51875
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   575 ----
[CW] collect: return: 153.19808, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 0.12632, qf2_loss: 0.12750, policy_loss: -23.79522, policy_entropy: -6.00583, alpha: 0.00523, time: 70.51554
[CW] ---------------------------

============================= JOB FEEDBACK =============================

NodeName=uc2n511
Job ID: 21914616
Array Job ID: 21914616_0
Cluster: uc2
User/Group: uprnr/stud
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 4
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 2-00:00:32 core-walltime
Job Wall-clock time: 12:00:08
Memory Utilized: 6.20 GB
Memory Efficiency: 10.57% of 58.59 GB
