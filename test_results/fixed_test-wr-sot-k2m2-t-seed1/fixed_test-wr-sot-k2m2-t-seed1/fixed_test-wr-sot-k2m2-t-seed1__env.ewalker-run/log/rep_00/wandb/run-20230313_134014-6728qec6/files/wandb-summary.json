{"collect/return": 153.19808372389525, "collect/steps": 1000.0, "collect/total_steps": 581000.0, "train/qf1_loss": 0.1263151414692402, "train/qf2_loss": 0.1274979018419981, "train/policy_loss": -23.795216102600097, "train/policy_entropy": -6.005829129219055, "train/alpha": 0.005233892030082643, "train/time": 70.51553773880005, "eval/return": 169.2444548113388, "eval/steps": 1000.0, "_timestamp": 1678754278.853456, "_runtime": 43064.54499411583, "_step": 575}