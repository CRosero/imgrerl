{"collect/return": 533.2625293806195, "collect/steps": 1000.0, "collect/total_steps": 510000.0, "train/qf1_loss": 6.643466858863831, "train/qf2_loss": 6.675616595745087, "train/policy_loss": -102.84224929809571, "train/policy_entropy": -6.029839429855347, "train/alpha": 0.025695578511804342, "train/time": 68.88411331176758, "eval/return": 288.0118870574479, "eval/steps": 1000.0, "_timestamp": 1678738777.1491525, "_runtime": 35970.886932611465, "_step": 504}