[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 26.45219, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.58256, qf2_loss: 1.57601, policy_loss: -7.87154, policy_entropy: 4.09837, alpha: 0.98504, time: 45.47025
[CW] eval: return: 25.49316, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 25.86303, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.08605, qf2_loss: 0.08606, policy_loss: -8.52167, policy_entropy: 4.10125, alpha: 0.95626, time: 32.91457
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 24.30658, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.07516, qf2_loss: 0.07520, policy_loss: -9.21254, policy_entropy: 4.10237, alpha: 0.92871, time: 32.79553
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 26.07395, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.06826, qf2_loss: 0.06831, policy_loss: -10.09755, policy_entropy: 4.10100, alpha: 0.90231, time: 33.08216
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 24.52007, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06351, qf2_loss: 0.06352, policy_loss: -11.04488, policy_entropy: 4.10132, alpha: 0.87699, time: 33.24933
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 26.39665, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06200, qf2_loss: 0.06199, policy_loss: -12.07632, policy_entropy: 4.10094, alpha: 0.85267, time: 32.91427
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 24.20455, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.07186, qf2_loss: 0.07188, policy_loss: -13.18743, policy_entropy: 4.10109, alpha: 0.82931, time: 33.38853
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 23.93569, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.08265, qf2_loss: 0.08274, policy_loss: -14.35907, policy_entropy: 4.10092, alpha: 0.80683, time: 32.82982
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 25.14571, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.07448, qf2_loss: 0.07463, policy_loss: -15.55645, policy_entropy: 4.10077, alpha: 0.78520, time: 32.96565
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 24.07387, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.08537, qf2_loss: 0.08563, policy_loss: -16.76020, policy_entropy: 4.10245, alpha: 0.76436, time: 32.81458
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 28.82412, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.09343, qf2_loss: 0.09379, policy_loss: -17.93678, policy_entropy: 4.09985, alpha: 0.74427, time: 32.66911
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 22.35390, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.08869, qf2_loss: 0.08902, policy_loss: -19.10657, policy_entropy: 4.10203, alpha: 0.72489, time: 33.22601
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 27.19851, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.07838, qf2_loss: 0.07855, policy_loss: -20.24507, policy_entropy: 4.10342, alpha: 0.70617, time: 33.56668
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 25.87235, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.08669, qf2_loss: 0.08687, policy_loss: -21.35928, policy_entropy: 4.10194, alpha: 0.68809, time: 33.56807
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 29.52903, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.09435, qf2_loss: 0.09448, policy_loss: -22.44551, policy_entropy: 4.10170, alpha: 0.67062, time: 33.60825
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 23.64667, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.11117, qf2_loss: 0.11135, policy_loss: -23.50626, policy_entropy: 4.10067, alpha: 0.65372, time: 32.99187
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 35.22008, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.08701, qf2_loss: 0.08707, policy_loss: -24.53703, policy_entropy: 4.10203, alpha: 0.63737, time: 33.66732
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 25.52537, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.07455, qf2_loss: 0.07464, policy_loss: -25.54525, policy_entropy: 4.10143, alpha: 0.62153, time: 33.69440
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 20.58623, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.08168, qf2_loss: 0.08177, policy_loss: -26.52194, policy_entropy: 4.10261, alpha: 0.60619, time: 33.55728
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 21.19475, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.09724, qf2_loss: 0.09739, policy_loss: -27.47111, policy_entropy: 4.10095, alpha: 0.59132, time: 33.53441
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 31.16417, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.08876, qf2_loss: 0.08880, policy_loss: -28.39382, policy_entropy: 4.10092, alpha: 0.57690, time: 33.44808
[CW] eval: return: 25.04118, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 24.87086, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.11814, qf2_loss: 0.11863, policy_loss: -29.29696, policy_entropy: 4.10156, alpha: 0.56291, time: 33.48787
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 23.84551, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.07101, qf2_loss: 0.07097, policy_loss: -30.16724, policy_entropy: 4.10102, alpha: 0.54934, time: 33.11672
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 28.52573, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.06935, qf2_loss: 0.06940, policy_loss: -31.01230, policy_entropy: 4.10127, alpha: 0.53615, time: 33.63645
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 25.40543, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.09814, qf2_loss: 0.09815, policy_loss: -31.83678, policy_entropy: 4.10177, alpha: 0.52335, time: 33.51940
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 25.84923, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.09908, qf2_loss: 0.09913, policy_loss: -32.63372, policy_entropy: 4.10177, alpha: 0.51091, time: 33.05725
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 25.18990, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.09965, qf2_loss: 0.09970, policy_loss: -33.40498, policy_entropy: 4.10186, alpha: 0.49881, time: 33.28643
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 25.56125, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.07361, qf2_loss: 0.07363, policy_loss: -34.14818, policy_entropy: 4.10181, alpha: 0.48705, time: 33.23040
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 25.05913, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.08899, qf2_loss: 0.08899, policy_loss: -34.88075, policy_entropy: 4.10147, alpha: 0.47561, time: 33.33978
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 21.48626, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.08563, qf2_loss: 0.08561, policy_loss: -35.57770, policy_entropy: 4.10133, alpha: 0.46448, time: 33.39526
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 32.70245, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.09603, qf2_loss: 0.09599, policy_loss: -36.25633, policy_entropy: 4.10240, alpha: 0.45365, time: 33.69120
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 30.80942, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.08550, qf2_loss: 0.08540, policy_loss: -36.92149, policy_entropy: 4.10102, alpha: 0.44311, time: 33.51820
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 21.81294, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.13290, qf2_loss: 0.13286, policy_loss: -37.54981, policy_entropy: 4.10111, alpha: 0.43284, time: 33.39093
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 26.87201, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.04774, qf2_loss: 0.04766, policy_loss: -38.17214, policy_entropy: 4.10300, alpha: 0.42284, time: 33.38630
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 32.02156, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.06857, qf2_loss: 0.06854, policy_loss: -38.76580, policy_entropy: 4.10148, alpha: 0.41309, time: 33.32741
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 26.57480, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.12444, qf2_loss: 0.12424, policy_loss: -39.34658, policy_entropy: 4.10154, alpha: 0.40360, time: 33.20344
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 20.49003, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.05344, qf2_loss: 0.05331, policy_loss: -39.90013, policy_entropy: 4.10211, alpha: 0.39434, time: 33.23747
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 23.30424, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.09951, qf2_loss: 0.09931, policy_loss: -40.43997, policy_entropy: 4.10225, alpha: 0.38532, time: 33.40848
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 22.49156, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.07233, qf2_loss: 0.07221, policy_loss: -40.95997, policy_entropy: 4.10125, alpha: 0.37653, time: 33.44386
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 22.45695, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.09350, qf2_loss: 0.09332, policy_loss: -41.46599, policy_entropy: 4.10208, alpha: 0.36795, time: 33.36902
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 22.65200, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.09622, qf2_loss: 0.09600, policy_loss: -41.94558, policy_entropy: 4.10168, alpha: 0.35958, time: 33.32853
[CW] eval: return: 25.24881, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 27.33208, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.07975, qf2_loss: 0.07958, policy_loss: -42.41895, policy_entropy: 4.10173, alpha: 0.35142, time: 33.57334
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 30.13994, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.05215, qf2_loss: 0.05202, policy_loss: -42.86346, policy_entropy: 4.10192, alpha: 0.34346, time: 33.64582
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 25.66349, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.11765, qf2_loss: 0.11738, policy_loss: -43.29601, policy_entropy: 4.10107, alpha: 0.33569, time: 33.13976
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 26.66435, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.07209, qf2_loss: 0.07189, policy_loss: -43.71274, policy_entropy: 4.10143, alpha: 0.32811, time: 33.73549
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 31.58418, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.07004, qf2_loss: 0.06984, policy_loss: -44.12364, policy_entropy: 4.10174, alpha: 0.32071, time: 33.60075
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 27.46050, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.07948, qf2_loss: 0.07928, policy_loss: -44.50457, policy_entropy: 4.10129, alpha: 0.31348, time: 33.54914
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 26.77438, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.09014, qf2_loss: 0.08992, policy_loss: -44.88661, policy_entropy: 4.10151, alpha: 0.30643, time: 33.61998
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 23.98671, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.11982, qf2_loss: 0.11945, policy_loss: -45.23934, policy_entropy: 4.10145, alpha: 0.29954, time: 33.94693
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 28.53843, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.03795, qf2_loss: 0.03788, policy_loss: -45.58641, policy_entropy: 4.10160, alpha: 0.29282, time: 33.83002
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 33.33848, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.07631, qf2_loss: 0.07614, policy_loss: -45.91225, policy_entropy: 4.10204, alpha: 0.28625, time: 33.41176
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 23.95395, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.07522, qf2_loss: 0.07499, policy_loss: -46.23250, policy_entropy: 4.10172, alpha: 0.27983, time: 33.59090
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 22.41633, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.06068, qf2_loss: 0.06056, policy_loss: -46.54446, policy_entropy: 4.10156, alpha: 0.27357, time: 33.77816
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 24.00697, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.08874, qf2_loss: 0.08857, policy_loss: -46.83149, policy_entropy: 4.10124, alpha: 0.26745, time: 33.69412
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 26.52650, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.11246, qf2_loss: 0.11217, policy_loss: -47.10711, policy_entropy: 4.10164, alpha: 0.26147, time: 33.78829
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 28.72448, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.03985, qf2_loss: 0.03981, policy_loss: -47.37240, policy_entropy: 4.10164, alpha: 0.25563, time: 33.80327
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 26.01432, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.08098, qf2_loss: 0.08080, policy_loss: -47.62766, policy_entropy: 4.10202, alpha: 0.24993, time: 33.71539
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 20.99544, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.06614, qf2_loss: 0.06596, policy_loss: -47.87328, policy_entropy: 4.10299, alpha: 0.24435, time: 33.47315
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 20.56835, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.08339, qf2_loss: 0.08317, policy_loss: -48.10574, policy_entropy: 4.10153, alpha: 0.23891, time: 33.74036
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 22.05755, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.06505, qf2_loss: 0.06487, policy_loss: -48.31991, policy_entropy: 4.10054, alpha: 0.23358, time: 33.83135
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 24.14910, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.08800, qf2_loss: 0.08785, policy_loss: -48.53354, policy_entropy: 4.10097, alpha: 0.22838, time: 33.82327
[CW] eval: return: 24.29286, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 29.23709, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.08645, qf2_loss: 0.08625, policy_loss: -48.73081, policy_entropy: 4.10206, alpha: 0.22330, time: 33.60684
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 35.19826, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.05194, qf2_loss: 0.05184, policy_loss: -48.92183, policy_entropy: 4.10196, alpha: 0.21833, time: 33.80785
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 26.13494, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.05857, qf2_loss: 0.05846, policy_loss: -49.09744, policy_entropy: 4.10121, alpha: 0.21348, time: 33.22773
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 24.98918, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.07226, qf2_loss: 0.07218, policy_loss: -49.27099, policy_entropy: 4.10158, alpha: 0.20873, time: 33.51497
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 23.33788, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.07855, qf2_loss: 0.07840, policy_loss: -49.42351, policy_entropy: 4.10296, alpha: 0.20409, time: 33.49351
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 23.67081, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.05499, qf2_loss: 0.05493, policy_loss: -49.57821, policy_entropy: 4.10203, alpha: 0.19956, time: 33.89528
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 25.27682, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.06709, qf2_loss: 0.06700, policy_loss: -49.72576, policy_entropy: 4.10175, alpha: 0.19513, time: 33.81277
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 20.95392, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.05986, qf2_loss: 0.05981, policy_loss: -49.85394, policy_entropy: 4.10135, alpha: 0.19080, time: 34.12390
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 24.77215, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.07892, qf2_loss: 0.07883, policy_loss: -49.97615, policy_entropy: 4.10133, alpha: 0.18656, time: 34.01611
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 25.27987, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.06308, qf2_loss: 0.06304, policy_loss: -50.08984, policy_entropy: 4.10245, alpha: 0.18242, time: 33.77538
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 22.27699, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.05634, qf2_loss: 0.05630, policy_loss: -50.19909, policy_entropy: 4.10191, alpha: 0.17838, time: 33.80706
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 24.76202, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.10440, qf2_loss: 0.10432, policy_loss: -50.29279, policy_entropy: 4.10121, alpha: 0.17442, time: 34.15371
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 23.10558, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.02853, qf2_loss: 0.02854, policy_loss: -50.38415, policy_entropy: 4.10042, alpha: 0.17055, time: 33.76189
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 24.14192, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.06736, qf2_loss: 0.06739, policy_loss: -50.45956, policy_entropy: 4.10156, alpha: 0.16677, time: 33.44431
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 25.30217, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.06555, qf2_loss: 0.06552, policy_loss: -50.54344, policy_entropy: 4.10142, alpha: 0.16308, time: 33.18780
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 21.03506, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.06260, qf2_loss: 0.06264, policy_loss: -50.60192, policy_entropy: 4.10118, alpha: 0.15946, time: 33.50371
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 22.05502, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.03840, qf2_loss: 0.03843, policy_loss: -50.66757, policy_entropy: 4.10149, alpha: 0.15593, time: 33.74323
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 24.38316, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.07019, qf2_loss: 0.07030, policy_loss: -50.72208, policy_entropy: 4.10090, alpha: 0.15248, time: 33.65924
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 25.13598, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.08509, qf2_loss: 0.08497, policy_loss: -50.76743, policy_entropy: 4.09998, alpha: 0.14910, time: 34.43246
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 26.36470, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.03345, qf2_loss: 0.03350, policy_loss: -50.81189, policy_entropy: 4.10125, alpha: 0.14580, time: 33.79785
[CW] eval: return: 24.91076, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 25.89036, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.06189, qf2_loss: 0.06195, policy_loss: -50.84439, policy_entropy: 4.10059, alpha: 0.14257, time: 33.53300
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 20.56259, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.05987, qf2_loss: 0.05993, policy_loss: -50.87361, policy_entropy: 4.10170, alpha: 0.13941, time: 33.63406
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 26.53452, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.04832, qf2_loss: 0.04836, policy_loss: -50.89454, policy_entropy: 4.10329, alpha: 0.13633, time: 33.75384
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 24.68410, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.10524, qf2_loss: 0.10439, policy_loss: -50.91348, policy_entropy: 4.10206, alpha: 0.13331, time: 33.74414
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 24.34618, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.01983, qf2_loss: 0.01992, policy_loss: -50.92366, policy_entropy: 4.10204, alpha: 0.13036, time: 33.56847
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 27.63406, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.03453, qf2_loss: 0.03473, policy_loss: -50.92772, policy_entropy: 4.10155, alpha: 0.12747, time: 33.87642
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 28.12076, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.04592, qf2_loss: 0.04611, policy_loss: -50.92810, policy_entropy: 4.10214, alpha: 0.12465, time: 33.74020
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 28.80378, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.07048, qf2_loss: 0.07072, policy_loss: -50.92071, policy_entropy: 4.10105, alpha: 0.12189, time: 33.97263
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 26.16353, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.04203, qf2_loss: 0.04213, policy_loss: -50.91405, policy_entropy: 4.10248, alpha: 0.11919, time: 33.60219
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 24.08117, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.08472, qf2_loss: 0.08492, policy_loss: -50.90145, policy_entropy: 4.10073, alpha: 0.11656, time: 34.13513
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 20.12465, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.02798, qf2_loss: 0.02803, policy_loss: -50.88152, policy_entropy: 4.10009, alpha: 0.11398, time: 34.05922
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 28.57611, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.05208, qf2_loss: 0.05218, policy_loss: -50.85709, policy_entropy: 4.10065, alpha: 0.11146, time: 34.04815
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 23.19328, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.04481, qf2_loss: 0.04494, policy_loss: -50.82856, policy_entropy: 4.10184, alpha: 0.10899, time: 33.54772
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 21.39208, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.07365, qf2_loss: 0.07385, policy_loss: -50.79559, policy_entropy: 4.10114, alpha: 0.10658, time: 33.67842
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 23.68877, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.03414, qf2_loss: 0.03422, policy_loss: -50.75414, policy_entropy: 4.10207, alpha: 0.10422, time: 33.56054
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 21.68612, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.05514, qf2_loss: 0.05517, policy_loss: -50.71162, policy_entropy: 4.10182, alpha: 0.10192, time: 33.60891
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 24.60434, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.03363, qf2_loss: 0.03372, policy_loss: -50.66848, policy_entropy: 4.10104, alpha: 0.09966, time: 33.66283
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 26.29158, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.06299, qf2_loss: 0.06327, policy_loss: -50.61475, policy_entropy: 4.10127, alpha: 0.09746, time: 34.20875
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 27.68908, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.03351, qf2_loss: 0.03363, policy_loss: -50.56052, policy_entropy: 4.10112, alpha: 0.09531, time: 34.06802
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 22.32441, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.05753, qf2_loss: 0.05768, policy_loss: -50.50414, policy_entropy: 4.10115, alpha: 0.09320, time: 33.90878
[CW] eval: return: 24.47255, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 26.43991, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.03506, qf2_loss: 0.03517, policy_loss: -50.44326, policy_entropy: 4.10190, alpha: 0.09114, time: 33.96715
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 22.10014, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.05327, qf2_loss: 0.05339, policy_loss: -50.37501, policy_entropy: 4.10241, alpha: 0.08912, time: 34.12010
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 25.65998, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.03563, qf2_loss: 0.03576, policy_loss: -50.30938, policy_entropy: 4.10247, alpha: 0.08715, time: 33.95542
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 23.60046, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.05866, qf2_loss: 0.05888, policy_loss: -50.23550, policy_entropy: 4.10122, alpha: 0.08522, time: 34.21128
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 24.78063, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.03140, qf2_loss: 0.03144, policy_loss: -50.16100, policy_entropy: 4.10141, alpha: 0.08334, time: 33.87871
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 23.28523, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.04567, qf2_loss: 0.04584, policy_loss: -50.08499, policy_entropy: 4.10205, alpha: 0.08149, time: 34.13059
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 24.81110, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.05986, qf2_loss: 0.06001, policy_loss: -50.00455, policy_entropy: 4.10234, alpha: 0.07969, time: 34.21863
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 21.53484, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.01447, qf2_loss: 0.01465, policy_loss: -49.91923, policy_entropy: 4.10156, alpha: 0.07793, time: 34.10711
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 29.64564, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.03705, qf2_loss: 0.03717, policy_loss: -49.83372, policy_entropy: 4.10046, alpha: 0.07621, time: 33.97111
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 26.32651, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.03969, qf2_loss: 0.03991, policy_loss: -49.74345, policy_entropy: 4.10172, alpha: 0.07452, time: 34.00079
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 26.32338, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.03832, qf2_loss: 0.03841, policy_loss: -49.65681, policy_entropy: 4.10130, alpha: 0.07287, time: 33.71753
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 23.41249, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.05793, qf2_loss: 0.05811, policy_loss: -49.55521, policy_entropy: 4.10213, alpha: 0.07126, time: 33.94791
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 23.60445, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.02488, qf2_loss: 0.02497, policy_loss: -49.46036, policy_entropy: 4.10187, alpha: 0.06969, time: 33.92669
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 26.45045, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.03605, qf2_loss: 0.03619, policy_loss: -49.35896, policy_entropy: 4.10169, alpha: 0.06814, time: 33.79247
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 25.66331, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.06127, qf2_loss: 0.06142, policy_loss: -49.25990, policy_entropy: 4.10018, alpha: 0.06664, time: 34.06928
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 22.27972, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.01640, qf2_loss: 0.01649, policy_loss: -49.15133, policy_entropy: 4.09919, alpha: 0.06516, time: 33.98232
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 25.56163, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.04616, qf2_loss: 0.04633, policy_loss: -49.04459, policy_entropy: 4.10159, alpha: 0.06372, time: 33.88653
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 24.96906, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.02687, qf2_loss: 0.02699, policy_loss: -48.93688, policy_entropy: 4.10085, alpha: 0.06231, time: 41.95512
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 23.68036, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.04611, qf2_loss: 0.04628, policy_loss: -48.82537, policy_entropy: 4.10020, alpha: 0.06094, time: 34.21305
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 26.64536, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.02102, qf2_loss: 0.02119, policy_loss: -48.70981, policy_entropy: 4.10166, alpha: 0.05959, time: 34.03347
[CW] eval: return: 23.79594, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 23.69058, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.04153, qf2_loss: 0.04167, policy_loss: -48.60288, policy_entropy: 4.10149, alpha: 0.05827, time: 33.72604
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 24.26868, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.03154, qf2_loss: 0.03171, policy_loss: -48.48093, policy_entropy: 4.10053, alpha: 0.05698, time: 34.11241
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 29.95087, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.03666, qf2_loss: 0.03684, policy_loss: -48.36169, policy_entropy: 4.09968, alpha: 0.05572, time: 34.04976
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 24.33562, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.04269, qf2_loss: 0.04291, policy_loss: -48.24255, policy_entropy: 4.09905, alpha: 0.05449, time: 34.03643
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 21.06607, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.02551, qf2_loss: 0.02566, policy_loss: -48.11867, policy_entropy: 4.10017, alpha: 0.05328, time: 34.48584
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 20.47473, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.03315, qf2_loss: 0.03329, policy_loss: -47.99445, policy_entropy: 4.10079, alpha: 0.05211, time: 33.86297
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 25.64872, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.04298, qf2_loss: 0.04322, policy_loss: -47.87097, policy_entropy: 4.10001, alpha: 0.05095, time: 33.58447
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 28.43309, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.01919, qf2_loss: 0.01926, policy_loss: -47.74461, policy_entropy: 4.10035, alpha: 0.04983, time: 34.14882
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 25.52031, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.02657, qf2_loss: 0.02672, policy_loss: -47.61642, policy_entropy: 4.09943, alpha: 0.04873, time: 34.41505
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 24.54372, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.03288, qf2_loss: 0.03298, policy_loss: -47.48192, policy_entropy: 4.09911, alpha: 0.04765, time: 34.45503
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 26.66633, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.03232, qf2_loss: 0.03240, policy_loss: -47.35250, policy_entropy: 4.10064, alpha: 0.04659, time: 33.78273
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 26.85704, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.02920, qf2_loss: 0.02930, policy_loss: -47.22108, policy_entropy: 4.10054, alpha: 0.04556, time: 33.72941
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 27.82229, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.03740, qf2_loss: 0.03753, policy_loss: -47.08687, policy_entropy: 4.09941, alpha: 0.04456, time: 34.12249
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 28.05043, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.01753, qf2_loss: 0.01763, policy_loss: -46.95581, policy_entropy: 4.09895, alpha: 0.04357, time: 34.19015
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 23.33697, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.03393, qf2_loss: 0.03408, policy_loss: -46.81840, policy_entropy: 4.09985, alpha: 0.04261, time: 33.94415
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 27.28499, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.02911, qf2_loss: 0.02929, policy_loss: -46.68263, policy_entropy: 4.10087, alpha: 0.04167, time: 34.12818
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 21.41753, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.04171, qf2_loss: 0.04147, policy_loss: -46.54104, policy_entropy: 4.10025, alpha: 0.04074, time: 34.15214
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 22.46475, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.01179, qf2_loss: 0.01188, policy_loss: -46.40372, policy_entropy: 4.09875, alpha: 0.03984, time: 34.61145
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 23.48014, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.02731, qf2_loss: 0.02757, policy_loss: -46.26040, policy_entropy: 4.09777, alpha: 0.03896, time: 33.73783
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 21.09929, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.02547, qf2_loss: 0.02566, policy_loss: -46.12124, policy_entropy: 4.09990, alpha: 0.03810, time: 34.30427
[CW] eval: return: 24.95083, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 24.59638, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.02727, qf2_loss: 0.02729, policy_loss: -45.97924, policy_entropy: 4.10054, alpha: 0.03726, time: 33.81643
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 25.56607, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.03489, qf2_loss: 0.03521, policy_loss: -45.83894, policy_entropy: 4.09902, alpha: 0.03643, time: 33.67662
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 23.87457, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.02095, qf2_loss: 0.02106, policy_loss: -45.69190, policy_entropy: 4.09926, alpha: 0.03563, time: 34.22341
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 26.79550, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.01909, qf2_loss: 0.01922, policy_loss: -45.54657, policy_entropy: 4.09847, alpha: 0.03484, time: 33.92915
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 25.02283, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.02630, qf2_loss: 0.02641, policy_loss: -45.40366, policy_entropy: 4.09850, alpha: 0.03407, time: 33.54633
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 23.80039, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.03310, qf2_loss: 0.03318, policy_loss: -45.25743, policy_entropy: 4.09839, alpha: 0.03332, time: 33.91901
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 26.02966, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.01810, qf2_loss: 0.01825, policy_loss: -45.10874, policy_entropy: 4.09792, alpha: 0.03258, time: 33.76996
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 29.13038, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.03078, qf2_loss: 0.03092, policy_loss: -44.96399, policy_entropy: 4.09736, alpha: 0.03186, time: 34.17489
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 27.51379, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.02314, qf2_loss: 0.02325, policy_loss: -44.81648, policy_entropy: 4.09877, alpha: 0.03116, time: 33.81245
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 23.52185, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.02150, qf2_loss: 0.02161, policy_loss: -44.66695, policy_entropy: 4.09802, alpha: 0.03047, time: 34.01551
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 23.05641, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.02158, qf2_loss: 0.02166, policy_loss: -44.51625, policy_entropy: 4.09718, alpha: 0.02979, time: 34.03232
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 24.90160, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.02784, qf2_loss: 0.02789, policy_loss: -44.36576, policy_entropy: 4.09593, alpha: 0.02913, time: 33.72610
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 22.95346, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.02491, qf2_loss: 0.02504, policy_loss: -44.21667, policy_entropy: 4.09567, alpha: 0.02849, time: 33.40307
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 29.62489, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.01686, qf2_loss: 0.01697, policy_loss: -44.06715, policy_entropy: 4.09845, alpha: 0.02786, time: 34.02422
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 22.22684, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.02166, qf2_loss: 0.02179, policy_loss: -43.91361, policy_entropy: 4.09562, alpha: 0.02725, time: 34.12639
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 23.08767, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.02910, qf2_loss: 0.02927, policy_loss: -43.76387, policy_entropy: 4.09548, alpha: 0.02664, time: 33.95982
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 23.40175, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.01796, qf2_loss: 0.01801, policy_loss: -43.61702, policy_entropy: 4.09461, alpha: 0.02605, time: 34.03513
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 26.22378, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.02171, qf2_loss: 0.02190, policy_loss: -43.46016, policy_entropy: 4.09662, alpha: 0.02548, time: 33.93365
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 24.90692, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.01861, qf2_loss: 0.01852, policy_loss: -43.30761, policy_entropy: 4.09588, alpha: 0.02491, time: 33.84417
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 24.33995, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.02441, qf2_loss: 0.02465, policy_loss: -43.15660, policy_entropy: 4.09543, alpha: 0.02436, time: 33.76857
[CW] eval: return: 25.15501, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 24.88631, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.02064, qf2_loss: 0.02068, policy_loss: -43.00404, policy_entropy: 4.09470, alpha: 0.02383, time: 33.66760
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 22.21419, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.02137, qf2_loss: 0.02143, policy_loss: -42.85041, policy_entropy: 4.09346, alpha: 0.02330, time: 33.71593
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 27.66793, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.01945, qf2_loss: 0.01934, policy_loss: -42.69544, policy_entropy: 4.09209, alpha: 0.02278, time: 33.75915
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 20.64727, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.03407, qf2_loss: 0.03448, policy_loss: -42.54051, policy_entropy: 4.09291, alpha: 0.02228, time: 34.04810
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 22.96872, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.00797, qf2_loss: 0.00807, policy_loss: -42.38605, policy_entropy: 4.09224, alpha: 0.02179, time: 33.98730
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 23.64626, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.02215, qf2_loss: 0.02216, policy_loss: -42.23009, policy_entropy: 4.09350, alpha: 0.02131, time: 33.91408
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 23.03475, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.01907, qf2_loss: 0.01919, policy_loss: -42.07454, policy_entropy: 4.09261, alpha: 0.02084, time: 34.22168
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 24.33909, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.01435, qf2_loss: 0.01435, policy_loss: -41.92254, policy_entropy: 4.09122, alpha: 0.02038, time: 34.18004
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 24.64708, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.02088, qf2_loss: 0.02098, policy_loss: -41.76786, policy_entropy: 4.09326, alpha: 0.01993, time: 33.32155
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 29.70870, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.01504, qf2_loss: 0.01524, policy_loss: -41.61002, policy_entropy: 4.09234, alpha: 0.01948, time: 33.17381
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 24.29093, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.02248, qf2_loss: 0.02239, policy_loss: -41.45607, policy_entropy: 4.09064, alpha: 0.01905, time: 33.80416
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 35.38821, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.01388, qf2_loss: 0.01408, policy_loss: -41.30890, policy_entropy: 4.09261, alpha: 0.01863, time: 33.98158
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 25.58953, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.02193, qf2_loss: 0.02194, policy_loss: -41.15019, policy_entropy: 4.09176, alpha: 0.01822, time: 34.10232
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 24.07322, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.01136, qf2_loss: 0.01149, policy_loss: -40.99415, policy_entropy: 4.09219, alpha: 0.01782, time: 33.82006
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 24.46301, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.03230, qf2_loss: 0.03232, policy_loss: -40.84057, policy_entropy: 4.08945, alpha: 0.01742, time: 33.79163
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 26.04020, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.01221, qf2_loss: 0.01233, policy_loss: -40.68896, policy_entropy: 4.08946, alpha: 0.01704, time: 33.52737
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 27.24821, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.01599, qf2_loss: 0.01585, policy_loss: -40.53545, policy_entropy: 4.08933, alpha: 0.01666, time: 33.70347
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 24.69796, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.01220, qf2_loss: 0.01216, policy_loss: -40.37871, policy_entropy: 4.08881, alpha: 0.01629, time: 33.76381
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 26.13827, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.01824, qf2_loss: 0.01842, policy_loss: -40.22694, policy_entropy: 4.08773, alpha: 0.01593, time: 33.90412
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 28.00573, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.01489, qf2_loss: 0.01464, policy_loss: -40.07053, policy_entropy: 4.09005, alpha: 0.01558, time: 33.73619
[CW] eval: return: 24.70337, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 21.72178, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.02115, qf2_loss: 0.02122, policy_loss: -39.91526, policy_entropy: 4.08861, alpha: 0.01524, time: 33.61155
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 24.99160, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.02094, qf2_loss: 0.02181, policy_loss: -39.76396, policy_entropy: 4.07969, alpha: 0.01490, time: 34.04285
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 23.42526, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.01815, qf2_loss: 0.01769, policy_loss: -39.61073, policy_entropy: 4.08856, alpha: 0.01457, time: 33.58380
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 22.49909, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.01132, qf2_loss: 0.01134, policy_loss: -39.46058, policy_entropy: 4.08463, alpha: 0.01425, time: 33.60757
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 30.17297, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.01721, qf2_loss: 0.01721, policy_loss: -39.30368, policy_entropy: 4.08624, alpha: 0.01393, time: 33.95452
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 24.11553, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.01494, qf2_loss: 0.01498, policy_loss: -39.14894, policy_entropy: 4.07641, alpha: 0.01363, time: 33.71539
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 26.55677, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.01405, qf2_loss: 0.01387, policy_loss: -39.00339, policy_entropy: 4.08049, alpha: 0.01332, time: 33.78964
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 31.15391, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.01571, qf2_loss: 0.01561, policy_loss: -38.83910, policy_entropy: 4.07928, alpha: 0.01303, time: 33.74690
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 23.15280, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.01844, qf2_loss: 0.01822, policy_loss: -38.69373, policy_entropy: 4.07414, alpha: 0.01274, time: 34.01464
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 29.61659, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.01568, qf2_loss: 0.01580, policy_loss: -38.54757, policy_entropy: 4.05988, alpha: 0.01246, time: 33.85752
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 22.80144, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.01662, qf2_loss: 0.01663, policy_loss: -38.39444, policy_entropy: 4.06452, alpha: 0.01219, time: 34.77661
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 28.62787, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.02337, qf2_loss: 0.02344, policy_loss: -38.24548, policy_entropy: 4.06576, alpha: 0.01192, time: 33.39364
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 22.52206, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.01208, qf2_loss: 0.01186, policy_loss: -38.10014, policy_entropy: 4.06046, alpha: 0.01165, time: 33.80950
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 30.25431, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.01515, qf2_loss: 0.01526, policy_loss: -37.94169, policy_entropy: 4.05044, alpha: 0.01140, time: 33.66181
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 23.96565, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.01704, qf2_loss: 0.01699, policy_loss: -37.79541, policy_entropy: 4.05267, alpha: 0.01115, time: 33.65654
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 21.41810, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.02045, qf2_loss: 0.02050, policy_loss: -37.64843, policy_entropy: 3.99544, alpha: 0.01090, time: 33.88078
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 27.63386, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.01406, qf2_loss: 0.01385, policy_loss: -37.49887, policy_entropy: 4.01197, alpha: 0.01066, time: 34.26653
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 27.24014, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.01989, qf2_loss: 0.01991, policy_loss: -37.34869, policy_entropy: 3.97552, alpha: 0.01043, time: 34.06477
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 33.23958, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.01532, qf2_loss: 0.01534, policy_loss: -37.20357, policy_entropy: 3.99119, alpha: 0.01020, time: 33.54353
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 23.68522, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.01749, qf2_loss: 0.01761, policy_loss: -37.05415, policy_entropy: 3.94841, alpha: 0.00997, time: 33.98238
[CW] eval: return: 25.37648, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 23.20865, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.01766, qf2_loss: 0.01743, policy_loss: -36.90995, policy_entropy: 3.93622, alpha: 0.00976, time: 33.63882
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 28.85670, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.01655, qf2_loss: 0.01646, policy_loss: -36.76292, policy_entropy: 3.91842, alpha: 0.00954, time: 33.61484
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 22.05943, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.03079, qf2_loss: 0.03080, policy_loss: -36.61303, policy_entropy: 3.89491, alpha: 0.00933, time: 33.82682
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 22.03151, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.00692, qf2_loss: 0.00698, policy_loss: -36.45864, policy_entropy: 3.91168, alpha: 0.00913, time: 33.79271
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 24.64833, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.01728, qf2_loss: 0.01710, policy_loss: -36.32732, policy_entropy: 3.79285, alpha: 0.00893, time: 33.95420
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 24.29395, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.01887, qf2_loss: 0.01871, policy_loss: -36.17789, policy_entropy: 3.74025, alpha: 0.00874, time: 33.59463
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 24.80414, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.01309, qf2_loss: 0.01332, policy_loss: -36.03599, policy_entropy: 3.50972, alpha: 0.00855, time: 33.59901
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 26.16683, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.02208, qf2_loss: 0.02178, policy_loss: -35.89073, policy_entropy: 3.41611, alpha: 0.00837, time: 34.01014
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 16.69053, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.01431, qf2_loss: 0.01446, policy_loss: -35.74680, policy_entropy: 3.55903, alpha: 0.00819, time: 33.69387
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 25.03138, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.01972, qf2_loss: 0.01948, policy_loss: -35.60222, policy_entropy: 3.42250, alpha: 0.00802, time: 34.01307
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 23.26172, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.02113, qf2_loss: 0.02081, policy_loss: -35.46437, policy_entropy: 3.27952, alpha: 0.00785, time: 33.68511
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 19.00934, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.01877, qf2_loss: 0.01917, policy_loss: -35.32458, policy_entropy: 3.41425, alpha: 0.00769, time: 33.76776
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 25.83723, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.01927, qf2_loss: 0.01894, policy_loss: -35.18195, policy_entropy: 3.13960, alpha: 0.00752, time: 33.62675
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 13.72663, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 0.02043, qf2_loss: 0.02014, policy_loss: -35.03559, policy_entropy: 3.02360, alpha: 0.00737, time: 33.93905
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 26.04745, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.02112, qf2_loss: 0.02074, policy_loss: -34.89703, policy_entropy: 3.38705, alpha: 0.00721, time: 33.65400
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 27.19883, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.02259, qf2_loss: 0.02258, policy_loss: -34.76053, policy_entropy: 3.04073, alpha: 0.00706, time: 33.85626
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 25.66109, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.01640, qf2_loss: 0.01625, policy_loss: -34.62215, policy_entropy: 3.02209, alpha: 0.00691, time: 33.91792
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 12.01391, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.01753, qf2_loss: 0.01707, policy_loss: -34.48408, policy_entropy: 2.21298, alpha: 0.00678, time: 33.80742
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 27.82750, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.02188, qf2_loss: 0.02167, policy_loss: -34.34334, policy_entropy: 2.57474, alpha: 0.00664, time: 33.70879
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 25.22320, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.01671, qf2_loss: 0.01642, policy_loss: -34.21095, policy_entropy: 2.31972, alpha: 0.00651, time: 39.00091
[CW] eval: return: 23.81088, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 27.04327, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.02857, qf2_loss: 0.02846, policy_loss: -34.06951, policy_entropy: 2.12519, alpha: 0.00638, time: 33.60399
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 11.00716, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.02008, qf2_loss: 0.01997, policy_loss: -33.93974, policy_entropy: 1.61164, alpha: 0.00626, time: 33.83483
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 32.47142, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.02153, qf2_loss: 0.02160, policy_loss: -33.80485, policy_entropy: 1.75753, alpha: 0.00614, time: 33.75576
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 43.25140, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.02584, qf2_loss: 0.02554, policy_loss: -33.68355, policy_entropy: 1.47197, alpha: 0.00603, time: 34.04256
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 41.78332, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.01825, qf2_loss: 0.01817, policy_loss: -33.55228, policy_entropy: 0.93663, alpha: 0.00592, time: 34.15979
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 51.57892, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.02632, qf2_loss: 0.02600, policy_loss: -33.42606, policy_entropy: -0.21920, alpha: 0.00582, time: 34.06521
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 29.06261, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.02427, qf2_loss: 0.02450, policy_loss: -33.27089, policy_entropy: 1.55660, alpha: 0.00573, time: 33.82445
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 30.87896, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 0.03433, qf2_loss: 0.03375, policy_loss: -33.16495, policy_entropy: 0.84352, alpha: 0.00562, time: 33.88179
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 53.20859, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.02021, qf2_loss: 0.02009, policy_loss: -33.03383, policy_entropy: 0.35287, alpha: 0.00552, time: 33.96154
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 41.02717, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.02034, qf2_loss: 0.02013, policy_loss: -32.92134, policy_entropy: -0.47507, alpha: 0.00543, time: 33.88503
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 36.23453, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 0.02463, qf2_loss: 0.02454, policy_loss: -32.79724, policy_entropy: -1.51166, alpha: 0.00536, time: 33.96132
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 41.99953, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.03306, qf2_loss: 0.03249, policy_loss: -32.67934, policy_entropy: -2.33571, alpha: 0.00529, time: 33.94164
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 40.39213, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.02908, qf2_loss: 0.02930, policy_loss: -32.56185, policy_entropy: -2.38768, alpha: 0.00524, time: 33.95082
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 46.93352, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.03939, qf2_loss: 0.03865, policy_loss: -32.44283, policy_entropy: -2.77088, alpha: 0.00518, time: 33.92479
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 42.03349, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.02685, qf2_loss: 0.02684, policy_loss: -32.33435, policy_entropy: -2.95175, alpha: 0.00513, time: 33.65960
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 46.59009, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.03454, qf2_loss: 0.03402, policy_loss: -32.21934, policy_entropy: -3.05527, alpha: 0.00508, time: 33.97460
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 24.27952, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 0.02924, qf2_loss: 0.02924, policy_loss: -32.10217, policy_entropy: -3.00840, alpha: 0.00503, time: 33.98512
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 25.10583, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 0.03154, qf2_loss: 0.03123, policy_loss: -31.97244, policy_entropy: -1.64023, alpha: 0.00497, time: 34.13325
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 43.37931, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 0.02737, qf2_loss: 0.02743, policy_loss: -31.85949, policy_entropy: -1.82204, alpha: 0.00490, time: 33.83351
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 26.87931, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.03703, qf2_loss: 0.03650, policy_loss: -31.75108, policy_entropy: -2.65562, alpha: 0.00483, time: 33.89226
[CW] eval: return: 22.64577, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 22.95941, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 0.04931, qf2_loss: 0.04871, policy_loss: -31.63414, policy_entropy: -1.80653, alpha: 0.00476, time: 33.85651
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 22.91995, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 0.03169, qf2_loss: 0.03143, policy_loss: -31.51845, policy_entropy: -1.80173, alpha: 0.00469, time: 33.44780
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 37.23451, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 0.03801, qf2_loss: 0.03790, policy_loss: -31.39994, policy_entropy: -1.39596, alpha: 0.00461, time: 34.05634
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 45.83364, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 0.03305, qf2_loss: 0.03296, policy_loss: -31.30944, policy_entropy: -2.15395, alpha: 0.00453, time: 34.04663
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 10.00768, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 0.03506, qf2_loss: 0.03508, policy_loss: -31.20744, policy_entropy: -2.45243, alpha: 0.00446, time: 33.59544
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 10.81833, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 0.03895, qf2_loss: 0.03850, policy_loss: -31.09776, policy_entropy: -1.57626, alpha: 0.00439, time: 33.77083
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 22.85327, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 0.03658, qf2_loss: 0.03635, policy_loss: -31.00567, policy_entropy: -1.12461, alpha: 0.00431, time: 34.08234
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 63.01317, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 0.04312, qf2_loss: 0.04291, policy_loss: -30.90602, policy_entropy: -1.49983, alpha: 0.00422, time: 34.01549
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 41.76611, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 0.03890, qf2_loss: 0.03903, policy_loss: -30.81248, policy_entropy: -1.59376, alpha: 0.00414, time: 33.95217
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 26.43322, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 0.04593, qf2_loss: 0.04587, policy_loss: -30.72824, policy_entropy: -2.10715, alpha: 0.00407, time: 33.85171
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 26.79440, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 0.04129, qf2_loss: 0.04096, policy_loss: -30.64714, policy_entropy: -3.45560, alpha: 0.00401, time: 33.46666
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 66.84387, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 0.04120, qf2_loss: 0.04119, policy_loss: -30.58876, policy_entropy: -3.61429, alpha: 0.00397, time: 33.55869
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 25.01807, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 0.04092, qf2_loss: 0.04096, policy_loss: -30.50875, policy_entropy: -3.76544, alpha: 0.00392, time: 34.33955
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 22.90624, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 0.03593, qf2_loss: 0.03620, policy_loss: -30.43355, policy_entropy: -4.08320, alpha: 0.00388, time: 33.88257
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 23.12996, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 0.05547, qf2_loss: 0.05562, policy_loss: -30.35103, policy_entropy: -3.77435, alpha: 0.00385, time: 33.93096
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 22.97442, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 0.03835, qf2_loss: 0.03973, policy_loss: -30.29039, policy_entropy: -3.98538, alpha: 0.00380, time: 34.37400
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 12.26635, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 0.03899, qf2_loss: 0.03934, policy_loss: -30.24970, policy_entropy: -4.60459, alpha: 0.00377, time: 34.60501
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 39.30858, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 0.04260, qf2_loss: 0.04309, policy_loss: -30.20195, policy_entropy: -5.19635, alpha: 0.00374, time: 34.00284
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 42.71384, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 0.05007, qf2_loss: 0.04963, policy_loss: -30.14704, policy_entropy: -5.37586, alpha: 0.00373, time: 34.00793
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 42.99518, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 0.04117, qf2_loss: 0.04136, policy_loss: -30.11558, policy_entropy: -6.22940, alpha: 0.00372, time: 34.01481
[CW] eval: return: 33.82066, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 37.33171, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 0.06693, qf2_loss: 0.06663, policy_loss: -30.04705, policy_entropy: -6.09070, alpha: 0.00373, time: 33.66177
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 56.01824, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 0.05880, qf2_loss: 0.05871, policy_loss: -29.99385, policy_entropy: -5.70666, alpha: 0.00372, time: 33.43035
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 67.01094, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 0.05804, qf2_loss: 0.05784, policy_loss: -29.97627, policy_entropy: -6.79089, alpha: 0.00373, time: 34.00680
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 66.07326, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 0.04993, qf2_loss: 0.05041, policy_loss: -29.94688, policy_entropy: -6.81470, alpha: 0.00375, time: 33.72977
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 61.56303, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 0.04854, qf2_loss: 0.04834, policy_loss: -29.90639, policy_entropy: -6.75775, alpha: 0.00377, time: 33.60358
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 33.10750, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 0.06052, qf2_loss: 0.06060, policy_loss: -29.82082, policy_entropy: -5.73538, alpha: 0.00379, time: 33.70662
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 52.73614, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 0.06245, qf2_loss: 0.06253, policy_loss: -29.79907, policy_entropy: -6.52093, alpha: 0.00378, time: 33.83748
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 21.20493, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 0.05251, qf2_loss: 0.05275, policy_loss: -29.76414, policy_entropy: -5.87004, alpha: 0.00380, time: 33.84608
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 55.92171, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 0.05392, qf2_loss: 0.05417, policy_loss: -29.72722, policy_entropy: -6.01068, alpha: 0.00378, time: 34.16956
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 24.81517, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 0.06623, qf2_loss: 0.06645, policy_loss: -29.69021, policy_entropy: -6.93403, alpha: 0.00380, time: 33.72819
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 81.66996, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 0.05100, qf2_loss: 0.05078, policy_loss: -29.63459, policy_entropy: -7.01396, alpha: 0.00385, time: 33.54641
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 76.20132, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 0.07126, qf2_loss: 0.07135, policy_loss: -29.63233, policy_entropy: -7.50939, alpha: 0.00389, time: 33.80014
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 82.68855, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 0.06076, qf2_loss: 0.06092, policy_loss: -29.55914, policy_entropy: -6.76114, alpha: 0.00395, time: 33.38904
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 66.83629, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 0.05406, qf2_loss: 0.05378, policy_loss: -29.53238, policy_entropy: -6.16491, alpha: 0.00397, time: 33.73877
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 47.56315, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 0.05773, qf2_loss: 0.05779, policy_loss: -29.46640, policy_entropy: -6.00342, alpha: 0.00398, time: 33.78987
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 44.43075, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 0.05794, qf2_loss: 0.05747, policy_loss: -29.41646, policy_entropy: -5.93470, alpha: 0.00398, time: 33.78814
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 82.72308, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 0.04977, qf2_loss: 0.04981, policy_loss: -29.35086, policy_entropy: -5.08695, alpha: 0.00396, time: 33.76809
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 70.83400, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 0.05118, qf2_loss: 0.05132, policy_loss: -29.29623, policy_entropy: -5.47030, alpha: 0.00393, time: 33.84783
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 33.82385, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 0.06092, qf2_loss: 0.06060, policy_loss: -29.23859, policy_entropy: -4.66873, alpha: 0.00387, time: 33.67469
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 71.30786, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 0.05251, qf2_loss: 0.05252, policy_loss: -29.18170, policy_entropy: -4.53522, alpha: 0.00379, time: 33.80830
[CW] eval: return: 36.15238, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 21.70964, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 0.05128, qf2_loss: 0.05118, policy_loss: -29.11104, policy_entropy: -4.90907, alpha: 0.00371, time: 33.70574
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 43.84932, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 0.12691, qf2_loss: 0.12625, policy_loss: -29.02342, policy_entropy: -4.05386, alpha: 0.00365, time: 33.75511
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 22.22209, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 0.05460, qf2_loss: 0.05505, policy_loss: -28.98359, policy_entropy: -4.81381, alpha: 0.00354, time: 33.53031
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 25.02382, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 0.04920, qf2_loss: 0.04928, policy_loss: -28.94000, policy_entropy: -5.16296, alpha: 0.00349, time: 33.84735
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 29.13174, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 0.04817, qf2_loss: 0.04798, policy_loss: -28.90359, policy_entropy: -5.51354, alpha: 0.00346, time: 33.83628
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 49.26855, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 0.04365, qf2_loss: 0.04385, policy_loss: -28.82509, policy_entropy: -4.67797, alpha: 0.00342, time: 33.71140
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 87.85922, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 0.05172, qf2_loss: 0.05191, policy_loss: -28.75758, policy_entropy: -4.67851, alpha: 0.00334, time: 33.71619
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 64.57373, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 0.04388, qf2_loss: 0.04481, policy_loss: -28.72509, policy_entropy: -4.70519, alpha: 0.00327, time: 33.56656
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 65.99210, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 0.07042, qf2_loss: 0.06975, policy_loss: -28.69608, policy_entropy: -4.51128, alpha: 0.00321, time: 33.26641
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 42.89592, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 0.04491, qf2_loss: 0.04514, policy_loss: -28.59706, policy_entropy: -4.40781, alpha: 0.00313, time: 33.58318
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 80.48556, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 0.05103, qf2_loss: 0.05147, policy_loss: -28.53464, policy_entropy: -4.14319, alpha: 0.00305, time: 35.87252
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 67.43760, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 0.04636, qf2_loss: 0.04627, policy_loss: -28.48674, policy_entropy: -4.90729, alpha: 0.00298, time: 33.77384
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 19.82345, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 0.04451, qf2_loss: 0.04432, policy_loss: -28.41041, policy_entropy: -4.97303, alpha: 0.00293, time: 33.64651
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 27.86141, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 0.04154, qf2_loss: 0.04139, policy_loss: -28.38680, policy_entropy: -4.84213, alpha: 0.00288, time: 33.53090
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 87.55747, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 0.04196, qf2_loss: 0.04194, policy_loss: -28.31161, policy_entropy: -5.44184, alpha: 0.00284, time: 33.47302
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 55.43514, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 0.04789, qf2_loss: 0.04814, policy_loss: -28.23472, policy_entropy: -5.70660, alpha: 0.00283, time: 33.66600
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 95.38217, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 0.05063, qf2_loss: 0.05035, policy_loss: -28.20829, policy_entropy: -5.41606, alpha: 0.00281, time: 33.49111
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 114.70885, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 0.04810, qf2_loss: 0.04840, policy_loss: -28.14511, policy_entropy: -6.06130, alpha: 0.00279, time: 33.76686
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 89.80184, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 0.04574, qf2_loss: 0.04578, policy_loss: -28.09471, policy_entropy: -5.99191, alpha: 0.00279, time: 33.78272
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 97.99607, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 0.04345, qf2_loss: 0.04268, policy_loss: -28.02427, policy_entropy: -6.28899, alpha: 0.00279, time: 33.61716
[CW] eval: return: 94.18890, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 103.61947, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 0.05732, qf2_loss: 0.05678, policy_loss: -27.97238, policy_entropy: -7.89639, alpha: 0.00285, time: 33.73549
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 66.85913, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 0.04742, qf2_loss: 0.04775, policy_loss: -27.91160, policy_entropy: -7.22497, alpha: 0.00294, time: 33.44591
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 98.98098, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 0.04614, qf2_loss: 0.04657, policy_loss: -27.88382, policy_entropy: -6.17685, alpha: 0.00300, time: 33.67153
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 91.57337, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 0.04690, qf2_loss: 0.04717, policy_loss: -27.85751, policy_entropy: -6.97638, alpha: 0.00303, time: 33.31191
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 127.37302, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 0.05068, qf2_loss: 0.04985, policy_loss: -27.79689, policy_entropy: -7.03750, alpha: 0.00308, time: 33.70448
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 37.74813, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 0.05867, qf2_loss: 0.05821, policy_loss: -27.73814, policy_entropy: -7.40076, alpha: 0.00317, time: 33.79402
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 86.43012, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 0.05757, qf2_loss: 0.05750, policy_loss: -27.69948, policy_entropy: -7.04919, alpha: 0.00327, time: 33.35046
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 111.64167, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 0.05205, qf2_loss: 0.05229, policy_loss: -27.64255, policy_entropy: -7.57415, alpha: 0.00337, time: 33.75029
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 104.37408, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 0.05126, qf2_loss: 0.05095, policy_loss: -27.61220, policy_entropy: -7.01556, alpha: 0.00347, time: 33.94698
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 133.27264, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 0.05108, qf2_loss: 0.05091, policy_loss: -27.56262, policy_entropy: -7.12174, alpha: 0.00357, time: 33.86156
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 86.12020, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 0.05867, qf2_loss: 0.05919, policy_loss: -27.53621, policy_entropy: -6.55209, alpha: 0.00365, time: 33.86888
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 141.77295, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 0.05806, qf2_loss: 0.05652, policy_loss: -27.48722, policy_entropy: -6.12709, alpha: 0.00369, time: 33.78174
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 57.76061, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 0.06602, qf2_loss: 0.06642, policy_loss: -27.44955, policy_entropy: -6.02524, alpha: 0.00369, time: 33.54167
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 134.27496, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 0.05107, qf2_loss: 0.05137, policy_loss: -27.41424, policy_entropy: -5.80981, alpha: 0.00369, time: 33.67059
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 102.48919, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 0.06710, qf2_loss: 0.06645, policy_loss: -27.39655, policy_entropy: -6.20589, alpha: 0.00367, time: 33.32382
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 128.77195, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 0.05862, qf2_loss: 0.05847, policy_loss: -27.37778, policy_entropy: -6.40434, alpha: 0.00371, time: 33.88858
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 26.99120, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 0.05317, qf2_loss: 0.05301, policy_loss: -27.33238, policy_entropy: -6.45701, alpha: 0.00377, time: 33.94201
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 107.13628, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 0.06142, qf2_loss: 0.06075, policy_loss: -27.31152, policy_entropy: -6.55154, alpha: 0.00383, time: 34.10776
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 118.34346, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 0.06251, qf2_loss: 0.06243, policy_loss: -27.22270, policy_entropy: -6.23670, alpha: 0.00389, time: 34.03456
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 130.99892, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 0.06121, qf2_loss: 0.06184, policy_loss: -27.24240, policy_entropy: -6.02305, alpha: 0.00390, time: 33.69324
[CW] eval: return: 126.29283, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 129.86056, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 0.08147, qf2_loss: 0.07924, policy_loss: -27.22183, policy_entropy: -6.39238, alpha: 0.00393, time: 36.02128
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 130.88506, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 0.07480, qf2_loss: 0.07590, policy_loss: -27.22289, policy_entropy: -6.41062, alpha: 0.00400, time: 33.47584
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 112.19577, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 0.06497, qf2_loss: 0.06451, policy_loss: -27.17836, policy_entropy: -6.72552, alpha: 0.00408, time: 33.58838
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 132.10767, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 0.07054, qf2_loss: 0.06970, policy_loss: -27.19458, policy_entropy: -6.40478, alpha: 0.00417, time: 33.59036
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 123.71785, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 0.06576, qf2_loss: 0.06554, policy_loss: -27.15186, policy_entropy: -6.40687, alpha: 0.00424, time: 33.93593
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 112.26245, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 0.06623, qf2_loss: 0.06529, policy_loss: -27.12126, policy_entropy: -6.43324, alpha: 0.00432, time: 33.44819
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 104.12980, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 0.06126, qf2_loss: 0.06083, policy_loss: -27.10659, policy_entropy: -6.38727, alpha: 0.00439, time: 33.49814
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 113.18932, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 0.06453, qf2_loss: 0.06550, policy_loss: -27.11077, policy_entropy: -5.67987, alpha: 0.00443, time: 33.90514
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 110.80077, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 0.07958, qf2_loss: 0.07723, policy_loss: -27.08931, policy_entropy: -5.73416, alpha: 0.00437, time: 33.62975
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 122.12256, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 0.07302, qf2_loss: 0.07400, policy_loss: -27.07976, policy_entropy: -6.08703, alpha: 0.00434, time: 33.48767
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 113.61141, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 0.06056, qf2_loss: 0.06075, policy_loss: -27.06190, policy_entropy: -6.21384, alpha: 0.00436, time: 33.64715
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 119.13404, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 0.06211, qf2_loss: 0.06255, policy_loss: -27.01332, policy_entropy: -5.86775, alpha: 0.00438, time: 33.39139
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 119.14445, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 0.06331, qf2_loss: 0.06244, policy_loss: -27.01918, policy_entropy: -5.97209, alpha: 0.00436, time: 33.68430
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 117.44931, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 0.06322, qf2_loss: 0.06296, policy_loss: -27.00702, policy_entropy: -5.95269, alpha: 0.00435, time: 33.54834
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 123.92892, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 0.09188, qf2_loss: 0.09056, policy_loss: -26.99759, policy_entropy: -5.77026, alpha: 0.00433, time: 33.48259
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 112.94089, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 0.05475, qf2_loss: 0.05476, policy_loss: -26.94968, policy_entropy: -5.91982, alpha: 0.00430, time: 33.74495
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 128.23064, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 0.05401, qf2_loss: 0.05397, policy_loss: -26.94619, policy_entropy: -5.89250, alpha: 0.00426, time: 33.92411
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 114.87949, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 0.04935, qf2_loss: 0.04968, policy_loss: -26.94459, policy_entropy: -5.98032, alpha: 0.00425, time: 33.87427
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 67.46153, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 0.05761, qf2_loss: 0.05725, policy_loss: -26.92445, policy_entropy: -5.89571, alpha: 0.00424, time: 33.29667
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 137.12429, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 0.05828, qf2_loss: 0.05856, policy_loss: -26.88587, policy_entropy: -5.99863, alpha: 0.00422, time: 33.56226
[CW] eval: return: 123.87607, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 118.37726, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 0.05758, qf2_loss: 0.05662, policy_loss: -26.85957, policy_entropy: -5.51200, alpha: 0.00418, time: 33.20093
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 127.22494, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 0.05092, qf2_loss: 0.05095, policy_loss: -26.84298, policy_entropy: -5.91488, alpha: 0.00410, time: 33.45617
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 105.62712, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 0.05232, qf2_loss: 0.05217, policy_loss: -26.84807, policy_entropy: -6.14774, alpha: 0.00411, time: 33.41815
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 135.64657, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 0.05803, qf2_loss: 0.05721, policy_loss: -26.83631, policy_entropy: -6.15691, alpha: 0.00415, time: 33.84830
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 136.90034, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 0.05280, qf2_loss: 0.05398, policy_loss: -26.80787, policy_entropy: -5.88396, alpha: 0.00416, time: 33.82474
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 127.93246, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 0.04504, qf2_loss: 0.04421, policy_loss: -26.83090, policy_entropy: -5.78661, alpha: 0.00411, time: 33.42836
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 125.40755, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 0.04833, qf2_loss: 0.04759, policy_loss: -26.75080, policy_entropy: -5.87264, alpha: 0.00407, time: 33.97360
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 113.82750, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 0.04864, qf2_loss: 0.04903, policy_loss: -26.74484, policy_entropy: -5.61179, alpha: 0.00402, time: 33.68211
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 112.02725, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 0.04230, qf2_loss: 0.04280, policy_loss: -26.69332, policy_entropy: -6.06421, alpha: 0.00397, time: 33.65043
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 113.70395, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 0.04428, qf2_loss: 0.04383, policy_loss: -26.71109, policy_entropy: -6.18742, alpha: 0.00401, time: 33.21784
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 115.29680, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 0.04751, qf2_loss: 0.04820, policy_loss: -26.63187, policy_entropy: -5.93892, alpha: 0.00402, time: 33.68911
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 104.61853, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 0.06382, qf2_loss: 0.06312, policy_loss: -26.60527, policy_entropy: -6.00681, alpha: 0.00401, time: 33.80546
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 108.86443, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 0.05282, qf2_loss: 0.05273, policy_loss: -26.52448, policy_entropy: -5.98706, alpha: 0.00403, time: 34.04682
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 119.63237, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 0.04196, qf2_loss: 0.04200, policy_loss: -26.58342, policy_entropy: -5.72447, alpha: 0.00398, time: 33.58608
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 124.56866, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 0.04263, qf2_loss: 0.04322, policy_loss: -26.54801, policy_entropy: -5.90411, alpha: 0.00394, time: 34.02417
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 125.99891, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 0.04670, qf2_loss: 0.04724, policy_loss: -26.49770, policy_entropy: -5.74178, alpha: 0.00392, time: 33.88405
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 137.33304, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 0.04510, qf2_loss: 0.04480, policy_loss: -26.50767, policy_entropy: -5.67259, alpha: 0.00385, time: 33.57045
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 131.27015, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 0.04773, qf2_loss: 0.04740, policy_loss: -26.48904, policy_entropy: -5.70701, alpha: 0.00378, time: 33.69149
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 132.31260, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 0.04737, qf2_loss: 0.04641, policy_loss: -26.44669, policy_entropy: -5.70718, alpha: 0.00373, time: 33.73187
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 145.59585, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 0.05036, qf2_loss: 0.05047, policy_loss: -26.38534, policy_entropy: -5.67332, alpha: 0.00369, time: 33.77655
[CW] eval: return: 112.93473, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 144.88352, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 0.05183, qf2_loss: 0.05142, policy_loss: -26.35957, policy_entropy: -5.78504, alpha: 0.00362, time: 33.65188
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 135.08901, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 0.04595, qf2_loss: 0.04586, policy_loss: -26.38874, policy_entropy: -5.65182, alpha: 0.00357, time: 33.81316
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 68.30278, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 0.06087, qf2_loss: 0.06147, policy_loss: -26.30846, policy_entropy: -6.22494, alpha: 0.00355, time: 33.84072
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 126.97428, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 0.05799, qf2_loss: 0.05711, policy_loss: -26.27081, policy_entropy: -6.25239, alpha: 0.00360, time: 34.05262
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 121.90480, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 0.05382, qf2_loss: 0.05389, policy_loss: -26.25354, policy_entropy: -5.91187, alpha: 0.00361, time: 33.55309
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 130.15354, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 0.04666, qf2_loss: 0.04634, policy_loss: -26.25299, policy_entropy: -6.26954, alpha: 0.00362, time: 33.41960
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 76.56605, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 0.05260, qf2_loss: 0.05248, policy_loss: -26.18624, policy_entropy: -6.15786, alpha: 0.00366, time: 33.75404
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 111.82895, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 0.04716, qf2_loss: 0.04701, policy_loss: -26.17042, policy_entropy: -6.31130, alpha: 0.00371, time: 33.55872
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 124.01505, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 0.05334, qf2_loss: 0.05308, policy_loss: -26.14894, policy_entropy: -6.44375, alpha: 0.00377, time: 33.61601
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 101.67463, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 0.05332, qf2_loss: 0.05331, policy_loss: -26.15780, policy_entropy: -6.49986, alpha: 0.00386, time: 33.58532
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 134.29933, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 0.05068, qf2_loss: 0.05107, policy_loss: -26.08298, policy_entropy: -6.48173, alpha: 0.00394, time: 34.01216
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 39.49346, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 0.05308, qf2_loss: 0.05322, policy_loss: -26.06968, policy_entropy: -6.66084, alpha: 0.00407, time: 33.49812
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 132.74242, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 0.06118, qf2_loss: 0.06130, policy_loss: -26.01539, policy_entropy: -6.87350, alpha: 0.00421, time: 33.62672
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 139.74160, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 0.05541, qf2_loss: 0.05568, policy_loss: -26.02128, policy_entropy: -6.65045, alpha: 0.00435, time: 33.24402
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 140.33352, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 0.05851, qf2_loss: 0.05704, policy_loss: -26.02164, policy_entropy: -6.83948, alpha: 0.00450, time: 33.66450
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 124.21670, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 0.05662, qf2_loss: 0.05737, policy_loss: -26.00475, policy_entropy: -6.84952, alpha: 0.00467, time: 33.81283
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 128.06800, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 0.07104, qf2_loss: 0.07119, policy_loss: -25.94901, policy_entropy: -6.55234, alpha: 0.00482, time: 33.76318
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 128.92124, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 0.05595, qf2_loss: 0.05566, policy_loss: -25.91712, policy_entropy: -6.32253, alpha: 0.00491, time: 33.63063
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 114.75669, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 0.05339, qf2_loss: 0.05333, policy_loss: -25.90347, policy_entropy: -6.01251, alpha: 0.00498, time: 33.82830
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 126.58753, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 0.06056, qf2_loss: 0.06036, policy_loss: -25.92023, policy_entropy: -5.86469, alpha: 0.00497, time: 33.79177
[CW] eval: return: 116.97277, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 104.05823, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 0.06386, qf2_loss: 0.06387, policy_loss: -25.88384, policy_entropy: -5.96606, alpha: 0.00495, time: 33.49232
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 124.74518, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 0.06548, qf2_loss: 0.06488, policy_loss: -25.84007, policy_entropy: -6.04709, alpha: 0.00494, time: 33.50005
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 119.82011, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 0.06438, qf2_loss: 0.06422, policy_loss: -25.86403, policy_entropy: -6.23599, alpha: 0.00497, time: 33.55944
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 127.74479, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 0.07796, qf2_loss: 0.07807, policy_loss: -25.80724, policy_entropy: -6.16489, alpha: 0.00503, time: 33.72581
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 136.18643, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 0.07309, qf2_loss: 0.07326, policy_loss: -25.73487, policy_entropy: -6.12175, alpha: 0.00507, time: 33.62581
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 114.85227, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 0.06711, qf2_loss: 0.06654, policy_loss: -25.81791, policy_entropy: -6.10480, alpha: 0.00511, time: 37.61121
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 109.82727, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 0.07601, qf2_loss: 0.07553, policy_loss: -25.79562, policy_entropy: -6.04236, alpha: 0.00510, time: 33.58328
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 129.53004, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 0.05816, qf2_loss: 0.05790, policy_loss: -25.73887, policy_entropy: -6.40159, alpha: 0.00520, time: 33.91053
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 120.60954, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 0.06000, qf2_loss: 0.05932, policy_loss: -25.79609, policy_entropy: -6.28847, alpha: 0.00530, time: 33.61859
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 112.16030, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 0.06035, qf2_loss: 0.06124, policy_loss: -25.74582, policy_entropy: -5.93975, alpha: 0.00533, time: 33.42720
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 124.63653, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 0.06605, qf2_loss: 0.06597, policy_loss: -25.73820, policy_entropy: -5.75992, alpha: 0.00532, time: 33.93704
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 135.81265, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 0.05996, qf2_loss: 0.06014, policy_loss: -25.71069, policy_entropy: -5.88332, alpha: 0.00524, time: 33.54761
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 127.93007, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 0.05624, qf2_loss: 0.05683, policy_loss: -25.68894, policy_entropy: -5.75890, alpha: 0.00518, time: 33.66459
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 138.39683, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 0.07497, qf2_loss: 0.07408, policy_loss: -25.74886, policy_entropy: -5.88317, alpha: 0.00512, time: 34.61353
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 125.03384, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 0.05840, qf2_loss: 0.05860, policy_loss: -25.65234, policy_entropy: -5.96495, alpha: 0.00511, time: 33.60559
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 128.71020, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 0.07330, qf2_loss: 0.07399, policy_loss: -25.68057, policy_entropy: -5.95296, alpha: 0.00508, time: 33.81303
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 109.83640, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 0.05760, qf2_loss: 0.05769, policy_loss: -25.67576, policy_entropy: -5.82741, alpha: 0.00505, time: 33.94412
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 144.84031, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 0.06528, qf2_loss: 0.06617, policy_loss: -25.64606, policy_entropy: -5.83103, alpha: 0.00500, time: 33.63349
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 128.44376, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 0.06317, qf2_loss: 0.06256, policy_loss: -25.64125, policy_entropy: -6.06322, alpha: 0.00496, time: 34.06497
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 150.51359, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 0.06189, qf2_loss: 0.06214, policy_loss: -25.64374, policy_entropy: -6.11997, alpha: 0.00501, time: 33.75575
[CW] eval: return: 142.82495, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 154.67705, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 0.05699, qf2_loss: 0.05757, policy_loss: -25.66352, policy_entropy: -6.14256, alpha: 0.00505, time: 33.69312
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 137.97862, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 0.05591, qf2_loss: 0.05594, policy_loss: -25.62805, policy_entropy: -5.96750, alpha: 0.00507, time: 33.55431
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 122.36720, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 0.06848, qf2_loss: 0.06818, policy_loss: -25.64366, policy_entropy: -5.93111, alpha: 0.00505, time: 33.80268
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 134.37617, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 0.06273, qf2_loss: 0.06299, policy_loss: -25.62684, policy_entropy: -5.87016, alpha: 0.00502, time: 33.90524
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 132.07173, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 0.06169, qf2_loss: 0.06139, policy_loss: -25.62561, policy_entropy: -5.93806, alpha: 0.00500, time: 33.41872
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 157.78219, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 0.05957, qf2_loss: 0.06014, policy_loss: -25.62995, policy_entropy: -5.91225, alpha: 0.00497, time: 33.80302
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 138.90544, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 0.06376, qf2_loss: 0.06372, policy_loss: -25.63159, policy_entropy: -5.94268, alpha: 0.00493, time: 33.40289
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 134.83255, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 0.05920, qf2_loss: 0.06014, policy_loss: -25.59795, policy_entropy: -5.99700, alpha: 0.00490, time: 34.01741
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 139.15363, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 0.07739, qf2_loss: 0.07622, policy_loss: -25.63561, policy_entropy: -6.05629, alpha: 0.00494, time: 33.62982
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 105.56248, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 0.07897, qf2_loss: 0.07888, policy_loss: -25.64977, policy_entropy: -5.96974, alpha: 0.00495, time: 33.47665
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 140.36423, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 0.05793, qf2_loss: 0.05775, policy_loss: -25.59039, policy_entropy: -5.67118, alpha: 0.00488, time: 33.86095
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 136.47746, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 0.06203, qf2_loss: 0.06247, policy_loss: -25.63608, policy_entropy: -6.13519, alpha: 0.00485, time: 34.17057
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 145.69888, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 0.06171, qf2_loss: 0.06150, policy_loss: -25.61610, policy_entropy: -5.87775, alpha: 0.00485, time: 33.70920
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 125.82379, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 0.07252, qf2_loss: 0.07215, policy_loss: -25.60379, policy_entropy: -5.77191, alpha: 0.00480, time: 33.41590
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 147.42273, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 0.07666, qf2_loss: 0.07665, policy_loss: -25.59881, policy_entropy: -5.67692, alpha: 0.00470, time: 33.60546
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 148.82539, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 0.06331, qf2_loss: 0.06322, policy_loss: -25.56219, policy_entropy: -5.82093, alpha: 0.00463, time: 33.60402
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 140.57844, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 0.05883, qf2_loss: 0.05907, policy_loss: -25.58080, policy_entropy: -5.84509, alpha: 0.00458, time: 33.90866
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 135.89165, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 0.06405, qf2_loss: 0.06408, policy_loss: -25.56922, policy_entropy: -5.93953, alpha: 0.00456, time: 33.39600
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 169.02886, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 0.06208, qf2_loss: 0.06209, policy_loss: -25.61183, policy_entropy: -5.82144, alpha: 0.00451, time: 33.57154
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 135.86961, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 0.07403, qf2_loss: 0.07350, policy_loss: -25.55995, policy_entropy: -5.93731, alpha: 0.00448, time: 33.67784
[CW] eval: return: 141.55002, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 133.51271, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 0.06140, qf2_loss: 0.06113, policy_loss: -25.58636, policy_entropy: -5.95013, alpha: 0.00445, time: 33.79390
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 146.99168, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 0.06470, qf2_loss: 0.06413, policy_loss: -25.59181, policy_entropy: -5.90139, alpha: 0.00444, time: 33.61507
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 131.37822, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 0.06736, qf2_loss: 0.06790, policy_loss: -25.55042, policy_entropy: -5.78792, alpha: 0.00439, time: 34.28266
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 149.45522, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 0.05706, qf2_loss: 0.05711, policy_loss: -25.53153, policy_entropy: -5.87667, alpha: 0.00434, time: 33.89696
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 33.36000, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 0.06697, qf2_loss: 0.06662, policy_loss: -25.50836, policy_entropy: -5.97025, alpha: 0.00433, time: 33.69711
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 144.06134, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 0.06696, qf2_loss: 0.06651, policy_loss: -25.53225, policy_entropy: -6.09452, alpha: 0.00431, time: 33.84652
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 40.85632, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 0.07353, qf2_loss: 0.07312, policy_loss: -25.53315, policy_entropy: -5.93051, alpha: 0.00433, time: 33.96052
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 162.37345, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 0.05621, qf2_loss: 0.05628, policy_loss: -25.50138, policy_entropy: -5.68442, alpha: 0.00428, time: 33.90269
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 135.00125, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 0.06292, qf2_loss: 0.06258, policy_loss: -25.50113, policy_entropy: -5.83317, alpha: 0.00422, time: 33.62569
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 154.13402, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 0.06083, qf2_loss: 0.06084, policy_loss: -25.50112, policy_entropy: -5.80205, alpha: 0.00415, time: 34.00273
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 139.00205, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 0.06510, qf2_loss: 0.06437, policy_loss: -25.49570, policy_entropy: -5.97134, alpha: 0.00413, time: 33.88750
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 148.62755, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 0.06350, qf2_loss: 0.06321, policy_loss: -25.51866, policy_entropy: -5.83994, alpha: 0.00410, time: 34.11770
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 145.86313, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 0.08037, qf2_loss: 0.07953, policy_loss: -25.48907, policy_entropy: -6.00639, alpha: 0.00408, time: 33.66898
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 106.98885, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 0.05688, qf2_loss: 0.05750, policy_loss: -25.49210, policy_entropy: -6.13142, alpha: 0.00410, time: 33.64400
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 155.45194, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 0.05695, qf2_loss: 0.05676, policy_loss: -25.46763, policy_entropy: -6.15539, alpha: 0.00414, time: 33.90210
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 159.40030, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 0.06041, qf2_loss: 0.06044, policy_loss: -25.49529, policy_entropy: -6.19701, alpha: 0.00418, time: 33.95834
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 155.05204, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 0.07029, qf2_loss: 0.06990, policy_loss: -25.48677, policy_entropy: -6.04542, alpha: 0.00421, time: 34.02615
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 163.08726, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 0.05722, qf2_loss: 0.05680, policy_loss: -25.48545, policy_entropy: -6.06327, alpha: 0.00422, time: 33.99324
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 153.21692, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 0.05854, qf2_loss: 0.05832, policy_loss: -25.45592, policy_entropy: -6.08515, alpha: 0.00425, time: 33.43723
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 140.77737, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 0.06000, qf2_loss: 0.06025, policy_loss: -25.47185, policy_entropy: -6.09506, alpha: 0.00428, time: 33.63313
[CW] eval: return: 145.55943, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 157.12687, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 0.06572, qf2_loss: 0.06513, policy_loss: -25.44570, policy_entropy: -6.06922, alpha: 0.00430, time: 33.76517
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 143.28855, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 0.06497, qf2_loss: 0.06532, policy_loss: -25.44986, policy_entropy: -5.92338, alpha: 0.00429, time: 33.65857
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 130.16392, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 0.06338, qf2_loss: 0.06254, policy_loss: -25.42679, policy_entropy: -5.83712, alpha: 0.00426, time: 33.70606
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 133.06847, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 0.06528, qf2_loss: 0.06542, policy_loss: -25.49109, policy_entropy: -6.10177, alpha: 0.00425, time: 33.87121
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 165.14879, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 0.07668, qf2_loss: 0.07604, policy_loss: -25.46126, policy_entropy: -6.24391, alpha: 0.00429, time: 33.64794
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 169.16899, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 0.05880, qf2_loss: 0.05890, policy_loss: -25.43579, policy_entropy: -6.01432, alpha: 0.00435, time: 33.58420
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 145.26050, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 0.06654, qf2_loss: 0.06590, policy_loss: -25.50787, policy_entropy: -6.18215, alpha: 0.00436, time: 33.95399
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 152.68779, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 0.06809, qf2_loss: 0.06778, policy_loss: -25.44320, policy_entropy: -6.15579, alpha: 0.00442, time: 34.02689
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 144.49018, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 0.07462, qf2_loss: 0.07477, policy_loss: -25.45647, policy_entropy: -5.97420, alpha: 0.00444, time: 33.73183
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 145.58198, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 0.06413, qf2_loss: 0.06350, policy_loss: -25.45695, policy_entropy: -6.08424, alpha: 0.00445, time: 33.89293
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 151.75333, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 0.06406, qf2_loss: 0.06361, policy_loss: -25.47922, policy_entropy: -6.30867, alpha: 0.00449, time: 33.93706
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 142.73044, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 0.06423, qf2_loss: 0.06453, policy_loss: -25.51529, policy_entropy: -6.07984, alpha: 0.00457, time: 33.80377
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 148.47936, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 0.06477, qf2_loss: 0.06503, policy_loss: -25.44288, policy_entropy: -5.97954, alpha: 0.00459, time: 33.85620
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 78.51784, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 0.06840, qf2_loss: 0.06782, policy_loss: -25.46338, policy_entropy: -5.82583, alpha: 0.00456, time: 33.61291
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 146.64875, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 0.06774, qf2_loss: 0.06753, policy_loss: -25.42991, policy_entropy: -5.81889, alpha: 0.00449, time: 33.97939
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 132.56454, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 0.06319, qf2_loss: 0.06257, policy_loss: -25.40963, policy_entropy: -5.72160, alpha: 0.00442, time: 33.88711
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 148.25584, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 0.07665, qf2_loss: 0.07687, policy_loss: -25.47233, policy_entropy: -6.08773, alpha: 0.00438, time: 33.80284
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 148.02210, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 0.07167, qf2_loss: 0.07124, policy_loss: -25.42199, policy_entropy: -6.07881, alpha: 0.00442, time: 33.52431
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 127.54468, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 0.07326, qf2_loss: 0.07295, policy_loss: -25.44520, policy_entropy: -6.03436, alpha: 0.00443, time: 33.76808
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 164.07227, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 0.06366, qf2_loss: 0.06377, policy_loss: -25.44615, policy_entropy: -5.97563, alpha: 0.00443, time: 33.64195
[CW] eval: return: 149.10839, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 136.66617, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 0.08007, qf2_loss: 0.07839, policy_loss: -25.44516, policy_entropy: -6.07669, alpha: 0.00443, time: 34.06568
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 144.27305, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 0.06822, qf2_loss: 0.06820, policy_loss: -25.46041, policy_entropy: -6.27501, alpha: 0.00451, time: 33.83832
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 152.56572, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 0.06846, qf2_loss: 0.06857, policy_loss: -25.54349, policy_entropy: -6.10021, alpha: 0.00456, time: 33.69481
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 146.36395, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 0.06929, qf2_loss: 0.06944, policy_loss: -25.46060, policy_entropy: -6.06441, alpha: 0.00460, time: 33.66965
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 106.01943, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 0.06923, qf2_loss: 0.06881, policy_loss: -25.44910, policy_entropy: -5.97147, alpha: 0.00459, time: 33.69755
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 158.56657, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 0.06732, qf2_loss: 0.06677, policy_loss: -25.45505, policy_entropy: -6.02422, alpha: 0.00457, time: 33.90945
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 158.71007, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 0.07195, qf2_loss: 0.07254, policy_loss: -25.39143, policy_entropy: -5.96090, alpha: 0.00459, time: 33.69935
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 132.99899, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 0.07002, qf2_loss: 0.06987, policy_loss: -25.45204, policy_entropy: -6.03214, alpha: 0.00458, time: 34.17636
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 161.12787, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 0.07441, qf2_loss: 0.07371, policy_loss: -25.38860, policy_entropy: -6.05670, alpha: 0.00461, time: 33.75012
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 155.94153, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 0.07181, qf2_loss: 0.07248, policy_loss: -25.44013, policy_entropy: -5.97637, alpha: 0.00461, time: 34.38086
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 172.74739, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 0.06665, qf2_loss: 0.06710, policy_loss: -25.42512, policy_entropy: -6.14371, alpha: 0.00462, time: 33.60829
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 151.02943, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 0.06678, qf2_loss: 0.06686, policy_loss: -25.43594, policy_entropy: -6.05072, alpha: 0.00465, time: 33.61077
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 162.43449, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 0.06726, qf2_loss: 0.06652, policy_loss: -25.36392, policy_entropy: -5.90955, alpha: 0.00467, time: 33.70083
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 154.08079, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 0.07078, qf2_loss: 0.07058, policy_loss: -25.42752, policy_entropy: -6.15486, alpha: 0.00464, time: 34.10463
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 159.63304, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 0.07252, qf2_loss: 0.07265, policy_loss: -25.37686, policy_entropy: -6.04140, alpha: 0.00469, time: 33.96125
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 157.83143, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 0.07428, qf2_loss: 0.07388, policy_loss: -25.47079, policy_entropy: -5.96073, alpha: 0.00469, time: 33.89166
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 104.32007, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 0.07869, qf2_loss: 0.07858, policy_loss: -25.41938, policy_entropy: -6.06843, alpha: 0.00470, time: 34.16799
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 154.93600, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 0.08009, qf2_loss: 0.08008, policy_loss: -25.42390, policy_entropy: -5.93675, alpha: 0.00472, time: 34.09107
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 164.23587, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 0.07612, qf2_loss: 0.07512, policy_loss: -25.43018, policy_entropy: -6.27212, alpha: 0.00474, time: 33.90093
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 168.92017, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 0.06863, qf2_loss: 0.06819, policy_loss: -25.47483, policy_entropy: -6.03416, alpha: 0.00481, time: 33.96963
[CW] eval: return: 158.17600, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 165.31080, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 0.08085, qf2_loss: 0.08086, policy_loss: -25.46708, policy_entropy: -5.89562, alpha: 0.00480, time: 33.81068
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 165.24634, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 0.08080, qf2_loss: 0.08223, policy_loss: -25.51259, policy_entropy: -6.03579, alpha: 0.00477, time: 33.14202
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 147.73595, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 0.08214, qf2_loss: 0.08100, policy_loss: -25.53099, policy_entropy: -5.96584, alpha: 0.00476, time: 33.41299
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 161.35962, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 0.07563, qf2_loss: 0.07519, policy_loss: -25.50023, policy_entropy: -6.29536, alpha: 0.00481, time: 33.88352
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 169.14299, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 0.08584, qf2_loss: 0.08617, policy_loss: -25.51550, policy_entropy: -6.05552, alpha: 0.00488, time: 33.88959
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 163.42437, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 0.07326, qf2_loss: 0.07334, policy_loss: -25.49413, policy_entropy: -6.24747, alpha: 0.00493, time: 34.15777
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 171.02539, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 0.07832, qf2_loss: 0.07849, policy_loss: -25.50307, policy_entropy: -6.03152, alpha: 0.00499, time: 33.57533
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 155.88881, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 0.08764, qf2_loss: 0.08637, policy_loss: -25.53878, policy_entropy: -5.91927, alpha: 0.00495, time: 35.69681
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 131.31111, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 0.07402, qf2_loss: 0.07474, policy_loss: -25.50115, policy_entropy: -5.97836, alpha: 0.00498, time: 33.72912
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 160.73804, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 0.10192, qf2_loss: 0.10029, policy_loss: -25.49079, policy_entropy: -5.91750, alpha: 0.00494, time: 33.92002
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 155.44266, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 0.08539, qf2_loss: 0.08556, policy_loss: -25.57044, policy_entropy: -5.91437, alpha: 0.00491, time: 34.00403
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 172.06759, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 0.07914, qf2_loss: 0.07921, policy_loss: -25.56789, policy_entropy: -5.97405, alpha: 0.00488, time: 33.95640
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 160.89355, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 0.08096, qf2_loss: 0.08148, policy_loss: -25.47378, policy_entropy: -6.01815, alpha: 0.00486, time: 33.89769
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 167.22923, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 0.07939, qf2_loss: 0.07947, policy_loss: -25.53192, policy_entropy: -6.11757, alpha: 0.00492, time: 34.01316
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 166.31254, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 0.07670, qf2_loss: 0.07627, policy_loss: -25.56296, policy_entropy: -6.08918, alpha: 0.00495, time: 33.72210
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 166.54857, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 0.08024, qf2_loss: 0.08042, policy_loss: -25.53063, policy_entropy: -6.11451, alpha: 0.00497, time: 33.76016
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 172.72642, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 0.07846, qf2_loss: 0.07790, policy_loss: -25.50175, policy_entropy: -5.96180, alpha: 0.00500, time: 34.26316
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 152.69358, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 0.07856, qf2_loss: 0.07859, policy_loss: -25.53910, policy_entropy: -6.09846, alpha: 0.00500, time: 34.15875
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 166.23641, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 0.08869, qf2_loss: 0.08878, policy_loss: -25.62633, policy_entropy: -6.07353, alpha: 0.00504, time: 34.13967
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 166.43572, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 0.09700, qf2_loss: 0.09645, policy_loss: -25.60007, policy_entropy: -6.21291, alpha: 0.00509, time: 33.80459
[CW] eval: return: 167.68579, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 174.32571, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 0.07825, qf2_loss: 0.07855, policy_loss: -25.59695, policy_entropy: -5.95929, alpha: 0.00514, time: 33.78018
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 170.11212, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 0.08152, qf2_loss: 0.08165, policy_loss: -25.63448, policy_entropy: -5.99051, alpha: 0.00514, time: 33.87784
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 178.20918, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 0.08091, qf2_loss: 0.08070, policy_loss: -25.69403, policy_entropy: -6.02430, alpha: 0.00512, time: 34.05712
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 169.42389, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 0.09861, qf2_loss: 0.09926, policy_loss: -25.65528, policy_entropy: -6.14934, alpha: 0.00516, time: 33.39847
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 166.16171, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 0.08443, qf2_loss: 0.08453, policy_loss: -25.66650, policy_entropy: -6.00928, alpha: 0.00520, time: 33.86808
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 168.86192, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 0.08512, qf2_loss: 0.08585, policy_loss: -25.63455, policy_entropy: -6.04797, alpha: 0.00521, time: 33.81816
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 168.89091, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 0.09121, qf2_loss: 0.08964, policy_loss: -25.66519, policy_entropy: -5.83689, alpha: 0.00520, time: 33.56383
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 165.18165, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 0.08866, qf2_loss: 0.08822, policy_loss: -25.61174, policy_entropy: -5.76401, alpha: 0.00510, time: 33.99502
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 173.34726, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 0.09258, qf2_loss: 0.09229, policy_loss: -25.68837, policy_entropy: -6.01811, alpha: 0.00506, time: 34.08237
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 176.68113, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 0.08756, qf2_loss: 0.08782, policy_loss: -25.66752, policy_entropy: -5.86969, alpha: 0.00505, time: 33.98760
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 166.06469, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 0.09142, qf2_loss: 0.09133, policy_loss: -25.73287, policy_entropy: -6.00817, alpha: 0.00500, time: 33.83372
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 160.32311, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 0.08471, qf2_loss: 0.08473, policy_loss: -25.68414, policy_entropy: -6.11466, alpha: 0.00504, time: 33.73691
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 176.86910, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 0.08593, qf2_loss: 0.08549, policy_loss: -25.69755, policy_entropy: -6.02927, alpha: 0.00507, time: 33.79182
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 157.36516, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 0.08861, qf2_loss: 0.08875, policy_loss: -25.78478, policy_entropy: -6.08885, alpha: 0.00509, time: 33.83570
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 175.08152, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 0.08658, qf2_loss: 0.08685, policy_loss: -25.74244, policy_entropy: -5.93536, alpha: 0.00511, time: 34.10909
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 52.36729, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 0.08625, qf2_loss: 0.08631, policy_loss: -25.73845, policy_entropy: -5.97008, alpha: 0.00507, time: 33.71603
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 164.54010, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 0.09074, qf2_loss: 0.09051, policy_loss: -25.78663, policy_entropy: -6.16350, alpha: 0.00508, time: 35.20919
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 177.74277, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 0.08412, qf2_loss: 0.08430, policy_loss: -25.75287, policy_entropy: -5.98005, alpha: 0.00512, time: 33.66783
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 169.33108, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 0.08599, qf2_loss: 0.08656, policy_loss: -25.78019, policy_entropy: -6.05723, alpha: 0.00514, time: 33.93334
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 134.43312, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 0.10163, qf2_loss: 0.10066, policy_loss: -25.85804, policy_entropy: -6.12924, alpha: 0.00516, time: 33.72251
[CW] eval: return: 174.05017, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 172.21428, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 0.08860, qf2_loss: 0.08936, policy_loss: -25.81875, policy_entropy: -5.92233, alpha: 0.00518, time: 33.73984
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 174.99498, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 0.09292, qf2_loss: 0.09328, policy_loss: -25.85381, policy_entropy: -5.89940, alpha: 0.00513, time: 33.71425
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 169.20697, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 0.09192, qf2_loss: 0.09186, policy_loss: -25.88560, policy_entropy: -6.02437, alpha: 0.00513, time: 33.82766
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 161.29822, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 0.09922, qf2_loss: 0.09955, policy_loss: -25.86263, policy_entropy: -5.90021, alpha: 0.00512, time: 33.77820
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 148.63483, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 0.08523, qf2_loss: 0.08538, policy_loss: -25.81488, policy_entropy: -6.06348, alpha: 0.00512, time: 33.46414
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 164.67910, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 0.10207, qf2_loss: 0.10153, policy_loss: -25.79667, policy_entropy: -6.13592, alpha: 0.00514, time: 33.87629
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 179.26785, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 0.08576, qf2_loss: 0.08590, policy_loss: -25.88634, policy_entropy: -6.17474, alpha: 0.00520, time: 34.29004
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 162.48320, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 0.08384, qf2_loss: 0.08326, policy_loss: -25.86771, policy_entropy: -6.20486, alpha: 0.00527, time: 34.08882
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 184.41450, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 0.08675, qf2_loss: 0.08655, policy_loss: -25.90506, policy_entropy: -6.14924, alpha: 0.00536, time: 34.22184
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 191.71361, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 0.09224, qf2_loss: 0.09281, policy_loss: -25.96653, policy_entropy: -6.15240, alpha: 0.00539, time: 34.11454
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 191.20765, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 0.08229, qf2_loss: 0.08222, policy_loss: -25.89073, policy_entropy: -6.11170, alpha: 0.00547, time: 33.94926
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 205.04061, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 0.08641, qf2_loss: 0.08658, policy_loss: -25.96507, policy_entropy: -6.08482, alpha: 0.00551, time: 33.53594
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 176.25009, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 0.08762, qf2_loss: 0.08733, policy_loss: -25.89178, policy_entropy: -6.02506, alpha: 0.00552, time: 33.56006
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 170.44252, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 0.10003, qf2_loss: 0.09879, policy_loss: -25.90678, policy_entropy: -5.96378, alpha: 0.00553, time: 33.86546
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 162.29128, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 0.10985, qf2_loss: 0.11154, policy_loss: -26.03173, policy_entropy: -6.08169, alpha: 0.00553, time: 33.96731
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 178.46651, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 0.08953, qf2_loss: 0.08976, policy_loss: -26.06255, policy_entropy: -6.29509, alpha: 0.00560, time: 33.84786
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 169.62668, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 0.09037, qf2_loss: 0.09073, policy_loss: -26.03902, policy_entropy: -6.09665, alpha: 0.00570, time: 33.64677
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 161.18432, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 0.09447, qf2_loss: 0.09435, policy_loss: -26.04035, policy_entropy: -6.02153, alpha: 0.00571, time: 33.73581
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 150.48165, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 0.10654, qf2_loss: 0.10629, policy_loss: -26.08939, policy_entropy: -5.97866, alpha: 0.00572, time: 33.98679
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 169.90206, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 0.09488, qf2_loss: 0.09373, policy_loss: -26.05824, policy_entropy: -5.92033, alpha: 0.00570, time: 33.77020
[CW] eval: return: 169.23949, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 167.64219, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 0.09140, qf2_loss: 0.09117, policy_loss: -26.06323, policy_entropy: -5.97683, alpha: 0.00567, time: 33.98043
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 170.80803, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 0.09721, qf2_loss: 0.09773, policy_loss: -26.03874, policy_entropy: -6.07909, alpha: 0.00570, time: 33.65797
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 165.45703, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 0.10259, qf2_loss: 0.10310, policy_loss: -26.10341, policy_entropy: -5.98545, alpha: 0.00572, time: 33.53445
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 162.21127, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 0.09494, qf2_loss: 0.09388, policy_loss: -26.13653, policy_entropy: -6.05041, alpha: 0.00570, time: 33.69668
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 139.84281, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 0.09218, qf2_loss: 0.09285, policy_loss: -26.17258, policy_entropy: -5.99119, alpha: 0.00573, time: 33.84303
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 163.62433, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 0.09938, qf2_loss: 0.09968, policy_loss: -26.18821, policy_entropy: -5.95147, alpha: 0.00571, time: 33.99353
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 154.85327, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 0.10621, qf2_loss: 0.10536, policy_loss: -26.21678, policy_entropy: -5.98820, alpha: 0.00570, time: 33.57176
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 165.09583, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 0.10904, qf2_loss: 0.10884, policy_loss: -26.21724, policy_entropy: -5.97153, alpha: 0.00568, time: 33.71398
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 186.07011, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 0.09504, qf2_loss: 0.09542, policy_loss: -26.19586, policy_entropy: -5.93451, alpha: 0.00569, time: 33.71424
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 151.24832, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 0.10725, qf2_loss: 0.10716, policy_loss: -26.24110, policy_entropy: -5.96411, alpha: 0.00565, time: 33.80749
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 162.47283, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 0.09632, qf2_loss: 0.09595, policy_loss: -26.29097, policy_entropy: -5.74849, alpha: 0.00559, time: 34.04943
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 172.89419, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 0.09460, qf2_loss: 0.09431, policy_loss: -26.25603, policy_entropy: -5.85471, alpha: 0.00551, time: 34.00456
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 182.81417, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 0.09851, qf2_loss: 0.09863, policy_loss: -26.18762, policy_entropy: -5.84498, alpha: 0.00544, time: 33.57004
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 175.80035, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 0.09136, qf2_loss: 0.09198, policy_loss: -26.27012, policy_entropy: -6.06553, alpha: 0.00545, time: 34.01441
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 168.58065, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 0.09748, qf2_loss: 0.09731, policy_loss: -26.30795, policy_entropy: -5.92483, alpha: 0.00543, time: 33.87839
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 176.61784, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 0.09758, qf2_loss: 0.09750, policy_loss: -26.42581, policy_entropy: -5.98860, alpha: 0.00542, time: 33.82931
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 169.30682, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 0.09157, qf2_loss: 0.09113, policy_loss: -26.38742, policy_entropy: -6.16990, alpha: 0.00544, time: 34.00862
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 139.04129, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 0.09955, qf2_loss: 0.09903, policy_loss: -26.42879, policy_entropy: -5.89210, alpha: 0.00545, time: 33.87890
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 186.52406, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 0.09240, qf2_loss: 0.09239, policy_loss: -26.33634, policy_entropy: -5.90187, alpha: 0.00541, time: 33.89055
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 170.69763, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 0.11041, qf2_loss: 0.10985, policy_loss: -26.45638, policy_entropy: -6.09211, alpha: 0.00542, time: 33.80172
[CW] eval: return: 176.26918, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 180.35284, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 0.09311, qf2_loss: 0.09213, policy_loss: -26.45802, policy_entropy: -6.13523, alpha: 0.00544, time: 33.69204
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 169.44647, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 0.09060, qf2_loss: 0.09090, policy_loss: -26.53149, policy_entropy: -6.01303, alpha: 0.00550, time: 33.71245
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 183.87948, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 0.09107, qf2_loss: 0.09103, policy_loss: -26.29743, policy_entropy: -5.85486, alpha: 0.00546, time: 33.60579
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 158.30072, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 0.10817, qf2_loss: 0.10917, policy_loss: -26.48398, policy_entropy: -6.10109, alpha: 0.00546, time: 33.98965
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 162.76244, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 0.10357, qf2_loss: 0.10314, policy_loss: -26.53664, policy_entropy: -5.99025, alpha: 0.00547, time: 33.79719
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 167.61910, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 0.09576, qf2_loss: 0.09610, policy_loss: -26.55375, policy_entropy: -6.23991, alpha: 0.00550, time: 33.89595
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 162.32753, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 0.09033, qf2_loss: 0.09017, policy_loss: -26.54947, policy_entropy: -6.14862, alpha: 0.00558, time: 34.38466
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 193.15850, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 0.09163, qf2_loss: 0.09179, policy_loss: -26.57318, policy_entropy: -5.91244, alpha: 0.00561, time: 33.69062
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 164.69362, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 0.10521, qf2_loss: 0.10586, policy_loss: -26.57210, policy_entropy: -5.97561, alpha: 0.00557, time: 33.95400
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 190.35839, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 0.09250, qf2_loss: 0.09217, policy_loss: -26.54407, policy_entropy: -5.89491, alpha: 0.00555, time: 33.68331
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 175.69425, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 0.10255, qf2_loss: 0.10334, policy_loss: -26.51947, policy_entropy: -5.87162, alpha: 0.00551, time: 33.89709
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 176.33051, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 0.09978, qf2_loss: 0.09902, policy_loss: -26.59642, policy_entropy: -5.95506, alpha: 0.00549, time: 35.45959
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 170.17929, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 0.10082, qf2_loss: 0.10084, policy_loss: -26.62218, policy_entropy: -6.04449, alpha: 0.00547, time: 33.97973
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 169.59029, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 0.09484, qf2_loss: 0.09463, policy_loss: -26.62605, policy_entropy: -5.98443, alpha: 0.00548, time: 33.79392
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 165.84772, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 0.09112, qf2_loss: 0.09134, policy_loss: -26.63878, policy_entropy: -5.84718, alpha: 0.00545, time: 34.10160
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 172.01913, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 0.09236, qf2_loss: 0.09239, policy_loss: -26.66990, policy_entropy: -6.17062, alpha: 0.00543, time: 34.14752
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 180.32478, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 0.10353, qf2_loss: 0.10404, policy_loss: -26.70100, policy_entropy: -6.16914, alpha: 0.00550, time: 34.35520
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 160.57079, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 0.09862, qf2_loss: 0.09890, policy_loss: -26.81171, policy_entropy: -6.22370, alpha: 0.00558, time: 34.08518
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 164.45649, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 0.11276, qf2_loss: 0.11290, policy_loss: -26.65000, policy_entropy: -6.17612, alpha: 0.00567, time: 34.13945
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 174.75475, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 0.09436, qf2_loss: 0.09385, policy_loss: -26.80690, policy_entropy: -5.96058, alpha: 0.00570, time: 34.14570
[CW] eval: return: 171.92055, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 165.82023, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 0.09489, qf2_loss: 0.09527, policy_loss: -26.75258, policy_entropy: -6.00162, alpha: 0.00570, time: 33.83675
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 160.40549, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 0.09649, qf2_loss: 0.09758, policy_loss: -26.82135, policy_entropy: -6.11333, alpha: 0.00573, time: 33.97766
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 161.94249, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 0.11175, qf2_loss: 0.11226, policy_loss: -26.70671, policy_entropy: -5.72012, alpha: 0.00571, time: 34.03242
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 179.40474, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 0.09605, qf2_loss: 0.09641, policy_loss: -26.75643, policy_entropy: -5.87212, alpha: 0.00561, time: 33.76222
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 156.05656, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 0.09344, qf2_loss: 0.09353, policy_loss: -26.87511, policy_entropy: -6.26145, alpha: 0.00563, time: 34.04440
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 172.54430, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 0.09896, qf2_loss: 0.09788, policy_loss: -26.85652, policy_entropy: -6.15553, alpha: 0.00570, time: 33.95064
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 166.68338, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 0.11791, qf2_loss: 0.11939, policy_loss: -26.89698, policy_entropy: -6.10187, alpha: 0.00575, time: 33.89911
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 184.52316, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 0.10448, qf2_loss: 0.10400, policy_loss: -26.81004, policy_entropy: -6.07007, alpha: 0.00579, time: 33.89545
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 160.41126, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 0.10392, qf2_loss: 0.10243, policy_loss: -26.79218, policy_entropy: -6.08411, alpha: 0.00582, time: 33.98496
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 162.15220, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 0.10469, qf2_loss: 0.10445, policy_loss: -26.86712, policy_entropy: -6.14670, alpha: 0.00587, time: 34.20120
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 172.14706, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 0.09300, qf2_loss: 0.09370, policy_loss: -26.86981, policy_entropy: -6.07619, alpha: 0.00592, time: 33.63388
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 162.69739, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 0.09662, qf2_loss: 0.09615, policy_loss: -26.94883, policy_entropy: -6.01049, alpha: 0.00594, time: 33.62934
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 175.08535, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 0.09847, qf2_loss: 0.09874, policy_loss: -26.94173, policy_entropy: -6.05703, alpha: 0.00595, time: 33.77643
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 175.45210, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 0.10434, qf2_loss: 0.10574, policy_loss: -26.95067, policy_entropy: -6.31400, alpha: 0.00603, time: 33.79587
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 177.01069, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 0.11349, qf2_loss: 0.11316, policy_loss: -26.87048, policy_entropy: -6.02224, alpha: 0.00611, time: 33.47872
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 177.55844, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 0.11358, qf2_loss: 0.11364, policy_loss: -26.95391, policy_entropy: -6.18857, alpha: 0.00614, time: 33.94546
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 175.35620, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 0.10136, qf2_loss: 0.10117, policy_loss: -26.92536, policy_entropy: -6.14036, alpha: 0.00623, time: 34.16693
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 160.95968, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 0.11242, qf2_loss: 0.11256, policy_loss: -27.05985, policy_entropy: -5.99946, alpha: 0.00628, time: 34.04825
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 176.21313, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 0.10093, qf2_loss: 0.10066, policy_loss: -27.07879, policy_entropy: -5.94538, alpha: 0.00625, time: 34.12922
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 174.38479, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 0.10338, qf2_loss: 0.10345, policy_loss: -27.05142, policy_entropy: -6.07567, alpha: 0.00626, time: 33.86676
[CW] eval: return: 172.88420, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 178.77167, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 0.10215, qf2_loss: 0.10106, policy_loss: -27.11636, policy_entropy: -6.03224, alpha: 0.00629, time: 34.13993
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 199.03954, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 0.10046, qf2_loss: 0.10076, policy_loss: -27.02163, policy_entropy: -6.04322, alpha: 0.00631, time: 33.89375
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 163.85202, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 0.10435, qf2_loss: 0.10418, policy_loss: -27.10402, policy_entropy: -5.95530, alpha: 0.00630, time: 33.82309
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 174.30045, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 0.10526, qf2_loss: 0.10498, policy_loss: -27.16474, policy_entropy: -5.93226, alpha: 0.00629, time: 33.59086
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 193.65410, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 0.10777, qf2_loss: 0.10846, policy_loss: -27.25972, policy_entropy: -6.05820, alpha: 0.00625, time: 34.00892
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 177.38988, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 0.10412, qf2_loss: 0.10458, policy_loss: -27.14414, policy_entropy: -5.94613, alpha: 0.00628, time: 34.08667
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 168.85371, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 0.10706, qf2_loss: 0.10678, policy_loss: -27.26908, policy_entropy: -5.92058, alpha: 0.00624, time: 34.21585
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 180.86043, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 0.11166, qf2_loss: 0.11051, policy_loss: -27.22122, policy_entropy: -5.98472, alpha: 0.00622, time: 33.53462
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 166.68786, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 0.10029, qf2_loss: 0.09825, policy_loss: -27.21282, policy_entropy: -6.10589, alpha: 0.00625, time: 33.77620
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 166.37575, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 0.10763, qf2_loss: 0.10968, policy_loss: -27.22512, policy_entropy: -6.01505, alpha: 0.00628, time: 34.21974
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 61.01981, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 0.11573, qf2_loss: 0.11566, policy_loss: -27.18964, policy_entropy: -5.91845, alpha: 0.00626, time: 33.82965
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 169.26365, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 0.10568, qf2_loss: 0.10616, policy_loss: -27.31129, policy_entropy: -6.06277, alpha: 0.00624, time: 33.97566
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 170.22700, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 0.09596, qf2_loss: 0.09611, policy_loss: -27.23794, policy_entropy: -5.91656, alpha: 0.00626, time: 33.40409
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 189.38354, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 0.10889, qf2_loss: 0.10835, policy_loss: -27.31466, policy_entropy: -5.87618, alpha: 0.00620, time: 33.74618
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 167.14212, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 0.09667, qf2_loss: 0.09587, policy_loss: -27.40442, policy_entropy: -5.92758, alpha: 0.00615, time: 33.62740
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 175.96128, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 0.09568, qf2_loss: 0.09549, policy_loss: -27.29397, policy_entropy: -5.86833, alpha: 0.00610, time: 33.70977
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 166.81017, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 0.10033, qf2_loss: 0.10098, policy_loss: -27.38379, policy_entropy: -6.00750, alpha: 0.00606, time: 33.92980
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 173.48871, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 0.09933, qf2_loss: 0.09966, policy_loss: -27.41200, policy_entropy: -5.80444, alpha: 0.00602, time: 34.15915
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 162.63001, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 0.10554, qf2_loss: 0.10608, policy_loss: -27.38375, policy_entropy: -6.16265, alpha: 0.00600, time: 34.08284
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 179.18751, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 0.11462, qf2_loss: 0.11311, policy_loss: -27.34309, policy_entropy: -6.00001, alpha: 0.00604, time: 34.10009
[CW] eval: return: 176.13403, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 165.19432, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 0.09932, qf2_loss: 0.09993, policy_loss: -27.45609, policy_entropy: -5.92982, alpha: 0.00605, time: 33.84516
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 176.03062, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 0.09863, qf2_loss: 0.09829, policy_loss: -27.43441, policy_entropy: -5.72178, alpha: 0.00596, time: 33.97625
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 184.51925, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 0.10217, qf2_loss: 0.10255, policy_loss: -27.50628, policy_entropy: -6.07799, alpha: 0.00590, time: 33.41114
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 201.36153, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 0.10572, qf2_loss: 0.10583, policy_loss: -27.48944, policy_entropy: -5.88870, alpha: 0.00590, time: 33.78218
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 199.88920, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 0.11047, qf2_loss: 0.11028, policy_loss: -27.49590, policy_entropy: -6.09120, alpha: 0.00590, time: 33.97728
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 167.97178, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 0.11374, qf2_loss: 0.11319, policy_loss: -27.46098, policy_entropy: -5.94617, alpha: 0.00592, time: 33.93544
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 186.09016, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 0.09874, qf2_loss: 0.09930, policy_loss: -27.52696, policy_entropy: -6.00680, alpha: 0.00589, time: 34.16965
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 206.22560, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 0.09709, qf2_loss: 0.09684, policy_loss: -27.51455, policy_entropy: -5.89087, alpha: 0.00589, time: 34.13973
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 167.05535, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 0.09281, qf2_loss: 0.09318, policy_loss: -27.52566, policy_entropy: -5.89143, alpha: 0.00584, time: 33.69522
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 196.17031, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 0.09994, qf2_loss: 0.10036, policy_loss: -27.55562, policy_entropy: -6.02312, alpha: 0.00582, time: 33.79365
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 189.98732, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 0.09665, qf2_loss: 0.09589, policy_loss: -27.56720, policy_entropy: -5.85067, alpha: 0.00581, time: 34.20133
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 198.66435, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 0.10815, qf2_loss: 0.10739, policy_loss: -27.58888, policy_entropy: -6.13356, alpha: 0.00580, time: 34.36577
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 173.24894, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 0.10278, qf2_loss: 0.10435, policy_loss: -27.62614, policy_entropy: -6.05501, alpha: 0.00582, time: 33.77787
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 194.61550, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 0.10798, qf2_loss: 0.10676, policy_loss: -27.67271, policy_entropy: -5.98617, alpha: 0.00584, time: 33.82334
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 166.95769, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 0.09287, qf2_loss: 0.09223, policy_loss: -27.55288, policy_entropy: -5.99958, alpha: 0.00584, time: 34.33787
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 182.39218, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 0.09298, qf2_loss: 0.09329, policy_loss: -27.61098, policy_entropy: -6.01256, alpha: 0.00583, time: 34.33575
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 163.20122, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 0.10094, qf2_loss: 0.09988, policy_loss: -27.66368, policy_entropy: -6.12282, alpha: 0.00585, time: 34.17734
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 190.47382, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 0.09933, qf2_loss: 0.09946, policy_loss: -27.58440, policy_entropy: -5.74595, alpha: 0.00584, time: 33.88570
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 196.51005, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 0.10592, qf2_loss: 0.10537, policy_loss: -27.75034, policy_entropy: -6.04757, alpha: 0.00578, time: 33.96479
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 194.90628, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 0.09531, qf2_loss: 0.09624, policy_loss: -27.72450, policy_entropy: -6.13028, alpha: 0.00582, time: 34.09307
[CW] eval: return: 184.52121, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 190.79834, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 0.10534, qf2_loss: 0.10472, policy_loss: -27.74931, policy_entropy: -6.23642, alpha: 0.00589, time: 34.00896
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 177.69143, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 0.11069, qf2_loss: 0.11067, policy_loss: -27.73927, policy_entropy: -6.15359, alpha: 0.00599, time: 33.98227
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 196.15755, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 0.10136, qf2_loss: 0.10169, policy_loss: -27.81704, policy_entropy: -6.06843, alpha: 0.00604, time: 33.93716
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 197.12990, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 0.10204, qf2_loss: 0.10306, policy_loss: -27.77459, policy_entropy: -5.99770, alpha: 0.00606, time: 34.16081
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 186.77025, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 0.10019, qf2_loss: 0.09891, policy_loss: -27.76867, policy_entropy: -6.04329, alpha: 0.00606, time: 35.41562
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 201.08049, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 0.10562, qf2_loss: 0.10509, policy_loss: -27.82415, policy_entropy: -5.94373, alpha: 0.00607, time: 33.89846
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 186.05939, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 0.09876, qf2_loss: 0.09868, policy_loss: -27.78552, policy_entropy: -5.90411, alpha: 0.00604, time: 34.30233
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 181.26037, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 0.09885, qf2_loss: 0.10045, policy_loss: -27.80047, policy_entropy: -5.73780, alpha: 0.00595, time: 34.17525
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 189.68966, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 0.09224, qf2_loss: 0.09056, policy_loss: -27.77019, policy_entropy: -5.92639, alpha: 0.00588, time: 34.36288
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 185.39326, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 0.09800, qf2_loss: 0.09791, policy_loss: -27.84332, policy_entropy: -5.97721, alpha: 0.00585, time: 33.99759
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 176.31961, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 0.11462, qf2_loss: 0.11407, policy_loss: -27.85922, policy_entropy: -5.88983, alpha: 0.00582, time: 33.97721
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 192.62952, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 0.10491, qf2_loss: 0.10420, policy_loss: -27.91220, policy_entropy: -6.03365, alpha: 0.00580, time: 34.06795
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 194.69919, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 0.09808, qf2_loss: 0.09845, policy_loss: -27.88328, policy_entropy: -5.83793, alpha: 0.00578, time: 33.91871
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 172.93637, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 0.10501, qf2_loss: 0.10592, policy_loss: -27.92978, policy_entropy: -6.07819, alpha: 0.00577, time: 33.86021
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 189.38262, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 0.10660, qf2_loss: 0.10615, policy_loss: -27.92849, policy_entropy: -5.79934, alpha: 0.00574, time: 33.89150
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 204.55269, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 0.09300, qf2_loss: 0.09291, policy_loss: -27.86141, policy_entropy: -5.94074, alpha: 0.00567, time: 34.16761
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 170.40114, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 0.09563, qf2_loss: 0.09685, policy_loss: -27.97054, policy_entropy: -6.06801, alpha: 0.00569, time: 34.45828
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 179.29690, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 0.10435, qf2_loss: 0.10328, policy_loss: -28.05227, policy_entropy: -6.15623, alpha: 0.00573, time: 35.10668
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 195.12549, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 0.12786, qf2_loss: 0.12624, policy_loss: -28.07172, policy_entropy: -5.98801, alpha: 0.00578, time: 33.70491
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 180.79068, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 0.11811, qf2_loss: 0.11922, policy_loss: -28.04557, policy_entropy: -5.75693, alpha: 0.00571, time: 33.83346
[CW] eval: return: 186.90450, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 188.78333, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 0.09860, qf2_loss: 0.09917, policy_loss: -28.04619, policy_entropy: -5.97637, alpha: 0.00565, time: 35.51194
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 191.79043, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 0.10275, qf2_loss: 0.10331, policy_loss: -28.01126, policy_entropy: -5.95150, alpha: 0.00563, time: 34.23310
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 172.45546, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 0.09340, qf2_loss: 0.09359, policy_loss: -28.09789, policy_entropy: -6.18933, alpha: 0.00566, time: 34.20449
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 205.49601, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 0.09629, qf2_loss: 0.09635, policy_loss: -28.10572, policy_entropy: -6.13698, alpha: 0.00572, time: 34.02049
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 189.54500, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 0.10650, qf2_loss: 0.10642, policy_loss: -28.11830, policy_entropy: -6.06690, alpha: 0.00576, time: 34.20447
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 200.63399, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 0.09544, qf2_loss: 0.09553, policy_loss: -28.13464, policy_entropy: -6.22600, alpha: 0.00583, time: 34.04468
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 189.31485, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 0.09793, qf2_loss: 0.09643, policy_loss: -28.17069, policy_entropy: -6.19431, alpha: 0.00592, time: 34.05386
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 166.23560, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 0.09867, qf2_loss: 0.09982, policy_loss: -28.22536, policy_entropy: -6.02956, alpha: 0.00599, time: 33.91660
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 170.09140, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 0.09604, qf2_loss: 0.09686, policy_loss: -28.11673, policy_entropy: -6.06020, alpha: 0.00601, time: 34.05542
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 200.01915, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 0.09829, qf2_loss: 0.09801, policy_loss: -28.22914, policy_entropy: -6.13289, alpha: 0.00603, time: 33.76635
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 196.93411, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 0.09875, qf2_loss: 0.09791, policy_loss: -28.23385, policy_entropy: -6.10209, alpha: 0.00611, time: 34.09123
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 195.15337, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 0.09518, qf2_loss: 0.09503, policy_loss: -28.25160, policy_entropy: -6.08723, alpha: 0.00616, time: 34.00453
[CW] ---------------------------
[CW] ---- Iteration:   673 ----
[CW] collect: return: 174.50511, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 0.10001, qf2_loss: 0.09982, policy_loss: -28.30653, policy_entropy: -5.85014, alpha: 0.00615, time: 33.96641
[CW] ---------------------------
[CW] ---- Iteration:   674 ----
[CW] collect: return: 176.17520, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 0.10388, qf2_loss: 0.10446, policy_loss: -28.22607, policy_entropy: -5.91725, alpha: 0.00608, time: 34.15769
[CW] ---------------------------
[CW] ---- Iteration:   675 ----
[CW] collect: return: 180.93820, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 0.16811, qf2_loss: 0.16445, policy_loss: -28.31479, policy_entropy: -5.81462, alpha: 0.00602, time: 34.06328
[CW] ---------------------------
[CW] ---- Iteration:   676 ----
[CW] collect: return: 183.98588, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 0.10765, qf2_loss: 0.10927, policy_loss: -28.39055, policy_entropy: -5.80556, alpha: 0.00595, time: 34.10996
[CW] ---------------------------
[CW] ---- Iteration:   677 ----
[CW] collect: return: 199.20108, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 0.09218, qf2_loss: 0.09240, policy_loss: -28.25068, policy_entropy: -5.87385, alpha: 0.00587, time: 34.24328
[CW] ---------------------------
[CW] ---- Iteration:   678 ----
[CW] collect: return: 189.13538, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 0.10331, qf2_loss: 0.10376, policy_loss: -28.30132, policy_entropy: -6.05752, alpha: 0.00583, time: 33.97340
[CW] ---------------------------
[CW] ---- Iteration:   679 ----
[CW] collect: return: 174.60277, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 0.09238, qf2_loss: 0.09209, policy_loss: -28.42542, policy_entropy: -6.01163, alpha: 0.00587, time: 34.11175
[CW] ---------------------------
[CW] ---- Iteration:   680 ----
[CW] collect: return: 177.99846, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 0.08999, qf2_loss: 0.09001, policy_loss: -28.33736, policy_entropy: -5.95901, alpha: 0.00584, time: 34.17204
[CW] eval: return: 189.80547, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   681 ----
[CW] collect: return: 184.88611, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 0.11737, qf2_loss: 0.11722, policy_loss: -28.33948, policy_entropy: -5.99130, alpha: 0.00585, time: 33.79826
[CW] ---------------------------
[CW] ---- Iteration:   682 ----
[CW] collect: return: 193.58031, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 0.12447, qf2_loss: 0.12503, policy_loss: -28.44020, policy_entropy: -5.75811, alpha: 0.00581, time: 33.84304
[CW] ---------------------------
[CW] ---- Iteration:   683 ----
[CW] collect: return: 199.85478, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 0.08874, qf2_loss: 0.08861, policy_loss: -28.36968, policy_entropy: -5.87412, alpha: 0.00572, time: 34.02480
[CW] ---------------------------
[CW] ---- Iteration:   684 ----
[CW] collect: return: 204.55540, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 0.08892, qf2_loss: 0.08853, policy_loss: -28.38936, policy_entropy: -5.93495, alpha: 0.00567, time: 34.41806
[CW] ---------------------------
[CW] ---- Iteration:   685 ----
[CW] collect: return: 206.99700, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 0.10895, qf2_loss: 0.10901, policy_loss: -28.37327, policy_entropy: -5.91422, alpha: 0.00564, time: 34.13310
[CW] ---------------------------
[CW] ---- Iteration:   686 ----
[CW] collect: return: 178.26312, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 0.09646, qf2_loss: 0.09555, policy_loss: -28.42716, policy_entropy: -6.07050, alpha: 0.00563, time: 33.83417
[CW] ---------------------------
[CW] ---- Iteration:   687 ----
[CW] collect: return: 199.67702, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 0.09164, qf2_loss: 0.09157, policy_loss: -28.44020, policy_entropy: -5.90956, alpha: 0.00563, time: 34.09544
[CW] ---------------------------
[CW] ---- Iteration:   688 ----
[CW] collect: return: 197.02457, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 0.09598, qf2_loss: 0.09582, policy_loss: -28.53151, policy_entropy: -6.09775, alpha: 0.00563, time: 33.79199
[CW] ---------------------------
[CW] ---- Iteration:   689 ----
[CW] collect: return: 200.35260, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 0.10114, qf2_loss: 0.10181, policy_loss: -28.43472, policy_entropy: -6.25816, alpha: 0.00571, time: 33.94217
[CW] ---------------------------
[CW] ---- Iteration:   690 ----
[CW] collect: return: 182.87685, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 0.11288, qf2_loss: 0.11308, policy_loss: -28.55300, policy_entropy: -6.31910, alpha: 0.00582, time: 34.18242
[CW] ---------------------------
[CW] ---- Iteration:   691 ----
[CW] collect: return: 186.74528, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 0.08966, qf2_loss: 0.08971, policy_loss: -28.51372, policy_entropy: -6.24747, alpha: 0.00595, time: 33.84086
[CW] ---------------------------
[CW] ---- Iteration:   692 ----
[CW] collect: return: 202.08203, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 0.09416, qf2_loss: 0.09424, policy_loss: -28.59674, policy_entropy: -6.08987, alpha: 0.00603, time: 34.59850
[CW] ---------------------------
[CW] ---- Iteration:   693 ----
[CW] collect: return: 197.61583, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 0.09250, qf2_loss: 0.09208, policy_loss: -28.58815, policy_entropy: -5.86156, alpha: 0.00602, time: 33.97424
[CW] ---------------------------
[CW] ---- Iteration:   694 ----
[CW] collect: return: 179.16294, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 0.09867, qf2_loss: 0.09810, policy_loss: -28.51777, policy_entropy: -6.04307, alpha: 0.00599, time: 34.22687
[CW] ---------------------------
[CW] ---- Iteration:   695 ----
[CW] collect: return: 201.74004, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 0.11142, qf2_loss: 0.11148, policy_loss: -28.56982, policy_entropy: -6.04211, alpha: 0.00601, time: 33.86393
[CW] ---------------------------
[CW] ---- Iteration:   696 ----
[CW] collect: return: 188.93376, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 0.10316, qf2_loss: 0.10210, policy_loss: -28.58580, policy_entropy: -6.17202, alpha: 0.00605, time: 33.56163
[CW] ---------------------------
[CW] ---- Iteration:   697 ----
[CW] collect: return: 186.56066, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 0.09695, qf2_loss: 0.09759, policy_loss: -28.61105, policy_entropy: -6.00234, alpha: 0.00610, time: 34.12083
[CW] ---------------------------
[CW] ---- Iteration:   698 ----
[CW] collect: return: 188.74830, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 0.09205, qf2_loss: 0.09224, policy_loss: -28.60421, policy_entropy: -6.05731, alpha: 0.00613, time: 34.36029
[CW] ---------------------------
[CW] ---- Iteration:   699 ----
[CW] collect: return: 194.31819, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 0.10358, qf2_loss: 0.10368, policy_loss: -28.65663, policy_entropy: -5.89635, alpha: 0.00612, time: 34.16049
[CW] ---------------------------
[CW] ---- Iteration:   700 ----
[CW] collect: return: 199.96753, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 0.10094, qf2_loss: 0.10065, policy_loss: -28.70167, policy_entropy: -5.92902, alpha: 0.00607, time: 34.10046
[CW] eval: return: 193.71598, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   701 ----
[CW] collect: return: 190.68815, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 0.09469, qf2_loss: 0.09417, policy_loss: -28.68919, policy_entropy: -5.95681, alpha: 0.00603, time: 33.79136
[CW] ---------------------------
[CW] ---- Iteration:   702 ----
[CW] collect: return: 199.89942, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 0.09496, qf2_loss: 0.09585, policy_loss: -28.73324, policy_entropy: -5.98551, alpha: 0.00603, time: 33.84808
[CW] ---------------------------
[CW] ---- Iteration:   703 ----
[CW] collect: return: 188.25094, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 0.12564, qf2_loss: 0.12519, policy_loss: -28.71354, policy_entropy: -6.09302, alpha: 0.00604, time: 34.22142
[CW] ---------------------------
[CW] ---- Iteration:   704 ----
[CW] collect: return: 204.78336, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 0.10333, qf2_loss: 0.10259, policy_loss: -28.73774, policy_entropy: -6.09261, alpha: 0.00606, time: 34.12591
[CW] ---------------------------
[CW] ---- Iteration:   705 ----
[CW] collect: return: 207.99050, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 0.10476, qf2_loss: 0.10479, policy_loss: -28.68811, policy_entropy: -6.07637, alpha: 0.00612, time: 34.01138
[CW] ---------------------------
[CW] ---- Iteration:   706 ----
[CW] collect: return: 194.51683, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 0.09488, qf2_loss: 0.09514, policy_loss: -28.75665, policy_entropy: -6.07942, alpha: 0.00618, time: 34.31275
[CW] ---------------------------
[CW] ---- Iteration:   707 ----
[CW] collect: return: 207.90188, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 0.10523, qf2_loss: 0.10506, policy_loss: -28.79763, policy_entropy: -6.01463, alpha: 0.00620, time: 34.19224
[CW] ---------------------------
[CW] ---- Iteration:   708 ----
[CW] collect: return: 187.85294, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 0.11006, qf2_loss: 0.10909, policy_loss: -28.81369, policy_entropy: -6.07545, alpha: 0.00621, time: 34.19097
[CW] ---------------------------
[CW] ---- Iteration:   709 ----
[CW] collect: return: 179.26373, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 0.10548, qf2_loss: 0.10509, policy_loss: -28.79416, policy_entropy: -6.01850, alpha: 0.00624, time: 34.10315
[CW] ---------------------------
[CW] ---- Iteration:   710 ----
[CW] collect: return: 199.74714, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 0.10049, qf2_loss: 0.10007, policy_loss: -28.88661, policy_entropy: -6.01071, alpha: 0.00625, time: 34.12062
[CW] ---------------------------
[CW] ---- Iteration:   711 ----
[CW] collect: return: 197.73374, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 0.10223, qf2_loss: 0.10249, policy_loss: -28.85306, policy_entropy: -6.08777, alpha: 0.00625, time: 34.15407
[CW] ---------------------------
[CW] ---- Iteration:   712 ----
[CW] collect: return: 194.17812, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 0.10656, qf2_loss: 0.10536, policy_loss: -28.90484, policy_entropy: -5.96280, alpha: 0.00627, time: 34.05908
[CW] ---------------------------
[CW] ---- Iteration:   713 ----
[CW] collect: return: 162.02166, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 0.10394, qf2_loss: 0.10322, policy_loss: -28.95363, policy_entropy: -6.08698, alpha: 0.00628, time: 33.90618
[CW] ---------------------------
[CW] ---- Iteration:   714 ----
[CW] collect: return: 187.33536, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 0.10017, qf2_loss: 0.10054, policy_loss: -28.92421, policy_entropy: -6.09842, alpha: 0.00634, time: 34.02811
[CW] ---------------------------
[CW] ---- Iteration:   715 ----
[CW] collect: return: 199.44385, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 0.10756, qf2_loss: 0.10758, policy_loss: -28.93367, policy_entropy: -6.16388, alpha: 0.00641, time: 34.15025
[CW] ---------------------------
[CW] ---- Iteration:   716 ----
[CW] collect: return: 187.02256, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 0.11049, qf2_loss: 0.11049, policy_loss: -29.07897, policy_entropy: -5.98037, alpha: 0.00645, time: 34.28768
[CW] ---------------------------
[CW] ---- Iteration:   717 ----
[CW] collect: return: 181.51537, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 0.10640, qf2_loss: 0.10650, policy_loss: -28.94242, policy_entropy: -5.97515, alpha: 0.00643, time: 34.00840
[CW] ---------------------------
[CW] ---- Iteration:   718 ----
[CW] collect: return: 201.05275, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 0.10614, qf2_loss: 0.10612, policy_loss: -28.93885, policy_entropy: -5.85607, alpha: 0.00641, time: 34.05122
[CW] ---------------------------
[CW] ---- Iteration:   719 ----
[CW] collect: return: 187.46681, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 0.10240, qf2_loss: 0.10128, policy_loss: -29.03020, policy_entropy: -5.91765, alpha: 0.00635, time: 34.40846
[CW] ---------------------------
[CW] ---- Iteration:   720 ----
[CW] collect: return: 198.54486, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 0.10490, qf2_loss: 0.10470, policy_loss: -29.02539, policy_entropy: -5.96226, alpha: 0.00631, time: 34.15485
[CW] eval: return: 188.82368, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   721 ----
[CW] collect: return: 183.91622, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 0.13053, qf2_loss: 0.13047, policy_loss: -29.17361, policy_entropy: -5.90071, alpha: 0.00629, time: 33.68779
[CW] ---------------------------
[CW] ---- Iteration:   722 ----
[CW] collect: return: 192.49789, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 0.11864, qf2_loss: 0.11759, policy_loss: -29.01756, policy_entropy: -5.95475, alpha: 0.00622, time: 33.86682
[CW] ---------------------------
[CW] ---- Iteration:   723 ----
[CW] collect: return: 195.63375, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 0.10829, qf2_loss: 0.10726, policy_loss: -29.01656, policy_entropy: -5.97764, alpha: 0.00622, time: 34.25604
[CW] ---------------------------
[CW] ---- Iteration:   724 ----
[CW] collect: return: 199.78536, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 0.09651, qf2_loss: 0.09648, policy_loss: -29.09394, policy_entropy: -6.06597, alpha: 0.00624, time: 34.08172
[CW] ---------------------------
[CW] ---- Iteration:   725 ----
[CW] collect: return: 199.84244, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 0.11368, qf2_loss: 0.11429, policy_loss: -29.11670, policy_entropy: -5.91953, alpha: 0.00624, time: 33.69670
[CW] ---------------------------
[CW] ---- Iteration:   726 ----
[CW] collect: return: 195.32978, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 0.10217, qf2_loss: 0.10166, policy_loss: -29.08978, policy_entropy: -5.98109, alpha: 0.00620, time: 33.77973
[CW] ---------------------------
[CW] ---- Iteration:   727 ----
[CW] collect: return: 197.78272, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 0.10909, qf2_loss: 0.10881, policy_loss: -29.11084, policy_entropy: -6.06381, alpha: 0.00620, time: 34.14328
[CW] ---------------------------
[CW] ---- Iteration:   728 ----
[CW] collect: return: 206.95637, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 0.09337, qf2_loss: 0.09309, policy_loss: -29.15843, policy_entropy: -6.16682, alpha: 0.00627, time: 34.52966
[CW] ---------------------------
[CW] ---- Iteration:   729 ----
[CW] collect: return: 189.97153, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 0.09890, qf2_loss: 0.09855, policy_loss: -29.12359, policy_entropy: -6.06245, alpha: 0.00632, time: 34.11981
[CW] ---------------------------
[CW] ---- Iteration:   730 ----
[CW] collect: return: 204.07128, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 0.09526, qf2_loss: 0.09535, policy_loss: -29.20786, policy_entropy: -5.91499, alpha: 0.00633, time: 33.63659
[CW] ---------------------------
[CW] ---- Iteration:   731 ----
[CW] collect: return: 193.57460, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 0.10902, qf2_loss: 0.10951, policy_loss: -29.23836, policy_entropy: -5.95794, alpha: 0.00629, time: 33.53581
[CW] ---------------------------
[CW] ---- Iteration:   732 ----
[CW] collect: return: 198.53109, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 0.10679, qf2_loss: 0.10687, policy_loss: -29.23624, policy_entropy: -5.87022, alpha: 0.00625, time: 34.00028
[CW] ---------------------------
[CW] ---- Iteration:   733 ----
[CW] collect: return: 194.09059, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 0.10740, qf2_loss: 0.10699, policy_loss: -29.10545, policy_entropy: -5.89868, alpha: 0.00620, time: 34.30452
[CW] ---------------------------
[CW] ---- Iteration:   734 ----
[CW] collect: return: 203.85755, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 0.10525, qf2_loss: 0.10543, policy_loss: -29.21638, policy_entropy: -5.99438, alpha: 0.00616, time: 34.14637
[CW] ---------------------------
[CW] ---- Iteration:   735 ----
[CW] collect: return: 205.17552, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 0.10402, qf2_loss: 0.10401, policy_loss: -29.26012, policy_entropy: -6.06274, alpha: 0.00619, time: 34.24003
[CW] ---------------------------
[CW] ---- Iteration:   736 ----
[CW] collect: return: 203.67470, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 0.10411, qf2_loss: 0.10408, policy_loss: -29.25645, policy_entropy: -5.97113, alpha: 0.00619, time: 36.41463
[CW] ---------------------------
[CW] ---- Iteration:   737 ----
[CW] collect: return: 190.00755, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 0.10597, qf2_loss: 0.10458, policy_loss: -29.35089, policy_entropy: -6.05318, alpha: 0.00619, time: 34.31728
[CW] ---------------------------
[CW] ---- Iteration:   738 ----
[CW] collect: return: 194.31528, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 0.10967, qf2_loss: 0.10976, policy_loss: -29.35974, policy_entropy: -6.08124, alpha: 0.00623, time: 34.33395
[CW] ---------------------------
[CW] ---- Iteration:   739 ----
[CW] collect: return: 197.27541, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 0.11538, qf2_loss: 0.11574, policy_loss: -29.37010, policy_entropy: -6.12154, alpha: 0.00626, time: 34.05994
[CW] ---------------------------
[CW] ---- Iteration:   740 ----
[CW] collect: return: 202.65191, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 0.11812, qf2_loss: 0.11695, policy_loss: -29.29063, policy_entropy: -5.99798, alpha: 0.00631, time: 34.22613
[CW] eval: return: 191.37049, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   741 ----
[CW] collect: return: 205.22317, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 0.10479, qf2_loss: 0.10506, policy_loss: -29.28605, policy_entropy: -6.01098, alpha: 0.00632, time: 34.17232
[CW] ---------------------------
[CW] ---- Iteration:   742 ----
[CW] collect: return: 197.91289, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 0.10944, qf2_loss: 0.10954, policy_loss: -29.44110, policy_entropy: -6.33106, alpha: 0.00639, time: 34.05704
[CW] ---------------------------
[CW] ---- Iteration:   743 ----
[CW] collect: return: 208.29248, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 0.11254, qf2_loss: 0.11160, policy_loss: -29.38140, policy_entropy: -6.06986, alpha: 0.00650, time: 34.45355
[CW] ---------------------------
[CW] ---- Iteration:   744 ----
[CW] collect: return: 192.27313, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 0.10425, qf2_loss: 0.10283, policy_loss: -29.45686, policy_entropy: -6.10778, alpha: 0.00655, time: 34.09775
[CW] ---------------------------
[CW] ---- Iteration:   745 ----
[CW] collect: return: 207.31956, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 0.11855, qf2_loss: 0.11868, policy_loss: -29.43371, policy_entropy: -6.16789, alpha: 0.00662, time: 34.18032
[CW] ---------------------------
[CW] ---- Iteration:   746 ----
[CW] collect: return: 199.40169, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 0.10903, qf2_loss: 0.10885, policy_loss: -29.42145, policy_entropy: -5.94675, alpha: 0.00666, time: 34.06260
[CW] ---------------------------
[CW] ---- Iteration:   747 ----
[CW] collect: return: 210.02330, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 0.11919, qf2_loss: 0.11868, policy_loss: -29.51678, policy_entropy: -5.81684, alpha: 0.00659, time: 35.78825
[CW] ---------------------------
[CW] ---- Iteration:   748 ----
[CW] collect: return: 197.08363, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 0.11494, qf2_loss: 0.11580, policy_loss: -29.56894, policy_entropy: -6.11064, alpha: 0.00657, time: 34.44468
[CW] ---------------------------
[CW] ---- Iteration:   749 ----
[CW] collect: return: 192.25870, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 0.10101, qf2_loss: 0.10078, policy_loss: -29.47893, policy_entropy: -6.07403, alpha: 0.00661, time: 34.11886
[CW] ---------------------------
[CW] ---- Iteration:   750 ----
[CW] collect: return: 207.03170, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 0.10746, qf2_loss: 0.10733, policy_loss: -29.50329, policy_entropy: -5.99229, alpha: 0.00665, time: 34.22308
[CW] ---------------------------
[CW] ---- Iteration:   751 ----
[CW] collect: return: 202.51745, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 0.10498, qf2_loss: 0.10505, policy_loss: -29.61954, policy_entropy: -6.02203, alpha: 0.00665, time: 33.57392
[CW] ---------------------------
[CW] ---- Iteration:   752 ----
[CW] collect: return: 194.90035, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 0.11845, qf2_loss: 0.11842, policy_loss: -29.48316, policy_entropy: -5.98351, alpha: 0.00663, time: 33.95039
[CW] ---------------------------
[CW] ---- Iteration:   753 ----
[CW] collect: return: 184.79983, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 0.11523, qf2_loss: 0.11492, policy_loss: -29.58001, policy_entropy: -6.06306, alpha: 0.00663, time: 33.83870
[CW] ---------------------------
[CW] ---- Iteration:   754 ----
[CW] collect: return: 202.49293, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 0.10677, qf2_loss: 0.10618, policy_loss: -29.67364, policy_entropy: -6.05001, alpha: 0.00667, time: 34.05096
[CW] ---------------------------
[CW] ---- Iteration:   755 ----
[CW] collect: return: 203.45403, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 0.10825, qf2_loss: 0.10754, policy_loss: -29.70906, policy_entropy: -6.06230, alpha: 0.00672, time: 33.76506
[CW] ---------------------------
[CW] ---- Iteration:   756 ----
[CW] collect: return: 200.42526, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 0.11106, qf2_loss: 0.11044, policy_loss: -29.65089, policy_entropy: -5.97033, alpha: 0.00672, time: 34.29614
[CW] ---------------------------
[CW] ---- Iteration:   757 ----
[CW] collect: return: 203.09602, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 0.11459, qf2_loss: 0.11459, policy_loss: -29.68437, policy_entropy: -6.06148, alpha: 0.00672, time: 34.37639
[CW] ---------------------------
[CW] ---- Iteration:   758 ----
[CW] collect: return: 208.20537, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 0.10692, qf2_loss: 0.10676, policy_loss: -29.68799, policy_entropy: -6.06381, alpha: 0.00674, time: 34.32878
[CW] ---------------------------
[CW] ---- Iteration:   759 ----
[CW] collect: return: 204.84243, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 0.11056, qf2_loss: 0.11020, policy_loss: -29.74309, policy_entropy: -5.98999, alpha: 0.00677, time: 34.02707
[CW] ---------------------------
[CW] ---- Iteration:   760 ----
[CW] collect: return: 201.83248, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 0.12196, qf2_loss: 0.12203, policy_loss: -29.70781, policy_entropy: -5.86819, alpha: 0.00675, time: 33.85802
[CW] eval: return: 205.59199, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   761 ----
[CW] collect: return: 208.91027, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 0.10203, qf2_loss: 0.10088, policy_loss: -29.80074, policy_entropy: -5.95249, alpha: 0.00669, time: 34.03539
[CW] ---------------------------
[CW] ---- Iteration:   762 ----
[CW] collect: return: 207.46843, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 0.11697, qf2_loss: 0.11771, policy_loss: -29.72034, policy_entropy: -6.02717, alpha: 0.00668, time: 34.42376
[CW] ---------------------------
[CW] ---- Iteration:   763 ----
[CW] collect: return: 201.92985, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 0.11443, qf2_loss: 0.11467, policy_loss: -29.78155, policy_entropy: -5.89983, alpha: 0.00667, time: 34.03491
[CW] ---------------------------
[CW] ---- Iteration:   764 ----
[CW] collect: return: 205.20340, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 0.10039, qf2_loss: 0.10010, policy_loss: -29.92708, policy_entropy: -6.00682, alpha: 0.00663, time: 33.94182
[CW] ---------------------------
[CW] ---- Iteration:   765 ----
[CW] collect: return: 209.97543, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 0.10235, qf2_loss: 0.10175, policy_loss: -29.78430, policy_entropy: -6.03127, alpha: 0.00666, time: 34.20543
[CW] ---------------------------
[CW] ---- Iteration:   766 ----
[CW] collect: return: 208.34113, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 0.10579, qf2_loss: 0.10536, policy_loss: -29.86936, policy_entropy: -5.92855, alpha: 0.00664, time: 33.91941
[CW] ---------------------------
[CW] ---- Iteration:   767 ----
[CW] collect: return: 200.12983, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 0.10062, qf2_loss: 0.09993, policy_loss: -29.90721, policy_entropy: -6.01354, alpha: 0.00662, time: 34.12457
[CW] ---------------------------
[CW] ---- Iteration:   768 ----
[CW] collect: return: 205.62060, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 0.11075, qf2_loss: 0.11081, policy_loss: -29.85395, policy_entropy: -5.98639, alpha: 0.00662, time: 34.00411
[CW] ---------------------------
[CW] ---- Iteration:   769 ----
[CW] collect: return: 202.38190, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 0.10200, qf2_loss: 0.10074, policy_loss: -30.03380, policy_entropy: -6.10456, alpha: 0.00664, time: 33.68502
[CW] ---------------------------
[CW] ---- Iteration:   770 ----
[CW] collect: return: 196.36028, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 0.10818, qf2_loss: 0.10875, policy_loss: -29.90101, policy_entropy: -6.06007, alpha: 0.00670, time: 33.99441
[CW] ---------------------------
[CW] ---- Iteration:   771 ----
[CW] collect: return: 212.62545, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 0.10532, qf2_loss: 0.10458, policy_loss: -29.89460, policy_entropy: -5.94812, alpha: 0.00671, time: 33.99866
[CW] ---------------------------
[CW] ---- Iteration:   772 ----
[CW] collect: return: 205.22218, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 0.10583, qf2_loss: 0.10511, policy_loss: -30.04562, policy_entropy: -6.01047, alpha: 0.00669, time: 34.21088
[CW] ---------------------------
[CW] ---- Iteration:   773 ----
[CW] collect: return: 199.84995, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 0.09803, qf2_loss: 0.09847, policy_loss: -29.98593, policy_entropy: -5.99453, alpha: 0.00668, time: 34.18368
[CW] ---------------------------
[CW] ---- Iteration:   774 ----
[CW] collect: return: 204.21720, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 0.11270, qf2_loss: 0.11016, policy_loss: -30.12535, policy_entropy: -6.00716, alpha: 0.00670, time: 34.29401
[CW] ---------------------------
[CW] ---- Iteration:   775 ----
[CW] collect: return: 207.15571, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 0.10661, qf2_loss: 0.10711, policy_loss: -30.07400, policy_entropy: -6.04198, alpha: 0.00670, time: 34.39481
[CW] ---------------------------
[CW] ---- Iteration:   776 ----
[CW] collect: return: 182.84277, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 0.10840, qf2_loss: 0.10903, policy_loss: -30.10794, policy_entropy: -6.20968, alpha: 0.00677, time: 34.35267
[CW] ---------------------------
[CW] ---- Iteration:   777 ----
[CW] collect: return: 199.64600, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 0.10405, qf2_loss: 0.10319, policy_loss: -30.14059, policy_entropy: -6.12107, alpha: 0.00686, time: 34.27463
[CW] ---------------------------
[CW] ---- Iteration:   778 ----
[CW] collect: return: 201.15591, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 0.09962, qf2_loss: 0.09918, policy_loss: -30.04935, policy_entropy: -5.98389, alpha: 0.00689, time: 34.14473
[CW] ---------------------------
[CW] ---- Iteration:   779 ----
[CW] collect: return: 196.26757, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 0.11360, qf2_loss: 0.11262, policy_loss: -30.10955, policy_entropy: -6.13262, alpha: 0.00692, time: 34.34731
[CW] ---------------------------
[CW] ---- Iteration:   780 ----
[CW] collect: return: 211.72558, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 0.10575, qf2_loss: 0.10630, policy_loss: -30.14849, policy_entropy: -5.94625, alpha: 0.00698, time: 34.23167
[CW] eval: return: 205.53484, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   781 ----
[CW] collect: return: 209.69250, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 0.11281, qf2_loss: 0.11217, policy_loss: -30.15263, policy_entropy: -5.94904, alpha: 0.00692, time: 34.05754
[CW] ---------------------------
[CW] ---- Iteration:   782 ----
[CW] collect: return: 206.86979, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 0.10988, qf2_loss: 0.10978, policy_loss: -30.24941, policy_entropy: -5.86210, alpha: 0.00686, time: 34.16045
[CW] ---------------------------
[CW] ---- Iteration:   783 ----
[CW] collect: return: 202.30368, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 0.10417, qf2_loss: 0.10353, policy_loss: -30.25480, policy_entropy: -6.07482, alpha: 0.00685, time: 34.24622
[CW] ---------------------------
[CW] ---- Iteration:   784 ----
[CW] collect: return: 193.70572, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 0.10936, qf2_loss: 0.10923, policy_loss: -30.24616, policy_entropy: -5.97203, alpha: 0.00687, time: 34.01477
[CW] ---------------------------
[CW] ---- Iteration:   785 ----
[CW] collect: return: 202.83935, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 0.09927, qf2_loss: 0.10012, policy_loss: -30.29310, policy_entropy: -5.92670, alpha: 0.00684, time: 33.98840
[CW] ---------------------------
[CW] ---- Iteration:   786 ----
[CW] collect: return: 206.75426, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 0.10724, qf2_loss: 0.10698, policy_loss: -30.26343, policy_entropy: -5.90876, alpha: 0.00679, time: 34.08756
[CW] ---------------------------
[CW] ---- Iteration:   787 ----
[CW] collect: return: 209.11828, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 0.10320, qf2_loss: 0.10267, policy_loss: -30.30999, policy_entropy: -5.82248, alpha: 0.00672, time: 34.41639
[CW] ---------------------------
[CW] ---- Iteration:   788 ----
[CW] collect: return: 173.43813, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 0.10341, qf2_loss: 0.10217, policy_loss: -30.37180, policy_entropy: -5.94625, alpha: 0.00664, time: 33.96832
[CW] ---------------------------
[CW] ---- Iteration:   789 ----
[CW] collect: return: 202.55375, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 0.11162, qf2_loss: 0.11224, policy_loss: -30.33390, policy_entropy: -5.85570, alpha: 0.00661, time: 33.82042
[CW] ---------------------------
[CW] ---- Iteration:   790 ----
[CW] collect: return: 190.91896, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 0.11124, qf2_loss: 0.11054, policy_loss: -30.39792, policy_entropy: -5.97149, alpha: 0.00654, time: 34.44412
[CW] ---------------------------
[CW] ---- Iteration:   791 ----
[CW] collect: return: 210.76296, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 0.12194, qf2_loss: 0.12146, policy_loss: -30.35672, policy_entropy: -6.04917, alpha: 0.00655, time: 33.65519
[CW] ---------------------------
[CW] ---- Iteration:   792 ----
[CW] collect: return: 212.28319, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 0.11293, qf2_loss: 0.11277, policy_loss: -30.43468, policy_entropy: -5.99009, alpha: 0.00658, time: 34.21218
[CW] ---------------------------
[CW] ---- Iteration:   793 ----
[CW] collect: return: 197.75806, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 0.09842, qf2_loss: 0.09891, policy_loss: -30.39795, policy_entropy: -6.00547, alpha: 0.00656, time: 34.18535
[CW] ---------------------------
[CW] ---- Iteration:   794 ----
[CW] collect: return: 207.47021, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 0.10486, qf2_loss: 0.10459, policy_loss: -30.46366, policy_entropy: -6.09755, alpha: 0.00661, time: 33.96939
[CW] ---------------------------
[CW] ---- Iteration:   795 ----
[CW] collect: return: 204.46617, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 0.11177, qf2_loss: 0.11165, policy_loss: -30.46600, policy_entropy: -5.90892, alpha: 0.00661, time: 34.03815
[CW] ---------------------------
[CW] ---- Iteration:   796 ----
[CW] collect: return: 190.16984, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 0.11733, qf2_loss: 0.11625, policy_loss: -30.51190, policy_entropy: -6.05136, alpha: 0.00658, time: 34.27293
[CW] ---------------------------
[CW] ---- Iteration:   797 ----
[CW] collect: return: 203.18123, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 0.10658, qf2_loss: 0.10633, policy_loss: -30.43624, policy_entropy: -5.94608, alpha: 0.00658, time: 34.27639
[CW] ---------------------------
[CW] ---- Iteration:   798 ----
[CW] collect: return: 186.68650, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 0.10993, qf2_loss: 0.10909, policy_loss: -30.46860, policy_entropy: -6.15224, alpha: 0.00661, time: 34.31621
[CW] ---------------------------
[CW] ---- Iteration:   799 ----
[CW] collect: return: 208.11520, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 0.10453, qf2_loss: 0.10339, policy_loss: -30.51999, policy_entropy: -5.96460, alpha: 0.00664, time: 34.16149
[CW] ---------------------------
[CW] ---- Iteration:   800 ----
[CW] collect: return: 203.13509, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 0.11526, qf2_loss: 0.11537, policy_loss: -30.52935, policy_entropy: -6.01241, alpha: 0.00663, time: 34.24248
[CW] eval: return: 196.68982, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   801 ----
[CW] collect: return: 202.39295, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 0.09931, qf2_loss: 0.09905, policy_loss: -30.49889, policy_entropy: -6.04377, alpha: 0.00664, time: 34.40055
[CW] ---------------------------
[CW] ---- Iteration:   802 ----
[CW] collect: return: 213.38161, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 0.09539, qf2_loss: 0.09505, policy_loss: -30.58399, policy_entropy: -6.11215, alpha: 0.00668, time: 34.07665
[CW] ---------------------------
[CW] ---- Iteration:   803 ----
[CW] collect: return: 203.61181, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 0.09618, qf2_loss: 0.09657, policy_loss: -30.55081, policy_entropy: -5.85857, alpha: 0.00670, time: 34.19621
[CW] ---------------------------
[CW] ---- Iteration:   804 ----
[CW] collect: return: 203.87306, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 0.11324, qf2_loss: 0.11293, policy_loss: -30.65105, policy_entropy: -5.90701, alpha: 0.00664, time: 34.18984
[CW] ---------------------------
[CW] ---- Iteration:   805 ----
[CW] collect: return: 214.55499, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 0.10864, qf2_loss: 0.10767, policy_loss: -30.69707, policy_entropy: -6.07834, alpha: 0.00661, time: 34.21813
[CW] ---------------------------
[CW] ---- Iteration:   806 ----
[CW] collect: return: 202.32366, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 0.10614, qf2_loss: 0.10592, policy_loss: -30.64080, policy_entropy: -5.97571, alpha: 0.00664, time: 33.99873
[CW] ---------------------------
[CW] ---- Iteration:   807 ----
[CW] collect: return: 203.91070, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 0.10059, qf2_loss: 0.09972, policy_loss: -30.70694, policy_entropy: -6.06740, alpha: 0.00665, time: 34.18389
[CW] ---------------------------
[CW] ---- Iteration:   808 ----
[CW] collect: return: 200.71748, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 0.10830, qf2_loss: 0.10816, policy_loss: -30.66557, policy_entropy: -6.01679, alpha: 0.00667, time: 34.34194
[CW] ---------------------------
[CW] ---- Iteration:   809 ----
[CW] collect: return: 191.83063, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 0.11275, qf2_loss: 0.11280, policy_loss: -30.78642, policy_entropy: -6.06357, alpha: 0.00668, time: 34.01673
[CW] ---------------------------
[CW] ---- Iteration:   810 ----
[CW] collect: return: 205.56385, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 0.09922, qf2_loss: 0.09888, policy_loss: -30.71917, policy_entropy: -6.03724, alpha: 0.00671, time: 33.84070
[CW] ---------------------------
[CW] ---- Iteration:   811 ----
[CW] collect: return: 213.71839, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 0.10228, qf2_loss: 0.10223, policy_loss: -30.66653, policy_entropy: -5.96565, alpha: 0.00672, time: 33.82596
[CW] ---------------------------
[CW] ---- Iteration:   812 ----
[CW] collect: return: 203.84039, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 0.10304, qf2_loss: 0.10111, policy_loss: -30.81072, policy_entropy: -5.94528, alpha: 0.00668, time: 34.43913
[CW] ---------------------------
[CW] ---- Iteration:   813 ----
[CW] collect: return: 211.47223, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 0.10082, qf2_loss: 0.10146, policy_loss: -30.76506, policy_entropy: -5.97512, alpha: 0.00667, time: 34.44095
[CW] ---------------------------
[CW] ---- Iteration:   814 ----
[CW] collect: return: 205.12500, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 0.10386, qf2_loss: 0.10425, policy_loss: -30.76901, policy_entropy: -5.99261, alpha: 0.00666, time: 34.18606
[CW] ---------------------------
[CW] ---- Iteration:   815 ----
[CW] collect: return: 205.04332, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 0.10898, qf2_loss: 0.10773, policy_loss: -30.90100, policy_entropy: -6.04351, alpha: 0.00667, time: 34.23218
[CW] ---------------------------
[CW] ---- Iteration:   816 ----
[CW] collect: return: 195.71640, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 0.10929, qf2_loss: 0.10980, policy_loss: -30.94809, policy_entropy: -6.14253, alpha: 0.00673, time: 34.38715
[CW] ---------------------------
[CW] ---- Iteration:   817 ----
[CW] collect: return: 212.98127, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 0.11810, qf2_loss: 0.11765, policy_loss: -30.87736, policy_entropy: -6.03630, alpha: 0.00676, time: 34.38068
[CW] ---------------------------
[CW] ---- Iteration:   818 ----
[CW] collect: return: 210.84567, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 0.10363, qf2_loss: 0.10265, policy_loss: -30.89362, policy_entropy: -6.02591, alpha: 0.00679, time: 34.20699
[CW] ---------------------------
[CW] ---- Iteration:   819 ----
[CW] collect: return: 208.47413, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 0.10133, qf2_loss: 0.10094, policy_loss: -30.83959, policy_entropy: -5.97264, alpha: 0.00678, time: 34.18520
[CW] ---------------------------
[CW] ---- Iteration:   820 ----
[CW] collect: return: 213.10748, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 0.10453, qf2_loss: 0.10363, policy_loss: -30.84967, policy_entropy: -6.03391, alpha: 0.00680, time: 35.74777
[CW] eval: return: 206.47918, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   821 ----
[CW] collect: return: 202.79104, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 0.10389, qf2_loss: 0.10478, policy_loss: -31.02950, policy_entropy: -6.04960, alpha: 0.00682, time: 34.20653
[CW] ---------------------------
[CW] ---- Iteration:   822 ----
[CW] collect: return: 203.85309, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 0.11070, qf2_loss: 0.10996, policy_loss: -30.87813, policy_entropy: -5.97037, alpha: 0.00681, time: 34.04456
[CW] ---------------------------
[CW] ---- Iteration:   823 ----
[CW] collect: return: 213.81655, steps: 1000.00000, total_steps: 829000.00000
[CW] train: qf1_loss: 0.10303, qf2_loss: 0.10320, policy_loss: -30.98088, policy_entropy: -6.01548, alpha: 0.00682, time: 34.32268
[CW] ---------------------------
[CW] ---- Iteration:   824 ----
[CW] collect: return: 210.08811, steps: 1000.00000, total_steps: 830000.00000
[CW] train: qf1_loss: 0.12185, qf2_loss: 0.12188, policy_loss: -30.95975, policy_entropy: -5.89458, alpha: 0.00680, time: 34.15442
[CW] ---------------------------
[CW] ---- Iteration:   825 ----
[CW] collect: return: 201.40615, steps: 1000.00000, total_steps: 831000.00000
[CW] train: qf1_loss: 0.12455, qf2_loss: 0.12373, policy_loss: -30.97877, policy_entropy: -5.95933, alpha: 0.00672, time: 34.19748
[CW] ---------------------------
[CW] ---- Iteration:   826 ----
[CW] collect: return: 203.29995, steps: 1000.00000, total_steps: 832000.00000
[CW] train: qf1_loss: 0.10718, qf2_loss: 0.10730, policy_loss: -31.06994, policy_entropy: -6.13667, alpha: 0.00676, time: 34.25458
[CW] ---------------------------
[CW] ---- Iteration:   827 ----
[CW] collect: return: 207.64766, steps: 1000.00000, total_steps: 833000.00000
[CW] train: qf1_loss: 0.10388, qf2_loss: 0.10301, policy_loss: -31.10128, policy_entropy: -6.04042, alpha: 0.00681, time: 34.34750
[CW] ---------------------------
[CW] ---- Iteration:   828 ----
[CW] collect: return: 208.93163, steps: 1000.00000, total_steps: 834000.00000
[CW] train: qf1_loss: 0.10033, qf2_loss: 0.09989, policy_loss: -31.10232, policy_entropy: -6.09623, alpha: 0.00686, time: 33.78897
[CW] ---------------------------
[CW] ---- Iteration:   829 ----
[CW] collect: return: 209.71168, steps: 1000.00000, total_steps: 835000.00000
[CW] train: qf1_loss: 0.10201, qf2_loss: 0.10159, policy_loss: -31.13257, policy_entropy: -6.13352, alpha: 0.00692, time: 34.01648
[CW] ---------------------------
[CW] ---- Iteration:   830 ----
[CW] collect: return: 195.86052, steps: 1000.00000, total_steps: 836000.00000
[CW] train: qf1_loss: 0.09908, qf2_loss: 0.09846, policy_loss: -30.97687, policy_entropy: -5.93535, alpha: 0.00695, time: 34.38205
[CW] ---------------------------
[CW] ---- Iteration:   831 ----
[CW] collect: return: 164.01597, steps: 1000.00000, total_steps: 837000.00000
[CW] train: qf1_loss: 0.11199, qf2_loss: 0.11076, policy_loss: -31.07121, policy_entropy: -6.05746, alpha: 0.00695, time: 34.19577
[CW] ---------------------------
[CW] ---- Iteration:   832 ----
[CW] collect: return: 205.38829, steps: 1000.00000, total_steps: 838000.00000
[CW] train: qf1_loss: 0.11012, qf2_loss: 0.11098, policy_loss: -31.13639, policy_entropy: -5.96437, alpha: 0.00694, time: 34.05395
[CW] ---------------------------
[CW] ---- Iteration:   833 ----
[CW] collect: return: 203.69419, steps: 1000.00000, total_steps: 839000.00000
[CW] train: qf1_loss: 0.10727, qf2_loss: 0.10646, policy_loss: -31.15325, policy_entropy: -5.96351, alpha: 0.00693, time: 34.29980
[CW] ---------------------------
[CW] ---- Iteration:   834 ----
[CW] collect: return: 203.76966, steps: 1000.00000, total_steps: 840000.00000
[CW] train: qf1_loss: 0.09763, qf2_loss: 0.09848, policy_loss: -31.04840, policy_entropy: -5.99219, alpha: 0.00691, time: 34.44167
[CW] ---------------------------
[CW] ---- Iteration:   835 ----
[CW] collect: return: 206.57460, steps: 1000.00000, total_steps: 841000.00000
[CW] train: qf1_loss: 0.09844, qf2_loss: 0.09764, policy_loss: -31.17426, policy_entropy: -5.86861, alpha: 0.00687, time: 34.13326
[CW] ---------------------------
[CW] ---- Iteration:   836 ----
[CW] collect: return: 196.39176, steps: 1000.00000, total_steps: 842000.00000
[CW] train: qf1_loss: 0.10896, qf2_loss: 0.10806, policy_loss: -31.11683, policy_entropy: -5.91584, alpha: 0.00682, time: 34.04401
[CW] ---------------------------
[CW] ---- Iteration:   837 ----
[CW] collect: return: 200.70812, steps: 1000.00000, total_steps: 843000.00000
[CW] train: qf1_loss: 0.10868, qf2_loss: 0.10859, policy_loss: -31.18142, policy_entropy: -5.91377, alpha: 0.00678, time: 33.89919
[CW] ---------------------------
[CW] ---- Iteration:   838 ----
[CW] collect: return: 198.28700, steps: 1000.00000, total_steps: 844000.00000
[CW] train: qf1_loss: 0.11020, qf2_loss: 0.11038, policy_loss: -31.15049, policy_entropy: -5.96630, alpha: 0.00674, time: 35.56937
[CW] ---------------------------
[CW] ---- Iteration:   839 ----
[CW] collect: return: 201.77165, steps: 1000.00000, total_steps: 845000.00000
[CW] train: qf1_loss: 0.10249, qf2_loss: 0.10225, policy_loss: -31.25879, policy_entropy: -6.06351, alpha: 0.00675, time: 34.37519
[CW] ---------------------------
[CW] ---- Iteration:   840 ----
[CW] collect: return: 210.59596, steps: 1000.00000, total_steps: 846000.00000
[CW] train: qf1_loss: 0.10134, qf2_loss: 0.09937, policy_loss: -31.18672, policy_entropy: -5.92956, alpha: 0.00675, time: 34.03608
[CW] eval: return: 200.83052, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   841 ----
[CW] collect: return: 201.08247, steps: 1000.00000, total_steps: 847000.00000
[CW] train: qf1_loss: 0.10715, qf2_loss: 0.10805, policy_loss: -31.23639, policy_entropy: -6.09315, alpha: 0.00675, time: 33.65410
[CW] ---------------------------
[CW] ---- Iteration:   842 ----
[CW] collect: return: 195.14035, steps: 1000.00000, total_steps: 848000.00000
[CW] train: qf1_loss: 0.09921, qf2_loss: 0.09857, policy_loss: -31.25999, policy_entropy: -5.99212, alpha: 0.00678, time: 33.99479
[CW] ---------------------------
[CW] ---- Iteration:   843 ----
[CW] collect: return: 192.85470, steps: 1000.00000, total_steps: 849000.00000
[CW] train: qf1_loss: 0.10034, qf2_loss: 0.10061, policy_loss: -31.30636, policy_entropy: -6.04341, alpha: 0.00678, time: 34.38614
[CW] ---------------------------
[CW] ---- Iteration:   844 ----
[CW] collect: return: 197.84443, steps: 1000.00000, total_steps: 850000.00000
[CW] train: qf1_loss: 0.12240, qf2_loss: 0.12123, policy_loss: -31.29939, policy_entropy: -5.91903, alpha: 0.00677, time: 34.12197
[CW] ---------------------------
[CW] ---- Iteration:   845 ----
[CW] collect: return: 204.62726, steps: 1000.00000, total_steps: 851000.00000
[CW] train: qf1_loss: 0.09637, qf2_loss: 0.09568, policy_loss: -31.39155, policy_entropy: -6.09612, alpha: 0.00677, time: 34.23320
[CW] ---------------------------
[CW] ---- Iteration:   846 ----
[CW] collect: return: 203.47163, steps: 1000.00000, total_steps: 852000.00000
[CW] train: qf1_loss: 0.10472, qf2_loss: 0.10511, policy_loss: -31.40813, policy_entropy: -6.13173, alpha: 0.00684, time: 34.02335
[CW] ---------------------------
[CW] ---- Iteration:   847 ----
[CW] collect: return: 207.15820, steps: 1000.00000, total_steps: 853000.00000
[CW] train: qf1_loss: 0.09928, qf2_loss: 0.09961, policy_loss: -31.37186, policy_entropy: -5.96764, alpha: 0.00687, time: 34.37159
[CW] ---------------------------
[CW] ---- Iteration:   848 ----
[CW] collect: return: 210.29173, steps: 1000.00000, total_steps: 854000.00000
[CW] train: qf1_loss: 0.10813, qf2_loss: 0.10747, policy_loss: -31.40681, policy_entropy: -5.99843, alpha: 0.00686, time: 33.92978
[CW] ---------------------------
[CW] ---- Iteration:   849 ----
[CW] collect: return: 197.79625, steps: 1000.00000, total_steps: 855000.00000
[CW] train: qf1_loss: 0.10494, qf2_loss: 0.10421, policy_loss: -31.43399, policy_entropy: -6.01689, alpha: 0.00686, time: 35.61663
[CW] ---------------------------
[CW] ---- Iteration:   850 ----
[CW] collect: return: 205.53732, steps: 1000.00000, total_steps: 856000.00000
[CW] train: qf1_loss: 0.10430, qf2_loss: 0.10371, policy_loss: -31.39859, policy_entropy: -6.07848, alpha: 0.00688, time: 33.80003
[CW] ---------------------------
[CW] ---- Iteration:   851 ----
[CW] collect: return: 210.55229, steps: 1000.00000, total_steps: 857000.00000
[CW] train: qf1_loss: 0.09964, qf2_loss: 0.09959, policy_loss: -31.41286, policy_entropy: -6.01154, alpha: 0.00693, time: 34.14210
[CW] ---------------------------
[CW] ---- Iteration:   852 ----
[CW] collect: return: 211.25118, steps: 1000.00000, total_steps: 858000.00000
[CW] train: qf1_loss: 0.11001, qf2_loss: 0.11021, policy_loss: -31.48300, policy_entropy: -5.98112, alpha: 0.00691, time: 33.99971
[CW] ---------------------------
[CW] ---- Iteration:   853 ----
[CW] collect: return: 206.23790, steps: 1000.00000, total_steps: 859000.00000
[CW] train: qf1_loss: 0.13368, qf2_loss: 0.13303, policy_loss: -31.41200, policy_entropy: -6.08490, alpha: 0.00692, time: 34.11686
[CW] ---------------------------
[CW] ---- Iteration:   854 ----
[CW] collect: return: 206.90564, steps: 1000.00000, total_steps: 860000.00000
[CW] train: qf1_loss: 0.17067, qf2_loss: 0.16863, policy_loss: -31.43372, policy_entropy: -6.07745, alpha: 0.00697, time: 33.69731
[CW] ---------------------------
[CW] ---- Iteration:   855 ----
[CW] collect: return: 198.19095, steps: 1000.00000, total_steps: 861000.00000
[CW] train: qf1_loss: 0.10066, qf2_loss: 0.10077, policy_loss: -31.44117, policy_entropy: -5.99927, alpha: 0.00699, time: 33.71238
[CW] ---------------------------
[CW] ---- Iteration:   856 ----
[CW] collect: return: 201.27160, steps: 1000.00000, total_steps: 862000.00000
[CW] train: qf1_loss: 0.09732, qf2_loss: 0.09744, policy_loss: -31.49699, policy_entropy: -6.05210, alpha: 0.00702, time: 34.06113
[CW] ---------------------------
[CW] ---- Iteration:   857 ----
[CW] collect: return: 203.64000, steps: 1000.00000, total_steps: 863000.00000
[CW] train: qf1_loss: 0.09680, qf2_loss: 0.09717, policy_loss: -31.53400, policy_entropy: -6.09736, alpha: 0.00707, time: 33.84445
[CW] ---------------------------
[CW] ---- Iteration:   858 ----
[CW] collect: return: 210.66707, steps: 1000.00000, total_steps: 864000.00000
[CW] train: qf1_loss: 0.09504, qf2_loss: 0.09445, policy_loss: -31.56768, policy_entropy: -6.04161, alpha: 0.00710, time: 33.97753
[CW] ---------------------------
[CW] ---- Iteration:   859 ----
[CW] collect: return: 207.78807, steps: 1000.00000, total_steps: 865000.00000
[CW] train: qf1_loss: 0.10647, qf2_loss: 0.10587, policy_loss: -31.64343, policy_entropy: -5.98575, alpha: 0.00712, time: 34.10976
[CW] ---------------------------
[CW] ---- Iteration:   860 ----
[CW] collect: return: 206.31461, steps: 1000.00000, total_steps: 866000.00000
[CW] train: qf1_loss: 0.10112, qf2_loss: 0.10092, policy_loss: -31.63090, policy_entropy: -5.99918, alpha: 0.00712, time: 33.89626
[CW] eval: return: 208.06229, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   861 ----
[CW] collect: return: 212.66611, steps: 1000.00000, total_steps: 867000.00000
[CW] train: qf1_loss: 0.09882, qf2_loss: 0.09825, policy_loss: -31.55776, policy_entropy: -6.00529, alpha: 0.00711, time: 34.03073
[CW] ---------------------------
[CW] ---- Iteration:   862 ----
[CW] collect: return: 190.63637, steps: 1000.00000, total_steps: 868000.00000
[CW] train: qf1_loss: 0.09855, qf2_loss: 0.09885, policy_loss: -31.61300, policy_entropy: -6.01115, alpha: 0.00711, time: 34.14252
[CW] ---------------------------
[CW] ---- Iteration:   863 ----
[CW] collect: return: 201.10550, steps: 1000.00000, total_steps: 869000.00000
[CW] train: qf1_loss: 0.11839, qf2_loss: 0.11948, policy_loss: -31.63838, policy_entropy: -6.08172, alpha: 0.00713, time: 34.25363
[CW] ---------------------------
[CW] ---- Iteration:   864 ----
[CW] collect: return: 188.15119, steps: 1000.00000, total_steps: 870000.00000
[CW] train: qf1_loss: 0.10390, qf2_loss: 0.10275, policy_loss: -31.58789, policy_entropy: -6.14695, alpha: 0.00720, time: 33.91499
[CW] ---------------------------
[CW] ---- Iteration:   865 ----
[CW] collect: return: 203.58608, steps: 1000.00000, total_steps: 871000.00000
[CW] train: qf1_loss: 0.10286, qf2_loss: 0.10181, policy_loss: -31.59200, policy_entropy: -6.05802, alpha: 0.00727, time: 33.48046
[CW] ---------------------------
[CW] ---- Iteration:   866 ----
[CW] collect: return: 206.71413, steps: 1000.00000, total_steps: 872000.00000
[CW] train: qf1_loss: 0.09886, qf2_loss: 0.09825, policy_loss: -31.55265, policy_entropy: -5.86442, alpha: 0.00727, time: 33.91856
[CW] ---------------------------
[CW] ---- Iteration:   867 ----
[CW] collect: return: 191.66791, steps: 1000.00000, total_steps: 873000.00000
[CW] train: qf1_loss: 0.10047, qf2_loss: 0.10044, policy_loss: -31.65141, policy_entropy: -5.99308, alpha: 0.00721, time: 34.14386
[CW] ---------------------------
[CW] ---- Iteration:   868 ----
[CW] collect: return: 205.56738, steps: 1000.00000, total_steps: 874000.00000
[CW] train: qf1_loss: 0.10174, qf2_loss: 0.10150, policy_loss: -31.66043, policy_entropy: -6.08138, alpha: 0.00724, time: 34.58175
[CW] ---------------------------
[CW] ---- Iteration:   869 ----
[CW] collect: return: 203.36790, steps: 1000.00000, total_steps: 875000.00000
[CW] train: qf1_loss: 0.10812, qf2_loss: 0.10913, policy_loss: -31.84330, policy_entropy: -5.96184, alpha: 0.00725, time: 34.25128
[CW] ---------------------------
[CW] ---- Iteration:   870 ----
[CW] collect: return: 193.97301, steps: 1000.00000, total_steps: 876000.00000
[CW] train: qf1_loss: 0.10307, qf2_loss: 0.10233, policy_loss: -31.74764, policy_entropy: -6.03038, alpha: 0.00725, time: 33.94029
[CW] ---------------------------
[CW] ---- Iteration:   871 ----
[CW] collect: return: 208.64254, steps: 1000.00000, total_steps: 877000.00000
[CW] train: qf1_loss: 0.11043, qf2_loss: 0.10855, policy_loss: -31.84476, policy_entropy: -5.96743, alpha: 0.00723, time: 34.16036
[CW] ---------------------------
[CW] ---- Iteration:   872 ----
[CW] collect: return: 182.75982, steps: 1000.00000, total_steps: 878000.00000
[CW] train: qf1_loss: 0.10032, qf2_loss: 0.09951, policy_loss: -31.77049, policy_entropy: -5.99605, alpha: 0.00722, time: 34.20094
[CW] ---------------------------
[CW] ---- Iteration:   873 ----
[CW] collect: return: 206.04796, steps: 1000.00000, total_steps: 879000.00000
[CW] train: qf1_loss: 0.12458, qf2_loss: 0.12445, policy_loss: -31.85503, policy_entropy: -6.09009, alpha: 0.00727, time: 34.29120
[CW] ---------------------------
[CW] ---- Iteration:   874 ----
[CW] collect: return: 204.64986, steps: 1000.00000, total_steps: 880000.00000
[CW] train: qf1_loss: 0.12308, qf2_loss: 0.12475, policy_loss: -31.75659, policy_entropy: -6.00187, alpha: 0.00728, time: 34.13369
[CW] ---------------------------
[CW] ---- Iteration:   875 ----
[CW] collect: return: 193.45787, steps: 1000.00000, total_steps: 881000.00000
[CW] train: qf1_loss: 0.11558, qf2_loss: 0.11509, policy_loss: -31.79844, policy_entropy: -6.03673, alpha: 0.00732, time: 34.28003
[CW] ---------------------------
[CW] ---- Iteration:   876 ----
[CW] collect: return: 198.31095, steps: 1000.00000, total_steps: 882000.00000
[CW] train: qf1_loss: 0.09724, qf2_loss: 0.09728, policy_loss: -31.77039, policy_entropy: -6.02390, alpha: 0.00730, time: 34.05072
[CW] ---------------------------
[CW] ---- Iteration:   877 ----
[CW] collect: return: 203.96745, steps: 1000.00000, total_steps: 883000.00000
[CW] train: qf1_loss: 0.09841, qf2_loss: 0.09826, policy_loss: -31.87928, policy_entropy: -5.97591, alpha: 0.00732, time: 34.08602
[CW] ---------------------------
[CW] ---- Iteration:   878 ----
[CW] collect: return: 205.05087, steps: 1000.00000, total_steps: 884000.00000
[CW] train: qf1_loss: 0.11043, qf2_loss: 0.11000, policy_loss: -31.82162, policy_entropy: -5.98688, alpha: 0.00732, time: 34.52510
[CW] ---------------------------
[CW] ---- Iteration:   879 ----
[CW] collect: return: 204.37233, steps: 1000.00000, total_steps: 885000.00000
[CW] train: qf1_loss: 0.10089, qf2_loss: 0.10025, policy_loss: -31.81555, policy_entropy: -6.04041, alpha: 0.00732, time: 34.48747
[CW] ---------------------------
[CW] ---- Iteration:   880 ----
[CW] collect: return: 208.28574, steps: 1000.00000, total_steps: 886000.00000
[CW] train: qf1_loss: 0.11723, qf2_loss: 0.11536, policy_loss: -31.95149, policy_entropy: -5.91685, alpha: 0.00735, time: 34.01444
[CW] eval: return: 203.43366, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   881 ----
[CW] collect: return: 209.56757, steps: 1000.00000, total_steps: 887000.00000
[CW] train: qf1_loss: 0.10344, qf2_loss: 0.10389, policy_loss: -31.90756, policy_entropy: -5.83614, alpha: 0.00725, time: 34.13024
[CW] ---------------------------
[CW] ---- Iteration:   882 ----
[CW] collect: return: 212.72706, steps: 1000.00000, total_steps: 888000.00000
[CW] train: qf1_loss: 0.09675, qf2_loss: 0.09623, policy_loss: -31.93381, policy_entropy: -5.95574, alpha: 0.00717, time: 33.69950
[CW] ---------------------------
[CW] ---- Iteration:   883 ----
[CW] collect: return: 186.76642, steps: 1000.00000, total_steps: 889000.00000
[CW] train: qf1_loss: 0.10168, qf2_loss: 0.10137, policy_loss: -31.96756, policy_entropy: -6.02149, alpha: 0.00716, time: 33.77664
[CW] ---------------------------
[CW] ---- Iteration:   884 ----
[CW] collect: return: 209.71700, steps: 1000.00000, total_steps: 890000.00000
[CW] train: qf1_loss: 0.10334, qf2_loss: 0.10286, policy_loss: -32.00764, policy_entropy: -5.96820, alpha: 0.00717, time: 33.73993
[CW] ---------------------------
[CW] ---- Iteration:   885 ----
[CW] collect: return: 210.30100, steps: 1000.00000, total_steps: 891000.00000
[CW] train: qf1_loss: 0.10491, qf2_loss: 0.10420, policy_loss: -31.94603, policy_entropy: -6.04796, alpha: 0.00715, time: 33.99674
[CW] ---------------------------
[CW] ---- Iteration:   886 ----
[CW] collect: return: 205.59390, steps: 1000.00000, total_steps: 892000.00000
[CW] train: qf1_loss: 0.10358, qf2_loss: 0.10332, policy_loss: -31.89172, policy_entropy: -6.12295, alpha: 0.00720, time: 34.37782
[CW] ---------------------------
[CW] ---- Iteration:   887 ----
[CW] collect: return: 207.60709, steps: 1000.00000, total_steps: 893000.00000
[CW] train: qf1_loss: 0.10211, qf2_loss: 0.10197, policy_loss: -31.98029, policy_entropy: -6.11824, alpha: 0.00729, time: 34.00711
[CW] ---------------------------
[CW] ---- Iteration:   888 ----
[CW] collect: return: 201.83507, steps: 1000.00000, total_steps: 894000.00000
[CW] train: qf1_loss: 0.11751, qf2_loss: 0.11830, policy_loss: -32.05857, policy_entropy: -5.93565, alpha: 0.00731, time: 34.19741
[CW] ---------------------------
[CW] ---- Iteration:   889 ----
[CW] collect: return: 203.65386, steps: 1000.00000, total_steps: 895000.00000
[CW] train: qf1_loss: 0.10544, qf2_loss: 0.10464, policy_loss: -32.04358, policy_entropy: -5.91969, alpha: 0.00726, time: 34.12885
[CW] ---------------------------
[CW] ---- Iteration:   890 ----
[CW] collect: return: 202.67256, steps: 1000.00000, total_steps: 896000.00000
[CW] train: qf1_loss: 0.10685, qf2_loss: 0.10567, policy_loss: -32.00228, policy_entropy: -5.92506, alpha: 0.00721, time: 34.20382
[CW] ---------------------------
[CW] ---- Iteration:   891 ----
[CW] collect: return: 207.60131, steps: 1000.00000, total_steps: 897000.00000
[CW] train: qf1_loss: 0.11226, qf2_loss: 0.11312, policy_loss: -32.03650, policy_entropy: -6.04581, alpha: 0.00719, time: 34.09569
[CW] ---------------------------
[CW] ---- Iteration:   892 ----
[CW] collect: return: 207.67426, steps: 1000.00000, total_steps: 898000.00000
[CW] train: qf1_loss: 0.11475, qf2_loss: 0.11371, policy_loss: -32.16770, policy_entropy: -6.07306, alpha: 0.00724, time: 33.94755
[CW] ---------------------------
[CW] ---- Iteration:   893 ----
[CW] collect: return: 211.25213, steps: 1000.00000, total_steps: 899000.00000
[CW] train: qf1_loss: 0.09695, qf2_loss: 0.09700, policy_loss: -32.06336, policy_entropy: -6.05661, alpha: 0.00727, time: 33.89111
[CW] ---------------------------
[CW] ---- Iteration:   894 ----
[CW] collect: return: 209.57568, steps: 1000.00000, total_steps: 900000.00000
[CW] train: qf1_loss: 0.10269, qf2_loss: 0.10247, policy_loss: -32.12970, policy_entropy: -6.04362, alpha: 0.00731, time: 33.86472
[CW] ---------------------------
[CW] ---- Iteration:   895 ----
[CW] collect: return: 203.69435, steps: 1000.00000, total_steps: 901000.00000
[CW] train: qf1_loss: 0.10774, qf2_loss: 0.10658, policy_loss: -32.16848, policy_entropy: -5.94740, alpha: 0.00730, time: 33.86026
[CW] ---------------------------
[CW] ---- Iteration:   896 ----
[CW] collect: return: 210.39316, steps: 1000.00000, total_steps: 902000.00000
[CW] train: qf1_loss: 0.09547, qf2_loss: 0.09479, policy_loss: -32.19478, policy_entropy: -6.01799, alpha: 0.00729, time: 34.04302
[CW] ---------------------------
[CW] ---- Iteration:   897 ----
[CW] collect: return: 203.03113, steps: 1000.00000, total_steps: 903000.00000
[CW] train: qf1_loss: 0.09638, qf2_loss: 0.09637, policy_loss: -32.18901, policy_entropy: -6.03658, alpha: 0.00731, time: 33.74009
[CW] ---------------------------
[CW] ---- Iteration:   898 ----
[CW] collect: return: 198.89759, steps: 1000.00000, total_steps: 904000.00000
[CW] train: qf1_loss: 0.11331, qf2_loss: 0.11335, policy_loss: -32.20443, policy_entropy: -6.08034, alpha: 0.00735, time: 33.79769
[CW] ---------------------------
[CW] ---- Iteration:   899 ----
[CW] collect: return: 199.80247, steps: 1000.00000, total_steps: 905000.00000
[CW] train: qf1_loss: 0.10461, qf2_loss: 0.10494, policy_loss: -32.19791, policy_entropy: -6.05645, alpha: 0.00741, time: 34.02295
[CW] ---------------------------
[CW] ---- Iteration:   900 ----
[CW] collect: return: 210.08005, steps: 1000.00000, total_steps: 906000.00000
[CW] train: qf1_loss: 0.10503, qf2_loss: 0.10483, policy_loss: -32.28033, policy_entropy: -6.06609, alpha: 0.00744, time: 33.78601
[CW] eval: return: 207.31388, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   901 ----
[CW] collect: return: 203.00580, steps: 1000.00000, total_steps: 907000.00000
[CW] train: qf1_loss: 0.10533, qf2_loss: 0.10565, policy_loss: -32.23266, policy_entropy: -6.02528, alpha: 0.00747, time: 34.11131
[CW] ---------------------------
[CW] ---- Iteration:   902 ----
[CW] collect: return: 203.70022, steps: 1000.00000, total_steps: 908000.00000
[CW] train: qf1_loss: 0.11406, qf2_loss: 0.11257, policy_loss: -32.30014, policy_entropy: -6.05425, alpha: 0.00749, time: 34.28709
[CW] ---------------------------
[CW] ---- Iteration:   903 ----
[CW] collect: return: 205.50286, steps: 1000.00000, total_steps: 909000.00000
[CW] train: qf1_loss: 0.10203, qf2_loss: 0.10198, policy_loss: -32.20120, policy_entropy: -6.02692, alpha: 0.00753, time: 34.02350
[CW] ---------------------------
[CW] ---- Iteration:   904 ----
[CW] collect: return: 204.31027, steps: 1000.00000, total_steps: 910000.00000
[CW] train: qf1_loss: 0.09767, qf2_loss: 0.09714, policy_loss: -32.31985, policy_entropy: -5.94222, alpha: 0.00753, time: 34.38028
[CW] ---------------------------
[CW] ---- Iteration:   905 ----
[CW] collect: return: 210.41895, steps: 1000.00000, total_steps: 911000.00000
[CW] train: qf1_loss: 0.10350, qf2_loss: 0.10349, policy_loss: -32.24495, policy_entropy: -5.95612, alpha: 0.00747, time: 34.22661
[CW] ---------------------------
[CW] ---- Iteration:   906 ----
[CW] collect: return: 195.67052, steps: 1000.00000, total_steps: 912000.00000
[CW] train: qf1_loss: 0.11514, qf2_loss: 0.11523, policy_loss: -32.35698, policy_entropy: -5.98711, alpha: 0.00745, time: 34.46900
[CW] ---------------------------
[CW] ---- Iteration:   907 ----
[CW] collect: return: 213.45282, steps: 1000.00000, total_steps: 913000.00000
[CW] train: qf1_loss: 0.11176, qf2_loss: 0.11046, policy_loss: -32.30890, policy_entropy: -5.93166, alpha: 0.00742, time: 34.21701
[CW] ---------------------------
[CW] ---- Iteration:   908 ----
[CW] collect: return: 206.07822, steps: 1000.00000, total_steps: 914000.00000
[CW] train: qf1_loss: 0.10647, qf2_loss: 0.10511, policy_loss: -32.30295, policy_entropy: -6.01088, alpha: 0.00742, time: 34.42569
[CW] ---------------------------
[CW] ---- Iteration:   909 ----
[CW] collect: return: 216.30892, steps: 1000.00000, total_steps: 915000.00000
[CW] train: qf1_loss: 0.10810, qf2_loss: 0.10771, policy_loss: -32.32246, policy_entropy: -6.04424, alpha: 0.00742, time: 34.92758
[CW] ---------------------------
[CW] ---- Iteration:   910 ----
[CW] collect: return: 208.78640, steps: 1000.00000, total_steps: 916000.00000
[CW] train: qf1_loss: 0.10807, qf2_loss: 0.10782, policy_loss: -32.34870, policy_entropy: -6.02003, alpha: 0.00745, time: 34.02683
[CW] ---------------------------
[CW] ---- Iteration:   911 ----
[CW] collect: return: 192.75893, steps: 1000.00000, total_steps: 917000.00000
[CW] train: qf1_loss: 0.09687, qf2_loss: 0.09720, policy_loss: -32.46001, policy_entropy: -6.04643, alpha: 0.00748, time: 34.26364
[CW] ---------------------------
[CW] ---- Iteration:   912 ----
[CW] collect: return: 210.81365, steps: 1000.00000, total_steps: 918000.00000
[CW] train: qf1_loss: 0.09455, qf2_loss: 0.09511, policy_loss: -32.38501, policy_entropy: -5.99619, alpha: 0.00749, time: 34.08889
[CW] ---------------------------
[CW] ---- Iteration:   913 ----
[CW] collect: return: 203.13546, steps: 1000.00000, total_steps: 919000.00000
[CW] train: qf1_loss: 0.10164, qf2_loss: 0.10130, policy_loss: -32.34981, policy_entropy: -5.95716, alpha: 0.00748, time: 34.27180
[CW] ---------------------------
[CW] ---- Iteration:   914 ----
[CW] collect: return: 203.32857, steps: 1000.00000, total_steps: 920000.00000
[CW] train: qf1_loss: 0.09757, qf2_loss: 0.09647, policy_loss: -32.45638, policy_entropy: -6.06971, alpha: 0.00748, time: 34.52478
[CW] ---------------------------
[CW] ---- Iteration:   915 ----
[CW] collect: return: 203.75662, steps: 1000.00000, total_steps: 921000.00000
[CW] train: qf1_loss: 0.10254, qf2_loss: 0.10219, policy_loss: -32.43054, policy_entropy: -5.97062, alpha: 0.00751, time: 34.41349
[CW] ---------------------------
[CW] ---- Iteration:   916 ----
[CW] collect: return: 198.38517, steps: 1000.00000, total_steps: 922000.00000
[CW] train: qf1_loss: 0.11016, qf2_loss: 0.10955, policy_loss: -32.45100, policy_entropy: -6.17719, alpha: 0.00753, time: 34.42399
[CW] ---------------------------
[CW] ---- Iteration:   917 ----
[CW] collect: return: 200.56957, steps: 1000.00000, total_steps: 923000.00000
[CW] train: qf1_loss: 0.12761, qf2_loss: 0.12827, policy_loss: -32.57360, policy_entropy: -6.06410, alpha: 0.00762, time: 34.26627
[CW] ---------------------------
[CW] ---- Iteration:   918 ----
[CW] collect: return: 204.87885, steps: 1000.00000, total_steps: 924000.00000
[CW] train: qf1_loss: 0.11420, qf2_loss: 0.11538, policy_loss: -32.59199, policy_entropy: -6.02647, alpha: 0.00767, time: 34.23697
[CW] ---------------------------
[CW] ---- Iteration:   919 ----
[CW] collect: return: 206.62512, steps: 1000.00000, total_steps: 925000.00000
[CW] train: qf1_loss: 0.09387, qf2_loss: 0.09282, policy_loss: -32.48815, policy_entropy: -6.04117, alpha: 0.00769, time: 34.58470
[CW] ---------------------------
[CW] ---- Iteration:   920 ----
[CW] collect: return: 208.93483, steps: 1000.00000, total_steps: 926000.00000
[CW] train: qf1_loss: 0.09639, qf2_loss: 0.09551, policy_loss: -32.47457, policy_entropy: -5.99414, alpha: 0.00770, time: 34.04080
[CW] eval: return: 205.15026, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   921 ----
[CW] collect: return: 209.48593, steps: 1000.00000, total_steps: 927000.00000
[CW] train: qf1_loss: 0.09169, qf2_loss: 0.09221, policy_loss: -32.56577, policy_entropy: -6.01693, alpha: 0.00770, time: 34.34977
[CW] ---------------------------
[CW] ---- Iteration:   922 ----
[CW] collect: return: 209.19301, steps: 1000.00000, total_steps: 928000.00000
[CW] train: qf1_loss: 0.08798, qf2_loss: 0.08809, policy_loss: -32.58323, policy_entropy: -5.85599, alpha: 0.00767, time: 34.06599
[CW] ---------------------------
[CW] ---- Iteration:   923 ----
[CW] collect: return: 196.95139, steps: 1000.00000, total_steps: 929000.00000
[CW] train: qf1_loss: 0.11465, qf2_loss: 0.11326, policy_loss: -32.60093, policy_entropy: -5.83889, alpha: 0.00755, time: 33.99867
[CW] ---------------------------
[CW] ---- Iteration:   924 ----
[CW] collect: return: 206.90734, steps: 1000.00000, total_steps: 930000.00000
[CW] train: qf1_loss: 0.11185, qf2_loss: 0.11292, policy_loss: -32.59163, policy_entropy: -5.91951, alpha: 0.00746, time: 34.13106
[CW] ---------------------------
[CW] ---- Iteration:   925 ----
[CW] collect: return: 208.98096, steps: 1000.00000, total_steps: 931000.00000
[CW] train: qf1_loss: 0.10126, qf2_loss: 0.09979, policy_loss: -32.63847, policy_entropy: -5.91418, alpha: 0.00742, time: 34.39958
[CW] ---------------------------
[CW] ---- Iteration:   926 ----
[CW] collect: return: 209.93443, steps: 1000.00000, total_steps: 932000.00000
[CW] train: qf1_loss: 0.09353, qf2_loss: 0.09406, policy_loss: -32.64063, policy_entropy: -6.00410, alpha: 0.00739, time: 33.81903
[CW] ---------------------------
[CW] ---- Iteration:   927 ----
[CW] collect: return: 209.75985, steps: 1000.00000, total_steps: 933000.00000
[CW] train: qf1_loss: 0.09258, qf2_loss: 0.09319, policy_loss: -32.62452, policy_entropy: -6.05573, alpha: 0.00738, time: 33.34720
[CW] ---------------------------
[CW] ---- Iteration:   928 ----
[CW] collect: return: 208.01570, steps: 1000.00000, total_steps: 934000.00000
[CW] train: qf1_loss: 0.09429, qf2_loss: 0.09419, policy_loss: -32.64746, policy_entropy: -6.08618, alpha: 0.00744, time: 34.02550
[CW] ---------------------------
[CW] ---- Iteration:   929 ----
[CW] collect: return: 212.70177, steps: 1000.00000, total_steps: 935000.00000
[CW] train: qf1_loss: 0.09998, qf2_loss: 0.09863, policy_loss: -32.69752, policy_entropy: -6.02668, alpha: 0.00747, time: 34.09671
[CW] ---------------------------
[CW] ---- Iteration:   930 ----
[CW] collect: return: 201.13668, steps: 1000.00000, total_steps: 936000.00000
[CW] train: qf1_loss: 0.09326, qf2_loss: 0.09418, policy_loss: -32.65989, policy_entropy: -6.00987, alpha: 0.00750, time: 33.94600
[CW] ---------------------------
[CW] ---- Iteration:   931 ----
[CW] collect: return: 206.65796, steps: 1000.00000, total_steps: 937000.00000
[CW] train: qf1_loss: 0.09770, qf2_loss: 0.09679, policy_loss: -32.68084, policy_entropy: -6.00778, alpha: 0.00750, time: 34.30231
[CW] ---------------------------
[CW] ---- Iteration:   932 ----
[CW] collect: return: 199.51471, steps: 1000.00000, total_steps: 938000.00000
[CW] train: qf1_loss: 0.10018, qf2_loss: 0.09988, policy_loss: -32.59600, policy_entropy: -5.96313, alpha: 0.00750, time: 33.99471
[CW] ---------------------------
[CW] ---- Iteration:   933 ----
[CW] collect: return: 208.40636, steps: 1000.00000, total_steps: 939000.00000
[CW] train: qf1_loss: 0.09604, qf2_loss: 0.09553, policy_loss: -32.74443, policy_entropy: -6.06140, alpha: 0.00750, time: 34.14986
[CW] ---------------------------
[CW] ---- Iteration:   934 ----
[CW] collect: return: 202.97103, steps: 1000.00000, total_steps: 940000.00000
[CW] train: qf1_loss: 0.09317, qf2_loss: 0.09259, policy_loss: -32.73792, policy_entropy: -6.03304, alpha: 0.00752, time: 34.87168
[CW] ---------------------------
[CW] ---- Iteration:   935 ----
[CW] collect: return: 205.11280, steps: 1000.00000, total_steps: 941000.00000
[CW] train: qf1_loss: 0.09859, qf2_loss: 0.09856, policy_loss: -32.77203, policy_entropy: -6.09759, alpha: 0.00757, time: 34.16280
[CW] ---------------------------
[CW] ---- Iteration:   936 ----
[CW] collect: return: 190.57069, steps: 1000.00000, total_steps: 942000.00000
[CW] train: qf1_loss: 0.09641, qf2_loss: 0.09606, policy_loss: -32.69466, policy_entropy: -5.93546, alpha: 0.00760, time: 34.03838
[CW] ---------------------------
[CW] ---- Iteration:   937 ----
[CW] collect: return: 212.88895, steps: 1000.00000, total_steps: 943000.00000
[CW] train: qf1_loss: 0.09019, qf2_loss: 0.08944, policy_loss: -32.87540, policy_entropy: -6.05006, alpha: 0.00759, time: 34.10946
[CW] ---------------------------
[CW] ---- Iteration:   938 ----
[CW] collect: return: 209.16660, steps: 1000.00000, total_steps: 944000.00000
[CW] train: qf1_loss: 0.09548, qf2_loss: 0.09468, policy_loss: -32.73666, policy_entropy: -6.06166, alpha: 0.00762, time: 34.07773
[CW] ---------------------------
[CW] ---- Iteration:   939 ----
[CW] collect: return: 202.72938, steps: 1000.00000, total_steps: 945000.00000
[CW] train: qf1_loss: 0.10348, qf2_loss: 0.10333, policy_loss: -32.73145, policy_entropy: -5.99746, alpha: 0.00767, time: 35.30575
[CW] ---------------------------
[CW] ---- Iteration:   940 ----
[CW] collect: return: 201.82174, steps: 1000.00000, total_steps: 946000.00000
[CW] train: qf1_loss: 0.10597, qf2_loss: 0.10457, policy_loss: -32.85777, policy_entropy: -5.97523, alpha: 0.00765, time: 36.43194
[CW] eval: return: 202.53462, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   941 ----
[CW] collect: return: 201.11609, steps: 1000.00000, total_steps: 947000.00000
[CW] train: qf1_loss: 0.12167, qf2_loss: 0.12262, policy_loss: -32.84073, policy_entropy: -6.03732, alpha: 0.00765, time: 33.99195
[CW] ---------------------------
[CW] ---- Iteration:   942 ----
[CW] collect: return: 193.38267, steps: 1000.00000, total_steps: 948000.00000
[CW] train: qf1_loss: 0.08962, qf2_loss: 0.09046, policy_loss: -32.89436, policy_entropy: -6.13065, alpha: 0.00771, time: 33.96524
[CW] ---------------------------
[CW] ---- Iteration:   943 ----
[CW] collect: return: 209.37623, steps: 1000.00000, total_steps: 949000.00000
[CW] train: qf1_loss: 0.08677, qf2_loss: 0.08716, policy_loss: -32.91879, policy_entropy: -6.01906, alpha: 0.00777, time: 34.36524
[CW] ---------------------------
[CW] ---- Iteration:   944 ----
[CW] collect: return: 206.89383, steps: 1000.00000, total_steps: 950000.00000
[CW] train: qf1_loss: 0.09205, qf2_loss: 0.09160, policy_loss: -32.94843, policy_entropy: -5.87983, alpha: 0.00774, time: 34.53510
[CW] ---------------------------
[CW] ---- Iteration:   945 ----
[CW] collect: return: 193.69939, steps: 1000.00000, total_steps: 951000.00000
[CW] train: qf1_loss: 0.09710, qf2_loss: 0.09624, policy_loss: -32.93953, policy_entropy: -5.88042, alpha: 0.00766, time: 34.39497
[CW] ---------------------------
[CW] ---- Iteration:   946 ----
[CW] collect: return: 197.77144, steps: 1000.00000, total_steps: 952000.00000
[CW] train: qf1_loss: 0.10670, qf2_loss: 0.10677, policy_loss: -32.97101, policy_entropy: -5.88422, alpha: 0.00756, time: 33.81689
[CW] ---------------------------
[CW] ---- Iteration:   947 ----
[CW] collect: return: 206.56250, steps: 1000.00000, total_steps: 953000.00000
[CW] train: qf1_loss: 0.09581, qf2_loss: 0.09465, policy_loss: -32.94229, policy_entropy: -5.88488, alpha: 0.00746, time: 34.02131
[CW] ---------------------------
[CW] ---- Iteration:   948 ----
[CW] collect: return: 203.18920, steps: 1000.00000, total_steps: 954000.00000
[CW] train: qf1_loss: 0.09267, qf2_loss: 0.09372, policy_loss: -32.84812, policy_entropy: -6.00885, alpha: 0.00744, time: 34.72635
[CW] ---------------------------
[CW] ---- Iteration:   949 ----
[CW] collect: return: 207.92727, steps: 1000.00000, total_steps: 955000.00000
[CW] train: qf1_loss: 0.10202, qf2_loss: 0.10089, policy_loss: -32.92734, policy_entropy: -5.95340, alpha: 0.00744, time: 34.46224
[CW] ---------------------------
[CW] ---- Iteration:   950 ----
[CW] collect: return: 203.16670, steps: 1000.00000, total_steps: 956000.00000
[CW] train: qf1_loss: 0.11124, qf2_loss: 0.11009, policy_loss: -32.88262, policy_entropy: -6.00916, alpha: 0.00741, time: 34.08825
[CW] ---------------------------
[CW] ---- Iteration:   951 ----
[CW] collect: return: 217.28585, steps: 1000.00000, total_steps: 957000.00000
[CW] train: qf1_loss: 0.10518, qf2_loss: 0.10575, policy_loss: -33.02209, policy_entropy: -5.98106, alpha: 0.00740, time: 33.64440
[CW] ---------------------------
[CW] ---- Iteration:   952 ----
[CW] collect: return: 191.95625, steps: 1000.00000, total_steps: 958000.00000
[CW] train: qf1_loss: 0.09725, qf2_loss: 0.09774, policy_loss: -32.95862, policy_entropy: -6.07664, alpha: 0.00743, time: 34.30179
[CW] ---------------------------
[CW] ---- Iteration:   953 ----
[CW] collect: return: 204.11047, steps: 1000.00000, total_steps: 959000.00000
[CW] train: qf1_loss: 0.10253, qf2_loss: 0.10068, policy_loss: -33.01267, policy_entropy: -6.00315, alpha: 0.00745, time: 34.76652
[CW] ---------------------------
[CW] ---- Iteration:   954 ----
[CW] collect: return: 202.86814, steps: 1000.00000, total_steps: 960000.00000
[CW] train: qf1_loss: 0.08358, qf2_loss: 0.08390, policy_loss: -33.05628, policy_entropy: -6.11913, alpha: 0.00749, time: 34.54462
[CW] ---------------------------
[CW] ---- Iteration:   955 ----
[CW] collect: return: 198.43537, steps: 1000.00000, total_steps: 961000.00000
[CW] train: qf1_loss: 0.08994, qf2_loss: 0.08918, policy_loss: -32.99021, policy_entropy: -6.03393, alpha: 0.00755, time: 34.19994
[CW] ---------------------------
[CW] ---- Iteration:   956 ----
[CW] collect: return: 200.64266, steps: 1000.00000, total_steps: 962000.00000
[CW] train: qf1_loss: 0.08845, qf2_loss: 0.08804, policy_loss: -33.07539, policy_entropy: -5.99946, alpha: 0.00758, time: 34.25229
[CW] ---------------------------
[CW] ---- Iteration:   957 ----
[CW] collect: return: 206.18485, steps: 1000.00000, total_steps: 963000.00000
[CW] train: qf1_loss: 0.08876, qf2_loss: 0.08809, policy_loss: -33.05601, policy_entropy: -5.91291, alpha: 0.00754, time: 34.35339
[CW] ---------------------------
[CW] ---- Iteration:   958 ----
[CW] collect: return: 202.13627, steps: 1000.00000, total_steps: 964000.00000
[CW] train: qf1_loss: 0.08929, qf2_loss: 0.08880, policy_loss: -33.03190, policy_entropy: -6.09752, alpha: 0.00755, time: 34.44189
[CW] ---------------------------
[CW] ---- Iteration:   959 ----
[CW] collect: return: 201.80574, steps: 1000.00000, total_steps: 965000.00000
[CW] train: qf1_loss: 0.08930, qf2_loss: 0.08937, policy_loss: -33.17471, policy_entropy: -6.00299, alpha: 0.00759, time: 34.28563
[CW] ---------------------------
[CW] ---- Iteration:   960 ----
[CW] collect: return: 205.21353, steps: 1000.00000, total_steps: 966000.00000
[CW] train: qf1_loss: 0.09007, qf2_loss: 0.09005, policy_loss: -33.08808, policy_entropy: -5.91046, alpha: 0.00756, time: 34.44641
[CW] eval: return: 206.44789, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   961 ----
[CW] collect: return: 199.69503, steps: 1000.00000, total_steps: 967000.00000
[CW] train: qf1_loss: 0.09122, qf2_loss: 0.09012, policy_loss: -33.06992, policy_entropy: -5.94595, alpha: 0.00750, time: 34.49940
[CW] ---------------------------
[CW] ---- Iteration:   962 ----
[CW] collect: return: 209.05648, steps: 1000.00000, total_steps: 968000.00000
[CW] train: qf1_loss: 0.09204, qf2_loss: 0.09136, policy_loss: -33.18517, policy_entropy: -6.00749, alpha: 0.00748, time: 34.32327
[CW] ---------------------------
[CW] ---- Iteration:   963 ----
[CW] collect: return: 204.41815, steps: 1000.00000, total_steps: 969000.00000
[CW] train: qf1_loss: 0.09966, qf2_loss: 0.10050, policy_loss: -33.14056, policy_entropy: -5.91705, alpha: 0.00746, time: 34.28248
[CW] ---------------------------
[CW] ---- Iteration:   964 ----
[CW] collect: return: 209.06677, steps: 1000.00000, total_steps: 970000.00000
[CW] train: qf1_loss: 0.09562, qf2_loss: 0.09460, policy_loss: -33.10319, policy_entropy: -5.94994, alpha: 0.00740, time: 33.97499
[CW] ---------------------------
[CW] ---- Iteration:   965 ----
[CW] collect: return: 202.85885, steps: 1000.00000, total_steps: 971000.00000
[CW] train: qf1_loss: 0.09605, qf2_loss: 0.09472, policy_loss: -33.19225, policy_entropy: -5.93561, alpha: 0.00738, time: 33.80347
[CW] ---------------------------
[CW] ---- Iteration:   966 ----
[CW] collect: return: 204.44710, steps: 1000.00000, total_steps: 972000.00000
[CW] train: qf1_loss: 0.09577, qf2_loss: 0.09614, policy_loss: -33.14600, policy_entropy: -6.01098, alpha: 0.00736, time: 34.17461
[CW] ---------------------------
[CW] ---- Iteration:   967 ----
[CW] collect: return: 203.95827, steps: 1000.00000, total_steps: 973000.00000
[CW] train: qf1_loss: 0.09531, qf2_loss: 0.09448, policy_loss: -33.13386, policy_entropy: -5.99830, alpha: 0.00735, time: 34.34227
[CW] ---------------------------
[CW] ---- Iteration:   968 ----
[CW] collect: return: 203.24324, steps: 1000.00000, total_steps: 974000.00000
[CW] train: qf1_loss: 0.09482, qf2_loss: 0.09464, policy_loss: -33.24983, policy_entropy: -5.90793, alpha: 0.00732, time: 34.22211
[CW] ---------------------------
[CW] ---- Iteration:   969 ----
[CW] collect: return: 204.87451, steps: 1000.00000, total_steps: 975000.00000
[CW] train: qf1_loss: 0.08667, qf2_loss: 0.08597, policy_loss: -33.20498, policy_entropy: -6.02419, alpha: 0.00730, time: 34.24913
[CW] ---------------------------
[CW] ---- Iteration:   970 ----
[CW] collect: return: 212.10005, steps: 1000.00000, total_steps: 976000.00000
[CW] train: qf1_loss: 0.08397, qf2_loss: 0.08405, policy_loss: -33.23667, policy_entropy: -5.98037, alpha: 0.00730, time: 34.02105
[CW] ---------------------------
[CW] ---- Iteration:   971 ----
[CW] collect: return: 213.32934, steps: 1000.00000, total_steps: 977000.00000
[CW] train: qf1_loss: 0.09425, qf2_loss: 0.09418, policy_loss: -33.31917, policy_entropy: -6.05395, alpha: 0.00731, time: 34.30260
[CW] ---------------------------
[CW] ---- Iteration:   972 ----
[CW] collect: return: 210.28103, steps: 1000.00000, total_steps: 978000.00000
[CW] train: qf1_loss: 0.08960, qf2_loss: 0.08900, policy_loss: -33.30210, policy_entropy: -6.00749, alpha: 0.00735, time: 34.26490
[CW] ---------------------------
[CW] ---- Iteration:   973 ----
[CW] collect: return: 201.26509, steps: 1000.00000, total_steps: 979000.00000
[CW] train: qf1_loss: 0.09383, qf2_loss: 0.09379, policy_loss: -33.20729, policy_entropy: -5.94484, alpha: 0.00732, time: 34.32111
[CW] ---------------------------
[CW] ---- Iteration:   974 ----
[CW] collect: return: 204.99980, steps: 1000.00000, total_steps: 980000.00000
[CW] train: qf1_loss: 0.08631, qf2_loss: 0.08665, policy_loss: -33.27455, policy_entropy: -6.02950, alpha: 0.00733, time: 34.36034
[CW] ---------------------------
[CW] ---- Iteration:   975 ----
[CW] collect: return: 199.23245, steps: 1000.00000, total_steps: 981000.00000
[CW] train: qf1_loss: 0.08607, qf2_loss: 0.08567, policy_loss: -33.27083, policy_entropy: -6.05207, alpha: 0.00734, time: 34.02041
[CW] ---------------------------
[CW] ---- Iteration:   976 ----
[CW] collect: return: 212.81391, steps: 1000.00000, total_steps: 982000.00000
[CW] train: qf1_loss: 0.08967, qf2_loss: 0.09011, policy_loss: -33.30315, policy_entropy: -6.12984, alpha: 0.00740, time: 34.10066
[CW] ---------------------------
[CW] ---- Iteration:   977 ----
[CW] collect: return: 214.43804, steps: 1000.00000, total_steps: 983000.00000
[CW] train: qf1_loss: 0.08830, qf2_loss: 0.08838, policy_loss: -33.37513, policy_entropy: -6.02554, alpha: 0.00746, time: 34.45075
[CW] ---------------------------
[CW] ---- Iteration:   978 ----
[CW] collect: return: 188.80512, steps: 1000.00000, total_steps: 984000.00000
[CW] train: qf1_loss: 0.08940, qf2_loss: 0.08969, policy_loss: -33.34465, policy_entropy: -5.92345, alpha: 0.00744, time: 34.48922
[CW] ---------------------------
[CW] ---- Iteration:   979 ----
[CW] collect: return: 201.24709, steps: 1000.00000, total_steps: 985000.00000
[CW] train: qf1_loss: 0.09333, qf2_loss: 0.09232, policy_loss: -33.39684, policy_entropy: -6.05758, alpha: 0.00741, time: 34.29624
[CW] ---------------------------
[CW] ---- Iteration:   980 ----
[CW] collect: return: 201.96097, steps: 1000.00000, total_steps: 986000.00000
[CW] train: qf1_loss: 0.09295, qf2_loss: 0.09293, policy_loss: -33.30133, policy_entropy: -6.03943, alpha: 0.00746, time: 34.96647
[CW] eval: return: 205.10500, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   981 ----
[CW] collect: return: 199.70512, steps: 1000.00000, total_steps: 987000.00000
[CW] train: qf1_loss: 0.09300, qf2_loss: 0.09076, policy_loss: -33.42103, policy_entropy: -5.92945, alpha: 0.00746, time: 34.18128
[CW] ---------------------------
[CW] ---- Iteration:   982 ----
[CW] collect: return: 195.33355, steps: 1000.00000, total_steps: 988000.00000
[CW] train: qf1_loss: 0.09067, qf2_loss: 0.09048, policy_loss: -33.39738, policy_entropy: -6.01926, alpha: 0.00743, time: 33.79496
[CW] ---------------------------
[CW] ---- Iteration:   983 ----
[CW] collect: return: 207.15129, steps: 1000.00000, total_steps: 989000.00000
[CW] train: qf1_loss: 0.08371, qf2_loss: 0.08290, policy_loss: -33.49791, policy_entropy: -6.01056, alpha: 0.00743, time: 33.67120
[CW] ---------------------------
[CW] ---- Iteration:   984 ----
[CW] collect: return: 206.38903, steps: 1000.00000, total_steps: 990000.00000
[CW] train: qf1_loss: 0.08967, qf2_loss: 0.09004, policy_loss: -33.32952, policy_entropy: -5.94552, alpha: 0.00743, time: 34.00283
[CW] ---------------------------
[CW] ---- Iteration:   985 ----
[CW] collect: return: 204.49520, steps: 1000.00000, total_steps: 991000.00000
[CW] train: qf1_loss: 0.09396, qf2_loss: 0.09432, policy_loss: -33.39180, policy_entropy: -5.98628, alpha: 0.00741, time: 34.61793
[CW] ---------------------------
[CW] ---- Iteration:   986 ----
[CW] collect: return: 204.67382, steps: 1000.00000, total_steps: 992000.00000
[CW] train: qf1_loss: 0.08685, qf2_loss: 0.08587, policy_loss: -33.48311, policy_entropy: -6.00346, alpha: 0.00740, time: 34.61703
[CW] ---------------------------
[CW] ---- Iteration:   987 ----
[CW] collect: return: 193.61368, steps: 1000.00000, total_steps: 993000.00000
[CW] train: qf1_loss: 0.08705, qf2_loss: 0.08717, policy_loss: -33.46651, policy_entropy: -5.89444, alpha: 0.00738, time: 34.78838
[CW] ---------------------------
[CW] ---- Iteration:   988 ----
[CW] collect: return: 207.24521, steps: 1000.00000, total_steps: 994000.00000
[CW] train: qf1_loss: 0.09025, qf2_loss: 0.08958, policy_loss: -33.50868, policy_entropy: -5.92695, alpha: 0.00730, time: 34.76143
[CW] ---------------------------
[CW] ---- Iteration:   989 ----
[CW] collect: return: 207.93611, steps: 1000.00000, total_steps: 995000.00000
[CW] train: qf1_loss: 0.08911, qf2_loss: 0.08867, policy_loss: -33.46301, policy_entropy: -6.01819, alpha: 0.00730, time: 34.28904
[CW] ---------------------------
[CW] ---- Iteration:   990 ----
[CW] collect: return: 202.21019, steps: 1000.00000, total_steps: 996000.00000
[CW] train: qf1_loss: 0.09572, qf2_loss: 0.09607, policy_loss: -33.47582, policy_entropy: -6.04634, alpha: 0.00731, time: 34.16521
[CW] ---------------------------
[CW] ---- Iteration:   991 ----
[CW] collect: return: 213.26522, steps: 1000.00000, total_steps: 997000.00000
[CW] train: qf1_loss: 0.08852, qf2_loss: 0.08818, policy_loss: -33.46874, policy_entropy: -6.01675, alpha: 0.00734, time: 34.19447
[CW] ---------------------------
[CW] ---- Iteration:   992 ----
[CW] collect: return: 200.87971, steps: 1000.00000, total_steps: 998000.00000
[CW] train: qf1_loss: 0.09005, qf2_loss: 0.08805, policy_loss: -33.48586, policy_entropy: -6.02806, alpha: 0.00733, time: 34.16819
[CW] ---------------------------
[CW] ---- Iteration:   993 ----
[CW] collect: return: 185.14126, steps: 1000.00000, total_steps: 999000.00000
[CW] train: qf1_loss: 0.08583, qf2_loss: 0.08668, policy_loss: -33.52213, policy_entropy: -5.87939, alpha: 0.00733, time: 35.23475
[CW] ---------------------------
[CW] ---- Iteration:   994 ----
[CW] collect: return: 202.32448, steps: 1000.00000, total_steps: 1000000.00000
[CW] train: qf1_loss: 0.08069, qf2_loss: 0.08079, policy_loss: -33.58302, policy_entropy: -5.90459, alpha: 0.00725, time: 34.18780
[CW] ---------------------------
[CW] ---- Iteration:   995 ----
[CW] collect: return: 198.80654, steps: 1000.00000, total_steps: 1001000.00000
[CW] train: qf1_loss: 0.08234, qf2_loss: 0.08054, policy_loss: -33.61097, policy_entropy: -6.00630, alpha: 0.00720, time: 34.37825
[CW] ---------------------------
[CW] ---- Iteration:   996 ----
[CW] collect: return: 207.23348, steps: 1000.00000, total_steps: 1002000.00000
[CW] train: qf1_loss: 0.09580, qf2_loss: 0.09477, policy_loss: -33.57089, policy_entropy: -5.92319, alpha: 0.00721, time: 34.07685
[CW] ---------------------------
[CW] ---- Iteration:   997 ----
[CW] collect: return: 206.54445, steps: 1000.00000, total_steps: 1003000.00000
[CW] train: qf1_loss: 0.08422, qf2_loss: 0.08371, policy_loss: -33.58738, policy_entropy: -5.98783, alpha: 0.00717, time: 34.46254
[CW] ---------------------------
[CW] ---- Iteration:   998 ----
[CW] collect: return: 204.34021, steps: 1000.00000, total_steps: 1004000.00000
[CW] train: qf1_loss: 0.09001, qf2_loss: 0.09052, policy_loss: -33.57898, policy_entropy: -5.94459, alpha: 0.00715, time: 34.67462
[CW] ---------------------------
[CW] ---- Iteration:   999 ----
[CW] collect: return: 193.15794, steps: 1000.00000, total_steps: 1005000.00000
[CW] train: qf1_loss: 0.09758, qf2_loss: 0.09659, policy_loss: -33.53577, policy_entropy: -5.85403, alpha: 0.00709, time: 34.37129
[CW] ---------------------------
[CW] ---- Iteration:  1000 ----
[CW] collect: return: 200.89023, steps: 1000.00000, total_steps: 1006000.00000
[CW] train: qf1_loss: 0.08435, qf2_loss: 0.08339, policy_loss: -33.60107, policy_entropy: -5.97826, alpha: 0.00703, time: 34.52706
[CW] eval: return: 208.19306, steps: 1000.00000
[CW] ---------------------------
