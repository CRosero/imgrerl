{"collect/return": 587.0714110219851, "collect/steps": 1000.0, "collect/total_steps": 508000.0, "train/qf1_loss": 9.67428524017334, "train/qf2_loss": 9.680174221992493, "train/policy_loss": -121.7474942779541, "train/policy_entropy": -6.141522188186645, "train/alpha": 0.04003548424690962, "train/time": 69.33248281478882, "eval/return": 467.1864634906582, "eval/steps": 1000.0, "_timestamp": 1678739067.647155, "_runtime": 35947.097208976746, "_step": 502}