{"collect/return": 843.7261414310196, "collect/steps": 1000.0, "collect/total_steps": 627000.0, "train/qf1_loss": 69.75773067474366, "train/qf2_loss": 69.81320339202881, "train/policy_loss": -541.5499291992187, "train/policy_entropy": -0.987995742559433, "train/alpha": 0.5790226483345031, "train/time": 44.44691705703735, "eval/return": 829.0441472098216, "eval/steps": 1000.0, "_timestamp": 1678761348.6419969, "_runtime": 28734.9617497921, "_step": 621}