{"collect/return": 589.7197480536997, "collect/steps": 1000.0, "collect/total_steps": 681000.0, "train/qf1_loss": 5.625088188648224, "train/qf2_loss": 5.571827964782715, "train/policy_loss": -168.7829832458496, "train/policy_entropy": -5.823335146903991, "train/alpha": 0.058062124773859976, "train/time": 51.49384260177612, "eval/return": 605.4110345769674, "eval/steps": 1000.0, "_timestamp": 1678738520.8042104, "_runtime": 35974.87101340294, "_step": 675}