[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 24.13289, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.75506, qf2_loss: 1.74257, policy_loss: -7.81870, policy_entropy: 4.09759, alpha: 0.98504, time: 58.71740
[CW] eval: return: 26.76596, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 25.67676, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.08347, qf2_loss: 0.08380, policy_loss: -8.50999, policy_entropy: 4.09970, alpha: 0.95626, time: 49.61225
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 31.25593, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.07558, qf2_loss: 0.07586, policy_loss: -9.21444, policy_entropy: 4.10096, alpha: 0.92871, time: 49.33267
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 24.38984, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.06813, qf2_loss: 0.06823, policy_loss: -10.14996, policy_entropy: 4.09947, alpha: 0.90231, time: 49.09072
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 24.46941, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06275, qf2_loss: 0.06277, policy_loss: -11.22267, policy_entropy: 4.10066, alpha: 0.87699, time: 49.08144
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 23.32973, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.05901, qf2_loss: 0.05892, policy_loss: -12.36009, policy_entropy: 4.10138, alpha: 0.85267, time: 49.64866
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 28.91037, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.06439, qf2_loss: 0.06370, policy_loss: -13.53607, policy_entropy: 4.10067, alpha: 0.82930, time: 49.52625
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 24.13725, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.07304, qf2_loss: 0.07240, policy_loss: -14.74289, policy_entropy: 4.10111, alpha: 0.80683, time: 49.30751
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 28.92898, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.07589, qf2_loss: 0.07568, policy_loss: -15.96723, policy_entropy: 4.10154, alpha: 0.78519, time: 49.16911
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 25.04013, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.07113, qf2_loss: 0.07102, policy_loss: -17.18486, policy_entropy: 4.10155, alpha: 0.76435, time: 49.76278
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 29.64146, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.06968, qf2_loss: 0.06959, policy_loss: -18.38121, policy_entropy: 4.10121, alpha: 0.74426, time: 49.69466
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 27.33639, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.07574, qf2_loss: 0.07565, policy_loss: -19.56069, policy_entropy: 4.10094, alpha: 0.72488, time: 49.72136
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 25.91471, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.07822, qf2_loss: 0.07811, policy_loss: -20.70784, policy_entropy: 4.10097, alpha: 0.70616, time: 49.88482
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 23.48343, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.07969, qf2_loss: 0.07956, policy_loss: -21.83098, policy_entropy: 4.10064, alpha: 0.68809, time: 50.03988
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 24.21233, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.06904, qf2_loss: 0.06891, policy_loss: -22.92136, policy_entropy: 4.10069, alpha: 0.67062, time: 50.08296
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 28.38971, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.07593, qf2_loss: 0.07568, policy_loss: -23.99037, policy_entropy: 4.10123, alpha: 0.65372, time: 49.66580
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 27.69565, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.07451, qf2_loss: 0.07431, policy_loss: -25.02908, policy_entropy: 4.10149, alpha: 0.63736, time: 49.65432
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 24.61053, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.07332, qf2_loss: 0.07305, policy_loss: -26.03576, policy_entropy: 4.10076, alpha: 0.62152, time: 49.97433
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 22.25537, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.07869, qf2_loss: 0.07844, policy_loss: -27.01535, policy_entropy: 4.10218, alpha: 0.60618, time: 50.29407
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 31.14865, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.05770, qf2_loss: 0.05746, policy_loss: -27.97122, policy_entropy: 4.10141, alpha: 0.59131, time: 50.31954
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 25.45257, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.07002, qf2_loss: 0.06979, policy_loss: -28.89308, policy_entropy: 4.10078, alpha: 0.57689, time: 49.73979
[CW] eval: return: 24.69895, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 23.91429, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.06996, qf2_loss: 0.06973, policy_loss: -29.79411, policy_entropy: 4.10179, alpha: 0.56290, time: 50.12819
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 24.31456, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.06691, qf2_loss: 0.06662, policy_loss: -30.66224, policy_entropy: 4.10077, alpha: 0.54933, time: 50.08318
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 24.83645, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.07825, qf2_loss: 0.07787, policy_loss: -31.51155, policy_entropy: 4.10114, alpha: 0.53615, time: 49.68488
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 24.14347, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.06396, qf2_loss: 0.06369, policy_loss: -32.33213, policy_entropy: 4.10078, alpha: 0.52334, time: 50.23540
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 25.62386, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.07122, qf2_loss: 0.07092, policy_loss: -33.12602, policy_entropy: 4.10078, alpha: 0.51090, time: 49.92226
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 20.63114, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.07183, qf2_loss: 0.07148, policy_loss: -33.89394, policy_entropy: 4.10065, alpha: 0.49881, time: 50.78604
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 29.64065, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.10369, qf2_loss: 0.10303, policy_loss: -34.64281, policy_entropy: 4.10174, alpha: 0.48705, time: 50.67863
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 25.49804, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.02873, qf2_loss: 0.02872, policy_loss: -35.36596, policy_entropy: 4.10150, alpha: 0.47561, time: 50.16752
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 24.06861, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.07242, qf2_loss: 0.07196, policy_loss: -36.05991, policy_entropy: 4.10178, alpha: 0.46448, time: 50.42913
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 27.62085, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.06082, qf2_loss: 0.06053, policy_loss: -36.74428, policy_entropy: 4.10088, alpha: 0.45364, time: 50.29627
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 32.39624, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.04737, qf2_loss: 0.04718, policy_loss: -37.39982, policy_entropy: 4.10151, alpha: 0.44310, time: 50.15658
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 24.94171, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.09111, qf2_loss: 0.09022, policy_loss: -38.03409, policy_entropy: 4.10114, alpha: 0.43283, time: 50.13595
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 24.88284, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.04145, qf2_loss: 0.04137, policy_loss: -38.64643, policy_entropy: 4.10105, alpha: 0.42283, time: 50.01688
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 24.31409, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.08154, qf2_loss: 0.08097, policy_loss: -39.24085, policy_entropy: 4.10146, alpha: 0.41309, time: 50.50552
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 26.39032, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.06593, qf2_loss: 0.06552, policy_loss: -39.81593, policy_entropy: 4.10186, alpha: 0.40359, time: 50.62644
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 23.92166, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.06093, qf2_loss: 0.06049, policy_loss: -40.36788, policy_entropy: 4.10236, alpha: 0.39434, time: 50.58638
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 23.63355, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.05298, qf2_loss: 0.05254, policy_loss: -40.91241, policy_entropy: 4.10134, alpha: 0.38532, time: 50.75865
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 28.72829, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.07774, qf2_loss: 0.07719, policy_loss: -41.42463, policy_entropy: 4.10208, alpha: 0.37652, time: 50.18500
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 23.99953, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.04933, qf2_loss: 0.04890, policy_loss: -41.92323, policy_entropy: 4.10146, alpha: 0.36794, time: 50.47281
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 27.05803, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.07072, qf2_loss: 0.07125, policy_loss: -42.40322, policy_entropy: 4.10220, alpha: 0.35958, time: 50.88552
[CW] eval: return: 24.50871, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 26.26512, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.04731, qf2_loss: 0.04645, policy_loss: -42.86898, policy_entropy: 4.10214, alpha: 0.35142, time: 50.78003
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 21.88684, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.05555, qf2_loss: 0.05475, policy_loss: -43.31494, policy_entropy: 4.10201, alpha: 0.34345, time: 50.44853
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 26.26753, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.05336, qf2_loss: 0.05280, policy_loss: -43.74924, policy_entropy: 4.10059, alpha: 0.33568, time: 50.89408
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 24.07226, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.05914, qf2_loss: 0.05863, policy_loss: -44.16211, policy_entropy: 4.10202, alpha: 0.32810, time: 50.76534
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 21.47965, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.04377, qf2_loss: 0.04340, policy_loss: -44.56580, policy_entropy: 4.10137, alpha: 0.32070, time: 50.60182
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 25.54445, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.07122, qf2_loss: 0.07048, policy_loss: -44.94631, policy_entropy: 4.10153, alpha: 0.31348, time: 50.93512
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 23.84952, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.04729, qf2_loss: 0.04670, policy_loss: -45.31460, policy_entropy: 4.10247, alpha: 0.30642, time: 50.64545
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 24.48706, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.05105, qf2_loss: 0.05051, policy_loss: -45.67253, policy_entropy: 4.10303, alpha: 0.29953, time: 50.50017
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 25.71963, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.05525, qf2_loss: 0.05466, policy_loss: -46.01184, policy_entropy: 4.10133, alpha: 0.29281, time: 50.84070
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 23.56991, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.08282, qf2_loss: 0.08190, policy_loss: -46.33454, policy_entropy: 4.10244, alpha: 0.28624, time: 50.73359
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 22.29504, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.02905, qf2_loss: 0.02877, policy_loss: -46.65256, policy_entropy: 4.10078, alpha: 0.27983, time: 50.49723
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 21.23335, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.06069, qf2_loss: 0.06002, policy_loss: -46.94880, policy_entropy: 4.10194, alpha: 0.27356, time: 50.74417
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 30.08950, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.05059, qf2_loss: 0.05016, policy_loss: -47.23701, policy_entropy: 4.10209, alpha: 0.26744, time: 50.70022
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 25.30328, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.05211, qf2_loss: 0.05148, policy_loss: -47.51511, policy_entropy: 4.10172, alpha: 0.26147, time: 50.70527
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 25.67266, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.05473, qf2_loss: 0.05404, policy_loss: -47.77524, policy_entropy: 4.10120, alpha: 0.25563, time: 50.44479
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 23.79748, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.05318, qf2_loss: 0.05262, policy_loss: -48.02889, policy_entropy: 4.10100, alpha: 0.24992, time: 50.56886
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 24.43590, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.04071, qf2_loss: 0.04026, policy_loss: -48.26112, policy_entropy: 4.10167, alpha: 0.24435, time: 50.63078
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 26.09763, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.08169, qf2_loss: 0.08075, policy_loss: -48.49373, policy_entropy: 4.10074, alpha: 0.23890, time: 50.53087
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 24.28479, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.03643, qf2_loss: 0.03588, policy_loss: -48.70619, policy_entropy: 4.10198, alpha: 0.23358, time: 50.44285
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 25.58463, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.04161, qf2_loss: 0.04119, policy_loss: -48.91204, policy_entropy: 4.10188, alpha: 0.22838, time: 50.52274
[CW] eval: return: 25.63837, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 28.15212, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.04275, qf2_loss: 0.04241, policy_loss: -49.10828, policy_entropy: 4.10210, alpha: 0.22330, time: 50.87322
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 23.84446, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.04634, qf2_loss: 0.04580, policy_loss: -49.29242, policy_entropy: 4.10274, alpha: 0.21833, time: 50.76365
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 30.22522, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.04856, qf2_loss: 0.04810, policy_loss: -49.46744, policy_entropy: 4.10070, alpha: 0.21347, time: 50.46758
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 36.06430, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.04841, qf2_loss: 0.04788, policy_loss: -49.63810, policy_entropy: 4.10130, alpha: 0.20873, time: 50.62484
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 26.19103, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.04312, qf2_loss: 0.04274, policy_loss: -49.79353, policy_entropy: 4.10120, alpha: 0.20409, time: 50.74074
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 25.05703, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.06967, qf2_loss: 0.06874, policy_loss: -49.93744, policy_entropy: 4.10175, alpha: 0.19956, time: 50.63876
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 26.46857, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.02372, qf2_loss: 0.02359, policy_loss: -50.07640, policy_entropy: 4.10118, alpha: 0.19512, time: 50.72233
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 25.32426, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.05752, qf2_loss: 0.05699, policy_loss: -50.20596, policy_entropy: 4.10062, alpha: 0.19079, time: 50.56773
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 27.66857, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.03223, qf2_loss: 0.03198, policy_loss: -50.32518, policy_entropy: 4.10118, alpha: 0.18656, time: 50.48611
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 27.82230, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.04679, qf2_loss: 0.04628, policy_loss: -50.43669, policy_entropy: 4.10204, alpha: 0.18242, time: 50.45989
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 26.94664, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.06007, qf2_loss: 0.05952, policy_loss: -50.54338, policy_entropy: 4.10036, alpha: 0.17837, time: 50.66428
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 22.49318, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.02372, qf2_loss: 0.02360, policy_loss: -50.63608, policy_entropy: 4.10091, alpha: 0.17442, time: 50.63527
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 21.92120, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.04302, qf2_loss: 0.04257, policy_loss: -50.72605, policy_entropy: 4.10167, alpha: 0.17055, time: 50.43677
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 30.17361, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.04968, qf2_loss: 0.04921, policy_loss: -50.80549, policy_entropy: 4.10112, alpha: 0.16677, time: 50.72363
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 25.36315, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.04243, qf2_loss: 0.04205, policy_loss: -50.87895, policy_entropy: 4.10115, alpha: 0.16307, time: 50.60900
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 27.60474, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.03690, qf2_loss: 0.03635, policy_loss: -50.94644, policy_entropy: 4.10105, alpha: 0.15946, time: 50.30671
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 19.83761, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.06952, qf2_loss: 0.06894, policy_loss: -51.00383, policy_entropy: 4.10128, alpha: 0.15593, time: 50.60886
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 24.54637, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.02911, qf2_loss: 0.02884, policy_loss: -51.05408, policy_entropy: 4.10083, alpha: 0.15247, time: 50.63740
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 25.62127, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.04015, qf2_loss: 0.03979, policy_loss: -51.10093, policy_entropy: 4.10077, alpha: 0.14910, time: 50.64618
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 23.11091, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.04903, qf2_loss: 0.04861, policy_loss: -51.14095, policy_entropy: 4.10062, alpha: 0.14580, time: 50.81594
[CW] eval: return: 25.04777, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 23.49652, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.02948, qf2_loss: 0.02919, policy_loss: -51.17215, policy_entropy: 4.10174, alpha: 0.14257, time: 50.77826
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 22.03311, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.04391, qf2_loss: 0.04352, policy_loss: -51.19644, policy_entropy: 4.10088, alpha: 0.13941, time: 50.63810
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 29.20496, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.03677, qf2_loss: 0.03655, policy_loss: -51.22330, policy_entropy: 4.10142, alpha: 0.13632, time: 50.31848
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 25.31839, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.04519, qf2_loss: 0.04485, policy_loss: -51.23185, policy_entropy: 4.10119, alpha: 0.13331, time: 50.48117
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 24.25785, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.03617, qf2_loss: 0.03629, policy_loss: -51.24531, policy_entropy: 4.10221, alpha: 0.13035, time: 50.66581
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 28.01395, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.03494, qf2_loss: 0.03433, policy_loss: -51.24505, policy_entropy: 4.10072, alpha: 0.12747, time: 50.71232
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 25.13227, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.03274, qf2_loss: 0.03267, policy_loss: -51.24769, policy_entropy: 4.10190, alpha: 0.12465, time: 50.60464
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 21.81727, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.05767, qf2_loss: 0.05692, policy_loss: -51.23820, policy_entropy: 4.10101, alpha: 0.12189, time: 50.68792
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 26.38106, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.01499, qf2_loss: 0.01503, policy_loss: -51.22311, policy_entropy: 4.10068, alpha: 0.11919, time: 51.04117
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 28.23416, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.07682, qf2_loss: 0.07577, policy_loss: -51.20868, policy_entropy: 4.10084, alpha: 0.11655, time: 50.87901
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 26.57578, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.01093, qf2_loss: 0.01097, policy_loss: -51.18650, policy_entropy: 4.10154, alpha: 0.11398, time: 50.85044
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 25.65938, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.03853, qf2_loss: 0.03846, policy_loss: -51.16345, policy_entropy: 4.10163, alpha: 0.11145, time: 50.54486
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 24.54496, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.03194, qf2_loss: 0.03188, policy_loss: -51.13114, policy_entropy: 4.10097, alpha: 0.10899, time: 50.89061
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 24.51580, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.04314, qf2_loss: 0.04281, policy_loss: -51.09492, policy_entropy: 4.10021, alpha: 0.10658, time: 50.81532
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 28.18143, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.04696, qf2_loss: 0.04694, policy_loss: -51.06129, policy_entropy: 4.10027, alpha: 0.10422, time: 50.83350
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 24.49011, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.01970, qf2_loss: 0.01953, policy_loss: -51.01361, policy_entropy: 4.10133, alpha: 0.10192, time: 50.63519
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 28.31978, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.05335, qf2_loss: 0.05282, policy_loss: -50.97232, policy_entropy: 4.10154, alpha: 0.09966, time: 50.61616
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 25.72469, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.01841, qf2_loss: 0.01824, policy_loss: -50.91678, policy_entropy: 4.10119, alpha: 0.09746, time: 50.52909
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 26.38748, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.05054, qf2_loss: 0.05055, policy_loss: -50.86094, policy_entropy: 4.10205, alpha: 0.09530, time: 50.50073
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 25.85635, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.02183, qf2_loss: 0.02153, policy_loss: -50.80113, policy_entropy: 4.10048, alpha: 0.09319, time: 50.94525
[CW] eval: return: 25.30016, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 20.63572, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.02667, qf2_loss: 0.02661, policy_loss: -50.74078, policy_entropy: 4.09993, alpha: 0.09113, time: 50.53459
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 26.61038, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.05041, qf2_loss: 0.05039, policy_loss: -50.67478, policy_entropy: 4.10135, alpha: 0.08912, time: 50.54852
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 23.15005, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.01834, qf2_loss: 0.01823, policy_loss: -50.60450, policy_entropy: 4.10111, alpha: 0.08715, time: 50.62473
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 22.90438, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.04121, qf2_loss: 0.04108, policy_loss: -50.53429, policy_entropy: 4.10101, alpha: 0.08522, time: 50.74803
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 22.65026, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.03263, qf2_loss: 0.03240, policy_loss: -50.46020, policy_entropy: 4.10034, alpha: 0.08334, time: 50.46584
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 25.61236, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.03610, qf2_loss: 0.03623, policy_loss: -50.38017, policy_entropy: 4.09933, alpha: 0.08149, time: 51.10230
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 28.16042, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.03521, qf2_loss: 0.03508, policy_loss: -50.29994, policy_entropy: 4.10029, alpha: 0.07969, time: 51.33877
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 24.19056, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.04112, qf2_loss: 0.04099, policy_loss: -50.21611, policy_entropy: 4.10043, alpha: 0.07793, time: 50.92521
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 25.34018, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.01946, qf2_loss: 0.01965, policy_loss: -50.12590, policy_entropy: 4.10084, alpha: 0.07620, time: 50.83029
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 22.59779, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.03257, qf2_loss: 0.03259, policy_loss: -50.03687, policy_entropy: 4.10030, alpha: 0.07452, time: 54.70627
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 25.87221, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.02887, qf2_loss: 0.02875, policy_loss: -49.94405, policy_entropy: 4.10034, alpha: 0.07287, time: 51.83613
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 23.68227, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.03052, qf2_loss: 0.03051, policy_loss: -49.85179, policy_entropy: 4.09981, alpha: 0.07126, time: 51.50626
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 27.99211, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.03960, qf2_loss: 0.03959, policy_loss: -49.75270, policy_entropy: 4.10001, alpha: 0.06968, time: 51.49782
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 27.35375, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.03319, qf2_loss: 0.03338, policy_loss: -49.65065, policy_entropy: 4.10159, alpha: 0.06814, time: 51.76946
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 30.72994, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.01885, qf2_loss: 0.01865, policy_loss: -49.54558, policy_entropy: 4.10030, alpha: 0.06664, time: 51.69478
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 23.62943, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.03424, qf2_loss: 0.03416, policy_loss: -49.44306, policy_entropy: 4.10112, alpha: 0.06516, time: 51.76374
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 32.08156, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.03683, qf2_loss: 0.03651, policy_loss: -49.33325, policy_entropy: 4.10043, alpha: 0.06372, time: 51.43511
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 31.78174, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.02671, qf2_loss: 0.02670, policy_loss: -49.23017, policy_entropy: 4.10095, alpha: 0.06231, time: 51.21163
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 24.65152, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.04465, qf2_loss: 0.04427, policy_loss: -49.10601, policy_entropy: 4.09986, alpha: 0.06093, time: 51.37229
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 22.44967, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.01680, qf2_loss: 0.01740, policy_loss: -48.99709, policy_entropy: 4.09942, alpha: 0.05959, time: 51.17127
[CW] eval: return: 24.92494, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 23.79540, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.02201, qf2_loss: 0.02235, policy_loss: -48.88309, policy_entropy: 4.10116, alpha: 0.05827, time: 51.42274
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 28.87349, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.03793, qf2_loss: 0.03784, policy_loss: -48.76480, policy_entropy: 4.10038, alpha: 0.05698, time: 51.47393
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 25.63536, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.02454, qf2_loss: 0.02452, policy_loss: -48.64569, policy_entropy: 4.09928, alpha: 0.05572, time: 50.95522
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 23.77221, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.03591, qf2_loss: 0.03622, policy_loss: -48.53055, policy_entropy: 4.10008, alpha: 0.05449, time: 51.02505
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 20.85520, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.02992, qf2_loss: 0.03003, policy_loss: -48.40477, policy_entropy: 4.10046, alpha: 0.05328, time: 51.28207
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 25.67546, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.02841, qf2_loss: 0.02844, policy_loss: -48.28115, policy_entropy: 4.09925, alpha: 0.05210, time: 51.13623
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 27.53455, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.03215, qf2_loss: 0.03164, policy_loss: -48.15760, policy_entropy: 4.09902, alpha: 0.05095, time: 51.02579
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 22.78135, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.02715, qf2_loss: 0.02785, policy_loss: -48.02990, policy_entropy: 4.09983, alpha: 0.04983, time: 51.07406
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 28.41904, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.01723, qf2_loss: 0.01739, policy_loss: -47.89861, policy_entropy: 4.10118, alpha: 0.04872, time: 50.90612
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 21.97935, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.04052, qf2_loss: 0.04054, policy_loss: -47.76987, policy_entropy: 4.09998, alpha: 0.04765, time: 50.63938
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 25.59670, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.03126, qf2_loss: 0.03131, policy_loss: -47.64351, policy_entropy: 4.09968, alpha: 0.04659, time: 50.77603
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 29.33831, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.02601, qf2_loss: 0.02592, policy_loss: -47.51095, policy_entropy: 4.09835, alpha: 0.04556, time: 50.79890
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 25.09371, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.03072, qf2_loss: 0.03094, policy_loss: -47.37898, policy_entropy: 4.10051, alpha: 0.04455, time: 50.99144
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 19.67912, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.01426, qf2_loss: 0.01368, policy_loss: -47.23619, policy_entropy: 4.10033, alpha: 0.04357, time: 50.37437
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 21.49448, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.04748, qf2_loss: 0.04789, policy_loss: -47.09977, policy_entropy: 4.09911, alpha: 0.04261, time: 50.79263
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 22.79597, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.01141, qf2_loss: 0.01207, policy_loss: -46.96623, policy_entropy: 4.09839, alpha: 0.04166, time: 51.01349
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 23.07824, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.03852, qf2_loss: 0.03863, policy_loss: -46.82909, policy_entropy: 4.09990, alpha: 0.04074, time: 50.81974
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 25.50453, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.02751, qf2_loss: 0.02768, policy_loss: -46.69518, policy_entropy: 4.09786, alpha: 0.03984, time: 50.46849
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 26.32909, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.02023, qf2_loss: 0.02026, policy_loss: -46.54915, policy_entropy: 4.09807, alpha: 0.03896, time: 50.39734
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 24.17002, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.02576, qf2_loss: 0.02623, policy_loss: -46.41426, policy_entropy: 4.09845, alpha: 0.03810, time: 50.51381
[CW] eval: return: 25.44848, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 25.90909, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.03612, qf2_loss: 0.03623, policy_loss: -46.27222, policy_entropy: 4.09911, alpha: 0.03726, time: 51.27797
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 24.94363, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.01731, qf2_loss: 0.01674, policy_loss: -46.12846, policy_entropy: 4.09805, alpha: 0.03643, time: 50.71320
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 20.52306, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.03858, qf2_loss: 0.03892, policy_loss: -45.98534, policy_entropy: 4.09925, alpha: 0.03563, time: 50.35026
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 24.15821, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.01754, qf2_loss: 0.01766, policy_loss: -45.83594, policy_entropy: 4.09869, alpha: 0.03484, time: 50.39917
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 22.90231, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.05135, qf2_loss: 0.05195, policy_loss: -45.69438, policy_entropy: 4.09540, alpha: 0.03407, time: 50.56075
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 25.45468, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.00963, qf2_loss: 0.00964, policy_loss: -45.54803, policy_entropy: 4.09734, alpha: 0.03332, time: 50.63243
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 26.47258, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.02600, qf2_loss: 0.02605, policy_loss: -45.40332, policy_entropy: 4.09841, alpha: 0.03258, time: 50.57436
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 24.58865, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.01996, qf2_loss: 0.01934, policy_loss: -45.25587, policy_entropy: 4.09780, alpha: 0.03186, time: 50.66058
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 25.77880, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.02663, qf2_loss: 0.02681, policy_loss: -45.10772, policy_entropy: 4.09622, alpha: 0.03115, time: 50.47154
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 23.14965, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.02887, qf2_loss: 0.02964, policy_loss: -44.95199, policy_entropy: 4.09849, alpha: 0.03047, time: 50.63903
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 24.51098, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.04065, qf2_loss: 0.04086, policy_loss: -44.80670, policy_entropy: 4.09668, alpha: 0.02979, time: 50.63429
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 24.04365, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.01288, qf2_loss: 0.01329, policy_loss: -44.66531, policy_entropy: 4.09848, alpha: 0.02913, time: 51.24956
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 24.44115, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.01911, qf2_loss: 0.01834, policy_loss: -44.51046, policy_entropy: 4.09755, alpha: 0.02849, time: 50.98673
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 27.24550, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.03326, qf2_loss: 0.03397, policy_loss: -44.36033, policy_entropy: 4.09438, alpha: 0.02786, time: 50.66058
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 24.54764, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.02068, qf2_loss: 0.02066, policy_loss: -44.21932, policy_entropy: 4.09726, alpha: 0.02724, time: 50.88996
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 20.94513, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.04362, qf2_loss: 0.04411, policy_loss: -44.05330, policy_entropy: 4.09786, alpha: 0.02664, time: 50.61318
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 25.99052, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.02192, qf2_loss: 0.02161, policy_loss: -43.91509, policy_entropy: 4.09464, alpha: 0.02605, time: 50.26209
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 21.89519, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.01953, qf2_loss: 0.01923, policy_loss: -43.76064, policy_entropy: 4.09554, alpha: 0.02548, time: 50.72902
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 23.60004, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.02982, qf2_loss: 0.03024, policy_loss: -43.61046, policy_entropy: 4.09563, alpha: 0.02491, time: 50.74478
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 23.40520, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.02545, qf2_loss: 0.02449, policy_loss: -43.45876, policy_entropy: 4.09572, alpha: 0.02436, time: 50.63787
[CW] eval: return: 25.53995, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 22.73558, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.02655, qf2_loss: 0.02729, policy_loss: -43.30040, policy_entropy: 4.09540, alpha: 0.02382, time: 50.62393
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 28.47373, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.02423, qf2_loss: 0.02413, policy_loss: -43.15928, policy_entropy: 4.09580, alpha: 0.02330, time: 50.63617
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 25.31839, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.02177, qf2_loss: 0.02172, policy_loss: -43.00532, policy_entropy: 4.09221, alpha: 0.02278, time: 50.15288
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 22.40650, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.03047, qf2_loss: 0.03079, policy_loss: -42.84254, policy_entropy: 4.09432, alpha: 0.02228, time: 50.57737
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 27.70737, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.01521, qf2_loss: 0.01511, policy_loss: -42.68808, policy_entropy: 4.09406, alpha: 0.02179, time: 50.80897
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 25.68429, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.03631, qf2_loss: 0.03617, policy_loss: -42.54161, policy_entropy: 4.09417, alpha: 0.02130, time: 50.82146
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 24.11482, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.02634, qf2_loss: 0.02700, policy_loss: -42.38285, policy_entropy: 4.09504, alpha: 0.02083, time: 50.51592
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 22.58283, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.01349, qf2_loss: 0.01332, policy_loss: -42.23688, policy_entropy: 4.09922, alpha: 0.02037, time: 50.64170
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 25.07900, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.02181, qf2_loss: 0.02185, policy_loss: -42.07829, policy_entropy: 4.09511, alpha: 0.01992, time: 50.35850
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 22.90431, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.03566, qf2_loss: 0.03551, policy_loss: -41.93364, policy_entropy: 4.09293, alpha: 0.01948, time: 50.40904
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 27.02487, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.02029, qf2_loss: 0.02070, policy_loss: -41.77671, policy_entropy: 4.09357, alpha: 0.01905, time: 50.72525
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 23.81771, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.03055, qf2_loss: 0.03023, policy_loss: -41.62417, policy_entropy: 4.09103, alpha: 0.01863, time: 50.72007
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 24.16911, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.01735, qf2_loss: 0.01732, policy_loss: -41.47184, policy_entropy: 4.09090, alpha: 0.01822, time: 50.64994
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 23.54491, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.02280, qf2_loss: 0.02317, policy_loss: -41.31832, policy_entropy: 4.08918, alpha: 0.01782, time: 50.55226
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 23.69542, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.02955, qf2_loss: 0.02924, policy_loss: -41.15805, policy_entropy: 4.08666, alpha: 0.01742, time: 50.45997
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 26.59515, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.02153, qf2_loss: 0.02155, policy_loss: -41.00553, policy_entropy: 4.09018, alpha: 0.01704, time: 50.80777
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 20.82050, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.02644, qf2_loss: 0.02612, policy_loss: -40.85515, policy_entropy: 4.08960, alpha: 0.01666, time: 50.43019
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 28.06801, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.02519, qf2_loss: 0.02584, policy_loss: -40.69838, policy_entropy: 4.08238, alpha: 0.01629, time: 50.69586
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 26.10729, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.02107, qf2_loss: 0.02128, policy_loss: -40.54987, policy_entropy: 4.08691, alpha: 0.01593, time: 50.35448
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 27.34742, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.01961, qf2_loss: 0.01998, policy_loss: -40.39760, policy_entropy: 4.09014, alpha: 0.01558, time: 50.82012
[CW] eval: return: 24.40803, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 26.54351, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.03343, qf2_loss: 0.03197, policy_loss: -40.24095, policy_entropy: 4.08642, alpha: 0.01524, time: 50.56276
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 26.35802, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.02960, qf2_loss: 0.03018, policy_loss: -40.08944, policy_entropy: 4.08473, alpha: 0.01490, time: 50.69813
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 33.13732, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.03152, qf2_loss: 0.03180, policy_loss: -39.94457, policy_entropy: 4.07649, alpha: 0.01457, time: 50.30896
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 23.93670, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.01404, qf2_loss: 0.01401, policy_loss: -39.79237, policy_entropy: 4.08097, alpha: 0.01425, time: 50.72746
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 22.47188, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.03245, qf2_loss: 0.03212, policy_loss: -39.63310, policy_entropy: 4.08562, alpha: 0.01393, time: 50.83428
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 30.12897, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.01414, qf2_loss: 0.01418, policy_loss: -39.48143, policy_entropy: 4.08099, alpha: 0.01363, time: 50.59319
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 21.54747, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.02218, qf2_loss: 0.02247, policy_loss: -39.33396, policy_entropy: 4.08078, alpha: 0.01332, time: 50.57457
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 22.95979, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.02610, qf2_loss: 0.02653, policy_loss: -39.18535, policy_entropy: 4.06799, alpha: 0.01303, time: 50.85887
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 29.15276, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.02227, qf2_loss: 0.02161, policy_loss: -39.04023, policy_entropy: 4.05051, alpha: 0.01274, time: 51.17836
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 22.43561, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.02470, qf2_loss: 0.02479, policy_loss: -38.88422, policy_entropy: 4.07843, alpha: 0.01246, time: 50.60303
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 21.00350, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.03031, qf2_loss: 0.03056, policy_loss: -38.73045, policy_entropy: 4.08994, alpha: 0.01219, time: 50.60497
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 23.77111, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.02095, qf2_loss: 0.02090, policy_loss: -38.58232, policy_entropy: 4.06998, alpha: 0.01192, time: 50.71939
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 22.45053, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.02190, qf2_loss: 0.02189, policy_loss: -38.43446, policy_entropy: 4.06853, alpha: 0.01165, time: 51.08372
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 25.17773, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.01831, qf2_loss: 0.01830, policy_loss: -38.27460, policy_entropy: 4.07482, alpha: 0.01140, time: 50.51871
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 21.42276, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.04189, qf2_loss: 0.04150, policy_loss: -38.13572, policy_entropy: 4.07010, alpha: 0.01114, time: 50.54191
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 28.63051, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.01966, qf2_loss: 0.01972, policy_loss: -37.99182, policy_entropy: 4.04695, alpha: 0.01090, time: 50.99779
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 23.18360, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.01880, qf2_loss: 0.01903, policy_loss: -37.83230, policy_entropy: 4.04738, alpha: 0.01066, time: 50.50323
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 25.29466, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.02021, qf2_loss: 0.02032, policy_loss: -37.68902, policy_entropy: 4.04536, alpha: 0.01042, time: 50.70730
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 29.61431, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.02911, qf2_loss: 0.02905, policy_loss: -37.54291, policy_entropy: 4.06319, alpha: 0.01019, time: 50.39825
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 24.70307, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.02186, qf2_loss: 0.02168, policy_loss: -37.39090, policy_entropy: 4.04367, alpha: 0.00997, time: 50.43387
[CW] eval: return: 26.23154, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 26.31480, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.02000, qf2_loss: 0.01992, policy_loss: -37.24543, policy_entropy: 4.03107, alpha: 0.00975, time: 50.83733
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 23.98961, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.03307, qf2_loss: 0.03333, policy_loss: -37.09851, policy_entropy: 4.03124, alpha: 0.00953, time: 50.64004
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 21.65308, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.02523, qf2_loss: 0.02481, policy_loss: -36.94277, policy_entropy: 4.05216, alpha: 0.00932, time: 50.18707
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 25.19842, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.02617, qf2_loss: 0.02598, policy_loss: -36.80723, policy_entropy: 4.02862, alpha: 0.00912, time: 50.64161
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 29.56772, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.01501, qf2_loss: 0.01506, policy_loss: -36.64957, policy_entropy: 4.02685, alpha: 0.00892, time: 51.12150
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 26.19660, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.02708, qf2_loss: 0.02716, policy_loss: -36.51219, policy_entropy: 4.05325, alpha: 0.00872, time: 50.85185
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 29.38665, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.02228, qf2_loss: 0.02231, policy_loss: -36.37958, policy_entropy: 4.03573, alpha: 0.00853, time: 50.67473
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 22.75419, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.02030, qf2_loss: 0.02045, policy_loss: -36.22695, policy_entropy: 3.98850, alpha: 0.00834, time: 50.85737
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 23.57513, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.03629, qf2_loss: 0.03591, policy_loss: -36.07560, policy_entropy: 4.00589, alpha: 0.00816, time: 50.79450
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 27.60975, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.01740, qf2_loss: 0.01744, policy_loss: -35.93424, policy_entropy: 3.96346, alpha: 0.00798, time: 50.99472
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 25.23827, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.01931, qf2_loss: 0.01921, policy_loss: -35.79052, policy_entropy: 3.95042, alpha: 0.00780, time: 50.91034
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 25.72359, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.04787, qf2_loss: 0.04772, policy_loss: -35.65566, policy_entropy: 3.77321, alpha: 0.00763, time: 51.04778
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 17.76281, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.02082, qf2_loss: 0.02082, policy_loss: -35.51014, policy_entropy: 3.88322, alpha: 0.00747, time: 50.98074
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 25.19667, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 0.01800, qf2_loss: 0.01812, policy_loss: -35.36292, policy_entropy: 3.91167, alpha: 0.00730, time: 50.44488
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 25.48129, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.01660, qf2_loss: 0.01636, policy_loss: -35.22161, policy_entropy: 3.88163, alpha: 0.00714, time: 50.19733
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 16.64912, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.02294, qf2_loss: 0.02271, policy_loss: -35.07895, policy_entropy: 3.78904, alpha: 0.00699, time: 50.50479
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 22.78131, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.02254, qf2_loss: 0.02249, policy_loss: -34.94605, policy_entropy: 3.81957, alpha: 0.00684, time: 50.35847
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 21.74300, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.03442, qf2_loss: 0.03419, policy_loss: -34.80646, policy_entropy: 3.71214, alpha: 0.00669, time: 50.32967
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 15.82157, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.01951, qf2_loss: 0.01941, policy_loss: -34.66299, policy_entropy: 3.75989, alpha: 0.00654, time: 50.30230
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 28.60329, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.02506, qf2_loss: 0.02492, policy_loss: -34.51806, policy_entropy: 3.77838, alpha: 0.00640, time: 50.40144
[CW] eval: return: 26.46968, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 25.12874, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.03128, qf2_loss: 0.03102, policy_loss: -34.38319, policy_entropy: 3.67090, alpha: 0.00626, time: 50.83024
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 24.04991, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.02433, qf2_loss: 0.02422, policy_loss: -34.23618, policy_entropy: 3.64934, alpha: 0.00613, time: 50.82043
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 24.70404, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.02082, qf2_loss: 0.02074, policy_loss: -34.10651, policy_entropy: 3.40197, alpha: 0.00600, time: 52.04844
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 25.38194, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.02671, qf2_loss: 0.02697, policy_loss: -33.96105, policy_entropy: 3.42802, alpha: 0.00587, time: 50.99123
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 23.25913, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.02514, qf2_loss: 0.02458, policy_loss: -33.83665, policy_entropy: 3.30975, alpha: 0.00575, time: 51.21163
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 25.72359, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.02545, qf2_loss: 0.02538, policy_loss: -33.70883, policy_entropy: 2.84912, alpha: 0.00563, time: 50.84125
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 24.80939, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.04272, qf2_loss: 0.04276, policy_loss: -33.55583, policy_entropy: 2.70218, alpha: 0.00551, time: 51.05219
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 24.25237, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 0.02475, qf2_loss: 0.02463, policy_loss: -33.44184, policy_entropy: 1.79615, alpha: 0.00541, time: 51.01643
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 26.03143, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.02511, qf2_loss: 0.02465, policy_loss: -33.29301, policy_entropy: 2.48218, alpha: 0.00531, time: 51.05393
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 23.84624, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.03047, qf2_loss: 0.03051, policy_loss: -33.17491, policy_entropy: 2.24806, alpha: 0.00520, time: 50.76395
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 25.05362, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 0.03440, qf2_loss: 0.03430, policy_loss: -33.03887, policy_entropy: 2.66641, alpha: 0.00510, time: 51.76377
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 16.47248, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.02932, qf2_loss: 0.02902, policy_loss: -32.90445, policy_entropy: 2.97637, alpha: 0.00499, time: 50.73045
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 23.72516, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.03498, qf2_loss: 0.03477, policy_loss: -32.78533, policy_entropy: 2.71277, alpha: 0.00489, time: 51.03414
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 24.44809, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.02692, qf2_loss: 0.02668, policy_loss: -32.65641, policy_entropy: 0.55154, alpha: 0.00479, time: 50.56883
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 25.95112, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.03467, qf2_loss: 0.03396, policy_loss: -32.55053, policy_entropy: -0.47521, alpha: 0.00472, time: 50.71238
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 37.27646, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.03984, qf2_loss: 0.04017, policy_loss: -32.41810, policy_entropy: -0.03415, alpha: 0.00466, time: 50.89289
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 34.27200, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 0.04713, qf2_loss: 0.04718, policy_loss: -32.30697, policy_entropy: 0.44978, alpha: 0.00458, time: 50.49629
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 32.34281, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 0.03746, qf2_loss: 0.03754, policy_loss: -32.19079, policy_entropy: -0.15587, alpha: 0.00451, time: 50.40112
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 31.94061, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 0.04096, qf2_loss: 0.04165, policy_loss: -32.09138, policy_entropy: -0.73472, alpha: 0.00444, time: 51.14919
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 26.81470, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.04282, qf2_loss: 0.04300, policy_loss: -31.97195, policy_entropy: -1.05471, alpha: 0.00438, time: 50.48671
[CW] eval: return: 26.93273, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 33.30281, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 0.04394, qf2_loss: 0.04365, policy_loss: -31.86578, policy_entropy: -0.73583, alpha: 0.00432, time: 51.23921
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 53.32492, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 0.05678, qf2_loss: 0.05639, policy_loss: -31.73113, policy_entropy: -0.81269, alpha: 0.00425, time: 50.67220
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 35.99757, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 0.05525, qf2_loss: 0.05538, policy_loss: -31.63191, policy_entropy: -1.30310, alpha: 0.00419, time: 50.44553
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 23.72741, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 0.04959, qf2_loss: 0.04963, policy_loss: -31.51478, policy_entropy: -0.83156, alpha: 0.00413, time: 50.52319
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 16.07881, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 0.04795, qf2_loss: 0.04839, policy_loss: -31.41192, policy_entropy: -1.51589, alpha: 0.00407, time: 50.17832
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 28.51744, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 0.05916, qf2_loss: 0.05957, policy_loss: -31.30478, policy_entropy: -1.61567, alpha: 0.00401, time: 50.36207
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 23.09213, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 0.08230, qf2_loss: 0.08124, policy_loss: -31.18235, policy_entropy: -1.51834, alpha: 0.00395, time: 50.56201
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 23.04616, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 0.05167, qf2_loss: 0.05214, policy_loss: -31.09342, policy_entropy: -2.51987, alpha: 0.00390, time: 50.40168
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 22.42659, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 0.06281, qf2_loss: 0.06315, policy_loss: -31.00419, policy_entropy: -1.75463, alpha: 0.00385, time: 50.25222
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 23.10353, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 0.06896, qf2_loss: 0.06945, policy_loss: -30.89330, policy_entropy: -1.19470, alpha: 0.00379, time: 50.32791
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 24.66415, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 0.07229, qf2_loss: 0.07190, policy_loss: -30.81692, policy_entropy: -1.98311, alpha: 0.00373, time: 50.37259
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 23.81179, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 0.07380, qf2_loss: 0.07461, policy_loss: -30.73006, policy_entropy: -4.10695, alpha: 0.00369, time: 50.52147
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 22.83172, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 0.07686, qf2_loss: 0.07623, policy_loss: -30.64995, policy_entropy: -5.23924, alpha: 0.00367, time: 50.64708
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 33.96307, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 0.08763, qf2_loss: 0.08817, policy_loss: -30.60024, policy_entropy: -6.68678, alpha: 0.00366, time: 50.45995
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 22.60554, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 0.10098, qf2_loss: 0.10042, policy_loss: -30.56841, policy_entropy: -7.57864, alpha: 0.00368, time: 50.32579
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 22.87622, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 0.09479, qf2_loss: 0.09564, policy_loss: -30.55078, policy_entropy: -7.63218, alpha: 0.00371, time: 50.30995
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 24.52545, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 0.10478, qf2_loss: 0.10412, policy_loss: -30.54925, policy_entropy: -7.32598, alpha: 0.00373, time: 50.68652
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 9.77172, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 0.11806, qf2_loss: 0.11835, policy_loss: -30.58063, policy_entropy: -6.84342, alpha: 0.00376, time: 50.57612
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 23.72052, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 0.13450, qf2_loss: 0.13540, policy_loss: -30.62107, policy_entropy: -6.97058, alpha: 0.00377, time: 50.39893
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 24.04888, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 0.16124, qf2_loss: 0.16006, policy_loss: -30.66447, policy_entropy: -7.92309, alpha: 0.00380, time: 50.72978
[CW] eval: return: 24.61361, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 25.70449, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 0.14504, qf2_loss: 0.14538, policy_loss: -30.72749, policy_entropy: -8.52135, alpha: 0.00385, time: 50.38439
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 10.67895, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 0.16567, qf2_loss: 0.16396, policy_loss: -30.75958, policy_entropy: -7.77982, alpha: 0.00390, time: 50.61393
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 14.41118, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 0.14929, qf2_loss: 0.14850, policy_loss: -30.82396, policy_entropy: -7.92149, alpha: 0.00394, time: 50.05341
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 9.86891, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 0.22692, qf2_loss: 0.22858, policy_loss: -30.88660, policy_entropy: -9.23072, alpha: 0.00401, time: 50.50578
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 14.79963, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 0.15649, qf2_loss: 0.15576, policy_loss: -30.95634, policy_entropy: -8.97219, alpha: 0.00410, time: 50.41636
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 81.95903, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 0.17120, qf2_loss: 0.17011, policy_loss: -30.90760, policy_entropy: -8.78282, alpha: 0.00418, time: 50.58342
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 44.19812, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 0.16226, qf2_loss: 0.16100, policy_loss: -30.98263, policy_entropy: -9.57360, alpha: 0.00429, time: 50.36713
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 66.24501, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 0.16577, qf2_loss: 0.16524, policy_loss: -31.04320, policy_entropy: -10.04087, alpha: 0.00441, time: 50.59939
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 85.06939, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 0.17504, qf2_loss: 0.17468, policy_loss: -31.12831, policy_entropy: -9.63142, alpha: 0.00455, time: 50.44707
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 46.65403, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 0.16250, qf2_loss: 0.16281, policy_loss: -31.04728, policy_entropy: -8.62683, alpha: 0.00468, time: 50.60795
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 72.73389, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 0.17523, qf2_loss: 0.17586, policy_loss: -31.10528, policy_entropy: -7.86629, alpha: 0.00477, time: 50.42065
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 25.53445, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 0.15380, qf2_loss: 0.15635, policy_loss: -30.90306, policy_entropy: -7.70209, alpha: 0.00484, time: 50.37036
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 17.44322, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 0.12130, qf2_loss: 0.12303, policy_loss: -30.91076, policy_entropy: -6.94054, alpha: 0.00492, time: 50.65527
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 19.85536, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 0.11840, qf2_loss: 0.11894, policy_loss: -30.93716, policy_entropy: -5.74054, alpha: 0.00494, time: 50.78884
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 27.29913, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 0.11904, qf2_loss: 0.11925, policy_loss: -30.94908, policy_entropy: -6.02919, alpha: 0.00492, time: 50.40680
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 42.06178, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 0.14841, qf2_loss: 0.14822, policy_loss: -30.98633, policy_entropy: -4.99721, alpha: 0.00490, time: 50.47301
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 27.60133, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 0.10503, qf2_loss: 0.10538, policy_loss: -30.95728, policy_entropy: -5.75871, alpha: 0.00484, time: 50.39896
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 26.69826, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 0.10920, qf2_loss: 0.10989, policy_loss: -31.00348, policy_entropy: -5.00244, alpha: 0.00485, time: 50.52583
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 23.90157, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 0.09708, qf2_loss: 0.09649, policy_loss: -30.93112, policy_entropy: -4.59347, alpha: 0.00477, time: 50.58670
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 19.52656, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 0.12628, qf2_loss: 0.12512, policy_loss: -30.97078, policy_entropy: -3.83361, alpha: 0.00468, time: 50.45228
[CW] eval: return: 21.24705, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 17.94798, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 0.10845, qf2_loss: 0.10935, policy_loss: -31.00825, policy_entropy: -6.03700, alpha: 0.00461, time: 50.77643
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 17.85379, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 0.09711, qf2_loss: 0.09688, policy_loss: -30.89240, policy_entropy: -5.48975, alpha: 0.00460, time: 50.55050
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 18.22233, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 0.10616, qf2_loss: 0.10409, policy_loss: -30.84138, policy_entropy: -4.70450, alpha: 0.00457, time: 50.60824
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 26.60413, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 0.11260, qf2_loss: 0.11342, policy_loss: -30.92550, policy_entropy: -4.97950, alpha: 0.00448, time: 50.71408
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 18.01380, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 0.08840, qf2_loss: 0.08813, policy_loss: -30.78247, policy_entropy: -4.00193, alpha: 0.00441, time: 50.77924
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 18.43172, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 0.12883, qf2_loss: 0.12748, policy_loss: -30.84808, policy_entropy: -5.08871, alpha: 0.00432, time: 50.59209
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 27.98722, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 0.09234, qf2_loss: 0.09173, policy_loss: -30.71844, policy_entropy: -3.99612, alpha: 0.00426, time: 50.42436
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 57.85834, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 0.09045, qf2_loss: 0.09049, policy_loss: -30.70575, policy_entropy: -3.52006, alpha: 0.00414, time: 50.42084
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 22.98782, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 0.08805, qf2_loss: 0.08645, policy_loss: -30.60151, policy_entropy: -3.45650, alpha: 0.00402, time: 50.46121
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 64.19183, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 0.09916, qf2_loss: 0.09830, policy_loss: -30.62767, policy_entropy: -4.23656, alpha: 0.00392, time: 50.76856
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 18.24014, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 0.10122, qf2_loss: 0.10238, policy_loss: -30.38460, policy_entropy: -2.62343, alpha: 0.00382, time: 50.69919
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 22.16586, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 0.07376, qf2_loss: 0.07334, policy_loss: -30.37440, policy_entropy: -3.08905, alpha: 0.00369, time: 51.37315
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 25.01431, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 0.07022, qf2_loss: 0.06944, policy_loss: -30.32310, policy_entropy: -3.90435, alpha: 0.00359, time: 51.23528
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 18.52699, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 0.08298, qf2_loss: 0.08312, policy_loss: -30.31076, policy_entropy: -4.23265, alpha: 0.00352, time: 51.20441
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 15.05605, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 0.08307, qf2_loss: 0.08340, policy_loss: -30.28242, policy_entropy: -4.99573, alpha: 0.00347, time: 50.72378
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 28.11807, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 0.07270, qf2_loss: 0.07239, policy_loss: -30.03497, policy_entropy: -4.26269, alpha: 0.00342, time: 50.96001
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 59.99257, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 0.07280, qf2_loss: 0.07190, policy_loss: -30.08789, policy_entropy: -4.13953, alpha: 0.00335, time: 50.73200
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 56.91161, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 0.07675, qf2_loss: 0.07623, policy_loss: -30.02540, policy_entropy: -4.07260, alpha: 0.00328, time: 50.83348
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 7.09716, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 0.06805, qf2_loss: 0.06815, policy_loss: -29.85252, policy_entropy: -5.17646, alpha: 0.00323, time: 51.28193
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 57.15799, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 0.05913, qf2_loss: 0.05975, policy_loss: -29.83107, policy_entropy: -5.76331, alpha: 0.00321, time: 51.39685
[CW] eval: return: 31.17790, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 5.55523, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 0.05976, qf2_loss: 0.05895, policy_loss: -29.72706, policy_entropy: -5.63784, alpha: 0.00320, time: 51.26724
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 4.46200, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 0.06478, qf2_loss: 0.06473, policy_loss: -29.70700, policy_entropy: -4.70364, alpha: 0.00318, time: 50.83633
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 4.36308, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 0.07349, qf2_loss: 0.07388, policy_loss: -29.44617, policy_entropy: -4.25269, alpha: 0.00312, time: 50.72003
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 43.65612, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 0.06271, qf2_loss: 0.06350, policy_loss: -29.36476, policy_entropy: -3.43081, alpha: 0.00304, time: 51.07986
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 43.71399, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 0.05020, qf2_loss: 0.04955, policy_loss: -29.40757, policy_entropy: -3.29816, alpha: 0.00295, time: 51.29595
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 41.45711, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 0.04719, qf2_loss: 0.04664, policy_loss: -29.29700, policy_entropy: -3.19923, alpha: 0.00285, time: 50.80616
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 40.03948, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 0.04823, qf2_loss: 0.04879, policy_loss: -29.14417, policy_entropy: -3.54665, alpha: 0.00278, time: 50.86356
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 44.08892, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 0.04880, qf2_loss: 0.04894, policy_loss: -29.08503, policy_entropy: -4.69018, alpha: 0.00271, time: 50.82835
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 43.60173, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 0.04255, qf2_loss: 0.04168, policy_loss: -28.97477, policy_entropy: -5.15420, alpha: 0.00268, time: 50.44086
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 43.57249, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 0.04137, qf2_loss: 0.04120, policy_loss: -28.83027, policy_entropy: -5.31295, alpha: 0.00266, time: 50.74437
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 59.73321, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 0.03807, qf2_loss: 0.03848, policy_loss: -28.85045, policy_entropy: -4.87137, alpha: 0.00263, time: 51.16629
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 44.24716, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 0.04018, qf2_loss: 0.03954, policy_loss: -28.69030, policy_entropy: -3.64441, alpha: 0.00258, time: 50.57054
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 43.88341, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 0.04079, qf2_loss: 0.04173, policy_loss: -28.63426, policy_entropy: -4.23774, alpha: 0.00251, time: 50.64204
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 6.11618, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 0.03891, qf2_loss: 0.03919, policy_loss: -28.58543, policy_entropy: -4.38639, alpha: 0.00247, time: 50.53482
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 44.16612, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 0.07927, qf2_loss: 0.07801, policy_loss: -28.50032, policy_entropy: -5.16207, alpha: 0.00243, time: 50.32409
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 60.46609, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 0.03361, qf2_loss: 0.03352, policy_loss: -28.36731, policy_entropy: -6.96741, alpha: 0.00243, time: 50.50589
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 44.46142, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 0.03621, qf2_loss: 0.03734, policy_loss: -28.38540, policy_entropy: -6.82269, alpha: 0.00246, time: 50.60571
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 24.26499, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 0.03305, qf2_loss: 0.03267, policy_loss: -28.18558, policy_entropy: -8.41986, alpha: 0.00251, time: 50.61460
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 43.15371, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 0.03829, qf2_loss: 0.03819, policy_loss: -28.13779, policy_entropy: -7.62742, alpha: 0.00258, time: 50.83067
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 44.43467, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 0.03611, qf2_loss: 0.03557, policy_loss: -28.10935, policy_entropy: -6.75089, alpha: 0.00262, time: 50.49428
[CW] eval: return: 34.91789, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 44.29912, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 0.03685, qf2_loss: 0.03744, policy_loss: -27.95796, policy_entropy: -5.09170, alpha: 0.00263, time: 50.43846
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 57.89240, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 0.03828, qf2_loss: 0.03859, policy_loss: -27.83969, policy_entropy: -5.33648, alpha: 0.00259, time: 50.87241
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 61.40967, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 0.05228, qf2_loss: 0.05294, policy_loss: -27.83375, policy_entropy: -4.33998, alpha: 0.00255, time: 50.45539
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 28.40200, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 0.05368, qf2_loss: 0.05244, policy_loss: -27.73391, policy_entropy: -4.67056, alpha: 0.00250, time: 50.85318
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 43.92654, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 0.03667, qf2_loss: 0.03750, policy_loss: -27.52084, policy_entropy: -5.04647, alpha: 0.00245, time: 50.58224
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 45.39351, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 0.03236, qf2_loss: 0.03213, policy_loss: -27.56644, policy_entropy: -5.47135, alpha: 0.00242, time: 50.46178
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 45.40769, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 0.04650, qf2_loss: 0.04582, policy_loss: -27.40390, policy_entropy: -6.96306, alpha: 0.00244, time: 50.80166
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 44.03129, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 0.03989, qf2_loss: 0.04089, policy_loss: -27.34724, policy_entropy: -6.56995, alpha: 0.00246, time: 50.36095
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 44.23777, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 0.03166, qf2_loss: 0.03128, policy_loss: -27.21438, policy_entropy: -5.23859, alpha: 0.00246, time: 50.62107
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 43.36837, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 0.03683, qf2_loss: 0.03755, policy_loss: -27.21055, policy_entropy: -4.64678, alpha: 0.00242, time: 50.72383
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 44.28317, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 0.04271, qf2_loss: 0.04249, policy_loss: -27.06774, policy_entropy: -5.46048, alpha: 0.00238, time: 50.47817
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 32.06203, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 0.04560, qf2_loss: 0.04697, policy_loss: -27.03450, policy_entropy: -5.13725, alpha: 0.00236, time: 50.85206
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 28.52232, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 0.04882, qf2_loss: 0.04732, policy_loss: -26.89919, policy_entropy: -6.08216, alpha: 0.00233, time: 50.75306
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 44.08447, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 0.04476, qf2_loss: 0.04548, policy_loss: -26.84212, policy_entropy: -3.28555, alpha: 0.00231, time: 50.80271
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 44.28570, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 0.03925, qf2_loss: 0.04021, policy_loss: -26.73542, policy_entropy: -3.18072, alpha: 0.00220, time: 50.30890
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 41.34008, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 0.03225, qf2_loss: 0.03226, policy_loss: -26.67821, policy_entropy: -4.17971, alpha: 0.00212, time: 50.95456
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 44.40921, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 0.03450, qf2_loss: 0.03404, policy_loss: -26.57783, policy_entropy: -4.86407, alpha: 0.00208, time: 50.88280
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 50.31157, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 0.04278, qf2_loss: 0.04298, policy_loss: -26.51366, policy_entropy: -7.11164, alpha: 0.00208, time: 50.97941
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 9.33480, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 0.04920, qf2_loss: 0.04966, policy_loss: -26.49106, policy_entropy: -8.73001, alpha: 0.00213, time: 50.48797
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 44.29292, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 0.05224, qf2_loss: 0.05233, policy_loss: -26.51198, policy_entropy: -6.66003, alpha: 0.00220, time: 50.89433
[CW] eval: return: 46.52968, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 43.00362, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 0.05284, qf2_loss: 0.05286, policy_loss: -26.33631, policy_entropy: -6.16068, alpha: 0.00220, time: 51.24235
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 46.37237, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 0.05273, qf2_loss: 0.05257, policy_loss: -26.29268, policy_entropy: -5.03937, alpha: 0.00219, time: 51.27906
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 50.29104, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 0.04325, qf2_loss: 0.04320, policy_loss: -26.31587, policy_entropy: -5.48577, alpha: 0.00216, time: 50.79520
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 89.13761, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 0.07293, qf2_loss: 0.07488, policy_loss: -26.23986, policy_entropy: -6.62565, alpha: 0.00216, time: 50.93897
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 17.90797, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 0.05275, qf2_loss: 0.05268, policy_loss: -26.12932, policy_entropy: -6.81443, alpha: 0.00219, time: 50.74976
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 29.03659, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 0.05046, qf2_loss: 0.05067, policy_loss: -25.98178, policy_entropy: -6.37520, alpha: 0.00221, time: 50.57941
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 17.88532, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 0.05771, qf2_loss: 0.05824, policy_loss: -25.97171, policy_entropy: -5.65301, alpha: 0.00222, time: 50.59363
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 17.56257, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 0.06369, qf2_loss: 0.06371, policy_loss: -25.79832, policy_entropy: -4.72961, alpha: 0.00220, time: 51.14236
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 75.74148, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 0.07387, qf2_loss: 0.07505, policy_loss: -25.80942, policy_entropy: -4.72486, alpha: 0.00213, time: 50.93270
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 39.37631, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 0.05258, qf2_loss: 0.05257, policy_loss: -25.73863, policy_entropy: -6.22642, alpha: 0.00212, time: 50.92370
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 33.97224, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 0.04846, qf2_loss: 0.04832, policy_loss: -25.82490, policy_entropy: -5.13665, alpha: 0.00211, time: 50.84175
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 44.88142, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 0.05324, qf2_loss: 0.05331, policy_loss: -25.51727, policy_entropy: -4.32187, alpha: 0.00206, time: 50.88494
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 62.78390, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 0.04761, qf2_loss: 0.04825, policy_loss: -25.45892, policy_entropy: -5.35056, alpha: 0.00202, time: 50.73817
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 49.63633, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 0.05144, qf2_loss: 0.05228, policy_loss: -25.40332, policy_entropy: -5.65868, alpha: 0.00200, time: 50.94081
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 44.37383, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 0.04759, qf2_loss: 0.04733, policy_loss: -25.35629, policy_entropy: -6.51570, alpha: 0.00201, time: 50.64012
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 52.02935, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 0.04370, qf2_loss: 0.04376, policy_loss: -25.32328, policy_entropy: -6.58766, alpha: 0.00202, time: 50.77720
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 61.03183, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 0.05433, qf2_loss: 0.05401, policy_loss: -25.25368, policy_entropy: -6.12753, alpha: 0.00205, time: 50.84028
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 46.67859, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 0.05225, qf2_loss: 0.05255, policy_loss: -25.26428, policy_entropy: -5.39254, alpha: 0.00204, time: 50.96089
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 58.73533, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 0.04949, qf2_loss: 0.05075, policy_loss: -25.10195, policy_entropy: -5.04989, alpha: 0.00201, time: 51.25147
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 45.94821, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 0.06672, qf2_loss: 0.06639, policy_loss: -25.11678, policy_entropy: -5.53721, alpha: 0.00197, time: 50.80397
[CW] eval: return: 56.98073, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 81.35194, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 0.06122, qf2_loss: 0.06111, policy_loss: -25.05384, policy_entropy: -6.07973, alpha: 0.00197, time: 52.49563
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 57.45678, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 0.07282, qf2_loss: 0.07475, policy_loss: -24.94057, policy_entropy: -5.96560, alpha: 0.00196, time: 51.36825
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 61.83467, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 0.05825, qf2_loss: 0.05779, policy_loss: -24.98589, policy_entropy: -6.43641, alpha: 0.00196, time: 51.40679
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 61.33377, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 0.05690, qf2_loss: 0.05641, policy_loss: -25.07110, policy_entropy: -6.74342, alpha: 0.00200, time: 50.78954
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 68.57721, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 0.05311, qf2_loss: 0.05398, policy_loss: -24.96781, policy_entropy: -6.89798, alpha: 0.00203, time: 50.65254
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 21.78040, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 0.05693, qf2_loss: 0.05786, policy_loss: -24.64342, policy_entropy: -6.59970, alpha: 0.00208, time: 50.97352
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 44.98120, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 0.05142, qf2_loss: 0.05178, policy_loss: -24.65074, policy_entropy: -7.12362, alpha: 0.00212, time: 50.73276
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 46.03699, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 0.05154, qf2_loss: 0.05272, policy_loss: -24.59450, policy_entropy: -6.17555, alpha: 0.00216, time: 50.72061
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 57.83781, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 0.05653, qf2_loss: 0.05628, policy_loss: -24.53814, policy_entropy: -6.41556, alpha: 0.00217, time: 50.65734
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 68.98559, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 0.05534, qf2_loss: 0.05434, policy_loss: -24.50705, policy_entropy: -6.20652, alpha: 0.00217, time: 52.42831
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 95.90125, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 0.05561, qf2_loss: 0.05572, policy_loss: -24.51007, policy_entropy: -6.10092, alpha: 0.00220, time: 51.10370
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 75.19844, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 0.06225, qf2_loss: 0.06385, policy_loss: -24.52159, policy_entropy: -5.37627, alpha: 0.00217, time: 50.87467
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 64.57540, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 0.06251, qf2_loss: 0.06303, policy_loss: -24.39916, policy_entropy: -6.05742, alpha: 0.00216, time: 51.17981
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 58.26796, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 0.05840, qf2_loss: 0.05844, policy_loss: -24.33799, policy_entropy: -6.31780, alpha: 0.00214, time: 51.17285
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 55.25925, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 0.07618, qf2_loss: 0.07589, policy_loss: -24.32556, policy_entropy: -5.85942, alpha: 0.00220, time: 50.63196
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 42.94045, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 0.05447, qf2_loss: 0.05487, policy_loss: -24.32598, policy_entropy: -7.28696, alpha: 0.00218, time: 50.72711
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 114.08546, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 0.06113, qf2_loss: 0.06172, policy_loss: -24.25534, policy_entropy: -8.02076, alpha: 0.00227, time: 50.72299
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 45.81899, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 0.06940, qf2_loss: 0.06890, policy_loss: -24.19317, policy_entropy: -6.41262, alpha: 0.00233, time: 50.96109
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 59.72895, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 0.05165, qf2_loss: 0.05181, policy_loss: -24.10926, policy_entropy: -5.92681, alpha: 0.00234, time: 50.85309
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 42.60965, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 0.07188, qf2_loss: 0.07329, policy_loss: -24.04212, policy_entropy: -4.89277, alpha: 0.00232, time: 50.78823
[CW] eval: return: 50.47420, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 28.47657, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 0.05450, qf2_loss: 0.05431, policy_loss: -23.99201, policy_entropy: -5.45312, alpha: 0.00227, time: 50.83882
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 46.24837, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 0.05762, qf2_loss: 0.05668, policy_loss: -23.88726, policy_entropy: -5.20011, alpha: 0.00224, time: 50.66216
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 51.97155, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 0.05185, qf2_loss: 0.05157, policy_loss: -23.93118, policy_entropy: -5.11050, alpha: 0.00220, time: 50.53721
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 23.34221, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 0.04613, qf2_loss: 0.04520, policy_loss: -23.85445, policy_entropy: -6.24561, alpha: 0.00217, time: 50.88179
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 52.95851, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 0.04592, qf2_loss: 0.04630, policy_loss: -23.82099, policy_entropy: -6.26613, alpha: 0.00220, time: 50.98631
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 17.09541, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 0.04552, qf2_loss: 0.04540, policy_loss: -23.74791, policy_entropy: -5.17493, alpha: 0.00218, time: 50.80323
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 62.93932, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 0.05173, qf2_loss: 0.05304, policy_loss: -23.70187, policy_entropy: -5.15507, alpha: 0.00215, time: 51.04373
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 88.78709, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 0.04404, qf2_loss: 0.04330, policy_loss: -23.72167, policy_entropy: -4.78042, alpha: 0.00209, time: 50.73093
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 14.14224, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 0.05241, qf2_loss: 0.05263, policy_loss: -23.53782, policy_entropy: -5.24437, alpha: 0.00204, time: 51.00556
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 64.14875, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 0.04683, qf2_loss: 0.04569, policy_loss: -23.57027, policy_entropy: -5.79153, alpha: 0.00202, time: 50.93759
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 66.31037, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 0.04524, qf2_loss: 0.04517, policy_loss: -23.57032, policy_entropy: -7.14995, alpha: 0.00204, time: 51.11623
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 76.55867, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 0.05558, qf2_loss: 0.05706, policy_loss: -23.45955, policy_entropy: -6.47973, alpha: 0.00208, time: 51.05989
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 43.61129, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 0.05482, qf2_loss: 0.05398, policy_loss: -23.46846, policy_entropy: -7.45713, alpha: 0.00212, time: 51.49730
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 71.51635, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 0.05167, qf2_loss: 0.05391, policy_loss: -23.31981, policy_entropy: -6.65472, alpha: 0.00219, time: 50.71677
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 22.46229, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 0.04904, qf2_loss: 0.04829, policy_loss: -23.25844, policy_entropy: -6.45375, alpha: 0.00221, time: 50.85542
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 100.56392, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 0.05494, qf2_loss: 0.05571, policy_loss: -23.28978, policy_entropy: -6.90580, alpha: 0.00224, time: 51.02850
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 78.30304, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 0.07082, qf2_loss: 0.06985, policy_loss: -23.18635, policy_entropy: -6.41022, alpha: 0.00229, time: 50.85652
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 45.40258, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 0.04731, qf2_loss: 0.04756, policy_loss: -23.03557, policy_entropy: -6.65463, alpha: 0.00233, time: 51.32882
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 43.45818, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 0.06536, qf2_loss: 0.06595, policy_loss: -23.14223, policy_entropy: -4.65849, alpha: 0.00231, time: 50.95474
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 54.31706, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 0.05845, qf2_loss: 0.05867, policy_loss: -23.04189, policy_entropy: -6.28032, alpha: 0.00228, time: 50.72646
[CW] eval: return: 79.27451, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 33.83811, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 0.05053, qf2_loss: 0.05130, policy_loss: -23.01267, policy_entropy: -7.26565, alpha: 0.00231, time: 51.01486
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 73.77302, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 0.05858, qf2_loss: 0.05842, policy_loss: -22.96237, policy_entropy: -7.95361, alpha: 0.00240, time: 50.67558
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 97.07261, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 0.05983, qf2_loss: 0.06098, policy_loss: -23.03203, policy_entropy: -7.77231, alpha: 0.00251, time: 50.59218
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 78.73491, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 0.05763, qf2_loss: 0.05766, policy_loss: -22.99071, policy_entropy: -7.74907, alpha: 0.00260, time: 51.05302
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 94.47792, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 0.08283, qf2_loss: 0.08218, policy_loss: -22.78821, policy_entropy: -7.28347, alpha: 0.00268, time: 51.08670
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 52.79570, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 0.05540, qf2_loss: 0.05530, policy_loss: -22.76141, policy_entropy: -8.05846, alpha: 0.00279, time: 51.26771
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 93.31313, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 0.06886, qf2_loss: 0.06810, policy_loss: -22.71122, policy_entropy: -7.11116, alpha: 0.00289, time: 51.25648
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 56.90455, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 0.07698, qf2_loss: 0.07880, policy_loss: -22.71786, policy_entropy: -7.41472, alpha: 0.00296, time: 50.87624
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 83.86955, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 0.06727, qf2_loss: 0.06744, policy_loss: -22.70650, policy_entropy: -7.14580, alpha: 0.00305, time: 51.04943
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 103.68664, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 0.07263, qf2_loss: 0.07201, policy_loss: -22.57993, policy_entropy: -7.03499, alpha: 0.00309, time: 50.46950
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 59.76307, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 0.06727, qf2_loss: 0.06892, policy_loss: -22.60719, policy_entropy: -6.66011, alpha: 0.00316, time: 50.84824
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 118.62413, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 0.06689, qf2_loss: 0.06623, policy_loss: -22.68540, policy_entropy: -6.53062, alpha: 0.00321, time: 50.91998
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 84.54847, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 0.07235, qf2_loss: 0.07200, policy_loss: -22.50839, policy_entropy: -6.85570, alpha: 0.00326, time: 51.01893
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 83.84094, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 0.07192, qf2_loss: 0.07199, policy_loss: -22.58858, policy_entropy: -7.26429, alpha: 0.00334, time: 50.97871
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 112.30589, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 0.10185, qf2_loss: 0.10224, policy_loss: -22.55675, policy_entropy: -6.66819, alpha: 0.00342, time: 50.79889
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 116.63152, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 0.06630, qf2_loss: 0.06607, policy_loss: -22.50178, policy_entropy: -7.71310, alpha: 0.00350, time: 50.96547
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 56.09028, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 0.08464, qf2_loss: 0.08513, policy_loss: -22.53551, policy_entropy: -7.70018, alpha: 0.00364, time: 51.03669
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 46.29227, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 0.07596, qf2_loss: 0.07551, policy_loss: -22.48941, policy_entropy: -7.22241, alpha: 0.00378, time: 51.10188
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 28.49021, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 0.06873, qf2_loss: 0.06861, policy_loss: -22.39997, policy_entropy: -6.86532, alpha: 0.00387, time: 51.01307
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 117.15168, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 0.07763, qf2_loss: 0.07743, policy_loss: -22.50499, policy_entropy: -7.21852, alpha: 0.00398, time: 50.90121
[CW] eval: return: 49.59811, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 92.66610, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 0.08744, qf2_loss: 0.08721, policy_loss: -22.40352, policy_entropy: -6.64872, alpha: 0.00408, time: 51.10728
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 51.02307, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 0.08124, qf2_loss: 0.08077, policy_loss: -22.48525, policy_entropy: -6.52206, alpha: 0.00413, time: 50.76247
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 109.52950, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 0.07482, qf2_loss: 0.07403, policy_loss: -22.43792, policy_entropy: -6.78391, alpha: 0.00420, time: 50.78202
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 115.91617, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 0.08724, qf2_loss: 0.08731, policy_loss: -22.53568, policy_entropy: -6.50041, alpha: 0.00428, time: 51.03567
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 46.55314, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 0.08034, qf2_loss: 0.07881, policy_loss: -22.45348, policy_entropy: -5.89820, alpha: 0.00431, time: 50.57514
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 55.94334, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 0.08351, qf2_loss: 0.08405, policy_loss: -22.44543, policy_entropy: -6.13020, alpha: 0.00431, time: 50.93508
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 65.38125, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 0.11451, qf2_loss: 0.11287, policy_loss: -22.40391, policy_entropy: -5.91751, alpha: 0.00431, time: 50.89513
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 102.77608, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 0.12154, qf2_loss: 0.12346, policy_loss: -22.41242, policy_entropy: -6.09532, alpha: 0.00433, time: 51.13597
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 97.53798, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 0.08450, qf2_loss: 0.08385, policy_loss: -22.48080, policy_entropy: -6.09909, alpha: 0.00434, time: 51.42894
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 46.55498, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 0.09404, qf2_loss: 0.09321, policy_loss: -22.40615, policy_entropy: -6.11625, alpha: 0.00435, time: 50.94871
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 35.29076, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 0.09270, qf2_loss: 0.09061, policy_loss: -22.42224, policy_entropy: -6.13707, alpha: 0.00435, time: 50.92788
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 62.42798, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 0.08236, qf2_loss: 0.08131, policy_loss: -22.44927, policy_entropy: -6.39945, alpha: 0.00438, time: 51.47076
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 78.75540, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 0.09037, qf2_loss: 0.08894, policy_loss: -22.45377, policy_entropy: -6.33873, alpha: 0.00446, time: 51.41443
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 106.03744, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 0.09465, qf2_loss: 0.09185, policy_loss: -22.47870, policy_entropy: -6.00055, alpha: 0.00447, time: 51.26562
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 74.09086, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 0.09393, qf2_loss: 0.09255, policy_loss: -22.42498, policy_entropy: -6.18719, alpha: 0.00450, time: 50.54996
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 121.66980, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 0.09547, qf2_loss: 0.09235, policy_loss: -22.68901, policy_entropy: -6.54887, alpha: 0.00456, time: 50.73979
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 95.09824, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 0.10278, qf2_loss: 0.10207, policy_loss: -22.58810, policy_entropy: -6.38382, alpha: 0.00463, time: 50.91510
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 98.79982, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 0.11033, qf2_loss: 0.10824, policy_loss: -22.63816, policy_entropy: -6.38612, alpha: 0.00469, time: 50.64754
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 124.04812, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 0.09832, qf2_loss: 0.09602, policy_loss: -22.72760, policy_entropy: -6.12607, alpha: 0.00477, time: 50.75305
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 143.65929, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 0.10399, qf2_loss: 0.10058, policy_loss: -22.77023, policy_entropy: -6.36363, alpha: 0.00479, time: 52.36708
[CW] eval: return: 116.06301, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 127.63238, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 0.10395, qf2_loss: 0.10163, policy_loss: -22.76833, policy_entropy: -6.39079, alpha: 0.00485, time: 51.13334
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 107.06022, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 0.11744, qf2_loss: 0.11494, policy_loss: -22.82665, policy_entropy: -6.01640, alpha: 0.00493, time: 50.70369
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 113.53553, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 0.10386, qf2_loss: 0.10171, policy_loss: -22.91970, policy_entropy: -6.32366, alpha: 0.00494, time: 50.78963
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 116.03923, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 0.09564, qf2_loss: 0.09426, policy_loss: -22.90410, policy_entropy: -6.11725, alpha: 0.00501, time: 50.85136
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 112.47935, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 0.09941, qf2_loss: 0.09748, policy_loss: -22.93586, policy_entropy: -5.89766, alpha: 0.00500, time: 51.08833
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 100.50718, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 0.09758, qf2_loss: 0.09616, policy_loss: -22.92314, policy_entropy: -5.69357, alpha: 0.00496, time: 50.86798
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 111.18503, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 0.11608, qf2_loss: 0.11629, policy_loss: -23.05111, policy_entropy: -5.73443, alpha: 0.00489, time: 50.64837
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 104.59220, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 0.09499, qf2_loss: 0.09446, policy_loss: -23.04267, policy_entropy: -5.55378, alpha: 0.00482, time: 50.64589
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 137.87636, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 0.09852, qf2_loss: 0.09661, policy_loss: -23.13733, policy_entropy: -6.01479, alpha: 0.00478, time: 51.00477
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 22.21952, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 0.10348, qf2_loss: 0.10354, policy_loss: -23.17544, policy_entropy: -5.78730, alpha: 0.00474, time: 50.76032
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 107.71139, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 0.09472, qf2_loss: 0.09361, policy_loss: -23.11708, policy_entropy: -5.63109, alpha: 0.00471, time: 50.54049
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 19.33777, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 0.10358, qf2_loss: 0.10548, policy_loss: -23.19995, policy_entropy: -5.30058, alpha: 0.00461, time: 50.55581
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 83.45986, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 0.09600, qf2_loss: 0.09522, policy_loss: -23.18224, policy_entropy: -5.58948, alpha: 0.00448, time: 50.77772
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 33.34768, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 0.09095, qf2_loss: 0.09202, policy_loss: -23.26453, policy_entropy: -5.48449, alpha: 0.00442, time: 50.83716
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 50.59334, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 0.08712, qf2_loss: 0.08581, policy_loss: -23.25433, policy_entropy: -5.40001, alpha: 0.00432, time: 50.47244
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 56.00426, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 0.08602, qf2_loss: 0.08595, policy_loss: -23.27627, policy_entropy: -5.80087, alpha: 0.00423, time: 51.19399
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 52.41806, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 0.10136, qf2_loss: 0.10221, policy_loss: -23.28929, policy_entropy: -6.52878, alpha: 0.00425, time: 50.83537
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 33.42440, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 0.08938, qf2_loss: 0.08928, policy_loss: -23.24839, policy_entropy: -6.21016, alpha: 0.00432, time: 50.85019
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 49.10373, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 0.08073, qf2_loss: 0.07959, policy_loss: -23.21002, policy_entropy: -6.03500, alpha: 0.00435, time: 50.78274
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 13.78783, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 0.08514, qf2_loss: 0.08659, policy_loss: -23.21288, policy_entropy: -5.71467, alpha: 0.00434, time: 51.37642
[CW] eval: return: 31.73675, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 13.72801, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 0.07133, qf2_loss: 0.07158, policy_loss: -23.24107, policy_entropy: -4.48303, alpha: 0.00421, time: 51.00064
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 61.98862, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 0.07156, qf2_loss: 0.07133, policy_loss: -23.19866, policy_entropy: -4.61160, alpha: 0.00399, time: 50.80640
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 48.68647, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 0.05862, qf2_loss: 0.05886, policy_loss: -23.18091, policy_entropy: -4.67506, alpha: 0.00382, time: 50.64012
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 53.19153, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 0.07068, qf2_loss: 0.07211, policy_loss: -23.16835, policy_entropy: -4.98362, alpha: 0.00369, time: 50.84036
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 28.38908, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 0.06007, qf2_loss: 0.05975, policy_loss: -23.13787, policy_entropy: -4.76853, alpha: 0.00358, time: 50.55481
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 78.94990, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 0.06354, qf2_loss: 0.06341, policy_loss: -23.11558, policy_entropy: -4.74400, alpha: 0.00348, time: 50.94574
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 35.38408, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 0.05527, qf2_loss: 0.05660, policy_loss: -23.06195, policy_entropy: -4.46954, alpha: 0.00336, time: 51.15661
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 33.33803, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 0.06764, qf2_loss: 0.06702, policy_loss: -22.99729, policy_entropy: -4.71590, alpha: 0.00325, time: 50.79832
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 68.18763, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 0.05197, qf2_loss: 0.05313, policy_loss: -23.05389, policy_entropy: -5.10160, alpha: 0.00317, time: 50.72058
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 30.47824, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 0.05092, qf2_loss: 0.05172, policy_loss: -22.99982, policy_entropy: -5.00597, alpha: 0.00310, time: 51.03726
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 63.63480, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 0.05159, qf2_loss: 0.05067, policy_loss: -23.00780, policy_entropy: -5.03917, alpha: 0.00304, time: 50.88547
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 64.14142, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 0.05187, qf2_loss: 0.05296, policy_loss: -22.89787, policy_entropy: -4.71774, alpha: 0.00297, time: 51.05343
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 27.29804, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 0.05555, qf2_loss: 0.05581, policy_loss: -22.87770, policy_entropy: -4.65578, alpha: 0.00289, time: 51.30569
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 23.56451, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 0.04758, qf2_loss: 0.04830, policy_loss: -22.86707, policy_entropy: -4.77996, alpha: 0.00281, time: 50.94221
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 23.32902, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 0.04953, qf2_loss: 0.04994, policy_loss: -22.85121, policy_entropy: -5.13965, alpha: 0.00274, time: 51.06394
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 22.81408, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 0.04124, qf2_loss: 0.04084, policy_loss: -22.78074, policy_entropy: -5.71941, alpha: 0.00272, time: 51.13231
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 23.66236, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 0.04445, qf2_loss: 0.04492, policy_loss: -22.76917, policy_entropy: -5.23482, alpha: 0.00268, time: 50.78519
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 24.34614, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 0.04470, qf2_loss: 0.04404, policy_loss: -22.75138, policy_entropy: -5.51614, alpha: 0.00265, time: 51.23055
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 22.30238, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 0.03977, qf2_loss: 0.03972, policy_loss: -22.66059, policy_entropy: -5.91169, alpha: 0.00263, time: 50.84579
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 125.54702, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 0.04694, qf2_loss: 0.04802, policy_loss: -22.66898, policy_entropy: -6.87570, alpha: 0.00265, time: 50.64642
[CW] eval: return: 104.47875, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 129.93335, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 0.04661, qf2_loss: 0.04637, policy_loss: -22.69921, policy_entropy: -7.46314, alpha: 0.00272, time: 51.55489
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 146.19523, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 0.04131, qf2_loss: 0.04032, policy_loss: -22.62042, policy_entropy: -7.11987, alpha: 0.00280, time: 51.39166
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 103.36147, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 0.03892, qf2_loss: 0.03996, policy_loss: -22.61204, policy_entropy: -7.14497, alpha: 0.00288, time: 50.57380
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 126.44367, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 0.04733, qf2_loss: 0.04816, policy_loss: -22.57520, policy_entropy: -7.19672, alpha: 0.00297, time: 50.75634
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 110.04148, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 0.04332, qf2_loss: 0.04330, policy_loss: -22.55971, policy_entropy: -7.08085, alpha: 0.00305, time: 51.05733
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 120.56820, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 0.03837, qf2_loss: 0.03906, policy_loss: -22.60857, policy_entropy: -7.14920, alpha: 0.00314, time: 50.83805
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 43.12636, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 0.04038, qf2_loss: 0.03928, policy_loss: -22.52535, policy_entropy: -6.80807, alpha: 0.00323, time: 50.89857
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 144.75373, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 0.05222, qf2_loss: 0.05343, policy_loss: -22.51142, policy_entropy: -6.52900, alpha: 0.00329, time: 51.01885
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 31.24361, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 0.03983, qf2_loss: 0.03975, policy_loss: -22.52209, policy_entropy: -6.50791, alpha: 0.00333, time: 51.36624
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 137.98464, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 0.03497, qf2_loss: 0.03465, policy_loss: -22.49665, policy_entropy: -6.36872, alpha: 0.00338, time: 50.98095
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 145.35428, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 0.03650, qf2_loss: 0.03723, policy_loss: -22.47565, policy_entropy: -6.34256, alpha: 0.00342, time: 50.98955
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 119.61777, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 0.04075, qf2_loss: 0.04100, policy_loss: -22.44402, policy_entropy: -6.18145, alpha: 0.00344, time: 50.96273
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 155.36415, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 0.04137, qf2_loss: 0.04102, policy_loss: -22.40800, policy_entropy: -6.71611, alpha: 0.00350, time: 51.03337
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 145.85570, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 0.03875, qf2_loss: 0.03998, policy_loss: -22.42200, policy_entropy: -6.69840, alpha: 0.00358, time: 51.02369
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 54.04502, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 0.04362, qf2_loss: 0.04262, policy_loss: -22.35376, policy_entropy: -6.33655, alpha: 0.00364, time: 51.32672
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 133.40563, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 0.05068, qf2_loss: 0.05070, policy_loss: -22.41058, policy_entropy: -6.55896, alpha: 0.00369, time: 51.17959
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 133.67476, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 0.04753, qf2_loss: 0.04843, policy_loss: -22.35103, policy_entropy: -6.16717, alpha: 0.00374, time: 50.88271
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 158.38016, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 0.04817, qf2_loss: 0.04904, policy_loss: -22.36898, policy_entropy: -5.88130, alpha: 0.00375, time: 50.88965
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 30.01712, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 0.04039, qf2_loss: 0.04058, policy_loss: -22.32614, policy_entropy: -5.75890, alpha: 0.00374, time: 51.48206
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 161.42530, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 0.04629, qf2_loss: 0.04623, policy_loss: -22.33305, policy_entropy: -5.79806, alpha: 0.00370, time: 51.05806
[CW] eval: return: 131.79539, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 158.51391, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 0.04188, qf2_loss: 0.04130, policy_loss: -22.32061, policy_entropy: -5.90029, alpha: 0.00367, time: 52.39982
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 141.70482, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 0.04731, qf2_loss: 0.04973, policy_loss: -22.26587, policy_entropy: -6.16731, alpha: 0.00369, time: 51.31502
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 154.79732, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 0.04033, qf2_loss: 0.04002, policy_loss: -22.29973, policy_entropy: -5.93575, alpha: 0.00369, time: 50.91396
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 150.62349, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 0.04169, qf2_loss: 0.04112, policy_loss: -22.29105, policy_entropy: -6.06259, alpha: 0.00369, time: 50.75460
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 145.56207, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 0.03863, qf2_loss: 0.03855, policy_loss: -22.29083, policy_entropy: -6.06435, alpha: 0.00370, time: 51.23899
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 30.31406, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 0.04490, qf2_loss: 0.04388, policy_loss: -22.23102, policy_entropy: -5.96253, alpha: 0.00371, time: 50.83847
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 127.44956, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 0.04483, qf2_loss: 0.04518, policy_loss: -22.18359, policy_entropy: -5.90444, alpha: 0.00370, time: 50.98594
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 36.18601, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 0.04492, qf2_loss: 0.04504, policy_loss: -22.19515, policy_entropy: -6.07209, alpha: 0.00370, time: 51.03471
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 143.14490, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 0.05578, qf2_loss: 0.05593, policy_loss: -22.18068, policy_entropy: -6.36237, alpha: 0.00371, time: 51.69909
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 154.69609, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 0.05368, qf2_loss: 0.05463, policy_loss: -22.19010, policy_entropy: -6.07864, alpha: 0.00376, time: 50.75572
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 29.11842, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 0.05916, qf2_loss: 0.05913, policy_loss: -22.10276, policy_entropy: -6.07057, alpha: 0.00377, time: 50.88428
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 158.57800, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 0.04949, qf2_loss: 0.04949, policy_loss: -22.17527, policy_entropy: -5.68838, alpha: 0.00377, time: 50.98145
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 50.13413, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 0.04203, qf2_loss: 0.04244, policy_loss: -22.07100, policy_entropy: -6.15129, alpha: 0.00374, time: 50.99254
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 149.97989, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 0.04116, qf2_loss: 0.04073, policy_loss: -22.13219, policy_entropy: -6.31958, alpha: 0.00377, time: 50.70418
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 68.30641, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 0.04595, qf2_loss: 0.04650, policy_loss: -22.12834, policy_entropy: -5.96310, alpha: 0.00381, time: 50.80781
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 136.12044, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 0.04301, qf2_loss: 0.04323, policy_loss: -22.04902, policy_entropy: -5.86703, alpha: 0.00379, time: 51.33715
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 128.51772, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 0.04334, qf2_loss: 0.04317, policy_loss: -22.15287, policy_entropy: -6.02537, alpha: 0.00377, time: 50.89239
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 134.05642, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 0.04877, qf2_loss: 0.04902, policy_loss: -22.06029, policy_entropy: -5.95624, alpha: 0.00376, time: 51.05327
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 61.66305, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 0.04342, qf2_loss: 0.04284, policy_loss: -22.08916, policy_entropy: -6.28358, alpha: 0.00379, time: 51.10007
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 147.57891, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 0.06839, qf2_loss: 0.06773, policy_loss: -22.09365, policy_entropy: -6.15029, alpha: 0.00383, time: 50.90560
[CW] eval: return: 84.45679, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 99.46105, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 0.05596, qf2_loss: 0.05532, policy_loss: -22.09285, policy_entropy: -6.39511, alpha: 0.00387, time: 50.90925
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 104.03766, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 0.04703, qf2_loss: 0.04710, policy_loss: -22.02372, policy_entropy: -5.78908, alpha: 0.00391, time: 50.88806
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 143.41474, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 0.04435, qf2_loss: 0.04434, policy_loss: -22.00195, policy_entropy: -5.86489, alpha: 0.00388, time: 50.57514
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 45.43842, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 0.04802, qf2_loss: 0.04828, policy_loss: -22.01342, policy_entropy: -6.09744, alpha: 0.00387, time: 50.71333
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 150.48004, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 0.06626, qf2_loss: 0.06543, policy_loss: -21.95495, policy_entropy: -5.89064, alpha: 0.00388, time: 51.02039
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 144.81347, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 0.06682, qf2_loss: 0.06711, policy_loss: -22.04475, policy_entropy: -6.01490, alpha: 0.00386, time: 50.82539
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 134.95302, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 0.04868, qf2_loss: 0.04798, policy_loss: -22.05476, policy_entropy: -6.41748, alpha: 0.00389, time: 50.75862
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 163.57083, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 0.05099, qf2_loss: 0.05118, policy_loss: -22.04719, policy_entropy: -6.13018, alpha: 0.00394, time: 51.15148
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 144.92621, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 0.05046, qf2_loss: 0.04973, policy_loss: -22.04313, policy_entropy: -6.16372, alpha: 0.00396, time: 51.08467
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 156.10663, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 0.05900, qf2_loss: 0.06024, policy_loss: -22.02653, policy_entropy: -6.44564, alpha: 0.00401, time: 51.09393
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 156.59540, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 0.05438, qf2_loss: 0.05365, policy_loss: -22.02530, policy_entropy: -6.33350, alpha: 0.00410, time: 50.86970
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 165.05628, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 0.05736, qf2_loss: 0.05744, policy_loss: -22.06689, policy_entropy: -6.08673, alpha: 0.00415, time: 51.06336
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 125.68348, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 0.04625, qf2_loss: 0.04574, policy_loss: -22.05757, policy_entropy: -6.18423, alpha: 0.00415, time: 51.09668
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 147.07271, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 0.05056, qf2_loss: 0.05079, policy_loss: -22.02022, policy_entropy: -6.06352, alpha: 0.00419, time: 51.17949
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 132.58119, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 0.04802, qf2_loss: 0.04741, policy_loss: -21.97186, policy_entropy: -5.81560, alpha: 0.00418, time: 51.46909
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 164.01116, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 0.05893, qf2_loss: 0.05869, policy_loss: -21.96022, policy_entropy: -6.16220, alpha: 0.00416, time: 51.18563
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 51.71863, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 0.05231, qf2_loss: 0.05125, policy_loss: -21.99728, policy_entropy: -6.55358, alpha: 0.00424, time: 50.86024
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 88.89677, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 0.05006, qf2_loss: 0.04957, policy_loss: -22.05239, policy_entropy: -6.42465, alpha: 0.00435, time: 51.00321
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 130.48626, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 0.05660, qf2_loss: 0.05684, policy_loss: -22.06896, policy_entropy: -6.35631, alpha: 0.00443, time: 51.17201
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 71.77499, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 0.05573, qf2_loss: 0.05607, policy_loss: -22.15141, policy_entropy: -6.12963, alpha: 0.00449, time: 51.33377
[CW] eval: return: 142.56456, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 137.74491, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 0.07817, qf2_loss: 0.07855, policy_loss: -22.06392, policy_entropy: -5.74722, alpha: 0.00449, time: 50.97887
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 144.04502, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 0.05433, qf2_loss: 0.05429, policy_loss: -22.06044, policy_entropy: -5.72050, alpha: 0.00443, time: 50.70527
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 113.02358, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 0.05592, qf2_loss: 0.05514, policy_loss: -22.05777, policy_entropy: -6.05718, alpha: 0.00438, time: 50.79735
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 98.46104, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 0.05345, qf2_loss: 0.05410, policy_loss: -22.09494, policy_entropy: -5.98440, alpha: 0.00440, time: 51.40718
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 111.44998, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 0.05406, qf2_loss: 0.05381, policy_loss: -22.11027, policy_entropy: -6.09580, alpha: 0.00440, time: 51.18403
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 70.76751, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 0.05772, qf2_loss: 0.05774, policy_loss: -22.17598, policy_entropy: -5.81021, alpha: 0.00440, time: 51.16692
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 166.97776, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 0.06520, qf2_loss: 0.06414, policy_loss: -22.09568, policy_entropy: -5.90731, alpha: 0.00436, time: 50.72243
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 148.52054, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 0.06954, qf2_loss: 0.06984, policy_loss: -22.15657, policy_entropy: -5.89878, alpha: 0.00435, time: 50.70615
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 157.36665, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 0.06098, qf2_loss: 0.06125, policy_loss: -22.13041, policy_entropy: -6.03455, alpha: 0.00433, time: 51.11335
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 118.73628, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 0.05879, qf2_loss: 0.05894, policy_loss: -22.14775, policy_entropy: -6.06309, alpha: 0.00434, time: 51.89571
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 163.18651, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 0.06794, qf2_loss: 0.06743, policy_loss: -22.14995, policy_entropy: -6.17956, alpha: 0.00437, time: 51.56844
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 158.89064, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 0.05679, qf2_loss: 0.05675, policy_loss: -22.16143, policy_entropy: -5.91683, alpha: 0.00438, time: 51.48630
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 152.50203, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 0.06347, qf2_loss: 0.06260, policy_loss: -22.12935, policy_entropy: -5.78330, alpha: 0.00434, time: 51.51912
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 150.84949, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 0.05901, qf2_loss: 0.05925, policy_loss: -22.21769, policy_entropy: -6.01440, alpha: 0.00432, time: 51.51937
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 157.35946, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 0.06293, qf2_loss: 0.06268, policy_loss: -22.21182, policy_entropy: -6.02779, alpha: 0.00433, time: 51.05006
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 134.02751, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 0.05755, qf2_loss: 0.05838, policy_loss: -22.13976, policy_entropy: -5.92390, alpha: 0.00433, time: 51.03222
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 141.01807, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 0.06963, qf2_loss: 0.06823, policy_loss: -22.10361, policy_entropy: -5.85493, alpha: 0.00429, time: 51.14235
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 146.21660, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 0.05823, qf2_loss: 0.05814, policy_loss: -22.18806, policy_entropy: -5.67288, alpha: 0.00425, time: 50.78001
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 140.33981, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 0.06835, qf2_loss: 0.06836, policy_loss: -22.17922, policy_entropy: -6.02761, alpha: 0.00422, time: 50.82566
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 167.69571, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 0.05908, qf2_loss: 0.05905, policy_loss: -22.27567, policy_entropy: -6.15915, alpha: 0.00423, time: 50.86636
[CW] eval: return: 148.29559, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 159.19586, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 0.06345, qf2_loss: 0.06219, policy_loss: -22.26481, policy_entropy: -5.93374, alpha: 0.00426, time: 51.19633
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 131.63988, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 0.06694, qf2_loss: 0.06684, policy_loss: -22.24582, policy_entropy: -5.74717, alpha: 0.00422, time: 51.11020
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 152.54146, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 0.06335, qf2_loss: 0.06305, policy_loss: -22.16771, policy_entropy: -6.08880, alpha: 0.00419, time: 50.74488
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 153.99046, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 0.06773, qf2_loss: 0.06685, policy_loss: -22.26883, policy_entropy: -6.36545, alpha: 0.00423, time: 51.23816
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 172.65599, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 0.05932, qf2_loss: 0.05919, policy_loss: -22.30887, policy_entropy: -6.19412, alpha: 0.00429, time: 50.89559
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 168.87275, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 0.06167, qf2_loss: 0.06163, policy_loss: -22.29372, policy_entropy: -6.27365, alpha: 0.00435, time: 51.13769
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 151.97910, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 0.06199, qf2_loss: 0.06286, policy_loss: -22.31891, policy_entropy: -5.77464, alpha: 0.00436, time: 51.22821
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 146.79665, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 0.06488, qf2_loss: 0.06432, policy_loss: -22.30394, policy_entropy: -5.62503, alpha: 0.00430, time: 50.99777
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 150.18926, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 0.06340, qf2_loss: 0.06404, policy_loss: -22.40350, policy_entropy: -6.03089, alpha: 0.00425, time: 51.02720
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 162.74091, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 0.06024, qf2_loss: 0.05888, policy_loss: -22.30645, policy_entropy: -6.36734, alpha: 0.00430, time: 51.05348
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 134.64464, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 0.05738, qf2_loss: 0.05806, policy_loss: -22.26465, policy_entropy: -6.19169, alpha: 0.00436, time: 51.88105
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 136.54290, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 0.06926, qf2_loss: 0.06838, policy_loss: -22.29246, policy_entropy: -6.25293, alpha: 0.00440, time: 51.08505
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 154.68749, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 0.07160, qf2_loss: 0.07088, policy_loss: -22.39050, policy_entropy: -6.03289, alpha: 0.00444, time: 51.00222
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 155.23785, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 0.06117, qf2_loss: 0.06140, policy_loss: -22.39405, policy_entropy: -6.25512, alpha: 0.00446, time: 50.93362
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 151.51886, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 0.06951, qf2_loss: 0.06924, policy_loss: -22.38606, policy_entropy: -6.24931, alpha: 0.00450, time: 50.77601
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 159.85754, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 0.05927, qf2_loss: 0.05883, policy_loss: -22.44910, policy_entropy: -6.21286, alpha: 0.00455, time: 51.00431
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 148.77930, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 0.07703, qf2_loss: 0.07764, policy_loss: -22.39726, policy_entropy: -6.17285, alpha: 0.00460, time: 51.14399
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 159.78648, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 0.06109, qf2_loss: 0.06117, policy_loss: -22.47132, policy_entropy: -6.22786, alpha: 0.00465, time: 51.43487
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 161.80951, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 0.06076, qf2_loss: 0.05928, policy_loss: -22.45155, policy_entropy: -6.18648, alpha: 0.00468, time: 50.74359
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 162.02411, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 0.07771, qf2_loss: 0.07784, policy_loss: -22.49876, policy_entropy: -6.03911, alpha: 0.00472, time: 50.74170
[CW] eval: return: 159.16378, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 145.97108, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 0.06950, qf2_loss: 0.06936, policy_loss: -22.49218, policy_entropy: -5.74031, alpha: 0.00469, time: 51.13017
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 157.72904, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 0.05861, qf2_loss: 0.05865, policy_loss: -22.44063, policy_entropy: -5.87267, alpha: 0.00465, time: 51.25985
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 161.14685, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 0.06092, qf2_loss: 0.06030, policy_loss: -22.43661, policy_entropy: -5.83019, alpha: 0.00462, time: 50.53994
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 160.34173, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 0.06808, qf2_loss: 0.06871, policy_loss: -22.55464, policy_entropy: -5.74015, alpha: 0.00457, time: 50.80827
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 156.18901, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 0.06997, qf2_loss: 0.06951, policy_loss: -22.54392, policy_entropy: -5.63746, alpha: 0.00451, time: 50.57383
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 167.20337, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 0.07595, qf2_loss: 0.07635, policy_loss: -22.50412, policy_entropy: -5.89562, alpha: 0.00445, time: 50.96511
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 154.58023, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 0.06234, qf2_loss: 0.06169, policy_loss: -22.61125, policy_entropy: -6.25111, alpha: 0.00447, time: 51.05033
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 143.32673, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 0.06816, qf2_loss: 0.06827, policy_loss: -22.45410, policy_entropy: -6.03064, alpha: 0.00450, time: 50.82746
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 157.65591, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 0.09086, qf2_loss: 0.09092, policy_loss: -22.58683, policy_entropy: -6.15026, alpha: 0.00452, time: 51.08631
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 133.37885, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 0.06594, qf2_loss: 0.06578, policy_loss: -22.73009, policy_entropy: -6.26022, alpha: 0.00455, time: 51.15993
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 156.85551, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 0.06606, qf2_loss: 0.06596, policy_loss: -22.63132, policy_entropy: -5.84494, alpha: 0.00458, time: 51.02737
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 149.90229, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 0.06784, qf2_loss: 0.06754, policy_loss: -22.60994, policy_entropy: -5.85138, alpha: 0.00455, time: 50.87778
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 37.80539, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 0.06285, qf2_loss: 0.06326, policy_loss: -22.70412, policy_entropy: -5.84067, alpha: 0.00451, time: 51.39412
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 143.62347, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 0.06069, qf2_loss: 0.06094, policy_loss: -22.61199, policy_entropy: -5.72140, alpha: 0.00445, time: 51.13154
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 151.21557, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 0.07738, qf2_loss: 0.07645, policy_loss: -22.60249, policy_entropy: -5.93896, alpha: 0.00444, time: 50.90781
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 162.05513, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 0.07633, qf2_loss: 0.07657, policy_loss: -22.71197, policy_entropy: -5.80164, alpha: 0.00439, time: 51.30526
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 152.18829, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 0.06295, qf2_loss: 0.06213, policy_loss: -22.69420, policy_entropy: -6.01443, alpha: 0.00438, time: 51.32436
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 157.83437, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 0.07575, qf2_loss: 0.07697, policy_loss: -22.76615, policy_entropy: -5.89638, alpha: 0.00437, time: 50.81079
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 161.38581, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 0.07822, qf2_loss: 0.07787, policy_loss: -22.74305, policy_entropy: -5.95410, alpha: 0.00436, time: 51.13793
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 167.76743, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 0.06641, qf2_loss: 0.06532, policy_loss: -22.74936, policy_entropy: -6.24694, alpha: 0.00438, time: 51.01146
[CW] eval: return: 152.55849, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 143.80023, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 0.06381, qf2_loss: 0.06421, policy_loss: -22.75719, policy_entropy: -5.98926, alpha: 0.00441, time: 51.48993
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 147.08698, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 0.06517, qf2_loss: 0.06485, policy_loss: -22.74061, policy_entropy: -5.90307, alpha: 0.00438, time: 51.06829
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 180.74035, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 0.07044, qf2_loss: 0.07021, policy_loss: -22.76425, policy_entropy: -5.80876, alpha: 0.00436, time: 50.94209
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 149.49451, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 0.07299, qf2_loss: 0.07333, policy_loss: -22.80318, policy_entropy: -5.80756, alpha: 0.00431, time: 51.59945
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 148.90433, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 0.07520, qf2_loss: 0.07453, policy_loss: -22.78318, policy_entropy: -5.69728, alpha: 0.00427, time: 51.21097
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 146.42772, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 0.06225, qf2_loss: 0.06220, policy_loss: -22.81879, policy_entropy: -5.90176, alpha: 0.00422, time: 51.51104
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 145.36801, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 0.07424, qf2_loss: 0.07372, policy_loss: -22.78354, policy_entropy: -5.85135, alpha: 0.00418, time: 51.10876
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 147.20238, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 0.07450, qf2_loss: 0.07401, policy_loss: -23.02026, policy_entropy: -5.99855, alpha: 0.00418, time: 51.20652
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 147.39034, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 0.08054, qf2_loss: 0.08131, policy_loss: -22.81538, policy_entropy: -6.06635, alpha: 0.00419, time: 51.37668
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 26.87357, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 0.06451, qf2_loss: 0.06424, policy_loss: -22.76229, policy_entropy: -5.86790, alpha: 0.00418, time: 50.88653
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 166.23978, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 0.07844, qf2_loss: 0.07803, policy_loss: -22.80387, policy_entropy: -5.87721, alpha: 0.00415, time: 50.80561
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 128.18198, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 0.07209, qf2_loss: 0.07331, policy_loss: -22.82279, policy_entropy: -5.91691, alpha: 0.00415, time: 50.91398
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 151.87660, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 0.06634, qf2_loss: 0.06495, policy_loss: -22.77804, policy_entropy: -6.04308, alpha: 0.00412, time: 50.96223
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 151.77516, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 0.07529, qf2_loss: 0.07455, policy_loss: -23.00944, policy_entropy: -6.19862, alpha: 0.00416, time: 50.85330
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 161.20100, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 0.07556, qf2_loss: 0.07664, policy_loss: -22.87220, policy_entropy: -5.97341, alpha: 0.00418, time: 51.23434
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 157.91260, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 0.07885, qf2_loss: 0.07825, policy_loss: -22.86654, policy_entropy: -5.78511, alpha: 0.00414, time: 51.54854
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 152.94028, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 0.07898, qf2_loss: 0.07875, policy_loss: -22.74899, policy_entropy: -5.72847, alpha: 0.00410, time: 51.25367
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 151.69605, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 0.07351, qf2_loss: 0.07272, policy_loss: -22.84605, policy_entropy: -5.97629, alpha: 0.00406, time: 51.07830
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 147.89965, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 0.08091, qf2_loss: 0.08192, policy_loss: -22.93249, policy_entropy: -6.06291, alpha: 0.00407, time: 51.13217
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 156.04530, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 0.08044, qf2_loss: 0.08125, policy_loss: -22.98806, policy_entropy: -6.32198, alpha: 0.00411, time: 51.63894
[CW] eval: return: 149.57422, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 152.14812, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 0.07514, qf2_loss: 0.07451, policy_loss: -22.83987, policy_entropy: -6.56549, alpha: 0.00420, time: 51.91006
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 165.79817, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 0.07116, qf2_loss: 0.07100, policy_loss: -22.93761, policy_entropy: -6.55193, alpha: 0.00432, time: 51.61927
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 155.61324, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 0.07183, qf2_loss: 0.07202, policy_loss: -22.99407, policy_entropy: -6.26679, alpha: 0.00440, time: 51.16942
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 156.18455, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 0.09818, qf2_loss: 0.09769, policy_loss: -23.00922, policy_entropy: -6.39329, alpha: 0.00447, time: 51.28512
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 137.88314, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 0.07309, qf2_loss: 0.07295, policy_loss: -22.93315, policy_entropy: -6.50644, alpha: 0.00458, time: 51.27112
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 160.88146, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 0.08874, qf2_loss: 0.08871, policy_loss: -23.12721, policy_entropy: -6.25738, alpha: 0.00468, time: 50.97365
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 147.84086, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 0.08392, qf2_loss: 0.08346, policy_loss: -22.94526, policy_entropy: -6.10815, alpha: 0.00472, time: 51.20055
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 146.45482, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 0.07130, qf2_loss: 0.07057, policy_loss: -22.93335, policy_entropy: -6.14462, alpha: 0.00474, time: 50.94609
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 140.44127, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 0.08022, qf2_loss: 0.08082, policy_loss: -22.99727, policy_entropy: -6.31053, alpha: 0.00480, time: 51.13534
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 177.44886, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 0.07357, qf2_loss: 0.07454, policy_loss: -23.04795, policy_entropy: -5.79579, alpha: 0.00482, time: 51.13194
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 168.44120, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 0.07633, qf2_loss: 0.07699, policy_loss: -23.02686, policy_entropy: -6.17865, alpha: 0.00480, time: 51.21503
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 174.33906, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 0.07502, qf2_loss: 0.07501, policy_loss: -23.08799, policy_entropy: -6.30036, alpha: 0.00487, time: 51.16529
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 163.64817, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 0.08997, qf2_loss: 0.08909, policy_loss: -23.09944, policy_entropy: -6.27191, alpha: 0.00495, time: 51.63197
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 152.66383, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 0.07560, qf2_loss: 0.07483, policy_loss: -23.10263, policy_entropy: -5.99467, alpha: 0.00498, time: 51.10848
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 156.97158, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 0.07520, qf2_loss: 0.07488, policy_loss: -23.09721, policy_entropy: -6.12344, alpha: 0.00500, time: 50.75218
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 154.78535, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 0.07895, qf2_loss: 0.07969, policy_loss: -23.03972, policy_entropy: -5.76668, alpha: 0.00499, time: 51.05998
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 153.87987, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 0.07644, qf2_loss: 0.07602, policy_loss: -23.12489, policy_entropy: -6.04835, alpha: 0.00496, time: 50.84275
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 165.71919, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 0.08207, qf2_loss: 0.08151, policy_loss: -23.11888, policy_entropy: -5.91211, alpha: 0.00496, time: 50.92020
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 163.08631, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 0.07789, qf2_loss: 0.07769, policy_loss: -23.21283, policy_entropy: -5.85587, alpha: 0.00493, time: 50.97865
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 149.54326, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 0.07968, qf2_loss: 0.07860, policy_loss: -23.25906, policy_entropy: -5.87499, alpha: 0.00489, time: 50.98118
[CW] eval: return: 156.51875, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 149.06682, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 0.08383, qf2_loss: 0.08287, policy_loss: -23.38391, policy_entropy: -5.97086, alpha: 0.00488, time: 51.82508
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 170.38634, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 0.08584, qf2_loss: 0.08568, policy_loss: -23.26679, policy_entropy: -5.84491, alpha: 0.00484, time: 51.03357
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 152.68072, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 0.07621, qf2_loss: 0.07586, policy_loss: -23.26578, policy_entropy: -6.08984, alpha: 0.00486, time: 50.93314
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 154.11151, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 0.08800, qf2_loss: 0.08819, policy_loss: -23.28606, policy_entropy: -5.91042, alpha: 0.00486, time: 51.56101
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 146.50591, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 0.08147, qf2_loss: 0.08131, policy_loss: -23.32965, policy_entropy: -5.32815, alpha: 0.00476, time: 51.03918
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 155.65750, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 0.08531, qf2_loss: 0.08434, policy_loss: -23.38117, policy_entropy: -5.59662, alpha: 0.00464, time: 50.99951
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 168.25057, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 0.09327, qf2_loss: 0.09341, policy_loss: -23.39456, policy_entropy: -5.62941, alpha: 0.00456, time: 51.05935
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 162.70415, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 0.07763, qf2_loss: 0.07746, policy_loss: -23.44841, policy_entropy: -5.93849, alpha: 0.00451, time: 51.35772
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 164.42980, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 0.08221, qf2_loss: 0.08028, policy_loss: -23.41729, policy_entropy: -6.00796, alpha: 0.00451, time: 51.14257
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 155.02172, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 0.08205, qf2_loss: 0.08170, policy_loss: -23.37726, policy_entropy: -5.91775, alpha: 0.00449, time: 51.58446
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 170.05837, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 0.08410, qf2_loss: 0.08468, policy_loss: -23.52210, policy_entropy: -6.05931, alpha: 0.00449, time: 50.84708
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 165.12368, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 0.09329, qf2_loss: 0.09334, policy_loss: -23.48019, policy_entropy: -5.91828, alpha: 0.00449, time: 51.15354
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 165.71704, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 0.07969, qf2_loss: 0.07874, policy_loss: -23.56040, policy_entropy: -6.00800, alpha: 0.00448, time: 51.17566
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 160.44971, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 0.07491, qf2_loss: 0.07419, policy_loss: -23.51237, policy_entropy: -5.83947, alpha: 0.00448, time: 51.10305
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 150.60862, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 0.08175, qf2_loss: 0.08183, policy_loss: -23.52531, policy_entropy: -5.78746, alpha: 0.00443, time: 51.45955
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 166.80370, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 0.07316, qf2_loss: 0.07369, policy_loss: -23.54400, policy_entropy: -5.79808, alpha: 0.00439, time: 51.03179
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 167.74980, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 0.07384, qf2_loss: 0.07307, policy_loss: -23.53946, policy_entropy: -5.83957, alpha: 0.00435, time: 50.88138
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 166.16942, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 0.07609, qf2_loss: 0.07573, policy_loss: -23.57026, policy_entropy: -5.87939, alpha: 0.00432, time: 51.26661
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 154.41647, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 0.08332, qf2_loss: 0.08377, policy_loss: -23.66692, policy_entropy: -5.89856, alpha: 0.00430, time: 50.95761
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 160.70756, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 0.08914, qf2_loss: 0.08833, policy_loss: -23.64135, policy_entropy: -5.70804, alpha: 0.00426, time: 51.06503
[CW] eval: return: 160.72538, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 158.74668, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 0.08133, qf2_loss: 0.08125, policy_loss: -23.68835, policy_entropy: -5.99671, alpha: 0.00423, time: 51.40107
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 168.54898, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 0.07868, qf2_loss: 0.07932, policy_loss: -23.56993, policy_entropy: -5.86093, alpha: 0.00421, time: 51.83606
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 160.59042, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 0.07219, qf2_loss: 0.07186, policy_loss: -23.74060, policy_entropy: -5.97383, alpha: 0.00420, time: 50.86236
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 160.89212, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 0.06919, qf2_loss: 0.06902, policy_loss: -23.66666, policy_entropy: -5.97250, alpha: 0.00419, time: 51.41577
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 30.91805, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 0.07246, qf2_loss: 0.07335, policy_loss: -23.68291, policy_entropy: -6.05938, alpha: 0.00420, time: 51.21249
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 168.97353, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 0.08186, qf2_loss: 0.08101, policy_loss: -23.60851, policy_entropy: -5.89066, alpha: 0.00418, time: 51.03773
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 154.12628, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 0.08980, qf2_loss: 0.08928, policy_loss: -23.70136, policy_entropy: -5.87737, alpha: 0.00417, time: 51.57611
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 167.86716, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 0.07442, qf2_loss: 0.07377, policy_loss: -23.71828, policy_entropy: -6.02923, alpha: 0.00415, time: 51.25566
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 152.57559, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 0.07540, qf2_loss: 0.07413, policy_loss: -23.63885, policy_entropy: -6.00848, alpha: 0.00415, time: 51.64367
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 155.64170, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 0.08152, qf2_loss: 0.08236, policy_loss: -23.71552, policy_entropy: -6.06713, alpha: 0.00416, time: 51.42107
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 157.11645, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 0.07628, qf2_loss: 0.07674, policy_loss: -23.81542, policy_entropy: -6.25435, alpha: 0.00419, time: 51.15585
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 171.85724, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 0.07954, qf2_loss: 0.07930, policy_loss: -23.65928, policy_entropy: -6.26679, alpha: 0.00425, time: 51.14033
[CW] ---------------------------
[CW] ---- Iteration:   673 ----
[CW] collect: return: 173.61046, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 0.08058, qf2_loss: 0.08046, policy_loss: -23.88558, policy_entropy: -6.31196, alpha: 0.00430, time: 50.88103
[CW] ---------------------------
[CW] ---- Iteration:   674 ----
[CW] collect: return: 162.78439, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 0.08092, qf2_loss: 0.08014, policy_loss: -24.00008, policy_entropy: -6.50311, alpha: 0.00440, time: 51.24091
[CW] ---------------------------
[CW] ---- Iteration:   675 ----
[CW] collect: return: 156.83002, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 0.08013, qf2_loss: 0.08036, policy_loss: -23.94954, policy_entropy: -6.24506, alpha: 0.00450, time: 50.93710
[CW] ---------------------------
[CW] ---- Iteration:   676 ----
[CW] collect: return: 153.61234, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 0.08578, qf2_loss: 0.08361, policy_loss: -23.81555, policy_entropy: -6.16864, alpha: 0.00453, time: 50.98655
[CW] ---------------------------
[CW] ---- Iteration:   677 ----
[CW] collect: return: 165.42399, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 0.07485, qf2_loss: 0.07514, policy_loss: -23.94001, policy_entropy: -6.24577, alpha: 0.00460, time: 50.81741
[CW] ---------------------------
[CW] ---- Iteration:   678 ----
[CW] collect: return: 166.30226, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 0.08464, qf2_loss: 0.08412, policy_loss: -23.94693, policy_entropy: -6.41183, alpha: 0.00467, time: 51.20735
[CW] ---------------------------
[CW] ---- Iteration:   679 ----
[CW] collect: return: 160.50157, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 0.07669, qf2_loss: 0.07590, policy_loss: -24.00813, policy_entropy: -6.21116, alpha: 0.00476, time: 51.37543
[CW] ---------------------------
[CW] ---- Iteration:   680 ----
[CW] collect: return: 163.07815, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 0.07952, qf2_loss: 0.07978, policy_loss: -23.94263, policy_entropy: -6.04023, alpha: 0.00479, time: 51.28348
[CW] eval: return: 158.78614, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   681 ----
[CW] collect: return: 177.21540, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 0.08945, qf2_loss: 0.08937, policy_loss: -23.86234, policy_entropy: -6.06820, alpha: 0.00480, time: 51.13140
[CW] ---------------------------
[CW] ---- Iteration:   682 ----
[CW] collect: return: 166.72880, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 0.08780, qf2_loss: 0.08732, policy_loss: -23.99578, policy_entropy: -5.89622, alpha: 0.00479, time: 50.95501
[CW] ---------------------------
[CW] ---- Iteration:   683 ----
[CW] collect: return: 172.09081, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 0.07960, qf2_loss: 0.07910, policy_loss: -24.06048, policy_entropy: -5.98784, alpha: 0.00478, time: 50.66240
[CW] ---------------------------
[CW] ---- Iteration:   684 ----
[CW] collect: return: 163.17936, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 0.08710, qf2_loss: 0.08733, policy_loss: -24.00941, policy_entropy: -5.93025, alpha: 0.00478, time: 51.26793
[CW] ---------------------------
[CW] ---- Iteration:   685 ----
[CW] collect: return: 160.66487, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 0.08142, qf2_loss: 0.08059, policy_loss: -24.03827, policy_entropy: -5.99302, alpha: 0.00477, time: 51.18954
[CW] ---------------------------
[CW] ---- Iteration:   686 ----
[CW] collect: return: 165.65381, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 0.07786, qf2_loss: 0.07782, policy_loss: -24.05851, policy_entropy: -5.99993, alpha: 0.00476, time: 51.15334
[CW] ---------------------------
[CW] ---- Iteration:   687 ----
[CW] collect: return: 146.44683, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 0.07886, qf2_loss: 0.07872, policy_loss: -24.08183, policy_entropy: -6.09128, alpha: 0.00476, time: 51.09305
[CW] ---------------------------
[CW] ---- Iteration:   688 ----
[CW] collect: return: 169.97242, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 0.08167, qf2_loss: 0.08174, policy_loss: -23.93138, policy_entropy: -6.00136, alpha: 0.00480, time: 50.88328
[CW] ---------------------------
[CW] ---- Iteration:   689 ----
[CW] collect: return: 156.14568, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 0.08357, qf2_loss: 0.08277, policy_loss: -24.15050, policy_entropy: -5.96899, alpha: 0.00479, time: 50.98422
[CW] ---------------------------
[CW] ---- Iteration:   690 ----
[CW] collect: return: 165.98206, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 0.08697, qf2_loss: 0.08649, policy_loss: -24.05804, policy_entropy: -5.75775, alpha: 0.00475, time: 51.24007
[CW] ---------------------------
[CW] ---- Iteration:   691 ----
[CW] collect: return: 159.16409, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 0.09548, qf2_loss: 0.09422, policy_loss: -24.14313, policy_entropy: -5.83306, alpha: 0.00469, time: 51.06915
[CW] ---------------------------
[CW] ---- Iteration:   692 ----
[CW] collect: return: 159.73733, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 0.09585, qf2_loss: 0.09711, policy_loss: -24.11513, policy_entropy: -5.85645, alpha: 0.00466, time: 50.94191
[CW] ---------------------------
[CW] ---- Iteration:   693 ----
[CW] collect: return: 152.95168, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 0.07634, qf2_loss: 0.07616, policy_loss: -24.17281, policy_entropy: -6.14901, alpha: 0.00465, time: 51.23761
[CW] ---------------------------
[CW] ---- Iteration:   694 ----
[CW] collect: return: 151.17399, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 0.09559, qf2_loss: 0.09435, policy_loss: -24.27121, policy_entropy: -5.80057, alpha: 0.00467, time: 51.40114
[CW] ---------------------------
[CW] ---- Iteration:   695 ----
[CW] collect: return: 166.68994, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 0.08749, qf2_loss: 0.08688, policy_loss: -24.26962, policy_entropy: -5.94947, alpha: 0.00462, time: 50.97106
[CW] ---------------------------
[CW] ---- Iteration:   696 ----
[CW] collect: return: 154.93850, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 0.07951, qf2_loss: 0.07855, policy_loss: -24.11170, policy_entropy: -5.99299, alpha: 0.00461, time: 51.13894
[CW] ---------------------------
[CW] ---- Iteration:   697 ----
[CW] collect: return: 162.08418, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 0.07973, qf2_loss: 0.07847, policy_loss: -24.21223, policy_entropy: -6.03015, alpha: 0.00461, time: 51.06895
[CW] ---------------------------
[CW] ---- Iteration:   698 ----
[CW] collect: return: 159.22908, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 0.08085, qf2_loss: 0.08131, policy_loss: -24.29560, policy_entropy: -5.95158, alpha: 0.00461, time: 50.91291
[CW] ---------------------------
[CW] ---- Iteration:   699 ----
[CW] collect: return: 160.01647, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 0.08297, qf2_loss: 0.08547, policy_loss: -24.17714, policy_entropy: -6.00972, alpha: 0.00461, time: 51.26100
[CW] ---------------------------
[CW] ---- Iteration:   700 ----
[CW] collect: return: 159.99576, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 0.09038, qf2_loss: 0.08944, policy_loss: -24.44034, policy_entropy: -5.90245, alpha: 0.00459, time: 51.59471
[CW] eval: return: 160.26445, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   701 ----
[CW] collect: return: 161.72035, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 0.08150, qf2_loss: 0.08127, policy_loss: -24.35868, policy_entropy: -5.72890, alpha: 0.00455, time: 51.61773
[CW] ---------------------------
[CW] ---- Iteration:   702 ----
[CW] collect: return: 163.45659, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 0.07532, qf2_loss: 0.07536, policy_loss: -24.30722, policy_entropy: -5.89043, alpha: 0.00449, time: 51.49621
[CW] ---------------------------
[CW] ---- Iteration:   703 ----
[CW] collect: return: 163.26448, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 0.07965, qf2_loss: 0.07854, policy_loss: -24.41756, policy_entropy: -5.79552, alpha: 0.00446, time: 50.94260
[CW] ---------------------------
[CW] ---- Iteration:   704 ----
[CW] collect: return: 167.34152, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 0.08830, qf2_loss: 0.08759, policy_loss: -24.46160, policy_entropy: -5.93314, alpha: 0.00442, time: 52.52149
[CW] ---------------------------
[CW] ---- Iteration:   705 ----
[CW] collect: return: 156.69199, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 0.08260, qf2_loss: 0.08309, policy_loss: -24.41274, policy_entropy: -5.76351, alpha: 0.00439, time: 51.59959
[CW] ---------------------------
[CW] ---- Iteration:   706 ----
[CW] collect: return: 148.68961, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 0.08325, qf2_loss: 0.08247, policy_loss: -24.44915, policy_entropy: -5.83750, alpha: 0.00433, time: 51.22785
[CW] ---------------------------
[CW] ---- Iteration:   707 ----
[CW] collect: return: 175.88350, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 0.07923, qf2_loss: 0.07776, policy_loss: -24.42708, policy_entropy: -6.01811, alpha: 0.00432, time: 51.31257
[CW] ---------------------------
[CW] ---- Iteration:   708 ----
[CW] collect: return: 178.81074, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 0.08055, qf2_loss: 0.07995, policy_loss: -24.44869, policy_entropy: -6.12287, alpha: 0.00433, time: 51.31950
[CW] ---------------------------
[CW] ---- Iteration:   709 ----
[CW] collect: return: 23.16629, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 0.08581, qf2_loss: 0.08557, policy_loss: -24.47334, policy_entropy: -6.15999, alpha: 0.00436, time: 51.30189
[CW] ---------------------------
[CW] ---- Iteration:   710 ----
[CW] collect: return: 159.13807, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 0.07362, qf2_loss: 0.07317, policy_loss: -24.43118, policy_entropy: -6.23255, alpha: 0.00442, time: 51.15102
[CW] ---------------------------
[CW] ---- Iteration:   711 ----
[CW] collect: return: 166.58297, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 0.08446, qf2_loss: 0.08355, policy_loss: -24.48477, policy_entropy: -6.13290, alpha: 0.00445, time: 51.16651
[CW] ---------------------------
[CW] ---- Iteration:   712 ----
[CW] collect: return: 163.20690, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 0.09685, qf2_loss: 0.09602, policy_loss: -24.50387, policy_entropy: -6.21620, alpha: 0.00450, time: 50.92689
[CW] ---------------------------
[CW] ---- Iteration:   713 ----
[CW] collect: return: 36.86185, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 0.07859, qf2_loss: 0.07864, policy_loss: -24.61664, policy_entropy: -6.32928, alpha: 0.00457, time: 51.26207
[CW] ---------------------------
[CW] ---- Iteration:   714 ----
[CW] collect: return: 167.36336, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 0.07754, qf2_loss: 0.07743, policy_loss: -24.30734, policy_entropy: -6.02823, alpha: 0.00463, time: 50.81063
[CW] ---------------------------
[CW] ---- Iteration:   715 ----
[CW] collect: return: 169.20402, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 0.09214, qf2_loss: 0.09121, policy_loss: -24.53380, policy_entropy: -6.09504, alpha: 0.00464, time: 52.17771
[CW] ---------------------------
[CW] ---- Iteration:   716 ----
[CW] collect: return: 162.18942, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 0.08015, qf2_loss: 0.08031, policy_loss: -24.44740, policy_entropy: -6.15457, alpha: 0.00466, time: 51.49125
[CW] ---------------------------
[CW] ---- Iteration:   717 ----
[CW] collect: return: 156.20219, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 0.07795, qf2_loss: 0.07762, policy_loss: -24.56225, policy_entropy: -6.28673, alpha: 0.00474, time: 51.31121
[CW] ---------------------------
[CW] ---- Iteration:   718 ----
[CW] collect: return: 167.72115, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 0.09054, qf2_loss: 0.09019, policy_loss: -24.74533, policy_entropy: -6.34255, alpha: 0.00483, time: 51.49081
[CW] ---------------------------
[CW] ---- Iteration:   719 ----
[CW] collect: return: 163.34223, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 0.08484, qf2_loss: 0.08503, policy_loss: -24.58947, policy_entropy: -5.85919, alpha: 0.00485, time: 51.04614
[CW] ---------------------------
[CW] ---- Iteration:   720 ----
[CW] collect: return: 160.03561, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 0.08203, qf2_loss: 0.08135, policy_loss: -24.56662, policy_entropy: -6.00246, alpha: 0.00484, time: 51.06296
[CW] eval: return: 165.02428, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   721 ----
[CW] collect: return: 171.85857, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 0.08294, qf2_loss: 0.08378, policy_loss: -24.57567, policy_entropy: -5.80901, alpha: 0.00482, time: 51.12615
[CW] ---------------------------
[CW] ---- Iteration:   722 ----
[CW] collect: return: 161.67312, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 0.08003, qf2_loss: 0.07833, policy_loss: -24.60640, policy_entropy: -6.08701, alpha: 0.00481, time: 51.49787
[CW] ---------------------------
[CW] ---- Iteration:   723 ----
[CW] collect: return: 165.90341, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 0.09113, qf2_loss: 0.09058, policy_loss: -24.58072, policy_entropy: -5.95939, alpha: 0.00482, time: 50.61611
[CW] ---------------------------
[CW] ---- Iteration:   724 ----
[CW] collect: return: 166.97462, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 0.08575, qf2_loss: 0.08683, policy_loss: -24.64854, policy_entropy: -5.80674, alpha: 0.00479, time: 51.55710
[CW] ---------------------------
[CW] ---- Iteration:   725 ----
[CW] collect: return: 159.73544, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 0.08368, qf2_loss: 0.08235, policy_loss: -24.71993, policy_entropy: -6.10908, alpha: 0.00476, time: 51.02526
[CW] ---------------------------
[CW] ---- Iteration:   726 ----
[CW] collect: return: 175.79921, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 0.08729, qf2_loss: 0.08571, policy_loss: -24.69596, policy_entropy: -6.11302, alpha: 0.00480, time: 51.02946
[CW] ---------------------------
[CW] ---- Iteration:   727 ----
[CW] collect: return: 159.50689, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 0.07813, qf2_loss: 0.07829, policy_loss: -24.86120, policy_entropy: -6.23516, alpha: 0.00483, time: 50.87716
[CW] ---------------------------
[CW] ---- Iteration:   728 ----
[CW] collect: return: 150.11424, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 0.08427, qf2_loss: 0.08410, policy_loss: -24.85148, policy_entropy: -6.06479, alpha: 0.00490, time: 50.93011
[CW] ---------------------------
[CW] ---- Iteration:   729 ----
[CW] collect: return: 156.31499, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 0.08342, qf2_loss: 0.08234, policy_loss: -24.80219, policy_entropy: -5.79760, alpha: 0.00488, time: 51.20386
[CW] ---------------------------
[CW] ---- Iteration:   730 ----
[CW] collect: return: 163.40229, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 0.08225, qf2_loss: 0.08224, policy_loss: -24.69926, policy_entropy: -6.06710, alpha: 0.00484, time: 50.92875
[CW] ---------------------------
[CW] ---- Iteration:   731 ----
[CW] collect: return: 161.04572, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 0.07960, qf2_loss: 0.07885, policy_loss: -24.89459, policy_entropy: -6.10340, alpha: 0.00487, time: 50.97336
[CW] ---------------------------
[CW] ---- Iteration:   732 ----
[CW] collect: return: 167.90600, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 0.08678, qf2_loss: 0.08740, policy_loss: -24.88873, policy_entropy: -6.08497, alpha: 0.00490, time: 51.52214
[CW] ---------------------------
[CW] ---- Iteration:   733 ----
[CW] collect: return: 166.51624, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 0.07178, qf2_loss: 0.07146, policy_loss: -24.85631, policy_entropy: -5.95285, alpha: 0.00491, time: 51.35830
[CW] ---------------------------
[CW] ---- Iteration:   734 ----
[CW] collect: return: 150.72024, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 0.06911, qf2_loss: 0.06805, policy_loss: -24.72839, policy_entropy: -5.85496, alpha: 0.00490, time: 50.71835
[CW] ---------------------------
[CW] ---- Iteration:   735 ----
[CW] collect: return: 164.20080, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 0.07954, qf2_loss: 0.07943, policy_loss: -24.87562, policy_entropy: -5.87603, alpha: 0.00485, time: 51.07547
[CW] ---------------------------
[CW] ---- Iteration:   736 ----
[CW] collect: return: 156.86702, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 0.07728, qf2_loss: 0.07699, policy_loss: -24.97703, policy_entropy: -5.86685, alpha: 0.00479, time: 51.24348
[CW] ---------------------------
[CW] ---- Iteration:   737 ----
[CW] collect: return: 161.84701, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 0.07913, qf2_loss: 0.08055, policy_loss: -24.78318, policy_entropy: -5.83215, alpha: 0.00475, time: 51.56277
[CW] ---------------------------
[CW] ---- Iteration:   738 ----
[CW] collect: return: 159.14138, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 0.08672, qf2_loss: 0.08400, policy_loss: -24.88082, policy_entropy: -5.86766, alpha: 0.00471, time: 51.75797
[CW] ---------------------------
[CW] ---- Iteration:   739 ----
[CW] collect: return: 170.35636, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 0.08097, qf2_loss: 0.08195, policy_loss: -24.93562, policy_entropy: -6.00659, alpha: 0.00469, time: 51.41902
[CW] ---------------------------
[CW] ---- Iteration:   740 ----
[CW] collect: return: 159.65927, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 0.07257, qf2_loss: 0.07295, policy_loss: -25.00709, policy_entropy: -6.08093, alpha: 0.00469, time: 51.35779
[CW] eval: return: 162.35146, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   741 ----
[CW] collect: return: 173.36531, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 0.09018, qf2_loss: 0.08862, policy_loss: -24.85975, policy_entropy: -6.15535, alpha: 0.00473, time: 51.16129
[CW] ---------------------------
[CW] ---- Iteration:   742 ----
[CW] collect: return: 154.06534, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 0.07700, qf2_loss: 0.07656, policy_loss: -24.92404, policy_entropy: -6.03868, alpha: 0.00477, time: 50.93828
[CW] ---------------------------
[CW] ---- Iteration:   743 ----
[CW] collect: return: 152.66016, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 0.07714, qf2_loss: 0.07745, policy_loss: -25.02879, policy_entropy: -6.13859, alpha: 0.00478, time: 50.85927
[CW] ---------------------------
[CW] ---- Iteration:   744 ----
[CW] collect: return: 162.18885, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 0.07591, qf2_loss: 0.07490, policy_loss: -25.10928, policy_entropy: -6.03845, alpha: 0.00483, time: 51.07154
[CW] ---------------------------
[CW] ---- Iteration:   745 ----
[CW] collect: return: 164.92282, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 0.07974, qf2_loss: 0.07965, policy_loss: -25.06602, policy_entropy: -6.09058, alpha: 0.00484, time: 51.17049
[CW] ---------------------------
[CW] ---- Iteration:   746 ----
[CW] collect: return: 166.82118, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 0.08602, qf2_loss: 0.08618, policy_loss: -25.08428, policy_entropy: -5.95672, alpha: 0.00486, time: 51.42232
[CW] ---------------------------
[CW] ---- Iteration:   747 ----
[CW] collect: return: 158.71349, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 0.08090, qf2_loss: 0.08004, policy_loss: -24.96223, policy_entropy: -6.03043, alpha: 0.00483, time: 50.80104
[CW] ---------------------------
[CW] ---- Iteration:   748 ----
[CW] collect: return: 160.22241, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 0.06883, qf2_loss: 0.06940, policy_loss: -24.94379, policy_entropy: -6.01619, alpha: 0.00485, time: 50.98739
[CW] ---------------------------
[CW] ---- Iteration:   749 ----
[CW] collect: return: 167.97046, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 0.07601, qf2_loss: 0.07514, policy_loss: -25.17723, policy_entropy: -5.99427, alpha: 0.00485, time: 51.29754
[CW] ---------------------------
[CW] ---- Iteration:   750 ----
[CW] collect: return: 157.93351, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 0.08671, qf2_loss: 0.08820, policy_loss: -25.21062, policy_entropy: -6.29123, alpha: 0.00488, time: 51.15929
[CW] ---------------------------
[CW] ---- Iteration:   751 ----
[CW] collect: return: 162.07135, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 0.06882, qf2_loss: 0.06806, policy_loss: -25.23868, policy_entropy: -6.23567, alpha: 0.00498, time: 50.76105
[CW] ---------------------------
[CW] ---- Iteration:   752 ----
[CW] collect: return: 166.57399, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 0.08829, qf2_loss: 0.08792, policy_loss: -25.10831, policy_entropy: -5.87119, alpha: 0.00501, time: 50.63121
[CW] ---------------------------
[CW] ---- Iteration:   753 ----
[CW] collect: return: 170.74374, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 0.07700, qf2_loss: 0.07645, policy_loss: -25.17676, policy_entropy: -6.07094, alpha: 0.00499, time: 51.00339
[CW] ---------------------------
[CW] ---- Iteration:   754 ----
[CW] collect: return: 173.75275, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 0.07203, qf2_loss: 0.07159, policy_loss: -25.17594, policy_entropy: -6.05083, alpha: 0.00502, time: 50.75280
[CW] ---------------------------
[CW] ---- Iteration:   755 ----
[CW] collect: return: 175.28287, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 0.07452, qf2_loss: 0.07473, policy_loss: -25.09702, policy_entropy: -5.89619, alpha: 0.00500, time: 51.20299
[CW] ---------------------------
[CW] ---- Iteration:   756 ----
[CW] collect: return: 173.47611, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 0.06904, qf2_loss: 0.06914, policy_loss: -25.26713, policy_entropy: -6.05562, alpha: 0.00499, time: 51.12942
[CW] ---------------------------
[CW] ---- Iteration:   757 ----
[CW] collect: return: 161.32459, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 0.08179, qf2_loss: 0.08099, policy_loss: -25.20089, policy_entropy: -6.21210, alpha: 0.00505, time: 51.45578
[CW] ---------------------------
[CW] ---- Iteration:   758 ----
[CW] collect: return: 166.08004, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 0.08117, qf2_loss: 0.08186, policy_loss: -25.18563, policy_entropy: -6.04686, alpha: 0.00510, time: 51.23325
[CW] ---------------------------
[CW] ---- Iteration:   759 ----
[CW] collect: return: 171.28634, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 0.07321, qf2_loss: 0.07293, policy_loss: -25.21876, policy_entropy: -6.01139, alpha: 0.00509, time: 51.28265
[CW] ---------------------------
[CW] ---- Iteration:   760 ----
[CW] collect: return: 167.76647, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 0.08938, qf2_loss: 0.08810, policy_loss: -25.37887, policy_entropy: -6.01283, alpha: 0.00512, time: 50.82955
[CW] eval: return: 168.70388, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   761 ----
[CW] collect: return: 166.95643, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 0.07569, qf2_loss: 0.07516, policy_loss: -25.42139, policy_entropy: -6.29644, alpha: 0.00515, time: 51.50081
[CW] ---------------------------
[CW] ---- Iteration:   762 ----
[CW] collect: return: 168.83391, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 0.07564, qf2_loss: 0.07722, policy_loss: -25.40127, policy_entropy: -5.96792, alpha: 0.00520, time: 51.40095
[CW] ---------------------------
[CW] ---- Iteration:   763 ----
[CW] collect: return: 165.03963, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 0.07315, qf2_loss: 0.07250, policy_loss: -25.45250, policy_entropy: -5.90206, alpha: 0.00517, time: 50.96233
[CW] ---------------------------
[CW] ---- Iteration:   764 ----
[CW] collect: return: 161.69259, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 0.07892, qf2_loss: 0.07929, policy_loss: -25.38955, policy_entropy: -5.77514, alpha: 0.00514, time: 51.10129
[CW] ---------------------------
[CW] ---- Iteration:   765 ----
[CW] collect: return: 168.46389, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 0.07876, qf2_loss: 0.07834, policy_loss: -25.43031, policy_entropy: -5.82147, alpha: 0.00508, time: 50.82597
[CW] ---------------------------
[CW] ---- Iteration:   766 ----
[CW] collect: return: 170.83413, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 0.07716, qf2_loss: 0.07809, policy_loss: -25.48498, policy_entropy: -5.83984, alpha: 0.00501, time: 50.96173
[CW] ---------------------------
[CW] ---- Iteration:   767 ----
[CW] collect: return: 172.62573, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 0.07918, qf2_loss: 0.07940, policy_loss: -25.33287, policy_entropy: -5.92778, alpha: 0.00496, time: 50.99205
[CW] ---------------------------
