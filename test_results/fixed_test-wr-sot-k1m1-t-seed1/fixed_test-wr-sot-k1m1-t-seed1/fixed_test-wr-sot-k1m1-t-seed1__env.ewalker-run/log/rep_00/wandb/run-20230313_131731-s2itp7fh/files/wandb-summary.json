{"collect/return": 172.62573377601802, "collect/steps": 1000.0, "collect/total_steps": 773000.0, "train/qf1_loss": 0.07918196003884077, "train/qf2_loss": 0.07939582265913486, "train/policy_loss": -25.332869758605955, "train/policy_entropy": -5.927781662940979, "train/alpha": 0.004959735912270844, "train/time": 50.992045164108276, "eval/return": 168.7038751523127, "eval/steps": 1000.0, "_timestamp": 1678752935.2331245, "_runtime": 43083.91177940369, "_step": 767}