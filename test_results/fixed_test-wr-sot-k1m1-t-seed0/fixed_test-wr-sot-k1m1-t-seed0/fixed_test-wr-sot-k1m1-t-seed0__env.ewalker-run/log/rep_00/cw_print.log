[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 23.55385, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.65047, qf2_loss: 1.64358, policy_loss: -7.80733, policy_entropy: 4.09807, alpha: 0.98504, time: 58.13733
[CW] eval: return: 24.96769, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 25.70709, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.08488, qf2_loss: 0.08501, policy_loss: -8.51995, policy_entropy: 4.10059, alpha: 0.95626, time: 50.70527
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 26.25819, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.07539, qf2_loss: 0.07559, policy_loss: -9.22422, policy_entropy: 4.10091, alpha: 0.92871, time: 50.66585
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 25.96600, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.06846, qf2_loss: 0.06854, policy_loss: -10.16619, policy_entropy: 4.10066, alpha: 0.90231, time: 50.65626
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 26.59113, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06375, qf2_loss: 0.06362, policy_loss: -11.23511, policy_entropy: 4.10206, alpha: 0.87699, time: 50.65212
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 22.39052, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06123, qf2_loss: 0.06073, policy_loss: -12.35409, policy_entropy: 4.10222, alpha: 0.85267, time: 50.63620
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 30.46633, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.07047, qf2_loss: 0.06805, policy_loss: -13.51316, policy_entropy: 4.10252, alpha: 0.82930, time: 50.63150
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 20.93958, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.07707, qf2_loss: 0.07610, policy_loss: -14.71409, policy_entropy: 4.10147, alpha: 0.80683, time: 50.65924
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 25.62165, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.07287, qf2_loss: 0.07303, policy_loss: -15.92386, policy_entropy: 4.10036, alpha: 0.78519, time: 50.67646
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 26.10026, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.08032, qf2_loss: 0.08097, policy_loss: -17.13687, policy_entropy: 4.10161, alpha: 0.76436, time: 50.65868
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 25.89649, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.08927, qf2_loss: 0.09027, policy_loss: -18.32725, policy_entropy: 4.10215, alpha: 0.74426, time: 50.68352
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 21.69955, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.08368, qf2_loss: 0.08463, policy_loss: -19.50126, policy_entropy: 4.10150, alpha: 0.72488, time: 50.68366
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 25.29779, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.09066, qf2_loss: 0.09181, policy_loss: -20.64454, policy_entropy: 4.10127, alpha: 0.70617, time: 50.68548
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 26.76386, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.06675, qf2_loss: 0.06734, policy_loss: -21.76587, policy_entropy: 4.10132, alpha: 0.68809, time: 50.70376
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 25.62875, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.08317, qf2_loss: 0.08419, policy_loss: -22.86527, policy_entropy: 4.10117, alpha: 0.67062, time: 50.66240
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 26.51864, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.06810, qf2_loss: 0.06887, policy_loss: -23.92354, policy_entropy: 4.10229, alpha: 0.65372, time: 50.68031
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 27.72229, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.07829, qf2_loss: 0.07921, policy_loss: -24.95930, policy_entropy: 4.10106, alpha: 0.63736, time: 50.70867
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 22.78725, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.10490, qf2_loss: 0.10628, policy_loss: -25.96455, policy_entropy: 4.10157, alpha: 0.62153, time: 50.65331
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 20.28215, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.07011, qf2_loss: 0.07074, policy_loss: -26.94860, policy_entropy: 4.10282, alpha: 0.60619, time: 50.70911
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 24.62972, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.06371, qf2_loss: 0.06425, policy_loss: -27.90173, policy_entropy: 4.10139, alpha: 0.59132, time: 50.70692
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 24.14287, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.08237, qf2_loss: 0.08316, policy_loss: -28.82350, policy_entropy: 4.10224, alpha: 0.57690, time: 50.68806
[CW] eval: return: 24.54591, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 26.83628, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.09798, qf2_loss: 0.09890, policy_loss: -29.72003, policy_entropy: 4.10220, alpha: 0.56291, time: 50.73408
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 22.53255, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.06358, qf2_loss: 0.06402, policy_loss: -30.59648, policy_entropy: 4.10307, alpha: 0.54933, time: 50.72891
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 22.51577, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.07941, qf2_loss: 0.07991, policy_loss: -31.44048, policy_entropy: 4.10254, alpha: 0.53615, time: 50.77013
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 23.80648, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.11261, qf2_loss: 0.11345, policy_loss: -32.25487, policy_entropy: 4.10072, alpha: 0.52334, time: 50.71775
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 23.89429, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.03677, qf2_loss: 0.03687, policy_loss: -33.04893, policy_entropy: 4.10091, alpha: 0.51090, time: 50.73894
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 26.58269, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.08759, qf2_loss: 0.08806, policy_loss: -33.82749, policy_entropy: 4.10115, alpha: 0.49881, time: 50.75579
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 23.95147, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.07840, qf2_loss: 0.07885, policy_loss: -34.56896, policy_entropy: 4.10078, alpha: 0.48705, time: 50.74291
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 28.84014, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.08072, qf2_loss: 0.08104, policy_loss: -35.29948, policy_entropy: 4.10237, alpha: 0.47561, time: 50.72867
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 23.98999, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.06426, qf2_loss: 0.06446, policy_loss: -35.99599, policy_entropy: 4.10182, alpha: 0.46448, time: 50.77312
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 27.66134, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.07329, qf2_loss: 0.07451, policy_loss: -36.67433, policy_entropy: 4.10202, alpha: 0.45365, time: 50.73409
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 28.93522, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.08530, qf2_loss: 0.08532, policy_loss: -37.33199, policy_entropy: 4.10165, alpha: 0.44310, time: 50.76415
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 21.65292, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.06281, qf2_loss: 0.06287, policy_loss: -37.97095, policy_entropy: 4.10186, alpha: 0.43283, time: 50.77244
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 20.96118, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.08589, qf2_loss: 0.08601, policy_loss: -38.58376, policy_entropy: 4.10119, alpha: 0.42283, time: 50.72176
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 27.10880, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.05675, qf2_loss: 0.05686, policy_loss: -39.17889, policy_entropy: 4.10193, alpha: 0.41309, time: 50.80298
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 27.55297, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.07361, qf2_loss: 0.07371, policy_loss: -39.75215, policy_entropy: 4.10248, alpha: 0.40360, time: 50.77046
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 21.09979, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.06898, qf2_loss: 0.06912, policy_loss: -40.30699, policy_entropy: 4.10143, alpha: 0.39434, time: 50.73868
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 21.08658, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.08970, qf2_loss: 0.08990, policy_loss: -40.84269, policy_entropy: 4.10231, alpha: 0.38532, time: 50.98259
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 30.57786, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.04438, qf2_loss: 0.04445, policy_loss: -41.36101, policy_entropy: 4.10235, alpha: 0.37653, time: 50.70177
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 21.25302, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.08891, qf2_loss: 0.08902, policy_loss: -41.85875, policy_entropy: 4.10245, alpha: 0.36795, time: 50.66017
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 25.07181, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.04730, qf2_loss: 0.04736, policy_loss: -42.33762, policy_entropy: 4.10153, alpha: 0.35958, time: 50.68641
[CW] eval: return: 25.33942, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 22.48223, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.08518, qf2_loss: 0.08531, policy_loss: -42.80753, policy_entropy: 4.10143, alpha: 0.35142, time: 50.77493
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 28.81732, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.08537, qf2_loss: 0.08546, policy_loss: -43.25049, policy_entropy: 4.10154, alpha: 0.34346, time: 50.69238
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 25.12745, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.03709, qf2_loss: 0.03712, policy_loss: -43.67887, policy_entropy: 4.10178, alpha: 0.33569, time: 50.73944
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 29.14831, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.11532, qf2_loss: 0.11541, policy_loss: -44.09853, policy_entropy: 4.10063, alpha: 0.32811, time: 50.77485
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 23.16426, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.03283, qf2_loss: 0.03286, policy_loss: -44.49326, policy_entropy: 4.10167, alpha: 0.32071, time: 50.70521
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 24.37282, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.04703, qf2_loss: 0.04705, policy_loss: -44.87850, policy_entropy: 4.10100, alpha: 0.31348, time: 50.73831
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 23.10512, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.07256, qf2_loss: 0.07255, policy_loss: -45.24583, policy_entropy: 4.10209, alpha: 0.30643, time: 50.70710
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 27.64272, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.05536, qf2_loss: 0.05534, policy_loss: -45.59952, policy_entropy: 4.10148, alpha: 0.29954, time: 50.73317
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 24.30123, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.07864, qf2_loss: 0.07862, policy_loss: -45.94342, policy_entropy: 4.10129, alpha: 0.29282, time: 50.78132
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 23.17988, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.03990, qf2_loss: 0.03988, policy_loss: -46.26977, policy_entropy: 4.10225, alpha: 0.28625, time: 50.71979
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 25.58542, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.06823, qf2_loss: 0.06758, policy_loss: -46.57849, policy_entropy: 4.10065, alpha: 0.27983, time: 50.80245
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 23.22218, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.06877, qf2_loss: 0.06924, policy_loss: -46.88202, policy_entropy: 4.10009, alpha: 0.27357, time: 50.80562
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 25.99797, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.07959, qf2_loss: 0.07982, policy_loss: -47.16702, policy_entropy: 4.10161, alpha: 0.26745, time: 50.74394
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 24.36182, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.04051, qf2_loss: 0.04056, policy_loss: -47.44461, policy_entropy: 4.10214, alpha: 0.26147, time: 50.76295
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 24.63427, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.04527, qf2_loss: 0.04533, policy_loss: -47.70577, policy_entropy: 4.10218, alpha: 0.25563, time: 50.70378
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 29.13138, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.08472, qf2_loss: 0.08472, policy_loss: -47.95534, policy_entropy: 4.10199, alpha: 0.24993, time: 50.77147
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 26.17223, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.04141, qf2_loss: 0.04152, policy_loss: -48.19216, policy_entropy: 4.10203, alpha: 0.24435, time: 50.66971
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 22.61790, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.05106, qf2_loss: 0.05108, policy_loss: -48.42079, policy_entropy: 4.10069, alpha: 0.23891, time: 50.57403
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 22.48843, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.05080, qf2_loss: 0.05077, policy_loss: -48.63292, policy_entropy: 4.10159, alpha: 0.23358, time: 50.68105
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 24.48465, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.05206, qf2_loss: 0.05212, policy_loss: -48.84297, policy_entropy: 4.10077, alpha: 0.22838, time: 50.52905
[CW] eval: return: 25.34871, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 26.80910, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.09662, qf2_loss: 0.09653, policy_loss: -49.03497, policy_entropy: 4.10088, alpha: 0.22330, time: 50.86392
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 23.46480, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.02107, qf2_loss: 0.02118, policy_loss: -49.21963, policy_entropy: 4.10177, alpha: 0.21833, time: 50.75310
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 23.56037, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.04617, qf2_loss: 0.04619, policy_loss: -49.39339, policy_entropy: 4.10209, alpha: 0.21348, time: 50.78696
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 23.69450, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.04814, qf2_loss: 0.04809, policy_loss: -49.56127, policy_entropy: 4.10127, alpha: 0.20873, time: 50.71255
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 24.29757, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.05821, qf2_loss: 0.05822, policy_loss: -49.71342, policy_entropy: 4.10221, alpha: 0.20409, time: 50.78415
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 27.20912, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.06930, qf2_loss: 0.06930, policy_loss: -49.86157, policy_entropy: 4.10149, alpha: 0.19956, time: 50.78072
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 23.25904, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.03914, qf2_loss: 0.03910, policy_loss: -49.99606, policy_entropy: 4.10204, alpha: 0.19513, time: 51.04877
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 25.34835, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.04795, qf2_loss: 0.04792, policy_loss: -50.12488, policy_entropy: 4.10144, alpha: 0.19080, time: 51.60087
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 33.23608, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.05755, qf2_loss: 0.05754, policy_loss: -50.24840, policy_entropy: 4.10031, alpha: 0.18656, time: 59.17310
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 24.25699, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.04520, qf2_loss: 0.04505, policy_loss: -50.35598, policy_entropy: 4.10183, alpha: 0.18242, time: 50.82850
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 21.75770, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.05607, qf2_loss: 0.05610, policy_loss: -50.45965, policy_entropy: 4.10213, alpha: 0.17838, time: 51.18611
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 30.68590, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.03331, qf2_loss: 0.03324, policy_loss: -50.55290, policy_entropy: 4.10166, alpha: 0.17442, time: 50.75604
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 24.34118, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.06020, qf2_loss: 0.06008, policy_loss: -50.63848, policy_entropy: 4.10247, alpha: 0.17055, time: 50.85628
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 25.60926, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.07996, qf2_loss: 0.07982, policy_loss: -50.71594, policy_entropy: 4.10249, alpha: 0.16677, time: 50.72885
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 25.93574, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.02493, qf2_loss: 0.02492, policy_loss: -50.78967, policy_entropy: 4.10156, alpha: 0.16308, time: 50.83393
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 21.69849, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.02861, qf2_loss: 0.02862, policy_loss: -50.85639, policy_entropy: 4.10002, alpha: 0.15946, time: 50.80892
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 24.53732, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.04393, qf2_loss: 0.04377, policy_loss: -50.90426, policy_entropy: 4.10116, alpha: 0.15593, time: 50.81178
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 25.66552, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.04293, qf2_loss: 0.04287, policy_loss: -50.96339, policy_entropy: 4.10123, alpha: 0.15248, time: 50.80142
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 29.33884, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.05047, qf2_loss: 0.05043, policy_loss: -51.00938, policy_entropy: 4.10229, alpha: 0.14910, time: 50.78416
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 23.72888, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.04801, qf2_loss: 0.04779, policy_loss: -51.04290, policy_entropy: 4.10196, alpha: 0.14580, time: 50.86276
[CW] eval: return: 24.82158, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 24.41479, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.03200, qf2_loss: 0.03188, policy_loss: -51.07994, policy_entropy: 4.10216, alpha: 0.14257, time: 50.89333
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 22.86607, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.04389, qf2_loss: 0.04365, policy_loss: -51.10319, policy_entropy: 4.10087, alpha: 0.13941, time: 50.76249
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 25.03196, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.05749, qf2_loss: 0.05726, policy_loss: -51.12642, policy_entropy: 4.10213, alpha: 0.13632, time: 50.88012
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 26.87451, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.02582, qf2_loss: 0.02566, policy_loss: -51.13475, policy_entropy: 4.10074, alpha: 0.13331, time: 50.75267
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 28.71250, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.03886, qf2_loss: 0.03865, policy_loss: -51.14467, policy_entropy: 4.10120, alpha: 0.13036, time: 50.79330
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 23.35508, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.03912, qf2_loss: 0.03903, policy_loss: -51.14665, policy_entropy: 4.10112, alpha: 0.12747, time: 50.82590
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 23.76035, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.06119, qf2_loss: 0.06115, policy_loss: -51.14507, policy_entropy: 4.10059, alpha: 0.12465, time: 50.87513
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 24.38819, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.02172, qf2_loss: 0.02160, policy_loss: -51.13534, policy_entropy: 4.10211, alpha: 0.12189, time: 50.85516
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 20.84628, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.03834, qf2_loss: 0.03842, policy_loss: -51.12065, policy_entropy: 4.10209, alpha: 0.11919, time: 50.76016
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 34.52564, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.03373, qf2_loss: 0.03372, policy_loss: -51.10690, policy_entropy: 4.10172, alpha: 0.11655, time: 50.89643
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 25.33011, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.06167, qf2_loss: 0.06104, policy_loss: -51.08330, policy_entropy: 4.10131, alpha: 0.11398, time: 50.79440
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 26.51796, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.03663, qf2_loss: 0.03672, policy_loss: -51.04906, policy_entropy: 4.10089, alpha: 0.11145, time: 50.81464
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 26.06866, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.04694, qf2_loss: 0.04677, policy_loss: -51.01918, policy_entropy: 4.10080, alpha: 0.10899, time: 50.84402
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 23.42903, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.02478, qf2_loss: 0.02444, policy_loss: -50.98380, policy_entropy: 4.10096, alpha: 0.10658, time: 50.76725
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 28.94467, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.02930, qf2_loss: 0.02955, policy_loss: -50.94571, policy_entropy: 4.10156, alpha: 0.10422, time: 50.88866
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 21.49804, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.05088, qf2_loss: 0.05047, policy_loss: -50.89642, policy_entropy: 4.10233, alpha: 0.10192, time: 50.78997
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 26.28762, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.02569, qf2_loss: 0.02560, policy_loss: -50.84777, policy_entropy: 4.10235, alpha: 0.09966, time: 50.86928
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 29.79286, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.05038, qf2_loss: 0.05023, policy_loss: -50.79675, policy_entropy: 4.10188, alpha: 0.09746, time: 50.80035
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 27.10740, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.01666, qf2_loss: 0.01645, policy_loss: -50.74268, policy_entropy: 4.10032, alpha: 0.09530, time: 50.89227
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 24.42757, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.04909, qf2_loss: 0.04924, policy_loss: -50.68051, policy_entropy: 4.10149, alpha: 0.09320, time: 50.85288
[CW] eval: return: 24.30215, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 27.41015, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.01520, qf2_loss: 0.01527, policy_loss: -50.61444, policy_entropy: 4.10186, alpha: 0.09113, time: 50.90807
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 23.77587, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.05096, qf2_loss: 0.05053, policy_loss: -50.54762, policy_entropy: 4.10153, alpha: 0.08912, time: 50.80411
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 24.41018, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.03543, qf2_loss: 0.03559, policy_loss: -50.47880, policy_entropy: 4.10078, alpha: 0.08715, time: 50.88290
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 25.52039, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.02066, qf2_loss: 0.02088, policy_loss: -50.40348, policy_entropy: 4.10077, alpha: 0.08522, time: 50.79243
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 22.33289, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.03134, qf2_loss: 0.03134, policy_loss: -50.32885, policy_entropy: 4.10177, alpha: 0.08334, time: 50.89669
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 27.79829, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.04495, qf2_loss: 0.04454, policy_loss: -50.25163, policy_entropy: 4.10109, alpha: 0.08149, time: 50.79251
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 23.43202, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.02485, qf2_loss: 0.02496, policy_loss: -50.16552, policy_entropy: 4.10087, alpha: 0.07969, time: 50.86566
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 26.31238, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.03557, qf2_loss: 0.03552, policy_loss: -50.08020, policy_entropy: 4.10133, alpha: 0.07793, time: 50.84669
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 23.14397, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.02977, qf2_loss: 0.02953, policy_loss: -49.99256, policy_entropy: 4.10206, alpha: 0.07620, time: 50.83479
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 21.85202, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.03285, qf2_loss: 0.03285, policy_loss: -49.89776, policy_entropy: 4.10098, alpha: 0.07452, time: 50.87721
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 27.45069, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.02549, qf2_loss: 0.02532, policy_loss: -49.79995, policy_entropy: 4.10122, alpha: 0.07287, time: 50.77489
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 21.55916, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.02596, qf2_loss: 0.02644, policy_loss: -49.70502, policy_entropy: 4.09989, alpha: 0.07126, time: 50.91929
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 26.40833, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.03361, qf2_loss: 0.03250, policy_loss: -49.60676, policy_entropy: 4.10080, alpha: 0.06968, time: 50.69363
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 26.50991, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.03606, qf2_loss: 0.03653, policy_loss: -49.50804, policy_entropy: 4.10206, alpha: 0.06814, time: 50.87421
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 29.29007, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.02000, qf2_loss: 0.02011, policy_loss: -49.40179, policy_entropy: 4.09994, alpha: 0.06664, time: 50.80828
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 21.47729, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.03794, qf2_loss: 0.03781, policy_loss: -49.29498, policy_entropy: 4.09958, alpha: 0.06516, time: 50.88319
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 23.73687, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.02205, qf2_loss: 0.02221, policy_loss: -49.18812, policy_entropy: 4.10270, alpha: 0.06372, time: 50.89225
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 21.23140, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.04540, qf2_loss: 0.04514, policy_loss: -49.07425, policy_entropy: 4.10036, alpha: 0.06231, time: 50.79996
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 24.18601, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.01698, qf2_loss: 0.01675, policy_loss: -48.96184, policy_entropy: 4.10089, alpha: 0.06093, time: 50.91513
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 27.64232, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.02842, qf2_loss: 0.02833, policy_loss: -48.85137, policy_entropy: 4.10001, alpha: 0.05959, time: 50.80813
[CW] eval: return: 24.81165, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 24.72562, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.03025, qf2_loss: 0.03038, policy_loss: -48.73317, policy_entropy: 4.10136, alpha: 0.05827, time: 50.96149
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 26.73284, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.02250, qf2_loss: 0.02241, policy_loss: -48.61424, policy_entropy: 4.10035, alpha: 0.05698, time: 50.92527
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 20.78333, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.03799, qf2_loss: 0.03801, policy_loss: -48.49468, policy_entropy: 4.10154, alpha: 0.05572, time: 50.87200
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 29.83807, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.02330, qf2_loss: 0.02322, policy_loss: -48.37000, policy_entropy: 4.10048, alpha: 0.05449, time: 50.87750
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 29.10442, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.01544, qf2_loss: 0.01544, policy_loss: -48.24611, policy_entropy: 4.10174, alpha: 0.05328, time: 50.87326
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 21.07843, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.02269, qf2_loss: 0.02270, policy_loss: -48.12218, policy_entropy: 4.10014, alpha: 0.05210, time: 50.77861
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 28.64977, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.04472, qf2_loss: 0.04495, policy_loss: -47.99879, policy_entropy: 4.10098, alpha: 0.05095, time: 53.23179
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 22.63037, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.02244, qf2_loss: 0.02240, policy_loss: -47.86747, policy_entropy: 4.09980, alpha: 0.04983, time: 50.85648
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 26.32196, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.01772, qf2_loss: 0.01780, policy_loss: -47.73866, policy_entropy: 4.10072, alpha: 0.04872, time: 50.96054
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 22.28669, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.03123, qf2_loss: 0.03104, policy_loss: -47.61012, policy_entropy: 4.10026, alpha: 0.04765, time: 50.78254
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 27.36514, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.02479, qf2_loss: 0.02482, policy_loss: -47.47397, policy_entropy: 4.10198, alpha: 0.04659, time: 50.82947
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 23.04132, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.02076, qf2_loss: 0.02082, policy_loss: -47.34644, policy_entropy: 4.10142, alpha: 0.04556, time: 50.90145
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 25.61597, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.03273, qf2_loss: 0.03245, policy_loss: -47.20983, policy_entropy: 4.09909, alpha: 0.04455, time: 50.83058
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 27.26364, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.01776, qf2_loss: 0.01811, policy_loss: -47.07340, policy_entropy: 4.10221, alpha: 0.04357, time: 50.89367
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 23.81755, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.02138, qf2_loss: 0.02153, policy_loss: -46.93937, policy_entropy: 4.10054, alpha: 0.04261, time: 50.80373
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 24.16261, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.02831, qf2_loss: 0.02795, policy_loss: -46.79867, policy_entropy: 4.10021, alpha: 0.04166, time: 50.88065
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 25.66195, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.02988, qf2_loss: 0.02967, policy_loss: -46.66345, policy_entropy: 4.09961, alpha: 0.04074, time: 50.91457
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 27.71602, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.01575, qf2_loss: 0.01576, policy_loss: -46.52117, policy_entropy: 4.09953, alpha: 0.03984, time: 50.84688
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 25.74384, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.02557, qf2_loss: 0.02541, policy_loss: -46.38405, policy_entropy: 4.09932, alpha: 0.03896, time: 51.55807
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 25.74697, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.02599, qf2_loss: 0.02647, policy_loss: -46.23630, policy_entropy: 4.09777, alpha: 0.03810, time: 56.09657
[CW] eval: return: 25.19558, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 21.41007, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.01729, qf2_loss: 0.01736, policy_loss: -46.09655, policy_entropy: 4.09911, alpha: 0.03726, time: 51.05682
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 24.70387, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.02427, qf2_loss: 0.02364, policy_loss: -45.95280, policy_entropy: 4.09810, alpha: 0.03643, time: 50.89911
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 25.58048, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.02538, qf2_loss: 0.02554, policy_loss: -45.80762, policy_entropy: 4.09849, alpha: 0.03563, time: 50.93678
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 23.66768, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.02518, qf2_loss: 0.02432, policy_loss: -45.66420, policy_entropy: 4.09837, alpha: 0.03484, time: 50.85257
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 26.13224, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.02147, qf2_loss: 0.02194, policy_loss: -45.52015, policy_entropy: 4.09994, alpha: 0.03407, time: 50.95624
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 29.81856, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.02053, qf2_loss: 0.02072, policy_loss: -45.37382, policy_entropy: 4.09948, alpha: 0.03332, time: 50.82666
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 26.48728, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.03003, qf2_loss: 0.03016, policy_loss: -45.22495, policy_entropy: 4.10160, alpha: 0.03258, time: 50.96765
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 28.76564, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.01405, qf2_loss: 0.01378, policy_loss: -45.08046, policy_entropy: 4.09797, alpha: 0.03186, time: 50.94280
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 25.40739, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.03022, qf2_loss: 0.02996, policy_loss: -44.93097, policy_entropy: 4.10033, alpha: 0.03115, time: 50.88532
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 24.88052, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.01783, qf2_loss: 0.01806, policy_loss: -44.78231, policy_entropy: 4.09782, alpha: 0.03047, time: 51.19063
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 24.83943, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.02161, qf2_loss: 0.02160, policy_loss: -44.63291, policy_entropy: 4.09907, alpha: 0.02979, time: 51.08764
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 24.54232, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.02721, qf2_loss: 0.02666, policy_loss: -44.48379, policy_entropy: 4.09899, alpha: 0.02913, time: 51.03222
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 24.82656, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.01917, qf2_loss: 0.01881, policy_loss: -44.33024, policy_entropy: 4.09853, alpha: 0.02849, time: 50.91669
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 26.42668, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.01900, qf2_loss: 0.02016, policy_loss: -44.17974, policy_entropy: 4.09744, alpha: 0.02786, time: 50.96915
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 22.66690, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.02628, qf2_loss: 0.02509, policy_loss: -44.03227, policy_entropy: 4.09421, alpha: 0.02724, time: 51.01913
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 22.46195, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.01417, qf2_loss: 0.01493, policy_loss: -43.88082, policy_entropy: 4.09683, alpha: 0.02664, time: 50.85260
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 23.18919, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.02215, qf2_loss: 0.02249, policy_loss: -43.72787, policy_entropy: 4.09752, alpha: 0.02605, time: 50.98389
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 27.74238, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.02823, qf2_loss: 0.02776, policy_loss: -43.58103, policy_entropy: 4.09903, alpha: 0.02548, time: 50.91968
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 28.89311, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.01224, qf2_loss: 0.01241, policy_loss: -43.42719, policy_entropy: 4.09731, alpha: 0.02491, time: 50.90798
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 24.81313, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.02948, qf2_loss: 0.02948, policy_loss: -43.27133, policy_entropy: 4.09605, alpha: 0.02436, time: 50.97052
[CW] eval: return: 25.02165, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 31.10460, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.01765, qf2_loss: 0.01807, policy_loss: -43.12638, policy_entropy: 4.09427, alpha: 0.02382, time: 51.04829
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 21.26601, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.02356, qf2_loss: 0.02316, policy_loss: -42.96700, policy_entropy: 4.09177, alpha: 0.02330, time: 50.88422
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 23.61061, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.01917, qf2_loss: 0.01908, policy_loss: -42.81748, policy_entropy: 4.09098, alpha: 0.02278, time: 51.00045
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 28.40220, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.01266, qf2_loss: 0.01296, policy_loss: -42.66371, policy_entropy: 4.09435, alpha: 0.02228, time: 50.92417
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 27.34501, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.04800, qf2_loss: 0.04719, policy_loss: -42.50935, policy_entropy: 4.08894, alpha: 0.02179, time: 50.90404
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 26.94726, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.00849, qf2_loss: 0.00855, policy_loss: -42.35152, policy_entropy: 4.08901, alpha: 0.02131, time: 50.97977
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 25.54543, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.01380, qf2_loss: 0.01401, policy_loss: -42.20298, policy_entropy: 4.09046, alpha: 0.02084, time: 50.73525
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 24.15377, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.01722, qf2_loss: 0.01766, policy_loss: -42.04910, policy_entropy: 4.08624, alpha: 0.02037, time: 51.24626
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 27.40946, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.02206, qf2_loss: 0.02189, policy_loss: -41.89629, policy_entropy: 4.09169, alpha: 0.01992, time: 51.06359
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 29.01468, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.01919, qf2_loss: 0.01931, policy_loss: -41.74480, policy_entropy: 4.08973, alpha: 0.01948, time: 51.08256
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 29.96241, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.02350, qf2_loss: 0.02357, policy_loss: -41.59462, policy_entropy: 4.07913, alpha: 0.01905, time: 50.91127
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 26.00864, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.01818, qf2_loss: 0.01817, policy_loss: -41.44159, policy_entropy: 4.07857, alpha: 0.01863, time: 50.99463
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 22.79072, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.01772, qf2_loss: 0.01775, policy_loss: -41.28572, policy_entropy: 4.07757, alpha: 0.01822, time: 51.01390
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 24.99339, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.01931, qf2_loss: 0.01945, policy_loss: -41.12832, policy_entropy: 4.08666, alpha: 0.01782, time: 50.90221
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 23.46348, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.02282, qf2_loss: 0.02249, policy_loss: -40.97573, policy_entropy: 4.08342, alpha: 0.01743, time: 51.05314
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 24.06795, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.02311, qf2_loss: 0.02347, policy_loss: -40.82298, policy_entropy: 4.07884, alpha: 0.01704, time: 50.90855
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 24.11790, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.01112, qf2_loss: 0.01074, policy_loss: -40.66860, policy_entropy: 4.08338, alpha: 0.01666, time: 50.99800
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 24.81893, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.02511, qf2_loss: 0.02505, policy_loss: -40.51851, policy_entropy: 4.08494, alpha: 0.01630, time: 50.99357
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 24.20544, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.02135, qf2_loss: 0.02120, policy_loss: -40.36422, policy_entropy: 4.08063, alpha: 0.01594, time: 50.92084
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 25.07539, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.01975, qf2_loss: 0.01937, policy_loss: -40.21126, policy_entropy: 4.06668, alpha: 0.01558, time: 51.02075
[CW] eval: return: 25.94440, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 26.88371, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.01380, qf2_loss: 0.01417, policy_loss: -40.05444, policy_entropy: 4.07963, alpha: 0.01524, time: 51.02290
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 26.84166, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.01549, qf2_loss: 0.01542, policy_loss: -39.90238, policy_entropy: 4.06909, alpha: 0.01490, time: 50.88404
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 23.81826, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.02000, qf2_loss: 0.01989, policy_loss: -39.75399, policy_entropy: 4.06374, alpha: 0.01457, time: 50.89906
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 24.43484, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.02305, qf2_loss: 0.02288, policy_loss: -39.59705, policy_entropy: 4.03661, alpha: 0.01425, time: 50.95174
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 22.32075, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.02081, qf2_loss: 0.02127, policy_loss: -39.45012, policy_entropy: 4.04576, alpha: 0.01394, time: 50.83615
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 24.22221, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.02014, qf2_loss: 0.01962, policy_loss: -39.29370, policy_entropy: 4.04514, alpha: 0.01363, time: 50.97554
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 24.81391, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.01837, qf2_loss: 0.01847, policy_loss: -39.14255, policy_entropy: 3.98613, alpha: 0.01333, time: 50.84632
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 27.64561, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.01741, qf2_loss: 0.01704, policy_loss: -38.99114, policy_entropy: 4.04317, alpha: 0.01304, time: 50.88265
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 24.46126, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.02270, qf2_loss: 0.02267, policy_loss: -38.84531, policy_entropy: 4.00352, alpha: 0.01275, time: 50.96618
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 25.52437, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.01735, qf2_loss: 0.01605, policy_loss: -38.67923, policy_entropy: 3.99941, alpha: 0.01247, time: 50.86150
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 22.50761, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.02022, qf2_loss: 0.02101, policy_loss: -38.53562, policy_entropy: 3.99534, alpha: 0.01220, time: 51.00636
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 22.98340, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.01884, qf2_loss: 0.01816, policy_loss: -38.38513, policy_entropy: 3.96761, alpha: 0.01193, time: 50.92967
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 24.26066, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.01643, qf2_loss: 0.01700, policy_loss: -38.23419, policy_entropy: 3.93722, alpha: 0.01167, time: 50.96758
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 24.70321, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.02117, qf2_loss: 0.02066, policy_loss: -38.08301, policy_entropy: 3.92010, alpha: 0.01141, time: 51.68901
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 24.74576, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.02029, qf2_loss: 0.02027, policy_loss: -37.93639, policy_entropy: 3.90187, alpha: 0.01116, time: 51.03603
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 27.98075, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.01880, qf2_loss: 0.01886, policy_loss: -37.79061, policy_entropy: 3.86730, alpha: 0.01092, time: 51.09594
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 29.08076, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.03479, qf2_loss: 0.03492, policy_loss: -37.64167, policy_entropy: 3.82111, alpha: 0.01068, time: 50.94715
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 22.96940, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.01931, qf2_loss: 0.01817, policy_loss: -37.49888, policy_entropy: 3.73500, alpha: 0.01045, time: 53.77262
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 25.15346, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.01731, qf2_loss: 0.01716, policy_loss: -37.34935, policy_entropy: 3.64574, alpha: 0.01023, time: 51.08397
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 24.53315, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.01505, qf2_loss: 0.01480, policy_loss: -37.20612, policy_entropy: 3.36336, alpha: 0.01001, time: 52.63048
[CW] eval: return: 24.18602, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 18.49754, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.02518, qf2_loss: 0.02473, policy_loss: -37.06303, policy_entropy: 3.45685, alpha: 0.00980, time: 51.13671
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 21.97611, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.02214, qf2_loss: 0.02177, policy_loss: -36.91103, policy_entropy: 3.55908, alpha: 0.00959, time: 51.02087
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 21.60271, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.02087, qf2_loss: 0.02195, policy_loss: -36.77077, policy_entropy: 3.54350, alpha: 0.00938, time: 50.93106
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 27.13933, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.02912, qf2_loss: 0.02821, policy_loss: -36.62836, policy_entropy: 3.66212, alpha: 0.00918, time: 50.93476
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 31.43237, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.02704, qf2_loss: 0.02714, policy_loss: -36.48989, policy_entropy: 3.74699, alpha: 0.00898, time: 51.00597
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 27.27370, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.02230, qf2_loss: 0.02199, policy_loss: -36.33481, policy_entropy: 3.59548, alpha: 0.00879, time: 50.91382
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 27.36258, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.03018, qf2_loss: 0.02866, policy_loss: -36.19169, policy_entropy: 3.68097, alpha: 0.00860, time: 55.83174
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 31.85419, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.02118, qf2_loss: 0.02084, policy_loss: -36.05495, policy_entropy: 3.42452, alpha: 0.00841, time: 50.98828
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 23.68634, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.02536, qf2_loss: 0.02624, policy_loss: -35.90892, policy_entropy: 3.28047, alpha: 0.00823, time: 51.01061
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 33.49049, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.03650, qf2_loss: 0.03744, policy_loss: -35.77145, policy_entropy: 3.56753, alpha: 0.00805, time: 51.07232
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 35.51195, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.02592, qf2_loss: 0.02498, policy_loss: -35.62452, policy_entropy: 3.29625, alpha: 0.00788, time: 50.92057
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 37.31692, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.03242, qf2_loss: 0.03160, policy_loss: -35.50437, policy_entropy: 2.76614, alpha: 0.00772, time: 51.02179
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 12.79716, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.02464, qf2_loss: 0.02457, policy_loss: -35.36596, policy_entropy: 2.39086, alpha: 0.00757, time: 50.93074
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 24.95328, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 0.02763, qf2_loss: 0.02799, policy_loss: -35.21473, policy_entropy: 2.30952, alpha: 0.00742, time: 51.02901
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 36.00042, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.02250, qf2_loss: 0.02091, policy_loss: -35.08134, policy_entropy: 2.41188, alpha: 0.00727, time: 51.10422
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 27.73751, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.03525, qf2_loss: 0.03752, policy_loss: -34.94058, policy_entropy: 1.70315, alpha: 0.00713, time: 50.87944
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 60.23730, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.03387, qf2_loss: 0.03201, policy_loss: -34.82368, policy_entropy: 2.03183, alpha: 0.00700, time: 51.10774
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 10.36705, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.03714, qf2_loss: 0.03738, policy_loss: -34.67926, policy_entropy: 1.43373, alpha: 0.00687, time: 51.01927
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 74.89660, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.02923, qf2_loss: 0.02927, policy_loss: -34.55127, policy_entropy: 0.93501, alpha: 0.00675, time: 50.96470
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 23.28764, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.03306, qf2_loss: 0.03384, policy_loss: -34.42805, policy_entropy: 0.98991, alpha: 0.00663, time: 51.07567
[CW] eval: return: 29.59999, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 21.58560, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.02981, qf2_loss: 0.02937, policy_loss: -34.29168, policy_entropy: 1.07646, alpha: 0.00651, time: 51.10638
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 38.11122, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.03572, qf2_loss: 0.03534, policy_loss: -34.16047, policy_entropy: 0.25916, alpha: 0.00640, time: 50.99996
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 29.42343, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.03914, qf2_loss: 0.03890, policy_loss: -34.05260, policy_entropy: 0.32597, alpha: 0.00630, time: 51.00156
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 29.54827, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.04032, qf2_loss: 0.03935, policy_loss: -33.92412, policy_entropy: -0.41231, alpha: 0.00619, time: 51.07789
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 60.27610, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.04004, qf2_loss: 0.04217, policy_loss: -33.82806, policy_entropy: -2.88598, alpha: 0.00611, time: 50.97487
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 22.30371, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.03706, qf2_loss: 0.03713, policy_loss: -33.70537, policy_entropy: -2.27699, alpha: 0.00606, time: 51.05048
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 36.25474, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.03719, qf2_loss: 0.03719, policy_loss: -33.59172, policy_entropy: -1.28353, alpha: 0.00599, time: 51.03331
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 24.13226, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 0.04575, qf2_loss: 0.04465, policy_loss: -33.46824, policy_entropy: -0.66064, alpha: 0.00590, time: 50.93682
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 30.81255, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.03353, qf2_loss: 0.03346, policy_loss: -33.36494, policy_entropy: -0.52716, alpha: 0.00580, time: 51.05952
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 32.11281, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.04352, qf2_loss: 0.04254, policy_loss: -33.24487, policy_entropy: -0.49684, alpha: 0.00570, time: 51.00616
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 23.35918, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 0.05796, qf2_loss: 0.05678, policy_loss: -33.16623, policy_entropy: -1.11203, alpha: 0.00561, time: 50.96036
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 43.34083, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.05451, qf2_loss: 0.05420, policy_loss: -33.05418, policy_entropy: -1.03657, alpha: 0.00552, time: 51.04383
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 23.12724, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.05560, qf2_loss: 0.05497, policy_loss: -32.94671, policy_entropy: -1.44151, alpha: 0.00543, time: 50.95372
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 22.71491, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.04560, qf2_loss: 0.04519, policy_loss: -32.83585, policy_entropy: -2.56189, alpha: 0.00536, time: 51.00705
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 34.31347, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.05131, qf2_loss: 0.05090, policy_loss: -32.75878, policy_entropy: -3.60461, alpha: 0.00530, time: 51.06341
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 10.18065, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.04680, qf2_loss: 0.04639, policy_loss: -32.66132, policy_entropy: -3.39255, alpha: 0.00526, time: 50.96228
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 43.46703, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 0.06498, qf2_loss: 0.06339, policy_loss: -32.60561, policy_entropy: -2.68804, alpha: 0.00520, time: 51.07527
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 46.88964, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 0.06906, qf2_loss: 0.06854, policy_loss: -32.47748, policy_entropy: -2.03114, alpha: 0.00513, time: 51.11978
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 42.89782, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 0.05732, qf2_loss: 0.05734, policy_loss: -32.41898, policy_entropy: -3.15718, alpha: 0.00505, time: 50.91539
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 10.18534, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.05332, qf2_loss: 0.05244, policy_loss: -32.32340, policy_entropy: -3.50492, alpha: 0.00500, time: 51.03303
[CW] eval: return: 23.70081, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 22.87779, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 0.05872, qf2_loss: 0.05807, policy_loss: -32.28929, policy_entropy: -4.81304, alpha: 0.00496, time: 50.67904
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 22.84805, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 0.07034, qf2_loss: 0.07032, policy_loss: -32.21809, policy_entropy: -4.70817, alpha: 0.00494, time: 51.94436
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 30.04996, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 0.07275, qf2_loss: 0.07338, policy_loss: -32.15309, policy_entropy: -4.08909, alpha: 0.00490, time: 50.72004
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 32.70392, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 0.08084, qf2_loss: 0.08113, policy_loss: -32.11107, policy_entropy: -4.41256, alpha: 0.00486, time: 50.68364
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 82.59508, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 0.08070, qf2_loss: 0.07979, policy_loss: -32.09238, policy_entropy: -6.57643, alpha: 0.00484, time: 50.70763
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 70.86428, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 0.07344, qf2_loss: 0.07360, policy_loss: -32.09774, policy_entropy: -7.34591, alpha: 0.00487, time: 50.56863
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 93.34537, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 0.07854, qf2_loss: 0.07809, policy_loss: -32.08307, policy_entropy: -7.52910, alpha: 0.00491, time: 50.65731
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 70.63571, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 0.09502, qf2_loss: 0.09458, policy_loss: -32.09731, policy_entropy: -7.31115, alpha: 0.00495, time: 50.58113
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 55.76291, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 0.09083, qf2_loss: 0.09114, policy_loss: -32.07301, policy_entropy: -7.32450, alpha: 0.00498, time: 50.55072
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 50.76331, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 0.08529, qf2_loss: 0.08510, policy_loss: -32.08981, policy_entropy: -7.23306, alpha: 0.00503, time: 50.69825
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 22.16482, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 0.07566, qf2_loss: 0.07571, policy_loss: -32.01394, policy_entropy: -7.02650, alpha: 0.00507, time: 50.61031
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 95.28029, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 0.10290, qf2_loss: 0.10295, policy_loss: -32.01766, policy_entropy: -6.52981, alpha: 0.00509, time: 50.56080
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 23.69160, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 0.08605, qf2_loss: 0.08611, policy_loss: -31.97454, policy_entropy: -6.20695, alpha: 0.00511, time: 50.55856
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 66.95332, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 0.12068, qf2_loss: 0.12007, policy_loss: -31.90364, policy_entropy: -4.83276, alpha: 0.00510, time: 50.53485
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 21.97470, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 0.09125, qf2_loss: 0.09054, policy_loss: -31.85998, policy_entropy: -3.47361, alpha: 0.00503, time: 50.63118
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 64.82910, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 0.09498, qf2_loss: 0.09336, policy_loss: -31.83795, policy_entropy: -4.06079, alpha: 0.00493, time: 50.60485
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 62.29282, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 0.09137, qf2_loss: 0.09034, policy_loss: -31.79118, policy_entropy: -3.63895, alpha: 0.00484, time: 50.50363
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 31.32483, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 0.09085, qf2_loss: 0.08806, policy_loss: -31.67606, policy_entropy: -3.39767, alpha: 0.00475, time: 50.56851
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 83.54092, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 0.08981, qf2_loss: 0.08905, policy_loss: -31.66730, policy_entropy: -3.74901, alpha: 0.00464, time: 50.56284
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 77.12265, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 0.11928, qf2_loss: 0.11832, policy_loss: -31.61449, policy_entropy: -4.19315, alpha: 0.00457, time: 50.53125
[CW] eval: return: 26.75005, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 76.84316, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 0.08873, qf2_loss: 0.08669, policy_loss: -31.55445, policy_entropy: -3.74122, alpha: 0.00449, time: 50.69513
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 23.05132, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 0.08205, qf2_loss: 0.08042, policy_loss: -31.53802, policy_entropy: -3.80705, alpha: 0.00440, time: 50.60030
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 56.10605, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 0.08698, qf2_loss: 0.08643, policy_loss: -31.45424, policy_entropy: -3.66678, alpha: 0.00432, time: 50.58434
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 22.34877, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 0.08192, qf2_loss: 0.08062, policy_loss: -31.44505, policy_entropy: -3.68131, alpha: 0.00423, time: 50.98800
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 22.21149, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 0.09838, qf2_loss: 0.09713, policy_loss: -31.39254, policy_entropy: -3.80216, alpha: 0.00414, time: 50.75714
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 22.88625, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 0.08163, qf2_loss: 0.08142, policy_loss: -31.33788, policy_entropy: -3.83060, alpha: 0.00406, time: 50.75411
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 26.74852, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 0.10198, qf2_loss: 0.10044, policy_loss: -31.26225, policy_entropy: -3.94075, alpha: 0.00399, time: 50.55731
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 17.89475, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 0.08581, qf2_loss: 0.08544, policy_loss: -31.23381, policy_entropy: -3.97169, alpha: 0.00392, time: 51.72978
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 27.91333, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 0.08538, qf2_loss: 0.08437, policy_loss: -31.21335, policy_entropy: -3.69405, alpha: 0.00384, time: 50.87795
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 28.53698, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 0.08172, qf2_loss: 0.08086, policy_loss: -31.19893, policy_entropy: -4.52199, alpha: 0.00376, time: 50.56017
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 23.64336, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 0.08252, qf2_loss: 0.08199, policy_loss: -31.08834, policy_entropy: -4.94181, alpha: 0.00373, time: 50.57841
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 45.12805, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 0.08936, qf2_loss: 0.08701, policy_loss: -31.08733, policy_entropy: -5.04035, alpha: 0.00369, time: 50.48404
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 26.02144, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 0.07401, qf2_loss: 0.07443, policy_loss: -30.93481, policy_entropy: -5.13141, alpha: 0.00365, time: 50.56668
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 55.06567, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 0.09146, qf2_loss: 0.09188, policy_loss: -30.90393, policy_entropy: -6.44815, alpha: 0.00363, time: 50.53671
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 64.68919, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 0.07277, qf2_loss: 0.07246, policy_loss: -30.81729, policy_entropy: -6.17152, alpha: 0.00366, time: 50.49405
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 61.66196, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 0.06615, qf2_loss: 0.06635, policy_loss: -30.78479, policy_entropy: -4.93115, alpha: 0.00364, time: 50.56092
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 60.80292, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 0.08069, qf2_loss: 0.07907, policy_loss: -30.74684, policy_entropy: -4.83396, alpha: 0.00359, time: 50.51735
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 37.28656, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 0.06656, qf2_loss: 0.06733, policy_loss: -30.63156, policy_entropy: -4.30102, alpha: 0.00354, time: 52.31698
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 29.13172, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 0.05804, qf2_loss: 0.05690, policy_loss: -30.57012, policy_entropy: -3.56667, alpha: 0.00345, time: 50.59980
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 37.74113, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 0.05725, qf2_loss: 0.05754, policy_loss: -30.54058, policy_entropy: -4.11440, alpha: 0.00336, time: 50.54980
[CW] eval: return: 40.35860, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 42.15015, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 0.05834, qf2_loss: 0.05915, policy_loss: -30.50610, policy_entropy: -4.04425, alpha: 0.00329, time: 50.73196
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 32.12847, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 0.05375, qf2_loss: 0.05280, policy_loss: -30.42896, policy_entropy: -4.78992, alpha: 0.00322, time: 50.54868
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 14.40180, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 0.06213, qf2_loss: 0.06216, policy_loss: -30.35659, policy_entropy: -5.02629, alpha: 0.00318, time: 50.59396
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 61.12051, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 0.06221, qf2_loss: 0.06013, policy_loss: -30.25804, policy_entropy: -5.66064, alpha: 0.00316, time: 50.71557
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 21.31123, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 0.05334, qf2_loss: 0.05590, policy_loss: -30.24644, policy_entropy: -4.09983, alpha: 0.00312, time: 50.66104
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 51.65099, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 0.04244, qf2_loss: 0.04341, policy_loss: -30.13687, policy_entropy: -4.42027, alpha: 0.00305, time: 50.66682
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 39.80546, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 0.04060, qf2_loss: 0.04096, policy_loss: -30.10813, policy_entropy: -5.05058, alpha: 0.00299, time: 50.50194
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 46.89561, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 0.05999, qf2_loss: 0.05834, policy_loss: -29.96381, policy_entropy: -4.72236, alpha: 0.00295, time: 50.61369
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 40.19693, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 0.03957, qf2_loss: 0.04007, policy_loss: -29.98515, policy_entropy: -4.55043, alpha: 0.00290, time: 50.58654
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 29.89433, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 0.03693, qf2_loss: 0.03723, policy_loss: -29.88608, policy_entropy: -4.91015, alpha: 0.00285, time: 50.53614
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 36.74084, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 0.04388, qf2_loss: 0.04430, policy_loss: -29.80684, policy_entropy: -5.57971, alpha: 0.00282, time: 50.28504
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 26.61435, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 0.04158, qf2_loss: 0.04097, policy_loss: -29.68002, policy_entropy: -5.15119, alpha: 0.00280, time: 50.37934
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 39.11278, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 0.04499, qf2_loss: 0.04527, policy_loss: -29.64978, policy_entropy: -5.59875, alpha: 0.00277, time: 50.79291
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 28.09417, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 0.04932, qf2_loss: 0.04932, policy_loss: -29.58832, policy_entropy: -5.37454, alpha: 0.00275, time: 50.97577
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 28.03794, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 0.03551, qf2_loss: 0.03555, policy_loss: -29.53314, policy_entropy: -5.83823, alpha: 0.00273, time: 50.88416
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 37.04096, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 0.03769, qf2_loss: 0.03864, policy_loss: -29.44025, policy_entropy: -6.30818, alpha: 0.00274, time: 50.93706
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 81.86966, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 0.03153, qf2_loss: 0.03135, policy_loss: -29.35689, policy_entropy: -5.70752, alpha: 0.00274, time: 50.88888
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 78.62652, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 0.03393, qf2_loss: 0.03370, policy_loss: -29.25329, policy_entropy: -5.69706, alpha: 0.00273, time: 50.80255
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 94.93367, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 0.03857, qf2_loss: 0.03836, policy_loss: -29.23544, policy_entropy: -6.12711, alpha: 0.00271, time: 50.97192
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 100.87684, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 0.05591, qf2_loss: 0.05625, policy_loss: -29.17120, policy_entropy: -6.45546, alpha: 0.00273, time: 50.90395
[CW] eval: return: 76.99719, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 67.89035, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 0.03722, qf2_loss: 0.03843, policy_loss: -29.05352, policy_entropy: -6.43690, alpha: 0.00275, time: 51.15096
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 87.33659, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 0.05242, qf2_loss: 0.04989, policy_loss: -29.05209, policy_entropy: -6.48383, alpha: 0.00278, time: 50.73763
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 98.56125, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 0.03480, qf2_loss: 0.03456, policy_loss: -28.95744, policy_entropy: -6.97205, alpha: 0.00283, time: 50.90075
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 81.62764, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 0.03907, qf2_loss: 0.03893, policy_loss: -28.94142, policy_entropy: -6.83505, alpha: 0.00289, time: 50.98082
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 79.64291, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 0.03515, qf2_loss: 0.03462, policy_loss: -28.86147, policy_entropy: -6.83847, alpha: 0.00294, time: 50.85951
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 109.86954, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 0.03602, qf2_loss: 0.03540, policy_loss: -28.85227, policy_entropy: -6.96810, alpha: 0.00301, time: 50.90580
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 111.87261, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 0.04931, qf2_loss: 0.04854, policy_loss: -28.75804, policy_entropy: -7.36437, alpha: 0.00309, time: 50.87470
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 91.29052, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 0.04797, qf2_loss: 0.04804, policy_loss: -28.75834, policy_entropy: -6.88744, alpha: 0.00319, time: 50.80866
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 109.06939, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 0.03813, qf2_loss: 0.03785, policy_loss: -28.69724, policy_entropy: -7.07275, alpha: 0.00327, time: 50.91937
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 103.93555, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 0.04244, qf2_loss: 0.04084, policy_loss: -28.64885, policy_entropy: -6.82410, alpha: 0.00336, time: 50.86839
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 91.11934, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 0.03851, qf2_loss: 0.03817, policy_loss: -28.58352, policy_entropy: -6.34519, alpha: 0.00342, time: 50.81866
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 100.37094, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 0.06532, qf2_loss: 0.06495, policy_loss: -28.50242, policy_entropy: -6.34694, alpha: 0.00345, time: 50.90315
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 61.76060, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 0.03883, qf2_loss: 0.03841, policy_loss: -28.49699, policy_entropy: -6.50064, alpha: 0.00349, time: 51.74545
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 107.63741, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 0.03852, qf2_loss: 0.03795, policy_loss: -28.45768, policy_entropy: -6.38808, alpha: 0.00355, time: 50.81651
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 104.59979, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 0.04571, qf2_loss: 0.04466, policy_loss: -28.35402, policy_entropy: -6.42586, alpha: 0.00360, time: 50.76879
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 102.66491, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 0.04236, qf2_loss: 0.04158, policy_loss: -28.27220, policy_entropy: -6.63948, alpha: 0.00365, time: 50.63877
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 76.35878, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 0.04309, qf2_loss: 0.04278, policy_loss: -28.26943, policy_entropy: -6.56941, alpha: 0.00373, time: 50.42905
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 95.07003, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 0.04810, qf2_loss: 0.04673, policy_loss: -28.24721, policy_entropy: -6.65963, alpha: 0.00382, time: 50.49304
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 96.39801, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 0.04385, qf2_loss: 0.04346, policy_loss: -28.18356, policy_entropy: -6.50462, alpha: 0.00390, time: 50.81636
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 100.58352, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 0.05003, qf2_loss: 0.04921, policy_loss: -28.09359, policy_entropy: -6.03074, alpha: 0.00395, time: 50.91198
[CW] eval: return: 90.72102, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 93.82474, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 0.04781, qf2_loss: 0.04714, policy_loss: -28.11859, policy_entropy: -6.33441, alpha: 0.00397, time: 50.99871
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 109.17630, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 0.04378, qf2_loss: 0.04275, policy_loss: -28.06734, policy_entropy: -6.33806, alpha: 0.00404, time: 50.86940
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 80.38632, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 0.04686, qf2_loss: 0.04530, policy_loss: -28.02970, policy_entropy: -6.49914, alpha: 0.00411, time: 50.78597
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 87.82311, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 0.04637, qf2_loss: 0.04619, policy_loss: -27.97183, policy_entropy: -6.12168, alpha: 0.00416, time: 50.75484
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 103.37416, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 0.07244, qf2_loss: 0.07143, policy_loss: -27.95241, policy_entropy: -6.26355, alpha: 0.00419, time: 50.82994
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 124.02554, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 0.04664, qf2_loss: 0.04517, policy_loss: -27.90371, policy_entropy: -6.10216, alpha: 0.00423, time: 50.77088
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 110.45170, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 0.04340, qf2_loss: 0.04233, policy_loss: -27.85644, policy_entropy: -6.10978, alpha: 0.00425, time: 50.81514
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 104.75997, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 0.06280, qf2_loss: 0.06210, policy_loss: -27.83943, policy_entropy: -6.07826, alpha: 0.00427, time: 50.84510
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 85.36478, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 0.04404, qf2_loss: 0.04295, policy_loss: -27.81790, policy_entropy: -6.12365, alpha: 0.00427, time: 50.79495
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 102.60048, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 0.04732, qf2_loss: 0.04588, policy_loss: -27.79346, policy_entropy: -6.33634, alpha: 0.00435, time: 50.84391
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 108.11398, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 0.05521, qf2_loss: 0.05477, policy_loss: -27.76174, policy_entropy: -6.44526, alpha: 0.00443, time: 50.86278
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 93.49057, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 0.04469, qf2_loss: 0.04354, policy_loss: -27.71176, policy_entropy: -6.44252, alpha: 0.00453, time: 50.76977
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 115.29054, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 0.05545, qf2_loss: 0.05436, policy_loss: -27.71445, policy_entropy: -6.08764, alpha: 0.00459, time: 50.88472
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 111.79370, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 0.05196, qf2_loss: 0.05052, policy_loss: -27.65703, policy_entropy: -6.56791, alpha: 0.00462, time: 50.91877
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 108.65748, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 0.04998, qf2_loss: 0.04900, policy_loss: -27.65814, policy_entropy: -6.96073, alpha: 0.00482, time: 51.16259
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 103.86300, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 0.05350, qf2_loss: 0.05379, policy_loss: -27.59831, policy_entropy: -6.22721, alpha: 0.00497, time: 51.45232
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 114.57270, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 0.05136, qf2_loss: 0.04898, policy_loss: -27.56853, policy_entropy: -6.50700, alpha: 0.00506, time: 51.37844
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 113.44903, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 0.04824, qf2_loss: 0.04640, policy_loss: -27.53138, policy_entropy: -6.02072, alpha: 0.00513, time: 51.53235
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 106.52287, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 0.05825, qf2_loss: 0.05729, policy_loss: -27.51275, policy_entropy: -6.08550, alpha: 0.00513, time: 51.11480
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 109.39243, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 0.05987, qf2_loss: 0.05829, policy_loss: -27.45891, policy_entropy: -6.49752, alpha: 0.00521, time: 51.02033
[CW] eval: return: 109.79900, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 109.27971, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 0.05373, qf2_loss: 0.05411, policy_loss: -27.44258, policy_entropy: -5.99421, alpha: 0.00526, time: 51.01450
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 107.29544, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 0.04502, qf2_loss: 0.04405, policy_loss: -27.42513, policy_entropy: -5.93580, alpha: 0.00526, time: 50.63073
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 121.11751, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 0.05078, qf2_loss: 0.04938, policy_loss: -27.39681, policy_entropy: -5.95223, alpha: 0.00525, time: 50.96189
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 117.94068, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 0.05958, qf2_loss: 0.05810, policy_loss: -27.35886, policy_entropy: -6.13855, alpha: 0.00524, time: 50.93365
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 117.66146, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 0.05130, qf2_loss: 0.05062, policy_loss: -27.35150, policy_entropy: -6.34010, alpha: 0.00531, time: 50.88761
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 127.38625, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 0.04552, qf2_loss: 0.04484, policy_loss: -27.32309, policy_entropy: -6.40488, alpha: 0.00542, time: 50.97710
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 121.99149, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 0.05532, qf2_loss: 0.05434, policy_loss: -27.29920, policy_entropy: -5.69790, alpha: 0.00545, time: 50.93010
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 112.80305, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 0.05605, qf2_loss: 0.05473, policy_loss: -27.24481, policy_entropy: -5.97919, alpha: 0.00541, time: 50.84806
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 109.05650, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 0.06263, qf2_loss: 0.06144, policy_loss: -27.24048, policy_entropy: -6.00353, alpha: 0.00539, time: 51.36032
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 133.98276, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 0.04954, qf2_loss: 0.04840, policy_loss: -27.22243, policy_entropy: -6.18705, alpha: 0.00543, time: 51.11200
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 124.11106, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 0.04970, qf2_loss: 0.04893, policy_loss: -27.19697, policy_entropy: -5.93120, alpha: 0.00545, time: 50.96929
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 108.79448, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 0.04603, qf2_loss: 0.04546, policy_loss: -27.18975, policy_entropy: -6.16177, alpha: 0.00543, time: 50.79069
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 123.19808, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 0.04798, qf2_loss: 0.04826, policy_loss: -27.15294, policy_entropy: -6.02896, alpha: 0.00548, time: 50.79949
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 127.02895, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 0.05188, qf2_loss: 0.05071, policy_loss: -27.11466, policy_entropy: -5.75594, alpha: 0.00545, time: 50.85176
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 130.65173, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 0.05195, qf2_loss: 0.05147, policy_loss: -27.08867, policy_entropy: -5.59874, alpha: 0.00536, time: 51.10909
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 127.51472, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 0.04638, qf2_loss: 0.04468, policy_loss: -27.08882, policy_entropy: -5.69273, alpha: 0.00526, time: 50.74185
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 153.48903, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 0.04953, qf2_loss: 0.04850, policy_loss: -27.04135, policy_entropy: -5.65162, alpha: 0.00518, time: 51.13590
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 125.15030, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 0.05832, qf2_loss: 0.05698, policy_loss: -27.02697, policy_entropy: -6.15474, alpha: 0.00514, time: 51.03960
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 128.76803, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 0.04818, qf2_loss: 0.04767, policy_loss: -26.99387, policy_entropy: -6.18311, alpha: 0.00518, time: 50.96505
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 120.90453, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 0.04732, qf2_loss: 0.04732, policy_loss: -27.00188, policy_entropy: -6.10632, alpha: 0.00522, time: 50.96099
[CW] eval: return: 131.60873, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 145.84447, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 0.05766, qf2_loss: 0.05689, policy_loss: -26.94003, policy_entropy: -6.21942, alpha: 0.00527, time: 51.28831
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 130.67598, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 0.05806, qf2_loss: 0.05680, policy_loss: -26.97572, policy_entropy: -6.22226, alpha: 0.00534, time: 50.97758
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 105.52700, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 0.04964, qf2_loss: 0.04872, policy_loss: -26.91168, policy_entropy: -5.99324, alpha: 0.00537, time: 50.94517
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 136.59176, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 0.05247, qf2_loss: 0.05188, policy_loss: -26.90935, policy_entropy: -5.85368, alpha: 0.00534, time: 50.87605
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 127.36521, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 0.05283, qf2_loss: 0.05309, policy_loss: -26.88554, policy_entropy: -6.07747, alpha: 0.00534, time: 50.94177
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 130.84033, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 0.05629, qf2_loss: 0.05602, policy_loss: -26.84727, policy_entropy: -6.00301, alpha: 0.00535, time: 50.94674
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 127.13691, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 0.05049, qf2_loss: 0.04943, policy_loss: -26.82848, policy_entropy: -5.84830, alpha: 0.00533, time: 50.87533
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 137.73251, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 0.04671, qf2_loss: 0.04578, policy_loss: -26.83195, policy_entropy: -5.89152, alpha: 0.00529, time: 50.92901
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 131.33403, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 0.04844, qf2_loss: 0.04759, policy_loss: -26.78790, policy_entropy: -5.77819, alpha: 0.00524, time: 51.00539
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 130.70895, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 0.06043, qf2_loss: 0.05890, policy_loss: -26.78256, policy_entropy: -6.18169, alpha: 0.00523, time: 50.86019
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 138.28892, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 0.05476, qf2_loss: 0.05437, policy_loss: -26.78367, policy_entropy: -6.09156, alpha: 0.00527, time: 52.39419
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 124.83623, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 0.05051, qf2_loss: 0.04961, policy_loss: -26.76203, policy_entropy: -6.04001, alpha: 0.00529, time: 50.99489
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 129.91433, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 0.05088, qf2_loss: 0.05072, policy_loss: -26.71637, policy_entropy: -6.13198, alpha: 0.00531, time: 50.86542
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 124.24847, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 0.04954, qf2_loss: 0.04914, policy_loss: -26.72327, policy_entropy: -5.79878, alpha: 0.00532, time: 50.94434
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 134.41705, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 0.04990, qf2_loss: 0.04914, policy_loss: -26.65630, policy_entropy: -5.92413, alpha: 0.00526, time: 50.96779
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 146.97829, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 0.06306, qf2_loss: 0.06194, policy_loss: -26.64923, policy_entropy: -5.43141, alpha: 0.00518, time: 50.88571
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 118.69934, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 0.05071, qf2_loss: 0.05060, policy_loss: -26.62032, policy_entropy: -6.03194, alpha: 0.00508, time: 50.99188
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 122.25352, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 0.04487, qf2_loss: 0.04508, policy_loss: -26.60428, policy_entropy: -5.98868, alpha: 0.00511, time: 50.95045
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 138.66282, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 0.04939, qf2_loss: 0.04867, policy_loss: -26.60144, policy_entropy: -5.94543, alpha: 0.00507, time: 50.87600
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 132.62658, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 0.04991, qf2_loss: 0.04900, policy_loss: -26.58085, policy_entropy: -6.15335, alpha: 0.00510, time: 50.96085
[CW] eval: return: 132.79120, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 115.26914, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 0.04872, qf2_loss: 0.04823, policy_loss: -26.60407, policy_entropy: -6.07361, alpha: 0.00513, time: 51.03270
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 147.12858, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 0.05088, qf2_loss: 0.05020, policy_loss: -26.53486, policy_entropy: -6.09615, alpha: 0.00516, time: 50.92904
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 135.44348, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 0.04544, qf2_loss: 0.04406, policy_loss: -26.53316, policy_entropy: -5.78717, alpha: 0.00515, time: 51.58564
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 148.05280, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 0.05046, qf2_loss: 0.05034, policy_loss: -26.53096, policy_entropy: -5.74485, alpha: 0.00506, time: 50.92087
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 146.99933, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 0.04665, qf2_loss: 0.04684, policy_loss: -26.48819, policy_entropy: -6.05297, alpha: 0.00501, time: 50.91093
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 145.65522, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 0.04629, qf2_loss: 0.04495, policy_loss: -26.46314, policy_entropy: -6.25356, alpha: 0.00508, time: 50.89810
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 137.12014, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 0.05647, qf2_loss: 0.05653, policy_loss: -26.45127, policy_entropy: -5.97784, alpha: 0.00511, time: 50.90605
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 130.75694, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 0.04428, qf2_loss: 0.04390, policy_loss: -26.46900, policy_entropy: -5.91705, alpha: 0.00511, time: 50.92161
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 139.33813, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 0.04799, qf2_loss: 0.04756, policy_loss: -26.44751, policy_entropy: -6.12185, alpha: 0.00510, time: 50.85988
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 153.44967, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 0.05267, qf2_loss: 0.05154, policy_loss: -26.42865, policy_entropy: -5.78407, alpha: 0.00509, time: 51.18475
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 148.58724, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 0.04567, qf2_loss: 0.04486, policy_loss: -26.42549, policy_entropy: -5.70644, alpha: 0.00502, time: 50.97062
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 143.83372, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 0.04766, qf2_loss: 0.04785, policy_loss: -26.38529, policy_entropy: -5.87623, alpha: 0.00495, time: 50.87398
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 155.72355, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 0.04356, qf2_loss: 0.04292, policy_loss: -26.38513, policy_entropy: -5.88514, alpha: 0.00491, time: 50.98462
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 137.73325, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 0.06399, qf2_loss: 0.06397, policy_loss: -26.34786, policy_entropy: -6.20798, alpha: 0.00491, time: 50.95210
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 141.90004, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 0.04682, qf2_loss: 0.04612, policy_loss: -26.34582, policy_entropy: -5.85557, alpha: 0.00495, time: 50.87550
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 143.76757, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 0.05494, qf2_loss: 0.05376, policy_loss: -26.32081, policy_entropy: -5.92254, alpha: 0.00490, time: 50.97558
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 144.00620, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 0.04270, qf2_loss: 0.04233, policy_loss: -26.32500, policy_entropy: -5.90561, alpha: 0.00489, time: 50.93117
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 142.54205, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 0.04799, qf2_loss: 0.04818, policy_loss: -26.28806, policy_entropy: -5.67585, alpha: 0.00483, time: 50.93087
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 143.83686, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 0.04906, qf2_loss: 0.04890, policy_loss: -26.26226, policy_entropy: -5.55952, alpha: 0.00473, time: 50.98167
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 97.57392, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 0.04954, qf2_loss: 0.04847, policy_loss: -26.26698, policy_entropy: -5.82165, alpha: 0.00462, time: 50.90529
[CW] eval: return: 145.91166, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 157.60130, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 0.04725, qf2_loss: 0.04746, policy_loss: -26.26351, policy_entropy: -5.82000, alpha: 0.00459, time: 51.06380
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 149.95113, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 0.05520, qf2_loss: 0.05481, policy_loss: -26.19896, policy_entropy: -5.86912, alpha: 0.00453, time: 50.89876
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 157.05897, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 0.04562, qf2_loss: 0.04491, policy_loss: -26.23406, policy_entropy: -6.15057, alpha: 0.00454, time: 51.04635
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 153.38304, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 0.04713, qf2_loss: 0.04657, policy_loss: -26.22570, policy_entropy: -6.09820, alpha: 0.00458, time: 51.02985
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 147.70979, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 0.06115, qf2_loss: 0.06187, policy_loss: -26.22203, policy_entropy: -6.12260, alpha: 0.00461, time: 54.19114
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 144.91074, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 0.05019, qf2_loss: 0.04949, policy_loss: -26.22733, policy_entropy: -5.94545, alpha: 0.00462, time: 51.08842
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 158.14418, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 0.04780, qf2_loss: 0.04769, policy_loss: -26.18265, policy_entropy: -6.21998, alpha: 0.00463, time: 51.04991
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 131.43568, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 0.05178, qf2_loss: 0.05097, policy_loss: -26.17097, policy_entropy: -6.22280, alpha: 0.00472, time: 50.95301
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 149.83267, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 0.05652, qf2_loss: 0.05555, policy_loss: -26.19282, policy_entropy: -6.12117, alpha: 0.00476, time: 51.09643
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 135.22091, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 0.05036, qf2_loss: 0.05056, policy_loss: -26.21618, policy_entropy: -6.10638, alpha: 0.00481, time: 51.02137
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 148.15123, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 0.04971, qf2_loss: 0.04938, policy_loss: -26.17787, policy_entropy: -5.87793, alpha: 0.00482, time: 50.99372
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 150.04924, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 0.04771, qf2_loss: 0.04793, policy_loss: -26.14846, policy_entropy: -6.02626, alpha: 0.00479, time: 51.09580
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 145.40400, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 0.05624, qf2_loss: 0.05561, policy_loss: -26.16757, policy_entropy: -5.91272, alpha: 0.00477, time: 51.05908
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 146.46632, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 0.05418, qf2_loss: 0.05413, policy_loss: -26.17327, policy_entropy: -6.02437, alpha: 0.00477, time: 51.06942
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 146.84055, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 0.05513, qf2_loss: 0.05395, policy_loss: -26.19888, policy_entropy: -6.00977, alpha: 0.00476, time: 51.19144
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 148.43892, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 0.05454, qf2_loss: 0.05457, policy_loss: -26.20214, policy_entropy: -6.19937, alpha: 0.00480, time: 51.08432
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 145.91169, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 0.05739, qf2_loss: 0.05713, policy_loss: -26.15993, policy_entropy: -5.99016, alpha: 0.00484, time: 51.03568
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 163.36272, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 0.05374, qf2_loss: 0.05452, policy_loss: -26.19757, policy_entropy: -6.24520, alpha: 0.00487, time: 51.10233
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 159.08612, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 0.05678, qf2_loss: 0.05691, policy_loss: -26.18123, policy_entropy: -6.09046, alpha: 0.00496, time: 51.06771
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 155.88531, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 0.05141, qf2_loss: 0.05029, policy_loss: -26.17897, policy_entropy: -6.10113, alpha: 0.00499, time: 51.04393
[CW] eval: return: 148.73758, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 140.83453, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 0.05345, qf2_loss: 0.05298, policy_loss: -26.18788, policy_entropy: -6.04553, alpha: 0.00503, time: 51.12877
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 144.96406, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 0.05961, qf2_loss: 0.05953, policy_loss: -26.18560, policy_entropy: -6.25677, alpha: 0.00508, time: 51.09588
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 156.91335, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 0.05498, qf2_loss: 0.05569, policy_loss: -26.18649, policy_entropy: -5.79892, alpha: 0.00510, time: 51.11670
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 166.27103, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 0.05420, qf2_loss: 0.05389, policy_loss: -26.18282, policy_entropy: -5.95727, alpha: 0.00505, time: 51.03843
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 144.98731, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 0.05337, qf2_loss: 0.05300, policy_loss: -26.20143, policy_entropy: -6.08432, alpha: 0.00503, time: 51.12148
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 150.09126, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 0.07345, qf2_loss: 0.07315, policy_loss: -26.18581, policy_entropy: -5.75587, alpha: 0.00505, time: 51.07311
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 156.00745, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 0.09193, qf2_loss: 0.09185, policy_loss: -26.19039, policy_entropy: -5.87780, alpha: 0.00494, time: 51.04944
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 149.11994, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 0.04969, qf2_loss: 0.04898, policy_loss: -26.20270, policy_entropy: -6.20690, alpha: 0.00496, time: 51.10558
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 167.15097, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 0.05221, qf2_loss: 0.05097, policy_loss: -26.19010, policy_entropy: -5.96915, alpha: 0.00499, time: 51.07878
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 146.75979, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 0.05539, qf2_loss: 0.05560, policy_loss: -26.18864, policy_entropy: -6.09327, alpha: 0.00501, time: 51.05265
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 150.20059, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 0.05259, qf2_loss: 0.05226, policy_loss: -26.19960, policy_entropy: -6.06327, alpha: 0.00504, time: 51.81360
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 156.42051, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 0.04730, qf2_loss: 0.04704, policy_loss: -26.17038, policy_entropy: -5.99123, alpha: 0.00505, time: 51.10549
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 155.00750, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 0.05032, qf2_loss: 0.05040, policy_loss: -26.20492, policy_entropy: -6.04322, alpha: 0.00506, time: 51.11786
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 152.32405, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 0.05683, qf2_loss: 0.05672, policy_loss: -26.20269, policy_entropy: -5.96864, alpha: 0.00505, time: 51.18452
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 161.47865, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 0.05687, qf2_loss: 0.05631, policy_loss: -26.20688, policy_entropy: -5.97717, alpha: 0.00505, time: 51.10525
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 151.83047, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 0.06396, qf2_loss: 0.06320, policy_loss: -26.23575, policy_entropy: -6.16879, alpha: 0.00507, time: 51.05672
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 165.95014, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 0.07081, qf2_loss: 0.06975, policy_loss: -26.22549, policy_entropy: -6.17010, alpha: 0.00514, time: 51.07367
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 157.97648, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 0.05104, qf2_loss: 0.05096, policy_loss: -26.21152, policy_entropy: -6.09731, alpha: 0.00523, time: 51.07092
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 147.82638, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 0.05536, qf2_loss: 0.05476, policy_loss: -26.21905, policy_entropy: -6.05260, alpha: 0.00522, time: 50.99382
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 161.68962, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 0.04954, qf2_loss: 0.04914, policy_loss: -26.24224, policy_entropy: -5.92754, alpha: 0.00524, time: 51.08974
[CW] eval: return: 153.57735, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 160.11267, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 0.05111, qf2_loss: 0.05147, policy_loss: -26.21986, policy_entropy: -5.83764, alpha: 0.00520, time: 52.51102
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 162.54795, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 0.05693, qf2_loss: 0.05684, policy_loss: -26.23752, policy_entropy: -6.12379, alpha: 0.00518, time: 51.06287
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 165.52266, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 0.04889, qf2_loss: 0.04855, policy_loss: -26.24603, policy_entropy: -6.08625, alpha: 0.00522, time: 51.01072
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 167.47267, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 0.05156, qf2_loss: 0.05179, policy_loss: -26.22940, policy_entropy: -6.04346, alpha: 0.00524, time: 51.06329
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 162.45600, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 0.05859, qf2_loss: 0.05777, policy_loss: -26.27129, policy_entropy: -6.03702, alpha: 0.00527, time: 51.08886
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 163.27352, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 0.06062, qf2_loss: 0.06027, policy_loss: -26.27091, policy_entropy: -5.80747, alpha: 0.00523, time: 51.01109
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 162.62782, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 0.05074, qf2_loss: 0.05027, policy_loss: -26.23532, policy_entropy: -5.93786, alpha: 0.00517, time: 51.35291
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 164.64175, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 0.06103, qf2_loss: 0.06062, policy_loss: -26.27732, policy_entropy: -5.96430, alpha: 0.00517, time: 51.11890
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 154.47456, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 0.06093, qf2_loss: 0.05891, policy_loss: -26.24052, policy_entropy: -5.70185, alpha: 0.00509, time: 51.06486
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 152.89159, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 0.05616, qf2_loss: 0.05758, policy_loss: -26.26571, policy_entropy: -5.77592, alpha: 0.00500, time: 51.07225
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 152.36743, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 0.05198, qf2_loss: 0.05132, policy_loss: -26.24780, policy_entropy: -5.94032, alpha: 0.00493, time: 51.11646
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 159.41002, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 0.05637, qf2_loss: 0.05577, policy_loss: -26.25290, policy_entropy: -6.03867, alpha: 0.00492, time: 51.06049
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 168.45701, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 0.05703, qf2_loss: 0.05814, policy_loss: -26.27604, policy_entropy: -6.08615, alpha: 0.00496, time: 53.03025
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 159.86415, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 0.05577, qf2_loss: 0.05463, policy_loss: -26.24151, policy_entropy: -5.98807, alpha: 0.00497, time: 51.10683
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 168.14159, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 0.05572, qf2_loss: 0.05569, policy_loss: -26.30686, policy_entropy: -5.89072, alpha: 0.00495, time: 51.07292
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 153.72990, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 0.06264, qf2_loss: 0.06177, policy_loss: -26.27258, policy_entropy: -5.90055, alpha: 0.00490, time: 51.10789
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 145.22391, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 0.05289, qf2_loss: 0.05276, policy_loss: -26.24379, policy_entropy: -6.06070, alpha: 0.00491, time: 51.08884
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 167.34272, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 0.04885, qf2_loss: 0.04882, policy_loss: -26.28101, policy_entropy: -6.08868, alpha: 0.00493, time: 51.02601
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 141.60503, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 0.05162, qf2_loss: 0.05176, policy_loss: -26.28280, policy_entropy: -5.94306, alpha: 0.00494, time: 51.07633
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 150.52040, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 0.06013, qf2_loss: 0.05899, policy_loss: -26.30844, policy_entropy: -6.06634, alpha: 0.00494, time: 51.10451
[CW] eval: return: 159.07918, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 163.82687, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 0.05850, qf2_loss: 0.05846, policy_loss: -26.30306, policy_entropy: -6.00415, alpha: 0.00495, time: 51.18258
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 162.14745, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 0.05772, qf2_loss: 0.05724, policy_loss: -26.29119, policy_entropy: -5.82627, alpha: 0.00494, time: 51.15647
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 149.66208, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 0.05696, qf2_loss: 0.05676, policy_loss: -26.29431, policy_entropy: -5.88338, alpha: 0.00487, time: 51.15619
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 148.22665, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 0.05501, qf2_loss: 0.05589, policy_loss: -26.28733, policy_entropy: -6.04263, alpha: 0.00485, time: 51.41357
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 161.26596, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 0.05267, qf2_loss: 0.05192, policy_loss: -26.29532, policy_entropy: -6.06291, alpha: 0.00487, time: 51.37711
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 160.42524, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 0.05383, qf2_loss: 0.05285, policy_loss: -26.32060, policy_entropy: -5.88266, alpha: 0.00487, time: 50.91446
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 170.81106, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 0.06508, qf2_loss: 0.06462, policy_loss: -26.32834, policy_entropy: -5.94522, alpha: 0.00483, time: 50.75749
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 165.85075, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 0.06687, qf2_loss: 0.06597, policy_loss: -26.30822, policy_entropy: -6.11308, alpha: 0.00483, time: 50.66342
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 162.33575, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 0.06476, qf2_loss: 0.06595, policy_loss: -26.32149, policy_entropy: -5.99733, alpha: 0.00488, time: 50.61747
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 166.78120, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 0.05648, qf2_loss: 0.05605, policy_loss: -26.29740, policy_entropy: -6.09337, alpha: 0.00489, time: 50.68449
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 173.59232, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 0.05654, qf2_loss: 0.05607, policy_loss: -26.32112, policy_entropy: -5.98078, alpha: 0.00489, time: 50.66473
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 163.05520, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 0.05324, qf2_loss: 0.05252, policy_loss: -26.35461, policy_entropy: -5.80714, alpha: 0.00487, time: 50.63495
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 163.09139, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 0.06052, qf2_loss: 0.05931, policy_loss: -26.34370, policy_entropy: -6.03189, alpha: 0.00481, time: 50.66869
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 153.95543, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 0.05200, qf2_loss: 0.05158, policy_loss: -26.37767, policy_entropy: -6.07194, alpha: 0.00483, time: 51.60845
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 171.66753, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 0.05837, qf2_loss: 0.05816, policy_loss: -26.39571, policy_entropy: -5.92321, alpha: 0.00483, time: 50.63311
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 162.13816, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 0.06106, qf2_loss: 0.06227, policy_loss: -26.40268, policy_entropy: -6.03583, alpha: 0.00482, time: 51.08841
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 164.70082, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 0.06428, qf2_loss: 0.06336, policy_loss: -26.37136, policy_entropy: -5.98451, alpha: 0.00484, time: 50.63931
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 158.09612, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 0.05937, qf2_loss: 0.05980, policy_loss: -26.36136, policy_entropy: -6.05864, alpha: 0.00484, time: 50.60271
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 160.99716, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 0.05769, qf2_loss: 0.05738, policy_loss: -26.36955, policy_entropy: -5.97605, alpha: 0.00484, time: 50.70736
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 142.87675, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 0.06489, qf2_loss: 0.06324, policy_loss: -26.39487, policy_entropy: -5.90786, alpha: 0.00484, time: 50.69765
[CW] eval: return: 160.07379, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 160.80738, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 0.05645, qf2_loss: 0.05590, policy_loss: -26.40152, policy_entropy: -5.93616, alpha: 0.00479, time: 50.77116
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 165.65128, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 0.05659, qf2_loss: 0.05657, policy_loss: -26.46688, policy_entropy: -5.97808, alpha: 0.00478, time: 50.61529
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 146.54304, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 0.05750, qf2_loss: 0.05767, policy_loss: -26.41827, policy_entropy: -5.79426, alpha: 0.00473, time: 50.67459
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 165.56746, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 0.06044, qf2_loss: 0.06091, policy_loss: -26.44660, policy_entropy: -5.92875, alpha: 0.00467, time: 50.71097
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 156.92410, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 0.05596, qf2_loss: 0.05467, policy_loss: -26.47683, policy_entropy: -5.87801, alpha: 0.00463, time: 50.64163
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 156.79226, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 0.05773, qf2_loss: 0.05718, policy_loss: -26.44881, policy_entropy: -5.89964, alpha: 0.00460, time: 50.68518
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 169.46118, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 0.06169, qf2_loss: 0.06145, policy_loss: -26.44251, policy_entropy: -5.96933, alpha: 0.00458, time: 50.66066
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 125.77800, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 0.07562, qf2_loss: 0.07539, policy_loss: -26.44969, policy_entropy: -6.10048, alpha: 0.00457, time: 50.60239
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 162.46080, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 0.06674, qf2_loss: 0.06640, policy_loss: -26.43860, policy_entropy: -6.06310, alpha: 0.00463, time: 50.57897
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 166.83870, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 0.06159, qf2_loss: 0.06120, policy_loss: -26.45973, policy_entropy: -5.94698, alpha: 0.00462, time: 50.64682
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 168.61372, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 0.05463, qf2_loss: 0.05392, policy_loss: -26.52382, policy_entropy: -5.97567, alpha: 0.00460, time: 50.62435
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 166.89914, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 0.05931, qf2_loss: 0.05929, policy_loss: -26.54019, policy_entropy: -5.91970, alpha: 0.00459, time: 50.60238
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 159.66932, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 0.07041, qf2_loss: 0.06919, policy_loss: -26.52100, policy_entropy: -6.06574, alpha: 0.00458, time: 50.68065
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 159.55949, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 0.05651, qf2_loss: 0.05715, policy_loss: -26.49078, policy_entropy: -5.87487, alpha: 0.00456, time: 50.58090
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 168.93701, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 0.05988, qf2_loss: 0.05933, policy_loss: -26.54658, policy_entropy: -6.03251, alpha: 0.00455, time: 50.55714
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 169.87191, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 0.06120, qf2_loss: 0.06012, policy_loss: -26.57118, policy_entropy: -6.00322, alpha: 0.00456, time: 50.64779
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 168.42163, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 0.06225, qf2_loss: 0.06240, policy_loss: -26.55656, policy_entropy: -5.97715, alpha: 0.00455, time: 49.95562
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 159.93449, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 0.05870, qf2_loss: 0.05835, policy_loss: -26.54480, policy_entropy: -5.96037, alpha: 0.00454, time: 50.35563
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 167.88727, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 0.05666, qf2_loss: 0.05639, policy_loss: -26.58453, policy_entropy: -6.00439, alpha: 0.00454, time: 49.19000
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 163.13967, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 0.06040, qf2_loss: 0.05955, policy_loss: -26.59457, policy_entropy: -5.86661, alpha: 0.00452, time: 49.06217
[CW] eval: return: 164.52681, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 158.84903, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 0.05455, qf2_loss: 0.05450, policy_loss: -26.60164, policy_entropy: -6.02579, alpha: 0.00449, time: 49.98637
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 157.13235, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 0.06342, qf2_loss: 0.06299, policy_loss: -26.61202, policy_entropy: -5.99449, alpha: 0.00451, time: 48.70537
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 169.34178, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 0.07150, qf2_loss: 0.07159, policy_loss: -26.59890, policy_entropy: -6.04628, alpha: 0.00450, time: 50.19635
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 164.95767, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 0.05568, qf2_loss: 0.05544, policy_loss: -26.62591, policy_entropy: -5.93188, alpha: 0.00450, time: 48.53508
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 168.82482, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 0.06237, qf2_loss: 0.06082, policy_loss: -26.63032, policy_entropy: -6.02661, alpha: 0.00450, time: 48.55090
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 155.35119, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 0.07066, qf2_loss: 0.07091, policy_loss: -26.63474, policy_entropy: -5.91881, alpha: 0.00447, time: 48.79217
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 163.38721, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 0.06047, qf2_loss: 0.06033, policy_loss: -26.66014, policy_entropy: -5.99407, alpha: 0.00446, time: 48.70389
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 165.22331, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 0.05733, qf2_loss: 0.05682, policy_loss: -26.69904, policy_entropy: -5.94557, alpha: 0.00445, time: 48.54992
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 165.23000, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 0.05590, qf2_loss: 0.05574, policy_loss: -26.68701, policy_entropy: -6.07191, alpha: 0.00446, time: 48.67079
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 167.72228, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 0.06730, qf2_loss: 0.06707, policy_loss: -26.67979, policy_entropy: -5.92259, alpha: 0.00446, time: 48.59487
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 167.28470, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 0.06016, qf2_loss: 0.05965, policy_loss: -26.67985, policy_entropy: -6.03249, alpha: 0.00444, time: 48.48383
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 166.85130, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 0.05920, qf2_loss: 0.05947, policy_loss: -26.69207, policy_entropy: -6.04142, alpha: 0.00446, time: 48.74794
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 163.91472, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 0.05536, qf2_loss: 0.05478, policy_loss: -26.73414, policy_entropy: -6.00188, alpha: 0.00447, time: 48.55581
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 166.60620, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 0.06320, qf2_loss: 0.06295, policy_loss: -26.75519, policy_entropy: -5.91347, alpha: 0.00444, time: 48.47966
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 166.16460, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 0.05744, qf2_loss: 0.05810, policy_loss: -26.74139, policy_entropy: -5.87180, alpha: 0.00441, time: 48.56672
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 161.84454, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 0.05633, qf2_loss: 0.05579, policy_loss: -26.81322, policy_entropy: -5.94785, alpha: 0.00437, time: 48.57379
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 166.94643, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 0.05715, qf2_loss: 0.05688, policy_loss: -26.73931, policy_entropy: -6.04571, alpha: 0.00437, time: 49.20607
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 161.93034, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 0.06253, qf2_loss: 0.06247, policy_loss: -26.79024, policy_entropy: -5.93214, alpha: 0.00437, time: 49.73347
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 168.84701, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 0.06153, qf2_loss: 0.06123, policy_loss: -26.82156, policy_entropy: -5.89714, alpha: 0.00434, time: 49.91279
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 168.86426, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 0.06508, qf2_loss: 0.06409, policy_loss: -26.79598, policy_entropy: -6.01960, alpha: 0.00431, time: 49.56723
[CW] eval: return: 165.85791, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 168.57085, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 0.06834, qf2_loss: 0.06844, policy_loss: -26.75932, policy_entropy: -6.09955, alpha: 0.00435, time: 49.11059
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 167.45321, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 0.06783, qf2_loss: 0.06755, policy_loss: -26.82393, policy_entropy: -5.93547, alpha: 0.00437, time: 49.06632
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 165.23877, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 0.06136, qf2_loss: 0.06043, policy_loss: -26.83238, policy_entropy: -5.83204, alpha: 0.00431, time: 49.11105
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 172.69467, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 0.06377, qf2_loss: 0.06333, policy_loss: -26.78677, policy_entropy: -5.96989, alpha: 0.00428, time: 49.66352
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 168.42046, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 0.06416, qf2_loss: 0.06471, policy_loss: -26.82073, policy_entropy: -5.98584, alpha: 0.00428, time: 49.10354
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 167.11785, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 0.05566, qf2_loss: 0.05556, policy_loss: -26.89657, policy_entropy: -6.01114, alpha: 0.00425, time: 48.99985
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 168.01954, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 0.06437, qf2_loss: 0.06331, policy_loss: -26.78384, policy_entropy: -6.15154, alpha: 0.00429, time: 49.00960
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 170.57662, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 0.06579, qf2_loss: 0.06549, policy_loss: -26.83978, policy_entropy: -6.07284, alpha: 0.00432, time: 49.00281
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 170.83619, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 0.05868, qf2_loss: 0.05801, policy_loss: -26.84923, policy_entropy: -6.04350, alpha: 0.00435, time: 49.05557
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 170.02861, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 0.07676, qf2_loss: 0.07739, policy_loss: -26.92359, policy_entropy: -6.06596, alpha: 0.00437, time: 49.02888
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 176.66817, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 0.06355, qf2_loss: 0.06323, policy_loss: -26.93335, policy_entropy: -6.22434, alpha: 0.00442, time: 48.98997
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 169.12336, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 0.06042, qf2_loss: 0.06045, policy_loss: -26.93681, policy_entropy: -5.97599, alpha: 0.00445, time: 49.06250
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 177.86440, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 0.06415, qf2_loss: 0.06313, policy_loss: -26.94328, policy_entropy: -6.06515, alpha: 0.00447, time: 48.99517
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 167.59058, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 0.06010, qf2_loss: 0.06007, policy_loss: -26.96431, policy_entropy: -6.00332, alpha: 0.00449, time: 48.99627
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 168.20876, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 0.06347, qf2_loss: 0.06296, policy_loss: -26.95181, policy_entropy: -6.00299, alpha: 0.00448, time: 49.04822
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 167.02443, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 0.05860, qf2_loss: 0.05678, policy_loss: -26.97627, policy_entropy: -5.98398, alpha: 0.00448, time: 49.02614
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 171.88111, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 0.06334, qf2_loss: 0.06394, policy_loss: -26.98175, policy_entropy: -6.00422, alpha: 0.00449, time: 49.03799
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 164.56415, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 0.06613, qf2_loss: 0.06525, policy_loss: -27.03739, policy_entropy: -6.08110, alpha: 0.00450, time: 50.69258
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 161.64703, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 0.06687, qf2_loss: 0.06662, policy_loss: -27.06927, policy_entropy: -6.01752, alpha: 0.00451, time: 49.00457
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 179.78999, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 0.06233, qf2_loss: 0.06253, policy_loss: -27.02840, policy_entropy: -6.04423, alpha: 0.00452, time: 49.18577
[CW] eval: return: 173.45053, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 170.75433, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 0.06026, qf2_loss: 0.05960, policy_loss: -27.00692, policy_entropy: -6.06018, alpha: 0.00455, time: 49.10275
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 177.76025, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 0.06787, qf2_loss: 0.06807, policy_loss: -27.10713, policy_entropy: -5.99048, alpha: 0.00455, time: 49.05170
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 172.19930, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 0.06282, qf2_loss: 0.06247, policy_loss: -27.02867, policy_entropy: -6.04053, alpha: 0.00455, time: 48.97343
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 176.46945, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 0.06623, qf2_loss: 0.06594, policy_loss: -27.14575, policy_entropy: -5.96665, alpha: 0.00456, time: 49.04716
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 180.20268, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 0.07573, qf2_loss: 0.07308, policy_loss: -27.10177, policy_entropy: -6.07406, alpha: 0.00456, time: 49.04617
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 175.83455, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 0.06395, qf2_loss: 0.06462, policy_loss: -27.18819, policy_entropy: -6.15248, alpha: 0.00462, time: 49.09592
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 181.60600, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 0.07357, qf2_loss: 0.07309, policy_loss: -27.19978, policy_entropy: -6.12135, alpha: 0.00466, time: 49.41627
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 183.80765, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 0.06689, qf2_loss: 0.06716, policy_loss: -27.17943, policy_entropy: -6.04285, alpha: 0.00471, time: 49.30748
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 182.22391, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 0.06906, qf2_loss: 0.06944, policy_loss: -27.17801, policy_entropy: -5.86724, alpha: 0.00469, time: 49.23627
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 185.14395, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 0.06918, qf2_loss: 0.06846, policy_loss: -27.18692, policy_entropy: -6.03872, alpha: 0.00466, time: 49.29638
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 191.78160, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 0.07114, qf2_loss: 0.07021, policy_loss: -27.24223, policy_entropy: -5.89534, alpha: 0.00466, time: 48.85052
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 187.28824, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 0.06842, qf2_loss: 0.06871, policy_loss: -27.20841, policy_entropy: -6.07697, alpha: 0.00465, time: 48.92275
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 174.74604, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 0.07339, qf2_loss: 0.07274, policy_loss: -27.28034, policy_entropy: -5.78585, alpha: 0.00463, time: 48.90284
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 175.27188, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 0.08523, qf2_loss: 0.08550, policy_loss: -27.17058, policy_entropy: -6.18803, alpha: 0.00459, time: 48.93452
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 187.02344, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 0.06936, qf2_loss: 0.06960, policy_loss: -27.30439, policy_entropy: -6.14903, alpha: 0.00468, time: 48.94600
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 182.69366, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 0.06531, qf2_loss: 0.06504, policy_loss: -27.32143, policy_entropy: -5.98995, alpha: 0.00470, time: 48.97050
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 182.00892, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 0.06639, qf2_loss: 0.06646, policy_loss: -27.29890, policy_entropy: -5.89565, alpha: 0.00469, time: 48.93641
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 180.66773, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 0.07390, qf2_loss: 0.07288, policy_loss: -27.24277, policy_entropy: -6.02728, alpha: 0.00467, time: 49.05620
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 187.67829, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 0.07010, qf2_loss: 0.06890, policy_loss: -27.32037, policy_entropy: -6.12367, alpha: 0.00467, time: 49.04994
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 182.89456, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 0.07315, qf2_loss: 0.07358, policy_loss: -27.35998, policy_entropy: -5.97848, alpha: 0.00472, time: 49.10695
[CW] eval: return: 187.57920, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 179.60302, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 0.06963, qf2_loss: 0.06911, policy_loss: -27.38494, policy_entropy: -6.03747, alpha: 0.00472, time: 49.05869
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 187.55417, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 0.09884, qf2_loss: 0.09971, policy_loss: -27.35146, policy_entropy: -5.86252, alpha: 0.00472, time: 48.74604
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 186.42785, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 0.07050, qf2_loss: 0.06938, policy_loss: -27.43091, policy_entropy: -6.09854, alpha: 0.00468, time: 48.71706
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 181.13701, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 0.06918, qf2_loss: 0.06880, policy_loss: -27.37372, policy_entropy: -6.19532, alpha: 0.00474, time: 48.68672
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 182.71963, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 0.07231, qf2_loss: 0.07158, policy_loss: -27.45875, policy_entropy: -6.13880, alpha: 0.00481, time: 48.74682
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 183.65290, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 0.06991, qf2_loss: 0.07040, policy_loss: -27.51927, policy_entropy: -5.95991, alpha: 0.00483, time: 48.72223
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 188.80040, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 0.07623, qf2_loss: 0.07595, policy_loss: -27.47614, policy_entropy: -5.97459, alpha: 0.00482, time: 49.10949
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 191.30874, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 0.07222, qf2_loss: 0.07096, policy_loss: -27.59904, policy_entropy: -6.04142, alpha: 0.00482, time: 48.78236
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 184.93839, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 0.08896, qf2_loss: 0.08822, policy_loss: -27.49634, policy_entropy: -6.18787, alpha: 0.00486, time: 48.73403
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 183.97404, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 0.07096, qf2_loss: 0.07081, policy_loss: -27.63619, policy_entropy: -6.01080, alpha: 0.00492, time: 50.95023
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 174.27592, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 0.07286, qf2_loss: 0.07230, policy_loss: -27.59702, policy_entropy: -6.14425, alpha: 0.00494, time: 49.01291
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 179.24093, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 0.07546, qf2_loss: 0.07551, policy_loss: -27.58090, policy_entropy: -6.27213, alpha: 0.00503, time: 48.89259
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 177.95881, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 0.07156, qf2_loss: 0.07140, policy_loss: -27.58515, policy_entropy: -5.91761, alpha: 0.00508, time: 48.69354
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 192.45317, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 0.08844, qf2_loss: 0.08805, policy_loss: -27.63972, policy_entropy: -6.00143, alpha: 0.00506, time: 48.86761
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 192.44203, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 0.07988, qf2_loss: 0.07961, policy_loss: -27.75239, policy_entropy: -5.94903, alpha: 0.00506, time: 48.92109
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 171.81199, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 0.07725, qf2_loss: 0.07652, policy_loss: -27.73858, policy_entropy: -5.96659, alpha: 0.00502, time: 50.66295
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 192.57342, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 0.07200, qf2_loss: 0.07246, policy_loss: -27.77694, policy_entropy: -5.91970, alpha: 0.00502, time: 48.75398
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 184.12065, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 0.08208, qf2_loss: 0.08241, policy_loss: -27.82237, policy_entropy: -5.89619, alpha: 0.00495, time: 48.75222
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 187.36517, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 0.08675, qf2_loss: 0.08712, policy_loss: -27.80150, policy_entropy: -6.27336, alpha: 0.00498, time: 48.98618
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 188.91094, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 0.07512, qf2_loss: 0.07511, policy_loss: -27.81990, policy_entropy: -6.08460, alpha: 0.00508, time: 48.87920
[CW] eval: return: 196.78987, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 193.77942, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 0.07557, qf2_loss: 0.07531, policy_loss: -27.77210, policy_entropy: -6.07189, alpha: 0.00511, time: 48.81524
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 182.25876, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 0.07912, qf2_loss: 0.07944, policy_loss: -27.81762, policy_entropy: -6.12301, alpha: 0.00516, time: 48.65902
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 176.34238, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 0.08119, qf2_loss: 0.07887, policy_loss: -27.85072, policy_entropy: -6.04442, alpha: 0.00517, time: 48.66550
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 187.79217, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 0.08213, qf2_loss: 0.08255, policy_loss: -27.99906, policy_entropy: -6.03346, alpha: 0.00521, time: 49.39817
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 133.99473, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 0.07626, qf2_loss: 0.07561, policy_loss: -27.96358, policy_entropy: -5.79197, alpha: 0.00518, time: 48.57255
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 193.98930, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 0.08298, qf2_loss: 0.08351, policy_loss: -28.04689, policy_entropy: -6.09405, alpha: 0.00514, time: 48.55575
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 188.71468, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 0.07544, qf2_loss: 0.07548, policy_loss: -27.98136, policy_entropy: -6.19467, alpha: 0.00517, time: 48.52612
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 196.90779, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 0.07496, qf2_loss: 0.07463, policy_loss: -28.07205, policy_entropy: -6.08092, alpha: 0.00525, time: 48.52579
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 187.65146, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 0.08219, qf2_loss: 0.08095, policy_loss: -28.00072, policy_entropy: -6.12668, alpha: 0.00529, time: 48.64454
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 195.71218, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 0.07926, qf2_loss: 0.07968, policy_loss: -28.02992, policy_entropy: -6.01754, alpha: 0.00533, time: 48.58813
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 194.08311, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 0.08326, qf2_loss: 0.08273, policy_loss: -28.01384, policy_entropy: -5.94659, alpha: 0.00535, time: 48.55440
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 195.14914, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 0.09082, qf2_loss: 0.09004, policy_loss: -28.08196, policy_entropy: -5.92450, alpha: 0.00530, time: 48.53851
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 196.40637, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 0.08849, qf2_loss: 0.08916, policy_loss: -28.12749, policy_entropy: -5.76945, alpha: 0.00525, time: 48.79783
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 191.33589, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 0.08911, qf2_loss: 0.08847, policy_loss: -28.17255, policy_entropy: -5.99205, alpha: 0.00520, time: 48.93254
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 193.11669, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 0.07944, qf2_loss: 0.07926, policy_loss: -28.25093, policy_entropy: -5.96614, alpha: 0.00516, time: 48.77818
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 187.03802, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 0.07298, qf2_loss: 0.07267, policy_loss: -28.18651, policy_entropy: -6.02379, alpha: 0.00516, time: 48.66362
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 201.86495, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 0.08486, qf2_loss: 0.08385, policy_loss: -28.19921, policy_entropy: -5.99529, alpha: 0.00519, time: 48.60607
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 181.94046, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 0.07833, qf2_loss: 0.07811, policy_loss: -28.29561, policy_entropy: -6.06116, alpha: 0.00518, time: 48.55460
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 200.44971, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 0.08187, qf2_loss: 0.08236, policy_loss: -28.30926, policy_entropy: -6.07266, alpha: 0.00521, time: 48.66164
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 184.77694, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 0.08424, qf2_loss: 0.08399, policy_loss: -28.29662, policy_entropy: -6.11004, alpha: 0.00526, time: 48.61367
[CW] eval: return: 195.91348, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 190.19872, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 0.08613, qf2_loss: 0.08550, policy_loss: -28.29923, policy_entropy: -5.98428, alpha: 0.00528, time: 48.88311
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 193.13718, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 0.07230, qf2_loss: 0.07199, policy_loss: -28.27416, policy_entropy: -6.13796, alpha: 0.00531, time: 48.89301
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 206.92178, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 0.08485, qf2_loss: 0.08349, policy_loss: -28.28834, policy_entropy: -6.00875, alpha: 0.00532, time: 48.76233
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 200.61812, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 0.08047, qf2_loss: 0.08003, policy_loss: -28.38609, policy_entropy: -6.08635, alpha: 0.00538, time: 48.54015
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 197.79633, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 0.08164, qf2_loss: 0.08331, policy_loss: -28.38183, policy_entropy: -6.04190, alpha: 0.00539, time: 48.43837
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 203.05522, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 0.07612, qf2_loss: 0.07594, policy_loss: -28.49457, policy_entropy: -6.07201, alpha: 0.00540, time: 48.37513
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 173.78440, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 0.08526, qf2_loss: 0.08349, policy_loss: -28.51680, policy_entropy: -5.99501, alpha: 0.00543, time: 48.86090
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 190.81325, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 0.09076, qf2_loss: 0.09160, policy_loss: -28.48170, policy_entropy: -5.97855, alpha: 0.00543, time: 48.38649
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 184.92934, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 0.09027, qf2_loss: 0.08769, policy_loss: -28.53737, policy_entropy: -6.20139, alpha: 0.00544, time: 48.36303
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 196.26383, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 0.07869, qf2_loss: 0.07976, policy_loss: -28.57438, policy_entropy: -6.06753, alpha: 0.00553, time: 51.77034
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 195.74178, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 0.08301, qf2_loss: 0.08320, policy_loss: -28.48943, policy_entropy: -5.97839, alpha: 0.00553, time: 48.57703
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 197.29657, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 0.08508, qf2_loss: 0.08570, policy_loss: -28.62900, policy_entropy: -6.07730, alpha: 0.00552, time: 48.56097
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 206.09023, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 0.09429, qf2_loss: 0.09251, policy_loss: -28.56160, policy_entropy: -6.10084, alpha: 0.00561, time: 48.56464
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 200.12522, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 0.08564, qf2_loss: 0.08636, policy_loss: -28.59392, policy_entropy: -5.93481, alpha: 0.00560, time: 48.44880
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 204.16881, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 0.07919, qf2_loss: 0.07895, policy_loss: -28.68173, policy_entropy: -6.06444, alpha: 0.00560, time: 48.49691
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 205.00668, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 0.08505, qf2_loss: 0.08445, policy_loss: -28.62284, policy_entropy: -6.01505, alpha: 0.00562, time: 48.45523
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 176.98349, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 0.08881, qf2_loss: 0.08841, policy_loss: -28.65312, policy_entropy: -5.83639, alpha: 0.00559, time: 48.41262
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 203.91084, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 0.10382, qf2_loss: 0.10232, policy_loss: -28.71898, policy_entropy: -5.90034, alpha: 0.00554, time: 48.46470
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 191.22822, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 0.08966, qf2_loss: 0.09029, policy_loss: -28.83705, policy_entropy: -5.75202, alpha: 0.00545, time: 48.47144
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 186.24751, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 0.08212, qf2_loss: 0.08215, policy_loss: -28.81143, policy_entropy: -6.09593, alpha: 0.00541, time: 48.41730
[CW] eval: return: 195.34534, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 195.49712, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 0.08672, qf2_loss: 0.08807, policy_loss: -28.80819, policy_entropy: -6.10395, alpha: 0.00546, time: 48.58090
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 184.53389, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 0.08339, qf2_loss: 0.08279, policy_loss: -28.88948, policy_entropy: -6.08901, alpha: 0.00549, time: 48.52959
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 203.26728, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 0.09858, qf2_loss: 0.09657, policy_loss: -28.99050, policy_entropy: -6.01779, alpha: 0.00551, time: 48.80687
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 194.70003, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 0.08373, qf2_loss: 0.08335, policy_loss: -28.80920, policy_entropy: -6.15694, alpha: 0.00558, time: 48.64346
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 198.31330, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 0.09271, qf2_loss: 0.09134, policy_loss: -29.02497, policy_entropy: -5.95805, alpha: 0.00559, time: 48.72029
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 192.20490, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 0.08497, qf2_loss: 0.08600, policy_loss: -28.89174, policy_entropy: -6.03347, alpha: 0.00558, time: 48.74761
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 191.25420, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 0.08297, qf2_loss: 0.08333, policy_loss: -28.96972, policy_entropy: -6.08415, alpha: 0.00562, time: 48.89939
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 203.94041, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 0.08902, qf2_loss: 0.08833, policy_loss: -29.04001, policy_entropy: -6.17371, alpha: 0.00568, time: 48.96167
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 193.12334, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 0.09105, qf2_loss: 0.09040, policy_loss: -29.04846, policy_entropy: -6.01462, alpha: 0.00570, time: 48.94944
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 205.82900, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 0.08770, qf2_loss: 0.08728, policy_loss: -29.13341, policy_entropy: -5.95309, alpha: 0.00570, time: 48.90382
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 186.70816, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 0.09040, qf2_loss: 0.09080, policy_loss: -29.17937, policy_entropy: -5.89431, alpha: 0.00568, time: 49.08013
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 199.27318, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 0.10637, qf2_loss: 0.10654, policy_loss: -29.08462, policy_entropy: -5.90965, alpha: 0.00563, time: 48.85843
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 197.71714, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 0.08337, qf2_loss: 0.08303, policy_loss: -29.19317, policy_entropy: -5.96992, alpha: 0.00559, time: 48.44694
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 194.57721, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 0.08276, qf2_loss: 0.08177, policy_loss: -29.19952, policy_entropy: -6.01693, alpha: 0.00560, time: 48.50757
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 195.13911, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 0.08127, qf2_loss: 0.08090, policy_loss: -29.16282, policy_entropy: -5.99713, alpha: 0.00560, time: 48.46986
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 172.74849, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 0.09060, qf2_loss: 0.09036, policy_loss: -29.21714, policy_entropy: -5.96147, alpha: 0.00557, time: 48.82638
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 195.13088, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 0.09774, qf2_loss: 0.09684, policy_loss: -29.34053, policy_entropy: -5.86559, alpha: 0.00556, time: 48.55019
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 205.84552, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 0.08321, qf2_loss: 0.08221, policy_loss: -29.23390, policy_entropy: -5.95865, alpha: 0.00553, time: 48.51303
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 199.08962, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 0.07617, qf2_loss: 0.07625, policy_loss: -29.29393, policy_entropy: -5.72542, alpha: 0.00543, time: 48.93021
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 211.28298, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 0.07430, qf2_loss: 0.07434, policy_loss: -29.37640, policy_entropy: -5.70368, alpha: 0.00536, time: 49.12364
[CW] eval: return: 92.76970, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 33.78584, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 0.07423, qf2_loss: 0.07504, policy_loss: -29.33432, policy_entropy: -5.74469, alpha: 0.00523, time: 48.98197
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 211.44697, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 0.07796, qf2_loss: 0.07815, policy_loss: -29.31905, policy_entropy: -5.64384, alpha: 0.00511, time: 49.63407
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 207.11062, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 0.08082, qf2_loss: 0.08076, policy_loss: -29.27848, policy_entropy: -6.10715, alpha: 0.00505, time: 50.00379
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 224.78302, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 0.10642, qf2_loss: 0.10492, policy_loss: -29.41734, policy_entropy: -6.07219, alpha: 0.00510, time: 48.93731
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 184.63156, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 0.09072, qf2_loss: 0.09076, policy_loss: -29.45449, policy_entropy: -6.24599, alpha: 0.00515, time: 48.92474
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 212.61150, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 0.08406, qf2_loss: 0.08437, policy_loss: -29.39844, policy_entropy: -6.20458, alpha: 0.00524, time: 48.92260
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 209.65490, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 0.09146, qf2_loss: 0.09025, policy_loss: -29.45652, policy_entropy: -6.32510, alpha: 0.00531, time: 48.63533
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 214.06669, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 0.08277, qf2_loss: 0.08178, policy_loss: -29.49395, policy_entropy: -6.32578, alpha: 0.00544, time: 49.63357
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 199.79860, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 0.08718, qf2_loss: 0.08737, policy_loss: -29.59508, policy_entropy: -6.13923, alpha: 0.00557, time: 48.49515
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 215.16994, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 0.10313, qf2_loss: 0.10217, policy_loss: -29.62443, policy_entropy: -6.21323, alpha: 0.00562, time: 48.94794
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 200.25451, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 0.09277, qf2_loss: 0.09220, policy_loss: -29.58396, policy_entropy: -5.99471, alpha: 0.00568, time: 48.84082
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 212.12691, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 0.09173, qf2_loss: 0.09383, policy_loss: -29.61968, policy_entropy: -6.06056, alpha: 0.00569, time: 48.90321
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 219.35562, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 0.08604, qf2_loss: 0.08487, policy_loss: -29.60841, policy_entropy: -6.01979, alpha: 0.00571, time: 48.47385
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 212.88584, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 0.08882, qf2_loss: 0.08852, policy_loss: -29.63553, policy_entropy: -6.00174, alpha: 0.00571, time: 48.48896
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 202.93285, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 0.11359, qf2_loss: 0.11495, policy_loss: -29.69811, policy_entropy: -6.44058, alpha: 0.00579, time: 48.59401
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 200.45388, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 0.09519, qf2_loss: 0.09420, policy_loss: -29.66926, policy_entropy: -6.11896, alpha: 0.00593, time: 48.86967
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 190.51743, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 0.11088, qf2_loss: 0.10807, policy_loss: -29.86071, policy_entropy: -6.02427, alpha: 0.00597, time: 48.93775
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 198.98388, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 0.08537, qf2_loss: 0.08592, policy_loss: -29.84050, policy_entropy: -5.92857, alpha: 0.00597, time: 48.93737
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 215.49487, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 0.08618, qf2_loss: 0.08693, policy_loss: -29.81921, policy_entropy: -5.89858, alpha: 0.00592, time: 48.45544
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 204.99813, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 0.09260, qf2_loss: 0.09163, policy_loss: -29.81158, policy_entropy: -5.76589, alpha: 0.00587, time: 48.55395
[CW] eval: return: 206.38050, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 215.53793, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 0.09048, qf2_loss: 0.09018, policy_loss: -29.92484, policy_entropy: -5.92640, alpha: 0.00575, time: 48.60013
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 217.36561, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 0.08910, qf2_loss: 0.08972, policy_loss: -29.95235, policy_entropy: -5.96611, alpha: 0.00575, time: 48.54162
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 225.54206, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 0.09327, qf2_loss: 0.09249, policy_loss: -29.89815, policy_entropy: -6.20959, alpha: 0.00579, time: 48.64448
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 198.91371, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 0.09934, qf2_loss: 0.09949, policy_loss: -29.97775, policy_entropy: -5.96778, alpha: 0.00584, time: 48.78225
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 205.99473, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 0.09798, qf2_loss: 0.09691, policy_loss: -30.01457, policy_entropy: -5.95035, alpha: 0.00582, time: 49.08083
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 197.40782, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 0.08551, qf2_loss: 0.08628, policy_loss: -30.04192, policy_entropy: -5.94663, alpha: 0.00578, time: 48.57891
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 214.27858, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 0.08677, qf2_loss: 0.08620, policy_loss: -30.03475, policy_entropy: -5.88865, alpha: 0.00576, time: 48.52738
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 205.20769, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 0.08768, qf2_loss: 0.08767, policy_loss: -30.04941, policy_entropy: -6.07215, alpha: 0.00574, time: 48.51790
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 212.64337, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 0.09442, qf2_loss: 0.09427, policy_loss: -30.03552, policy_entropy: -6.11289, alpha: 0.00577, time: 48.49186
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 212.18202, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 0.09097, qf2_loss: 0.08997, policy_loss: -30.20451, policy_entropy: -5.78767, alpha: 0.00576, time: 48.51268
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 214.60830, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 0.10401, qf2_loss: 0.10401, policy_loss: -30.15496, policy_entropy: -5.90565, alpha: 0.00567, time: 48.50581
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 195.84073, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 0.09104, qf2_loss: 0.09213, policy_loss: -30.17751, policy_entropy: -5.95410, alpha: 0.00566, time: 48.57483
[CW] ---------------------------
[CW] ---- Iteration:   673 ----
[CW] collect: return: 208.05230, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 0.09906, qf2_loss: 0.09759, policy_loss: -30.09340, policy_entropy: -5.90859, alpha: 0.00562, time: 48.51199
[CW] ---------------------------
[CW] ---- Iteration:   674 ----
[CW] collect: return: 212.29535, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 0.10038, qf2_loss: 0.10147, policy_loss: -30.22487, policy_entropy: -5.90708, alpha: 0.00557, time: 48.47982
[CW] ---------------------------
[CW] ---- Iteration:   675 ----
[CW] collect: return: 223.19780, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 0.08551, qf2_loss: 0.08537, policy_loss: -30.27503, policy_entropy: -5.97822, alpha: 0.00553, time: 48.57613
[CW] ---------------------------
[CW] ---- Iteration:   676 ----
[CW] collect: return: 206.06058, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 0.09156, qf2_loss: 0.09023, policy_loss: -30.45352, policy_entropy: -5.99090, alpha: 0.00552, time: 48.49207
[CW] ---------------------------
[CW] ---- Iteration:   677 ----
[CW] collect: return: 212.46516, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 0.08340, qf2_loss: 0.08307, policy_loss: -30.34775, policy_entropy: -5.95014, alpha: 0.00552, time: 48.46774
[CW] ---------------------------
[CW] ---- Iteration:   678 ----
[CW] collect: return: 189.95317, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 0.08521, qf2_loss: 0.08560, policy_loss: -30.45121, policy_entropy: -6.12404, alpha: 0.00554, time: 48.49247
[CW] ---------------------------
[CW] ---- Iteration:   679 ----
[CW] collect: return: 219.32384, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 0.09698, qf2_loss: 0.09726, policy_loss: -30.39495, policy_entropy: -6.04711, alpha: 0.00558, time: 48.43188
[CW] ---------------------------
[CW] ---- Iteration:   680 ----
[CW] collect: return: 207.31106, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 0.09654, qf2_loss: 0.09582, policy_loss: -30.44969, policy_entropy: -6.04413, alpha: 0.00561, time: 48.43337
[CW] eval: return: 212.28894, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   681 ----
[CW] collect: return: 152.73771, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 0.09229, qf2_loss: 0.09163, policy_loss: -30.45701, policy_entropy: -6.05675, alpha: 0.00562, time: 48.83474
[CW] ---------------------------
[CW] ---- Iteration:   682 ----
[CW] collect: return: 223.46730, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 0.08804, qf2_loss: 0.08876, policy_loss: -30.51165, policy_entropy: -6.16637, alpha: 0.00568, time: 50.89581
[CW] ---------------------------
[CW] ---- Iteration:   683 ----
[CW] collect: return: 204.63250, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 0.09713, qf2_loss: 0.09701, policy_loss: -30.43546, policy_entropy: -6.22736, alpha: 0.00578, time: 48.85338
[CW] ---------------------------
[CW] ---- Iteration:   684 ----
[CW] collect: return: 225.13043, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 0.09787, qf2_loss: 0.09820, policy_loss: -30.58039, policy_entropy: -6.34551, alpha: 0.00592, time: 48.53023
[CW] ---------------------------
[CW] ---- Iteration:   685 ----
[CW] collect: return: 209.80288, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 0.10984, qf2_loss: 0.10970, policy_loss: -30.55363, policy_entropy: -5.96756, alpha: 0.00602, time: 48.65163
[CW] ---------------------------
[CW] ---- Iteration:   686 ----
[CW] collect: return: 217.92962, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 0.09652, qf2_loss: 0.09544, policy_loss: -30.51862, policy_entropy: -5.90452, alpha: 0.00600, time: 48.53634
[CW] ---------------------------
[CW] ---- Iteration:   687 ----
[CW] collect: return: 199.62788, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 0.10225, qf2_loss: 0.10257, policy_loss: -30.62491, policy_entropy: -6.04551, alpha: 0.00597, time: 48.51716
[CW] ---------------------------
[CW] ---- Iteration:   688 ----
[CW] collect: return: 224.88518, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 0.09979, qf2_loss: 0.09861, policy_loss: -30.63376, policy_entropy: -6.08803, alpha: 0.00600, time: 48.51670
[CW] ---------------------------
[CW] ---- Iteration:   689 ----
[CW] collect: return: 222.26061, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 0.09682, qf2_loss: 0.09670, policy_loss: -30.71729, policy_entropy: -6.00969, alpha: 0.00602, time: 48.48172
[CW] ---------------------------
[CW] ---- Iteration:   690 ----
[CW] collect: return: 204.67683, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 0.10526, qf2_loss: 0.10523, policy_loss: -30.74659, policy_entropy: -6.25833, alpha: 0.00610, time: 48.50484
[CW] ---------------------------
[CW] ---- Iteration:   691 ----
[CW] collect: return: 221.47105, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 0.09643, qf2_loss: 0.09645, policy_loss: -30.76031, policy_entropy: -6.03574, alpha: 0.00618, time: 48.57027
[CW] ---------------------------
[CW] ---- Iteration:   692 ----
[CW] collect: return: 220.98176, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 0.11053, qf2_loss: 0.11016, policy_loss: -30.83642, policy_entropy: -5.95617, alpha: 0.00618, time: 48.58047
[CW] ---------------------------
[CW] ---- Iteration:   693 ----
[CW] collect: return: 212.73872, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 0.09796, qf2_loss: 0.09839, policy_loss: -30.81248, policy_entropy: -6.18040, alpha: 0.00621, time: 48.73708
[CW] ---------------------------
[CW] ---- Iteration:   694 ----
[CW] collect: return: 220.62372, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 0.09895, qf2_loss: 0.09832, policy_loss: -30.89257, policy_entropy: -6.03667, alpha: 0.00628, time: 49.04475
[CW] ---------------------------
[CW] ---- Iteration:   695 ----
[CW] collect: return: 222.61464, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 0.10255, qf2_loss: 0.10283, policy_loss: -30.94173, policy_entropy: -5.92593, alpha: 0.00626, time: 48.55668
[CW] ---------------------------
[CW] ---- Iteration:   696 ----
[CW] collect: return: 211.91981, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 0.10236, qf2_loss: 0.10248, policy_loss: -30.94008, policy_entropy: -5.90756, alpha: 0.00622, time: 48.59253
[CW] ---------------------------
[CW] ---- Iteration:   697 ----
[CW] collect: return: 215.37753, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 0.10412, qf2_loss: 0.10311, policy_loss: -31.03850, policy_entropy: -5.92220, alpha: 0.00617, time: 48.79692
[CW] ---------------------------
[CW] ---- Iteration:   698 ----
[CW] collect: return: 227.75074, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 0.09772, qf2_loss: 0.09731, policy_loss: -30.93382, policy_entropy: -5.96781, alpha: 0.00617, time: 48.60353
[CW] ---------------------------
[CW] ---- Iteration:   699 ----
[CW] collect: return: 205.77832, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 0.09057, qf2_loss: 0.09011, policy_loss: -31.03323, policy_entropy: -5.72871, alpha: 0.00606, time: 48.60090
[CW] ---------------------------
[CW] ---- Iteration:   700 ----
[CW] collect: return: 226.46011, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 0.10012, qf2_loss: 0.10073, policy_loss: -31.05976, policy_entropy: -5.88126, alpha: 0.00596, time: 48.52811
[CW] eval: return: 215.16348, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   701 ----
[CW] collect: return: 227.60305, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 0.10654, qf2_loss: 0.10719, policy_loss: -31.08764, policy_entropy: -5.79272, alpha: 0.00589, time: 48.68502
[CW] ---------------------------
[CW] ---- Iteration:   702 ----
[CW] collect: return: 218.50474, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 0.14208, qf2_loss: 0.14199, policy_loss: -31.18290, policy_entropy: -6.03257, alpha: 0.00581, time: 48.59847
[CW] ---------------------------
[CW] ---- Iteration:   703 ----
[CW] collect: return: 228.62309, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 0.10181, qf2_loss: 0.09945, policy_loss: -31.10978, policy_entropy: -6.34479, alpha: 0.00592, time: 49.45609
[CW] ---------------------------
[CW] ---- Iteration:   704 ----
[CW] collect: return: 216.72986, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 0.09993, qf2_loss: 0.09997, policy_loss: -31.22285, policy_entropy: -6.13206, alpha: 0.00602, time: 48.54629
[CW] ---------------------------
[CW] ---- Iteration:   705 ----
[CW] collect: return: 227.70059, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 0.10498, qf2_loss: 0.10510, policy_loss: -31.21071, policy_entropy: -6.21729, alpha: 0.00612, time: 48.54856
[CW] ---------------------------
[CW] ---- Iteration:   706 ----
[CW] collect: return: 200.67726, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 0.10560, qf2_loss: 0.10723, policy_loss: -31.32682, policy_entropy: -5.96409, alpha: 0.00616, time: 48.63372
[CW] ---------------------------
[CW] ---- Iteration:   707 ----
[CW] collect: return: 209.66138, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 0.10822, qf2_loss: 0.10800, policy_loss: -31.37166, policy_entropy: -6.03886, alpha: 0.00619, time: 48.51557
[CW] ---------------------------
[CW] ---- Iteration:   708 ----
[CW] collect: return: 227.29498, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 0.10443, qf2_loss: 0.10496, policy_loss: -31.39691, policy_entropy: -5.93734, alpha: 0.00617, time: 48.57554
[CW] ---------------------------
[CW] ---- Iteration:   709 ----
[CW] collect: return: 220.27352, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 0.09925, qf2_loss: 0.09882, policy_loss: -31.33809, policy_entropy: -5.90664, alpha: 0.00613, time: 48.72181
[CW] ---------------------------
[CW] ---- Iteration:   710 ----
[CW] collect: return: 228.41189, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 0.09750, qf2_loss: 0.09732, policy_loss: -31.32457, policy_entropy: -5.94547, alpha: 0.00609, time: 48.59258
[CW] ---------------------------
[CW] ---- Iteration:   711 ----
[CW] collect: return: 225.29528, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 0.09796, qf2_loss: 0.09651, policy_loss: -31.37389, policy_entropy: -5.94018, alpha: 0.00606, time: 48.58801
[CW] ---------------------------
[CW] ---- Iteration:   712 ----
[CW] collect: return: 232.92052, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 0.10401, qf2_loss: 0.10360, policy_loss: -31.43794, policy_entropy: -6.00669, alpha: 0.00604, time: 48.82370
[CW] ---------------------------
[CW] ---- Iteration:   713 ----
[CW] collect: return: 219.92098, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 0.10465, qf2_loss: 0.10453, policy_loss: -31.61921, policy_entropy: -5.95279, alpha: 0.00602, time: 48.55757
[CW] ---------------------------
[CW] ---- Iteration:   714 ----
[CW] collect: return: 227.73028, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 0.10565, qf2_loss: 0.10548, policy_loss: -31.54826, policy_entropy: -6.10982, alpha: 0.00606, time: 48.56905
[CW] ---------------------------
[CW] ---- Iteration:   715 ----
[CW] collect: return: 210.92845, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 0.11765, qf2_loss: 0.11678, policy_loss: -31.55790, policy_entropy: -6.10708, alpha: 0.00610, time: 48.57358
[CW] ---------------------------
[CW] ---- Iteration:   716 ----
[CW] collect: return: 219.54024, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 0.10831, qf2_loss: 0.10802, policy_loss: -31.60366, policy_entropy: -5.90384, alpha: 0.00612, time: 50.63706
[CW] ---------------------------
[CW] ---- Iteration:   717 ----
[CW] collect: return: 223.41070, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 0.10415, qf2_loss: 0.10417, policy_loss: -31.57006, policy_entropy: -5.98367, alpha: 0.00606, time: 48.61783
[CW] ---------------------------
[CW] ---- Iteration:   718 ----
[CW] collect: return: 216.53329, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 0.09876, qf2_loss: 0.09956, policy_loss: -31.65294, policy_entropy: -6.01002, alpha: 0.00609, time: 48.53740
[CW] ---------------------------
[CW] ---- Iteration:   719 ----
[CW] collect: return: 226.24501, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 0.10113, qf2_loss: 0.10152, policy_loss: -31.74660, policy_entropy: -6.04949, alpha: 0.00609, time: 48.57062
[CW] ---------------------------
[CW] ---- Iteration:   720 ----
[CW] collect: return: 235.46279, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 0.11114, qf2_loss: 0.11050, policy_loss: -31.76846, policy_entropy: -6.05449, alpha: 0.00612, time: 48.56911
[CW] eval: return: 231.07403, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   721 ----
[CW] collect: return: 232.02740, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 0.10734, qf2_loss: 0.10853, policy_loss: -31.71170, policy_entropy: -6.05532, alpha: 0.00616, time: 48.60321
[CW] ---------------------------
[CW] ---- Iteration:   722 ----
[CW] collect: return: 218.03271, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 0.11334, qf2_loss: 0.11245, policy_loss: -31.76699, policy_entropy: -5.99461, alpha: 0.00616, time: 48.51831
[CW] ---------------------------
[CW] ---- Iteration:   723 ----
[CW] collect: return: 223.93657, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 0.11649, qf2_loss: 0.11432, policy_loss: -31.83660, policy_entropy: -5.93700, alpha: 0.00616, time: 48.48191
[CW] ---------------------------
[CW] ---- Iteration:   724 ----
[CW] collect: return: 217.26607, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 0.10064, qf2_loss: 0.10116, policy_loss: -31.74603, policy_entropy: -5.81973, alpha: 0.00609, time: 48.59967
[CW] ---------------------------
[CW] ---- Iteration:   725 ----
[CW] collect: return: 226.00470, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 0.09585, qf2_loss: 0.09607, policy_loss: -31.83010, policy_entropy: -5.87462, alpha: 0.00600, time: 48.49909
[CW] ---------------------------
[CW] ---- Iteration:   726 ----
[CW] collect: return: 219.29256, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 0.10344, qf2_loss: 0.10430, policy_loss: -31.92631, policy_entropy: -6.02992, alpha: 0.00597, time: 48.51088
[CW] ---------------------------
[CW] ---- Iteration:   727 ----
[CW] collect: return: 194.54199, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 0.10638, qf2_loss: 0.10589, policy_loss: -31.88578, policy_entropy: -5.99760, alpha: 0.00597, time: 48.62005
[CW] ---------------------------
[CW] ---- Iteration:   728 ----
[CW] collect: return: 218.43982, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 0.10309, qf2_loss: 0.10330, policy_loss: -31.88872, policy_entropy: -6.00742, alpha: 0.00599, time: 48.53237
[CW] ---------------------------
[CW] ---- Iteration:   729 ----
[CW] collect: return: 213.42667, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 0.11297, qf2_loss: 0.10949, policy_loss: -31.91671, policy_entropy: -5.92108, alpha: 0.00597, time: 48.57633
[CW] ---------------------------
[CW] ---- Iteration:   730 ----
[CW] collect: return: 225.39176, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 0.10735, qf2_loss: 0.10910, policy_loss: -31.97046, policy_entropy: -5.96872, alpha: 0.00593, time: 48.63270
[CW] ---------------------------
[CW] ---- Iteration:   731 ----
[CW] collect: return: 235.83788, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 0.09853, qf2_loss: 0.09808, policy_loss: -32.02546, policy_entropy: -6.21988, alpha: 0.00597, time: 48.58446
[CW] ---------------------------
[CW] ---- Iteration:   732 ----
[CW] collect: return: 222.67160, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 0.11203, qf2_loss: 0.11249, policy_loss: -32.13848, policy_entropy: -6.06521, alpha: 0.00606, time: 49.23081
[CW] ---------------------------
[CW] ---- Iteration:   733 ----
[CW] collect: return: 210.60350, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 0.11057, qf2_loss: 0.11163, policy_loss: -32.06429, policy_entropy: -6.05524, alpha: 0.00608, time: 48.61238
[CW] ---------------------------
[CW] ---- Iteration:   734 ----
[CW] collect: return: 213.52011, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 0.10253, qf2_loss: 0.10103, policy_loss: -32.07595, policy_entropy: -6.25332, alpha: 0.00617, time: 48.57798
[CW] ---------------------------
[CW] ---- Iteration:   735 ----
[CW] collect: return: 213.78848, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 0.10505, qf2_loss: 0.10558, policy_loss: -32.04434, policy_entropy: -6.00251, alpha: 0.00625, time: 48.62918
[CW] ---------------------------
[CW] ---- Iteration:   736 ----
[CW] collect: return: 217.55256, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 0.12641, qf2_loss: 0.12583, policy_loss: -32.12037, policy_entropy: -6.02070, alpha: 0.00628, time: 48.60246
[CW] ---------------------------
[CW] ---- Iteration:   737 ----
[CW] collect: return: 222.43546, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 0.10289, qf2_loss: 0.10151, policy_loss: -32.19512, policy_entropy: -5.98048, alpha: 0.00626, time: 48.63199
[CW] ---------------------------
[CW] ---- Iteration:   738 ----
[CW] collect: return: 218.91710, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 0.11524, qf2_loss: 0.11492, policy_loss: -32.31542, policy_entropy: -6.03702, alpha: 0.00626, time: 48.63409
[CW] ---------------------------
[CW] ---- Iteration:   739 ----
[CW] collect: return: 221.09567, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 0.11506, qf2_loss: 0.11535, policy_loss: -32.25893, policy_entropy: -6.04722, alpha: 0.00630, time: 48.56751
[CW] ---------------------------
[CW] ---- Iteration:   740 ----
[CW] collect: return: 233.03949, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 0.10295, qf2_loss: 0.10229, policy_loss: -32.35177, policy_entropy: -5.97709, alpha: 0.00629, time: 48.62696
[CW] eval: return: 233.36652, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   741 ----
[CW] collect: return: 234.06024, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 0.10888, qf2_loss: 0.10736, policy_loss: -32.40901, policy_entropy: -6.13861, alpha: 0.00634, time: 50.86950
[CW] ---------------------------
[CW] ---- Iteration:   742 ----
[CW] collect: return: 196.12339, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 0.10345, qf2_loss: 0.10375, policy_loss: -32.32611, policy_entropy: -6.05422, alpha: 0.00638, time: 48.62960
[CW] ---------------------------
[CW] ---- Iteration:   743 ----
[CW] collect: return: 212.24139, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 0.12826, qf2_loss: 0.13010, policy_loss: -32.43334, policy_entropy: -5.88673, alpha: 0.00639, time: 48.60405
[CW] ---------------------------
[CW] ---- Iteration:   744 ----
[CW] collect: return: 236.77655, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 0.10002, qf2_loss: 0.10186, policy_loss: -32.43746, policy_entropy: -5.99847, alpha: 0.00635, time: 48.58834
[CW] ---------------------------
[CW] ---- Iteration:   745 ----
[CW] collect: return: 247.30092, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 0.10366, qf2_loss: 0.10306, policy_loss: -32.42756, policy_entropy: -5.89791, alpha: 0.00632, time: 48.64407
[CW] ---------------------------
[CW] ---- Iteration:   746 ----
[CW] collect: return: 225.52955, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 0.10460, qf2_loss: 0.10520, policy_loss: -32.50380, policy_entropy: -6.05692, alpha: 0.00629, time: 48.62467
[CW] ---------------------------
[CW] ---- Iteration:   747 ----
[CW] collect: return: 248.01255, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 0.11412, qf2_loss: 0.11348, policy_loss: -32.54489, policy_entropy: -5.97220, alpha: 0.00630, time: 48.59891
[CW] ---------------------------
[CW] ---- Iteration:   748 ----
[CW] collect: return: 227.32269, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 0.10195, qf2_loss: 0.10226, policy_loss: -32.59352, policy_entropy: -6.04916, alpha: 0.00630, time: 48.62363
[CW] ---------------------------
[CW] ---- Iteration:   749 ----
[CW] collect: return: 221.80555, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 0.10917, qf2_loss: 0.10671, policy_loss: -32.62874, policy_entropy: -6.14906, alpha: 0.00638, time: 48.86991
[CW] ---------------------------
[CW] ---- Iteration:   750 ----
[CW] collect: return: 237.54924, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 0.11273, qf2_loss: 0.11547, policy_loss: -32.62662, policy_entropy: -6.02433, alpha: 0.00641, time: 48.63382
[CW] ---------------------------
[CW] ---- Iteration:   751 ----
[CW] collect: return: 225.89847, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 0.10806, qf2_loss: 0.10765, policy_loss: -32.76701, policy_entropy: -6.02041, alpha: 0.00644, time: 48.62483
[CW] ---------------------------
[CW] ---- Iteration:   752 ----
[CW] collect: return: 237.42496, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 0.11318, qf2_loss: 0.11397, policy_loss: -32.63196, policy_entropy: -5.89014, alpha: 0.00643, time: 48.54488
[CW] ---------------------------
[CW] ---- Iteration:   753 ----
[CW] collect: return: 221.76856, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 0.15450, qf2_loss: 0.15283, policy_loss: -32.75019, policy_entropy: -5.98553, alpha: 0.00636, time: 48.57307
[CW] ---------------------------
[CW] ---- Iteration:   754 ----
[CW] collect: return: 224.07589, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 0.12180, qf2_loss: 0.12262, policy_loss: -32.78238, policy_entropy: -6.18024, alpha: 0.00638, time: 48.51970
[CW] ---------------------------
[CW] ---- Iteration:   755 ----
[CW] collect: return: 225.87764, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 0.11705, qf2_loss: 0.11788, policy_loss: -32.85045, policy_entropy: -6.13588, alpha: 0.00652, time: 48.54842
[CW] ---------------------------
[CW] ---- Iteration:   756 ----
[CW] collect: return: 242.87792, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 0.10014, qf2_loss: 0.09949, policy_loss: -32.81898, policy_entropy: -5.95967, alpha: 0.00655, time: 49.19773
[CW] ---------------------------
[CW] ---- Iteration:   757 ----
[CW] collect: return: 228.83033, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 0.11180, qf2_loss: 0.11291, policy_loss: -32.86184, policy_entropy: -6.03964, alpha: 0.00653, time: 48.89926
[CW] ---------------------------
[CW] ---- Iteration:   758 ----
[CW] collect: return: 230.13343, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 0.10957, qf2_loss: 0.10941, policy_loss: -32.92357, policy_entropy: -6.01999, alpha: 0.00656, time: 48.82653
[CW] ---------------------------
[CW] ---- Iteration:   759 ----
[CW] collect: return: 207.69365, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 0.10675, qf2_loss: 0.10691, policy_loss: -33.01550, policy_entropy: -5.96263, alpha: 0.00656, time: 48.58436
[CW] ---------------------------
[CW] ---- Iteration:   760 ----
[CW] collect: return: 230.97224, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 0.11129, qf2_loss: 0.11148, policy_loss: -32.92018, policy_entropy: -6.11384, alpha: 0.00658, time: 48.59222
[CW] eval: return: 234.29401, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   761 ----
[CW] collect: return: 228.03979, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 0.10781, qf2_loss: 0.10838, policy_loss: -33.05580, policy_entropy: -6.00872, alpha: 0.00663, time: 48.61181
[CW] ---------------------------
[CW] ---- Iteration:   762 ----
[CW] collect: return: 224.92226, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 0.11380, qf2_loss: 0.11354, policy_loss: -32.87811, policy_entropy: -5.99442, alpha: 0.00663, time: 48.47316
[CW] ---------------------------
[CW] ---- Iteration:   763 ----
[CW] collect: return: 232.87079, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 0.10514, qf2_loss: 0.10541, policy_loss: -33.15161, policy_entropy: -6.01747, alpha: 0.00663, time: 48.57072
[CW] ---------------------------
[CW] ---- Iteration:   764 ----
[CW] collect: return: 230.73539, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 0.11729, qf2_loss: 0.11509, policy_loss: -33.00263, policy_entropy: -5.80387, alpha: 0.00660, time: 48.60942
[CW] ---------------------------
[CW] ---- Iteration:   765 ----
[CW] collect: return: 228.66450, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 0.10447, qf2_loss: 0.10464, policy_loss: -33.12953, policy_entropy: -5.89207, alpha: 0.00647, time: 48.55826
[CW] ---------------------------
[CW] ---- Iteration:   766 ----
[CW] collect: return: 246.24578, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 0.10332, qf2_loss: 0.10512, policy_loss: -33.11801, policy_entropy: -5.97987, alpha: 0.00643, time: 48.98039
[CW] ---------------------------
[CW] ---- Iteration:   767 ----
[CW] collect: return: 238.65642, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 0.11210, qf2_loss: 0.11300, policy_loss: -33.19588, policy_entropy: -5.99416, alpha: 0.00644, time: 48.85538
[CW] ---------------------------
[CW] ---- Iteration:   768 ----
[CW] collect: return: 234.56221, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 0.11065, qf2_loss: 0.10956, policy_loss: -33.29668, policy_entropy: -6.04538, alpha: 0.00644, time: 48.94050
[CW] ---------------------------
[CW] ---- Iteration:   769 ----
[CW] collect: return: 231.05488, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 0.10851, qf2_loss: 0.10775, policy_loss: -33.22990, policy_entropy: -6.00498, alpha: 0.00646, time: 50.28315
[CW] ---------------------------
[CW] ---- Iteration:   770 ----
[CW] collect: return: 232.30012, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 0.12176, qf2_loss: 0.12203, policy_loss: -33.25961, policy_entropy: -5.94985, alpha: 0.00643, time: 48.91404
[CW] ---------------------------
[CW] ---- Iteration:   771 ----
[CW] collect: return: 245.53185, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 0.10559, qf2_loss: 0.10630, policy_loss: -33.28308, policy_entropy: -6.05378, alpha: 0.00645, time: 48.94241
[CW] ---------------------------
[CW] ---- Iteration:   772 ----
[CW] collect: return: 236.74824, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 0.10975, qf2_loss: 0.11029, policy_loss: -33.27719, policy_entropy: -6.01523, alpha: 0.00648, time: 48.76764
[CW] ---------------------------
[CW] ---- Iteration:   773 ----
[CW] collect: return: 235.59195, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 0.11146, qf2_loss: 0.10900, policy_loss: -33.35967, policy_entropy: -6.10465, alpha: 0.00649, time: 48.68053
[CW] ---------------------------
[CW] ---- Iteration:   774 ----
[CW] collect: return: 246.36405, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 0.11094, qf2_loss: 0.10961, policy_loss: -33.39482, policy_entropy: -5.97854, alpha: 0.00653, time: 48.69678
[CW] ---------------------------
[CW] ---- Iteration:   775 ----
[CW] collect: return: 247.87901, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 0.10373, qf2_loss: 0.10454, policy_loss: -33.42727, policy_entropy: -6.07229, alpha: 0.00657, time: 48.57007
[CW] ---------------------------
[CW] ---- Iteration:   776 ----
[CW] collect: return: 248.23994, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 0.13553, qf2_loss: 0.13683, policy_loss: -33.36773, policy_entropy: -6.02111, alpha: 0.00660, time: 48.51351
[CW] ---------------------------
[CW] ---- Iteration:   777 ----
[CW] collect: return: 242.17197, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 0.10669, qf2_loss: 0.10731, policy_loss: -33.46348, policy_entropy: -6.00576, alpha: 0.00661, time: 49.44373
[CW] ---------------------------
[CW] ---- Iteration:   778 ----
[CW] collect: return: 245.24771, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 0.09915, qf2_loss: 0.09913, policy_loss: -33.58779, policy_entropy: -6.01785, alpha: 0.00659, time: 49.03936
[CW] ---------------------------
[CW] ---- Iteration:   779 ----
[CW] collect: return: 238.51259, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 0.10591, qf2_loss: 0.10546, policy_loss: -33.45203, policy_entropy: -5.93211, alpha: 0.00660, time: 48.67284
[CW] ---------------------------
[CW] ---- Iteration:   780 ----
[CW] collect: return: 240.87407, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 0.09779, qf2_loss: 0.09752, policy_loss: -33.61415, policy_entropy: -5.86497, alpha: 0.00651, time: 48.68552
[CW] eval: return: 238.79826, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   781 ----
[CW] collect: return: 246.77813, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 0.10675, qf2_loss: 0.10769, policy_loss: -33.56296, policy_entropy: -5.95783, alpha: 0.00647, time: 48.72541
[CW] ---------------------------
[CW] ---- Iteration:   782 ----
[CW] collect: return: 228.14320, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 0.10802, qf2_loss: 0.10742, policy_loss: -33.66538, policy_entropy: -6.03191, alpha: 0.00644, time: 48.65261
[CW] ---------------------------
