{"collect/return": 228.14320484921336, "collect/steps": 1000.0, "collect/total_steps": 788000.0, "train/qf1_loss": 0.10802128251641989, "train/qf2_loss": 0.10742443434894085, "train/policy_loss": -33.665376358032226, "train/policy_entropy": -6.031905932426453, "train/alpha": 0.0064397802390158175, "train/time": 48.652605056762695, "eval/return": 238.7982566977502, "eval/steps": 1000.0, "_timestamp": 1678752662.4683917, "_runtime": 43076.36813759804, "_step": 782}