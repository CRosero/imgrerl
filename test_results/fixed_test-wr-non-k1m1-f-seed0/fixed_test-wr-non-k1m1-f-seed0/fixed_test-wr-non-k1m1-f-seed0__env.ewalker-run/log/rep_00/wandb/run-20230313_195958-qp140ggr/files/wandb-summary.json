{"collect/return": 113.10787746845745, "collect/steps": 1000.0, "collect/total_steps": 1006000.0, "train/qf1_loss": 0.10315043441951274, "train/qf2_loss": 0.10316285125911236, "train/policy_loss": -20.395776634216308, "train/policy_entropy": -6.085317993164063, "train/alpha": 0.00722851695958525, "train/time": 33.34436631202698, "eval/return": 111.73064930404071, "eval/steps": 1000.0, "_timestamp": 1678771486.4878676, "_runtime": 37487.74287366867, "_step": 1000, "_wandb": {"runtime": 37487}}