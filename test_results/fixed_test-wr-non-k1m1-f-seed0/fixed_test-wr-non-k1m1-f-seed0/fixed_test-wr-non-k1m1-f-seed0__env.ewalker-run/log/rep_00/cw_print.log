[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 24.90927, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.79321, qf2_loss: 1.80300, policy_loss: -7.78228, policy_entropy: 4.09787, alpha: 0.98504, time: 36.63320
[CW] eval: return: 25.11618, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 23.46926, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.08478, qf2_loss: 0.08564, policy_loss: -8.48256, policy_entropy: 4.10089, alpha: 0.95626, time: 31.97710
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 25.18213, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.07410, qf2_loss: 0.07472, policy_loss: -9.16259, policy_entropy: 4.10155, alpha: 0.92871, time: 32.85136
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 23.07016, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.06563, qf2_loss: 0.06578, policy_loss: -10.06681, policy_entropy: 4.10099, alpha: 0.90231, time: 32.64875
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 24.75301, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06172, qf2_loss: 0.06170, policy_loss: -11.09334, policy_entropy: 4.10116, alpha: 0.87698, time: 32.68171
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 27.03248, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.05951, qf2_loss: 0.05958, policy_loss: -12.20278, policy_entropy: 4.10135, alpha: 0.85267, time: 32.86089
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 24.43282, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.07608, qf2_loss: 0.07748, policy_loss: -13.35992, policy_entropy: 4.10225, alpha: 0.82930, time: 32.65897
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 28.15123, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.06024, qf2_loss: 0.06069, policy_loss: -14.56200, policy_entropy: 4.10074, alpha: 0.80683, time: 32.87185
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 23.25894, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.07720, qf2_loss: 0.07804, policy_loss: -15.77339, policy_entropy: 4.10215, alpha: 0.78519, time: 32.31489
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 29.04985, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.07356, qf2_loss: 0.07413, policy_loss: -16.98152, policy_entropy: 4.10091, alpha: 0.76435, time: 32.91476
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 23.95956, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.07426, qf2_loss: 0.07481, policy_loss: -18.17742, policy_entropy: 4.10072, alpha: 0.74426, time: 32.58797
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 25.54641, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.07907, qf2_loss: 0.07974, policy_loss: -19.35278, policy_entropy: 4.10080, alpha: 0.72488, time: 32.68223
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 24.76555, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.07454, qf2_loss: 0.07517, policy_loss: -20.50234, policy_entropy: 4.10113, alpha: 0.70617, time: 32.77281
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 25.20231, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.07425, qf2_loss: 0.07486, policy_loss: -21.62464, policy_entropy: 4.10082, alpha: 0.68809, time: 32.96258
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 24.84425, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.08864, qf2_loss: 0.08963, policy_loss: -22.72161, policy_entropy: 4.10171, alpha: 0.67062, time: 32.57890
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 25.02451, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.05917, qf2_loss: 0.05960, policy_loss: -23.79405, policy_entropy: 4.10241, alpha: 0.65372, time: 33.05588
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 25.58289, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.07483, qf2_loss: 0.07548, policy_loss: -24.82364, policy_entropy: 4.10108, alpha: 0.63736, time: 32.26202
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 28.77673, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.07070, qf2_loss: 0.07131, policy_loss: -25.84292, policy_entropy: 4.10216, alpha: 0.62153, time: 32.26949
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 20.95978, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.07673, qf2_loss: 0.07750, policy_loss: -26.82453, policy_entropy: 4.10138, alpha: 0.60619, time: 32.19723
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 27.00506, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.07214, qf2_loss: 0.07288, policy_loss: -27.77945, policy_entropy: 4.10222, alpha: 0.59132, time: 32.46669
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 25.07264, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.06577, qf2_loss: 0.06634, policy_loss: -28.70490, policy_entropy: 4.10138, alpha: 0.57690, time: 33.08532
[CW] eval: return: 25.74697, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 26.38303, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.07922, qf2_loss: 0.07995, policy_loss: -29.60833, policy_entropy: 4.10175, alpha: 0.56291, time: 32.83063
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 26.44043, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.08412, qf2_loss: 0.08508, policy_loss: -30.47970, policy_entropy: 4.10144, alpha: 0.54933, time: 32.40709
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 24.77104, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.06090, qf2_loss: 0.06138, policy_loss: -31.32491, policy_entropy: 4.10056, alpha: 0.53615, time: 32.53305
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 25.00747, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.07860, qf2_loss: 0.07942, policy_loss: -32.14956, policy_entropy: 4.10268, alpha: 0.52334, time: 32.44318
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 24.38730, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.05891, qf2_loss: 0.05942, policy_loss: -32.94330, policy_entropy: 4.10238, alpha: 0.51090, time: 32.30432
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 24.09800, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.06730, qf2_loss: 0.06794, policy_loss: -33.71668, policy_entropy: 4.10156, alpha: 0.49881, time: 32.70265
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 31.81355, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.08562, qf2_loss: 0.08666, policy_loss: -34.46464, policy_entropy: 4.10180, alpha: 0.48705, time: 32.69192
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 24.21440, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.06694, qf2_loss: 0.06765, policy_loss: -35.18963, policy_entropy: 4.10230, alpha: 0.47561, time: 32.50295
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 26.88415, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.07235, qf2_loss: 0.07281, policy_loss: -35.88670, policy_entropy: 4.10122, alpha: 0.46448, time: 32.41250
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 25.20616, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.08045, qf2_loss: 0.08147, policy_loss: -36.56872, policy_entropy: 4.10154, alpha: 0.45365, time: 32.67002
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 22.48882, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.07475, qf2_loss: 0.07557, policy_loss: -37.22527, policy_entropy: 4.10157, alpha: 0.44310, time: 32.70712
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 22.77847, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.02896, qf2_loss: 0.02907, policy_loss: -37.85907, policy_entropy: 4.10327, alpha: 0.43283, time: 32.44166
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 24.64977, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.07738, qf2_loss: 0.07845, policy_loss: -38.47299, policy_entropy: 4.10258, alpha: 0.42283, time: 32.73787
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 25.50343, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.07566, qf2_loss: 0.07594, policy_loss: -39.06050, policy_entropy: 4.10134, alpha: 0.41309, time: 32.73963
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 27.96776, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.07283, qf2_loss: 0.07389, policy_loss: -39.64285, policy_entropy: 4.10135, alpha: 0.40359, time: 32.61680
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 26.40568, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.05834, qf2_loss: 0.05884, policy_loss: -40.19711, policy_entropy: 4.10058, alpha: 0.39434, time: 32.56797
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 28.54517, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.06584, qf2_loss: 0.06657, policy_loss: -40.73481, policy_entropy: 4.10155, alpha: 0.38532, time: 32.33653
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 21.28937, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.06109, qf2_loss: 0.06178, policy_loss: -41.25582, policy_entropy: 4.10057, alpha: 0.37652, time: 32.56029
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 22.03923, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.08421, qf2_loss: 0.08512, policy_loss: -41.75172, policy_entropy: 4.10095, alpha: 0.36795, time: 32.90874
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 21.51746, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.05133, qf2_loss: 0.05182, policy_loss: -42.23025, policy_entropy: 4.10093, alpha: 0.35958, time: 33.03327
[CW] eval: return: 24.38374, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 21.82412, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.05819, qf2_loss: 0.05862, policy_loss: -42.69945, policy_entropy: 4.10105, alpha: 0.35142, time: 32.92789
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 29.27268, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.03604, qf2_loss: 0.03640, policy_loss: -43.14759, policy_entropy: 4.10194, alpha: 0.34346, time: 32.94500
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 26.74792, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.10720, qf2_loss: 0.10854, policy_loss: -43.57382, policy_entropy: 4.10223, alpha: 0.33569, time: 33.19399
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 25.85580, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.02334, qf2_loss: 0.02346, policy_loss: -43.99407, policy_entropy: 4.10140, alpha: 0.32811, time: 33.03038
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 22.74397, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.07329, qf2_loss: 0.07398, policy_loss: -44.38939, policy_entropy: 4.10106, alpha: 0.32070, time: 33.10049
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 24.57412, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.07313, qf2_loss: 0.07381, policy_loss: -44.77291, policy_entropy: 4.10047, alpha: 0.31348, time: 33.07151
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 25.46481, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.05714, qf2_loss: 0.05749, policy_loss: -45.14774, policy_entropy: 4.10155, alpha: 0.30643, time: 33.21210
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 23.83370, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.04827, qf2_loss: 0.04901, policy_loss: -45.49712, policy_entropy: 4.10115, alpha: 0.29954, time: 33.12859
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 28.86905, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.07938, qf2_loss: 0.08008, policy_loss: -45.84493, policy_entropy: 4.10209, alpha: 0.29282, time: 33.05857
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 23.49969, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.03763, qf2_loss: 0.03793, policy_loss: -46.16785, policy_entropy: 4.10122, alpha: 0.28625, time: 33.20506
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 24.32561, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.07394, qf2_loss: 0.07455, policy_loss: -46.48302, policy_entropy: 4.10173, alpha: 0.27983, time: 33.18431
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 25.22480, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.04373, qf2_loss: 0.04409, policy_loss: -46.78523, policy_entropy: 4.10158, alpha: 0.27357, time: 33.08917
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 23.45737, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.06770, qf2_loss: 0.06835, policy_loss: -47.07560, policy_entropy: 4.10082, alpha: 0.26745, time: 33.23020
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 25.78004, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.06927, qf2_loss: 0.06999, policy_loss: -47.34920, policy_entropy: 4.10115, alpha: 0.26147, time: 33.25832
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 30.28945, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.06133, qf2_loss: 0.06179, policy_loss: -47.60633, policy_entropy: 4.10088, alpha: 0.25563, time: 33.06656
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 28.18071, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.03550, qf2_loss: 0.03589, policy_loss: -47.85665, policy_entropy: 4.10126, alpha: 0.24993, time: 33.05938
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 23.21649, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.05341, qf2_loss: 0.05377, policy_loss: -48.10206, policy_entropy: 4.10129, alpha: 0.24435, time: 33.48018
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 27.18276, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.05681, qf2_loss: 0.05722, policy_loss: -48.32920, policy_entropy: 4.10062, alpha: 0.23891, time: 33.06629
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 28.04549, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.05987, qf2_loss: 0.06041, policy_loss: -48.54746, policy_entropy: 4.10130, alpha: 0.23358, time: 32.84286
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 26.27094, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.06881, qf2_loss: 0.06943, policy_loss: -48.75490, policy_entropy: 4.10077, alpha: 0.22838, time: 33.32486
[CW] eval: return: 24.48348, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 26.47902, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.02931, qf2_loss: 0.02947, policy_loss: -48.95476, policy_entropy: 4.10039, alpha: 0.22330, time: 32.96011
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 24.03294, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.05455, qf2_loss: 0.05513, policy_loss: -49.13130, policy_entropy: 4.10041, alpha: 0.21833, time: 33.04382
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 23.44121, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.05959, qf2_loss: 0.06017, policy_loss: -49.31282, policy_entropy: 4.10176, alpha: 0.21348, time: 32.92181
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 23.11017, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.07223, qf2_loss: 0.07286, policy_loss: -49.47919, policy_entropy: 4.10190, alpha: 0.20873, time: 32.88392
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 24.97539, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.02465, qf2_loss: 0.02477, policy_loss: -49.63458, policy_entropy: 4.10247, alpha: 0.20409, time: 33.00567
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 21.81396, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.05702, qf2_loss: 0.05782, policy_loss: -49.78292, policy_entropy: 4.10030, alpha: 0.19956, time: 32.88666
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 25.82895, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.06900, qf2_loss: 0.06970, policy_loss: -49.91516, policy_entropy: 4.10168, alpha: 0.19513, time: 32.99145
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 25.01246, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.02946, qf2_loss: 0.02964, policy_loss: -50.05225, policy_entropy: 4.10081, alpha: 0.19080, time: 32.90546
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 22.03561, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.04971, qf2_loss: 0.04989, policy_loss: -50.16479, policy_entropy: 4.10117, alpha: 0.18656, time: 33.11901
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 27.32949, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.05784, qf2_loss: 0.05835, policy_loss: -50.27938, policy_entropy: 4.10014, alpha: 0.18242, time: 32.95221
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 21.90843, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.04461, qf2_loss: 0.04485, policy_loss: -50.38510, policy_entropy: 4.10310, alpha: 0.17838, time: 32.88347
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 24.24473, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.05863, qf2_loss: 0.05921, policy_loss: -50.47932, policy_entropy: 4.10111, alpha: 0.17442, time: 32.97403
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 31.17478, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.05498, qf2_loss: 0.05553, policy_loss: -50.57288, policy_entropy: 4.10081, alpha: 0.17055, time: 33.09419
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 22.19604, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.02403, qf2_loss: 0.02414, policy_loss: -50.64692, policy_entropy: 4.10144, alpha: 0.16677, time: 32.69109
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 24.60447, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.08667, qf2_loss: 0.08717, policy_loss: -50.72258, policy_entropy: 4.10038, alpha: 0.16308, time: 32.93020
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 21.39475, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.01453, qf2_loss: 0.01457, policy_loss: -50.78288, policy_entropy: 4.10086, alpha: 0.15946, time: 33.11244
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 21.93396, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.04517, qf2_loss: 0.04559, policy_loss: -50.84130, policy_entropy: 4.09985, alpha: 0.15593, time: 32.76971
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 30.18429, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.04109, qf2_loss: 0.04138, policy_loss: -50.90031, policy_entropy: 4.10126, alpha: 0.15248, time: 32.87826
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 24.23019, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.06381, qf2_loss: 0.06443, policy_loss: -50.94259, policy_entropy: 4.10167, alpha: 0.14910, time: 33.21029
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 22.12531, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.03963, qf2_loss: 0.03980, policy_loss: -50.98228, policy_entropy: 4.10107, alpha: 0.14580, time: 32.79941
[CW] eval: return: 24.38102, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 24.83627, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.05628, qf2_loss: 0.05646, policy_loss: -51.01163, policy_entropy: 4.10124, alpha: 0.14257, time: 32.98958
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 22.63842, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.03205, qf2_loss: 0.03238, policy_loss: -51.04156, policy_entropy: 4.10114, alpha: 0.13941, time: 33.13295
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 21.61459, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.04896, qf2_loss: 0.04922, policy_loss: -51.05909, policy_entropy: 4.09941, alpha: 0.13633, time: 32.85216
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 24.45591, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.04553, qf2_loss: 0.04587, policy_loss: -51.08267, policy_entropy: 4.10252, alpha: 0.13331, time: 32.91238
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 24.16397, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.04746, qf2_loss: 0.04772, policy_loss: -51.08839, policy_entropy: 4.10100, alpha: 0.13036, time: 33.30330
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 28.16882, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.05600, qf2_loss: 0.05630, policy_loss: -51.09419, policy_entropy: 4.10090, alpha: 0.12747, time: 32.75348
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 26.99679, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.02512, qf2_loss: 0.02536, policy_loss: -51.08791, policy_entropy: 4.10067, alpha: 0.12465, time: 32.88183
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 27.52042, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.03816, qf2_loss: 0.03841, policy_loss: -51.07717, policy_entropy: 4.10040, alpha: 0.12189, time: 32.63977
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 22.99793, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.04446, qf2_loss: 0.04460, policy_loss: -51.07192, policy_entropy: 4.10094, alpha: 0.11919, time: 32.97202
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 26.12143, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.03417, qf2_loss: 0.03424, policy_loss: -51.05712, policy_entropy: 4.10067, alpha: 0.11656, time: 33.06491
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 25.21339, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.05686, qf2_loss: 0.05736, policy_loss: -51.02834, policy_entropy: 4.10012, alpha: 0.11398, time: 32.97217
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 24.39790, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.04365, qf2_loss: 0.04387, policy_loss: -51.00925, policy_entropy: 4.10033, alpha: 0.11146, time: 32.83189
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 22.12060, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.04318, qf2_loss: 0.04338, policy_loss: -50.97869, policy_entropy: 4.10084, alpha: 0.10899, time: 32.69617
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 22.23741, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.03421, qf2_loss: 0.03445, policy_loss: -50.94290, policy_entropy: 4.10117, alpha: 0.10658, time: 32.90089
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 25.64229, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.05733, qf2_loss: 0.05738, policy_loss: -50.90802, policy_entropy: 4.10107, alpha: 0.10422, time: 32.53307
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 21.06991, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.01740, qf2_loss: 0.01738, policy_loss: -50.85652, policy_entropy: 4.10100, alpha: 0.10192, time: 32.81430
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 30.37934, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.04165, qf2_loss: 0.04179, policy_loss: -50.81287, policy_entropy: 4.10096, alpha: 0.09966, time: 32.51788
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 23.02779, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.08149, qf2_loss: 0.08175, policy_loss: -50.76604, policy_entropy: 4.10022, alpha: 0.09746, time: 32.51756
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 27.20138, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.02872, qf2_loss: 0.03090, policy_loss: -50.69946, policy_entropy: 4.10089, alpha: 0.09531, time: 32.82061
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 29.61430, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.02424, qf2_loss: 0.02355, policy_loss: -50.64779, policy_entropy: 4.10103, alpha: 0.09320, time: 32.73213
[CW] eval: return: 24.55707, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 25.33251, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.02581, qf2_loss: 0.02564, policy_loss: -50.58108, policy_entropy: 4.10123, alpha: 0.09114, time: 33.05561
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 24.42181, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.03850, qf2_loss: 0.03849, policy_loss: -50.51558, policy_entropy: 4.09988, alpha: 0.08912, time: 33.09894
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 24.69414, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.04134, qf2_loss: 0.04130, policy_loss: -50.44705, policy_entropy: 4.10161, alpha: 0.08715, time: 33.22825
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 25.78508, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.03795, qf2_loss: 0.03793, policy_loss: -50.37425, policy_entropy: 4.10013, alpha: 0.08522, time: 32.92480
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 24.71415, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.03878, qf2_loss: 0.03867, policy_loss: -50.29710, policy_entropy: 4.09987, alpha: 0.08334, time: 32.66027
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 25.17708, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.04096, qf2_loss: 0.04147, policy_loss: -50.22351, policy_entropy: 4.10032, alpha: 0.08149, time: 33.07817
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 28.64313, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.02592, qf2_loss: 0.02555, policy_loss: -50.14233, policy_entropy: 4.10125, alpha: 0.07969, time: 32.94755
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 23.62201, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.05173, qf2_loss: 0.05193, policy_loss: -50.05937, policy_entropy: 4.09957, alpha: 0.07793, time: 33.00456
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 22.44201, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.02621, qf2_loss: 0.02622, policy_loss: -49.95902, policy_entropy: 4.10103, alpha: 0.07621, time: 33.18949
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 25.92152, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.04968, qf2_loss: 0.05002, policy_loss: -49.87607, policy_entropy: 4.09988, alpha: 0.07452, time: 32.67810
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 28.94817, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.02387, qf2_loss: 0.02400, policy_loss: -49.78427, policy_entropy: 4.10044, alpha: 0.07287, time: 33.39592
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 28.96778, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.04113, qf2_loss: 0.04287, policy_loss: -49.68894, policy_entropy: 4.09879, alpha: 0.07126, time: 32.91468
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 23.48601, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.02778, qf2_loss: 0.02693, policy_loss: -49.59078, policy_entropy: 4.10036, alpha: 0.06969, time: 32.69935
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 25.77621, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.04727, qf2_loss: 0.04714, policy_loss: -49.48821, policy_entropy: 4.09968, alpha: 0.06814, time: 33.05559
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 30.01406, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.02874, qf2_loss: 0.02792, policy_loss: -49.39189, policy_entropy: 4.09922, alpha: 0.06664, time: 32.87045
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 23.23698, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.04338, qf2_loss: 0.04404, policy_loss: -49.28551, policy_entropy: 4.09945, alpha: 0.06516, time: 33.11732
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 23.90032, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.02295, qf2_loss: 0.02301, policy_loss: -49.18050, policy_entropy: 4.10069, alpha: 0.06372, time: 32.77549
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 30.80257, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.04269, qf2_loss: 0.04315, policy_loss: -49.06663, policy_entropy: 4.09923, alpha: 0.06231, time: 33.23309
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 27.98545, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.02367, qf2_loss: 0.02358, policy_loss: -48.95869, policy_entropy: 4.10019, alpha: 0.06094, time: 32.61201
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 27.45986, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.04884, qf2_loss: 0.04891, policy_loss: -48.84882, policy_entropy: 4.10118, alpha: 0.05959, time: 33.08976
[CW] eval: return: 25.69514, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 25.39519, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.02118, qf2_loss: 0.02116, policy_loss: -48.73235, policy_entropy: 4.09980, alpha: 0.05827, time: 33.17208
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 28.68724, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.03282, qf2_loss: 0.03289, policy_loss: -48.62013, policy_entropy: 4.10053, alpha: 0.05698, time: 32.79068
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 23.91989, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.04150, qf2_loss: 0.04172, policy_loss: -48.49798, policy_entropy: 4.09977, alpha: 0.05572, time: 32.98976
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 23.74961, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.02566, qf2_loss: 0.02561, policy_loss: -48.37585, policy_entropy: 4.10009, alpha: 0.05449, time: 32.95985
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 26.05690, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.04685, qf2_loss: 0.04689, policy_loss: -48.25909, policy_entropy: 4.10072, alpha: 0.05328, time: 32.83535
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 22.11912, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.02485, qf2_loss: 0.02485, policy_loss: -48.11890, policy_entropy: 4.10040, alpha: 0.05211, time: 33.23194
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 25.45181, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.02347, qf2_loss: 0.02378, policy_loss: -47.99945, policy_entropy: 4.09902, alpha: 0.05095, time: 32.92320
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 23.77308, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.04119, qf2_loss: 0.04117, policy_loss: -47.88453, policy_entropy: 4.09875, alpha: 0.04983, time: 33.18331
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 24.88159, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.02605, qf2_loss: 0.02624, policy_loss: -47.75141, policy_entropy: 4.09895, alpha: 0.04872, time: 32.91439
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 26.91147, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.03192, qf2_loss: 0.03184, policy_loss: -47.61251, policy_entropy: 4.09912, alpha: 0.04765, time: 33.17747
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 24.24997, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.03258, qf2_loss: 0.03285, policy_loss: -47.48431, policy_entropy: 4.09960, alpha: 0.04659, time: 33.09695
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 23.70134, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.02932, qf2_loss: 0.02882, policy_loss: -47.35441, policy_entropy: 4.09805, alpha: 0.04556, time: 32.93184
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 24.67360, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.03219, qf2_loss: 0.03275, policy_loss: -47.21810, policy_entropy: 4.09753, alpha: 0.04456, time: 33.30573
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 25.25673, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.03029, qf2_loss: 0.03036, policy_loss: -47.08229, policy_entropy: 4.09928, alpha: 0.04357, time: 33.17906
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 31.88557, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.04706, qf2_loss: 0.04755, policy_loss: -46.95258, policy_entropy: 4.09890, alpha: 0.04261, time: 33.32848
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 27.15155, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.01253, qf2_loss: 0.01252, policy_loss: -46.80904, policy_entropy: 4.09728, alpha: 0.04167, time: 33.37705
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 24.84810, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.03567, qf2_loss: 0.03591, policy_loss: -46.67452, policy_entropy: 4.09808, alpha: 0.04074, time: 33.12195
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 22.86039, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.03105, qf2_loss: 0.03099, policy_loss: -46.53925, policy_entropy: 4.09796, alpha: 0.03984, time: 33.22865
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 23.71424, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.03527, qf2_loss: 0.03569, policy_loss: -46.38901, policy_entropy: 4.09860, alpha: 0.03896, time: 32.90550
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 28.60004, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.02297, qf2_loss: 0.02312, policy_loss: -46.25715, policy_entropy: 4.09629, alpha: 0.03810, time: 32.82090
[CW] eval: return: 26.53201, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 25.83857, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.03288, qf2_loss: 0.03315, policy_loss: -46.11246, policy_entropy: 4.09628, alpha: 0.03726, time: 33.25709
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 28.80683, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.02410, qf2_loss: 0.02397, policy_loss: -45.96569, policy_entropy: 4.09614, alpha: 0.03644, time: 32.85694
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 33.00540, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.04041, qf2_loss: 0.04052, policy_loss: -45.83238, policy_entropy: 4.09560, alpha: 0.03563, time: 33.06218
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 35.29353, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.01912, qf2_loss: 0.01920, policy_loss: -45.69853, policy_entropy: 4.09204, alpha: 0.03484, time: 33.38673
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 23.76445, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.04857, qf2_loss: 0.04919, policy_loss: -45.54000, policy_entropy: 4.09574, alpha: 0.03407, time: 32.73131
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 24.35898, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.01137, qf2_loss: 0.01126, policy_loss: -45.39667, policy_entropy: 4.09434, alpha: 0.03332, time: 33.13210
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 23.89626, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.03925, qf2_loss: 0.03973, policy_loss: -45.24674, policy_entropy: 4.09275, alpha: 0.03258, time: 32.89410
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 25.88770, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.01783, qf2_loss: 0.01773, policy_loss: -45.09633, policy_entropy: 4.09177, alpha: 0.03186, time: 33.19523
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 26.21488, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.04390, qf2_loss: 0.04454, policy_loss: -44.95587, policy_entropy: 4.09296, alpha: 0.03116, time: 33.04932
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 23.49912, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.01119, qf2_loss: 0.01102, policy_loss: -44.80161, policy_entropy: 4.09462, alpha: 0.03047, time: 33.20700
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 23.28225, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.02718, qf2_loss: 0.02731, policy_loss: -44.65275, policy_entropy: 4.09103, alpha: 0.02980, time: 33.31401
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 26.89971, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.05488, qf2_loss: 0.05598, policy_loss: -44.50596, policy_entropy: 4.09190, alpha: 0.02914, time: 33.13510
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 24.37429, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.02482, qf2_loss: 0.02459, policy_loss: -44.35956, policy_entropy: 4.09422, alpha: 0.02849, time: 33.35970
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 26.07547, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.01382, qf2_loss: 0.01389, policy_loss: -44.20221, policy_entropy: 4.08882, alpha: 0.02786, time: 33.14146
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 22.49404, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.02658, qf2_loss: 0.02693, policy_loss: -44.05578, policy_entropy: 4.08962, alpha: 0.02725, time: 33.30135
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 26.52808, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.02781, qf2_loss: 0.02822, policy_loss: -43.89795, policy_entropy: 4.08682, alpha: 0.02665, time: 33.09432
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 29.21693, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.04352, qf2_loss: 0.04352, policy_loss: -43.74222, policy_entropy: 4.08870, alpha: 0.02606, time: 33.06336
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 24.71008, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.01196, qf2_loss: 0.01193, policy_loss: -43.59164, policy_entropy: 4.09024, alpha: 0.02548, time: 33.27282
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 22.89866, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.02181, qf2_loss: 0.02216, policy_loss: -43.43330, policy_entropy: 4.09172, alpha: 0.02492, time: 33.00396
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 24.68349, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.02714, qf2_loss: 0.02729, policy_loss: -43.29203, policy_entropy: 4.08840, alpha: 0.02437, time: 32.98412
[CW] eval: return: 25.29941, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 28.73216, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.03406, qf2_loss: 0.03487, policy_loss: -43.13411, policy_entropy: 4.08939, alpha: 0.02383, time: 33.57478
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 23.21365, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.01853, qf2_loss: 0.01853, policy_loss: -42.98931, policy_entropy: 4.08640, alpha: 0.02330, time: 33.27673
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 29.10929, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.03864, qf2_loss: 0.03882, policy_loss: -42.82740, policy_entropy: 4.09192, alpha: 0.02279, time: 33.13841
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 22.27922, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.01941, qf2_loss: 0.01994, policy_loss: -42.67739, policy_entropy: 4.08086, alpha: 0.02228, time: 33.15102
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 24.04309, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.02085, qf2_loss: 0.02098, policy_loss: -42.53086, policy_entropy: 4.08496, alpha: 0.02179, time: 33.26788
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 23.20449, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.02837, qf2_loss: 0.02860, policy_loss: -42.37593, policy_entropy: 4.09055, alpha: 0.02131, time: 33.00465
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 25.96080, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.02467, qf2_loss: 0.02495, policy_loss: -42.20796, policy_entropy: 4.08698, alpha: 0.02084, time: 33.19112
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 33.25824, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.02916, qf2_loss: 0.02940, policy_loss: -42.06524, policy_entropy: 4.07999, alpha: 0.02038, time: 33.10944
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 23.28690, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.02641, qf2_loss: 0.02670, policy_loss: -41.91245, policy_entropy: 4.08118, alpha: 0.01993, time: 33.32524
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 26.77318, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.01404, qf2_loss: 0.01416, policy_loss: -41.75874, policy_entropy: 4.08432, alpha: 0.01949, time: 33.36568
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 26.88351, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.03133, qf2_loss: 0.03185, policy_loss: -41.60722, policy_entropy: 4.08398, alpha: 0.01906, time: 33.33426
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 30.57142, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.03494, qf2_loss: 0.03500, policy_loss: -41.45359, policy_entropy: 4.08264, alpha: 0.01864, time: 33.21232
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 29.24500, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.01745, qf2_loss: 0.01760, policy_loss: -41.29409, policy_entropy: 4.08422, alpha: 0.01822, time: 33.49054
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 28.89547, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.03014, qf2_loss: 0.03063, policy_loss: -41.13436, policy_entropy: 4.06137, alpha: 0.01782, time: 33.34985
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 26.51353, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.01230, qf2_loss: 0.01233, policy_loss: -40.98107, policy_entropy: 4.07720, alpha: 0.01743, time: 33.40811
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 25.76627, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.03801, qf2_loss: 0.03911, policy_loss: -40.83938, policy_entropy: 4.08322, alpha: 0.01704, time: 33.28191
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 22.40166, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.01896, qf2_loss: 0.01886, policy_loss: -40.68448, policy_entropy: 4.07358, alpha: 0.01667, time: 33.21038
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 24.99317, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.02164, qf2_loss: 0.02199, policy_loss: -40.52565, policy_entropy: 4.07968, alpha: 0.01630, time: 33.37802
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 23.08190, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.03220, qf2_loss: 0.03285, policy_loss: -40.38221, policy_entropy: 4.07910, alpha: 0.01594, time: 33.09871
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 30.36322, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.01283, qf2_loss: 0.01276, policy_loss: -40.22605, policy_entropy: 4.07800, alpha: 0.01559, time: 33.19077
[CW] eval: return: 25.29985, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 22.50102, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.02707, qf2_loss: 0.02761, policy_loss: -40.06538, policy_entropy: 4.06181, alpha: 0.01524, time: 33.56731
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 25.34314, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.01986, qf2_loss: 0.01955, policy_loss: -39.92852, policy_entropy: 4.06560, alpha: 0.01491, time: 33.09391
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 27.39135, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.03916, qf2_loss: 0.04041, policy_loss: -39.75966, policy_entropy: 4.07572, alpha: 0.01458, time: 33.36972
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 21.82843, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.01118, qf2_loss: 0.01127, policy_loss: -39.62054, policy_entropy: 4.07239, alpha: 0.01425, time: 33.35531
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 25.19846, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.03432, qf2_loss: 0.03547, policy_loss: -39.46317, policy_entropy: 4.06239, alpha: 0.01394, time: 33.23516
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 25.14740, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.02348, qf2_loss: 0.02291, policy_loss: -39.30926, policy_entropy: 4.04814, alpha: 0.01363, time: 33.07847
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 22.76707, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.01592, qf2_loss: 0.01634, policy_loss: -39.17235, policy_entropy: 4.05439, alpha: 0.01333, time: 33.18325
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 32.68452, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.03218, qf2_loss: 0.03270, policy_loss: -39.00911, policy_entropy: 4.04033, alpha: 0.01304, time: 32.99616
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 26.44795, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.01719, qf2_loss: 0.01756, policy_loss: -38.86211, policy_entropy: 4.05285, alpha: 0.01275, time: 32.86329
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 23.63464, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.01961, qf2_loss: 0.01960, policy_loss: -38.70698, policy_entropy: 4.03865, alpha: 0.01247, time: 33.00814
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 23.12453, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.02393, qf2_loss: 0.02452, policy_loss: -38.55903, policy_entropy: 4.05352, alpha: 0.01219, time: 32.91069
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 26.76556, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.03491, qf2_loss: 0.03543, policy_loss: -38.42144, policy_entropy: 4.01372, alpha: 0.01192, time: 32.94427
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 27.40018, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.01496, qf2_loss: 0.01517, policy_loss: -38.25851, policy_entropy: 4.02333, alpha: 0.01166, time: 32.95456
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 28.70294, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.01216, qf2_loss: 0.01241, policy_loss: -38.11402, policy_entropy: 4.04475, alpha: 0.01140, time: 33.02189
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 22.30000, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.03158, qf2_loss: 0.03232, policy_loss: -37.95410, policy_entropy: 4.03838, alpha: 0.01115, time: 33.23921
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 23.68963, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.01801, qf2_loss: 0.01804, policy_loss: -37.81856, policy_entropy: 4.00911, alpha: 0.01091, time: 32.98628
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 25.95880, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.02846, qf2_loss: 0.02874, policy_loss: -37.67002, policy_entropy: 4.02937, alpha: 0.01067, time: 32.65867
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 33.41558, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.01641, qf2_loss: 0.01670, policy_loss: -37.51686, policy_entropy: 4.00212, alpha: 0.01043, time: 32.78343
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 23.13188, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.02232, qf2_loss: 0.02254, policy_loss: -37.36845, policy_entropy: 4.00556, alpha: 0.01020, time: 33.49155
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 31.39622, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.02958, qf2_loss: 0.03033, policy_loss: -37.22756, policy_entropy: 4.04687, alpha: 0.00998, time: 32.86506
[CW] eval: return: 25.72091, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 23.82715, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.01329, qf2_loss: 0.01347, policy_loss: -37.08629, policy_entropy: 4.02111, alpha: 0.00976, time: 33.37360
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 31.28694, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.02491, qf2_loss: 0.02515, policy_loss: -36.93805, policy_entropy: 3.99653, alpha: 0.00954, time: 33.21397
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 27.04265, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.01950, qf2_loss: 0.01979, policy_loss: -36.78471, policy_entropy: 3.98797, alpha: 0.00933, time: 33.27635
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 24.36239, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.02526, qf2_loss: 0.02560, policy_loss: -36.64708, policy_entropy: 3.97439, alpha: 0.00913, time: 33.20715
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 32.33455, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.01826, qf2_loss: 0.01843, policy_loss: -36.49362, policy_entropy: 3.94357, alpha: 0.00893, time: 33.20435
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 27.11031, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.01893, qf2_loss: 0.01928, policy_loss: -36.35316, policy_entropy: 3.91655, alpha: 0.00873, time: 37.09459
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 27.60609, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.03918, qf2_loss: 0.04042, policy_loss: -36.20292, policy_entropy: 3.88943, alpha: 0.00854, time: 34.63380
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 23.73265, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.01326, qf2_loss: 0.01295, policy_loss: -36.06665, policy_entropy: 3.82586, alpha: 0.00835, time: 33.45042
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 27.62294, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.01571, qf2_loss: 0.01602, policy_loss: -35.91422, policy_entropy: 3.87714, alpha: 0.00817, time: 33.45935
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 35.82624, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.02959, qf2_loss: 0.03007, policy_loss: -35.78168, policy_entropy: 3.81752, alpha: 0.00799, time: 33.30814
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 25.96845, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.01741, qf2_loss: 0.01760, policy_loss: -35.63268, policy_entropy: 3.70884, alpha: 0.00782, time: 33.48005
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 23.52311, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.01524, qf2_loss: 0.01535, policy_loss: -35.49530, policy_entropy: 3.25779, alpha: 0.00765, time: 33.39246
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 25.52481, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.02075, qf2_loss: 0.02131, policy_loss: -35.35394, policy_entropy: 3.47330, alpha: 0.00750, time: 33.18148
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 25.12783, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 0.02113, qf2_loss: 0.02131, policy_loss: -35.20267, policy_entropy: 3.57959, alpha: 0.00734, time: 33.31265
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 25.76725, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.02386, qf2_loss: 0.02452, policy_loss: -35.06802, policy_entropy: 3.61639, alpha: 0.00718, time: 33.31952
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 30.03750, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.02358, qf2_loss: 0.02374, policy_loss: -34.93814, policy_entropy: 3.11239, alpha: 0.00703, time: 33.53171
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 25.81950, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.01703, qf2_loss: 0.01737, policy_loss: -34.79867, policy_entropy: 3.23070, alpha: 0.00688, time: 33.52712
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 25.88483, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.02725, qf2_loss: 0.02740, policy_loss: -34.65970, policy_entropy: 2.70870, alpha: 0.00674, time: 33.29474
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 25.95856, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.01621, qf2_loss: 0.01660, policy_loss: -34.52708, policy_entropy: 3.23627, alpha: 0.00660, time: 33.54060
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 13.52606, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.02120, qf2_loss: 0.02168, policy_loss: -34.37370, policy_entropy: 3.31091, alpha: 0.00646, time: 32.84613
[CW] eval: return: 23.62791, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 24.36656, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.01889, qf2_loss: 0.01905, policy_loss: -34.24952, policy_entropy: 2.96438, alpha: 0.00632, time: 33.27782
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 25.46660, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.02540, qf2_loss: 0.02590, policy_loss: -34.10041, policy_entropy: 3.06204, alpha: 0.00619, time: 33.07856
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 35.45442, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.02646, qf2_loss: 0.02706, policy_loss: -33.97537, policy_entropy: 3.10531, alpha: 0.00606, time: 32.77205
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 23.59293, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.01073, qf2_loss: 0.01072, policy_loss: -33.83711, policy_entropy: 3.48615, alpha: 0.00593, time: 33.21560
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 24.35907, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.01801, qf2_loss: 0.01836, policy_loss: -33.69936, policy_entropy: 3.17736, alpha: 0.00580, time: 33.14818
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 21.21040, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.02573, qf2_loss: 0.02640, policy_loss: -33.57963, policy_entropy: 1.46524, alpha: 0.00569, time: 32.79659
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 42.85065, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.02521, qf2_loss: 0.02558, policy_loss: -33.44985, policy_entropy: 2.70941, alpha: 0.00558, time: 32.88048
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 23.84586, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 0.02674, qf2_loss: 0.02688, policy_loss: -33.30679, policy_entropy: 3.53699, alpha: 0.00546, time: 32.75758
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 30.52982, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.01005, qf2_loss: 0.01009, policy_loss: -33.18417, policy_entropy: 1.84939, alpha: 0.00534, time: 32.81510
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 10.05081, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.01654, qf2_loss: 0.01710, policy_loss: -33.03894, policy_entropy: 1.54195, alpha: 0.00525, time: 32.51667
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 23.25366, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 0.02541, qf2_loss: 0.02565, policy_loss: -32.92489, policy_entropy: 1.35847, alpha: 0.00515, time: 32.87107
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 24.39142, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.02283, qf2_loss: 0.02332, policy_loss: -32.78868, policy_entropy: 1.93084, alpha: 0.00505, time: 32.95809
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 28.80179, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.02232, qf2_loss: 0.02252, policy_loss: -32.66321, policy_entropy: 1.44965, alpha: 0.00496, time: 32.82650
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 16.27957, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.02266, qf2_loss: 0.02312, policy_loss: -32.52263, policy_entropy: 1.96718, alpha: 0.00486, time: 32.91873
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 30.80004, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.01396, qf2_loss: 0.01399, policy_loss: -32.40411, policy_entropy: 1.32044, alpha: 0.00477, time: 32.75564
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 49.86871, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.02731, qf2_loss: 0.02752, policy_loss: -32.28362, policy_entropy: 0.76547, alpha: 0.00468, time: 32.94161
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 35.49295, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 0.01939, qf2_loss: 0.01985, policy_loss: -32.15673, policy_entropy: 0.74120, alpha: 0.00460, time: 32.99211
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 42.40605, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 0.01876, qf2_loss: 0.01892, policy_loss: -32.03933, policy_entropy: 0.17369, alpha: 0.00452, time: 32.77125
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 64.81792, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 0.02515, qf2_loss: 0.02539, policy_loss: -31.90526, policy_entropy: -0.43336, alpha: 0.00445, time: 32.65463
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 58.79419, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.02720, qf2_loss: 0.02754, policy_loss: -31.80921, policy_entropy: -0.65708, alpha: 0.00438, time: 32.63486
[CW] eval: return: 42.12152, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 28.10316, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 0.01932, qf2_loss: 0.02017, policy_loss: -31.67786, policy_entropy: -0.50824, alpha: 0.00431, time: 32.99431
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 63.14325, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 0.01837, qf2_loss: 0.01805, policy_loss: -31.54059, policy_entropy: -0.70306, alpha: 0.00425, time: 32.74002
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 24.59191, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 0.03302, qf2_loss: 0.03361, policy_loss: -31.44847, policy_entropy: -0.70251, alpha: 0.00418, time: 32.71338
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 65.26784, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 0.02282, qf2_loss: 0.02306, policy_loss: -31.32973, policy_entropy: -1.21883, alpha: 0.00412, time: 32.97596
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 29.97224, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 0.02551, qf2_loss: 0.02587, policy_loss: -31.21519, policy_entropy: -1.22228, alpha: 0.00405, time: 33.18315
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 82.87377, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 0.02053, qf2_loss: 0.02098, policy_loss: -31.10502, policy_entropy: -2.63152, alpha: 0.00400, time: 33.23546
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 23.32724, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 0.03992, qf2_loss: 0.04050, policy_loss: -31.00051, policy_entropy: -2.36420, alpha: 0.00396, time: 33.24839
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 45.91602, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 0.02152, qf2_loss: 0.02148, policy_loss: -30.89031, policy_entropy: -2.71251, alpha: 0.00391, time: 33.47213
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 50.77068, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 0.02345, qf2_loss: 0.02355, policy_loss: -30.79016, policy_entropy: -3.03330, alpha: 0.00387, time: 33.58447
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 42.48509, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 0.02348, qf2_loss: 0.02382, policy_loss: -30.69017, policy_entropy: -3.07147, alpha: 0.00383, time: 33.96760
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 58.41564, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 0.02587, qf2_loss: 0.02587, policy_loss: -30.58535, policy_entropy: -3.14115, alpha: 0.00379, time: 33.28714
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 48.35223, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 0.03677, qf2_loss: 0.03745, policy_loss: -30.49521, policy_entropy: -3.05110, alpha: 0.00375, time: 33.23931
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 45.26337, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 0.04077, qf2_loss: 0.04110, policy_loss: -30.39648, policy_entropy: -3.37021, alpha: 0.00371, time: 33.25801
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 44.69278, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 0.02632, qf2_loss: 0.02654, policy_loss: -30.32111, policy_entropy: -3.88223, alpha: 0.00367, time: 33.23762
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 44.84038, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 0.02857, qf2_loss: 0.02881, policy_loss: -30.23176, policy_entropy: -4.47363, alpha: 0.00364, time: 33.43997
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 44.54109, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 0.02887, qf2_loss: 0.02920, policy_loss: -30.14276, policy_entropy: -4.27073, alpha: 0.00362, time: 33.22175
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 45.58343, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 0.03357, qf2_loss: 0.03345, policy_loss: -30.04872, policy_entropy: -4.62192, alpha: 0.00359, time: 33.24399
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 43.39463, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 0.04187, qf2_loss: 0.04227, policy_loss: -29.95835, policy_entropy: -3.88092, alpha: 0.00356, time: 33.28805
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 12.88190, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 0.04168, qf2_loss: 0.04225, policy_loss: -29.86994, policy_entropy: -4.36080, alpha: 0.00352, time: 33.46190
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 37.78438, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 0.02913, qf2_loss: 0.02956, policy_loss: -29.82178, policy_entropy: -5.36954, alpha: 0.00350, time: 32.88844
[CW] eval: return: 39.98717, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 81.81467, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 0.04054, qf2_loss: 0.04153, policy_loss: -29.76822, policy_entropy: -6.25460, alpha: 0.00350, time: 33.61801
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 21.58462, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 0.04595, qf2_loss: 0.04701, policy_loss: -29.68731, policy_entropy: -4.70946, alpha: 0.00349, time: 33.77823
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 38.20615, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 0.04147, qf2_loss: 0.04215, policy_loss: -29.62621, policy_entropy: -5.99954, alpha: 0.00347, time: 33.50103
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 38.78924, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 0.05148, qf2_loss: 0.05233, policy_loss: -29.55521, policy_entropy: -5.39963, alpha: 0.00347, time: 33.64895
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 17.84999, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 0.03921, qf2_loss: 0.04058, policy_loss: -29.46938, policy_entropy: -5.00944, alpha: 0.00346, time: 33.45613
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 48.82166, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 0.04185, qf2_loss: 0.04286, policy_loss: -29.45419, policy_entropy: -5.26829, alpha: 0.00344, time: 33.75448
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 18.03509, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 0.04781, qf2_loss: 0.04858, policy_loss: -29.39162, policy_entropy: -4.67080, alpha: 0.00341, time: 33.43299
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 30.42108, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 0.04187, qf2_loss: 0.04295, policy_loss: -29.33416, policy_entropy: -5.23046, alpha: 0.00338, time: 33.65019
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 31.92562, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 0.07219, qf2_loss: 0.07334, policy_loss: -29.24935, policy_entropy: -4.51479, alpha: 0.00336, time: 33.54729
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 32.03699, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 0.03812, qf2_loss: 0.03885, policy_loss: -29.19716, policy_entropy: -4.72761, alpha: 0.00332, time: 33.42805
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 22.80922, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 0.04030, qf2_loss: 0.04080, policy_loss: -29.12625, policy_entropy: -4.63142, alpha: 0.00329, time: 33.22056
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 17.15236, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 0.03971, qf2_loss: 0.04066, policy_loss: -29.06299, policy_entropy: -4.54116, alpha: 0.00325, time: 33.60713
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 28.70909, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 0.04022, qf2_loss: 0.04093, policy_loss: -28.98974, policy_entropy: -4.63804, alpha: 0.00322, time: 33.76032
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 17.67327, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 0.04272, qf2_loss: 0.04250, policy_loss: -28.90824, policy_entropy: -4.65722, alpha: 0.00318, time: 33.34523
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 42.86302, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 0.04271, qf2_loss: 0.04375, policy_loss: -28.85543, policy_entropy: -4.78761, alpha: 0.00314, time: 33.35879
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 24.06719, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 0.04256, qf2_loss: 0.04270, policy_loss: -28.74634, policy_entropy: -4.77191, alpha: 0.00311, time: 33.42884
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 27.00962, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 0.03444, qf2_loss: 0.03491, policy_loss: -28.67936, policy_entropy: -4.90686, alpha: 0.00307, time: 33.60385
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 27.11588, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 0.03049, qf2_loss: 0.03067, policy_loss: -28.58480, policy_entropy: -4.86609, alpha: 0.00304, time: 33.26556
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 40.87214, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 0.03356, qf2_loss: 0.03386, policy_loss: -28.53165, policy_entropy: -5.10335, alpha: 0.00301, time: 33.41452
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 28.96115, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 0.03707, qf2_loss: 0.03761, policy_loss: -28.46482, policy_entropy: -5.44332, alpha: 0.00298, time: 32.95411
[CW] eval: return: 33.07484, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 27.89151, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 0.02682, qf2_loss: 0.02696, policy_loss: -28.43471, policy_entropy: -5.62288, alpha: 0.00297, time: 33.38650
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 43.33442, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 0.02298, qf2_loss: 0.02310, policy_loss: -28.31910, policy_entropy: -4.64091, alpha: 0.00294, time: 33.47330
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 27.31239, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 0.02514, qf2_loss: 0.02541, policy_loss: -28.26014, policy_entropy: -4.46297, alpha: 0.00289, time: 33.34435
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 37.73231, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 0.02887, qf2_loss: 0.02933, policy_loss: -28.16950, policy_entropy: -4.32428, alpha: 0.00285, time: 33.21277
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 35.36961, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 0.03754, qf2_loss: 0.03829, policy_loss: -28.08532, policy_entropy: -4.55506, alpha: 0.00278, time: 33.16514
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 27.67057, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 0.02411, qf2_loss: 0.02426, policy_loss: -28.02936, policy_entropy: -3.81622, alpha: 0.00273, time: 32.65533
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 31.52096, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 0.01879, qf2_loss: 0.01883, policy_loss: -27.91338, policy_entropy: -4.34775, alpha: 0.00267, time: 33.16598
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 26.91124, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 0.02297, qf2_loss: 0.02299, policy_loss: -27.85818, policy_entropy: -4.59911, alpha: 0.00262, time: 33.28101
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 24.52199, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 0.04864, qf2_loss: 0.04956, policy_loss: -27.79410, policy_entropy: -4.07823, alpha: 0.00257, time: 33.28819
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 46.28713, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 0.01709, qf2_loss: 0.01703, policy_loss: -27.69110, policy_entropy: -4.30510, alpha: 0.00252, time: 33.16269
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 27.87086, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 0.01529, qf2_loss: 0.01524, policy_loss: -27.61070, policy_entropy: -4.49057, alpha: 0.00247, time: 33.39000
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 27.28332, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 0.01615, qf2_loss: 0.01605, policy_loss: -27.50441, policy_entropy: -4.25748, alpha: 0.00243, time: 32.81156
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 76.43947, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 0.02687, qf2_loss: 0.02757, policy_loss: -27.43901, policy_entropy: -4.13405, alpha: 0.00237, time: 33.10503
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 37.14585, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 0.02791, qf2_loss: 0.02837, policy_loss: -27.36794, policy_entropy: -4.62979, alpha: 0.00232, time: 32.85409
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 28.24681, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 0.02157, qf2_loss: 0.02185, policy_loss: -27.26368, policy_entropy: -4.73926, alpha: 0.00229, time: 33.06842
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 24.34424, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 0.02396, qf2_loss: 0.02395, policy_loss: -27.20175, policy_entropy: -4.81158, alpha: 0.00225, time: 33.11425
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 26.65514, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 0.01829, qf2_loss: 0.01829, policy_loss: -27.10036, policy_entropy: -4.91275, alpha: 0.00222, time: 33.24204
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 49.08076, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 0.02196, qf2_loss: 0.02184, policy_loss: -27.01453, policy_entropy: -5.12275, alpha: 0.00220, time: 32.98276
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 32.14559, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 0.02356, qf2_loss: 0.02427, policy_loss: -26.90498, policy_entropy: -4.61586, alpha: 0.00216, time: 32.97231
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 28.60422, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 0.02208, qf2_loss: 0.02258, policy_loss: -26.83466, policy_entropy: -5.32446, alpha: 0.00213, time: 32.71929
[CW] eval: return: 30.22825, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 50.17036, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 0.02234, qf2_loss: 0.02230, policy_loss: -26.75971, policy_entropy: -5.39096, alpha: 0.00211, time: 33.03875
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 23.21411, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 0.01895, qf2_loss: 0.01916, policy_loss: -26.64583, policy_entropy: -5.40379, alpha: 0.00209, time: 32.85141
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 46.64185, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 0.02640, qf2_loss: 0.02678, policy_loss: -26.56729, policy_entropy: -6.86651, alpha: 0.00209, time: 33.18728
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 44.07580, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 0.02807, qf2_loss: 0.02884, policy_loss: -26.49854, policy_entropy: -6.71928, alpha: 0.00212, time: 32.82164
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 32.43803, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 0.01813, qf2_loss: 0.01822, policy_loss: -26.38536, policy_entropy: -5.64243, alpha: 0.00213, time: 33.32051
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 30.82835, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 0.01877, qf2_loss: 0.01883, policy_loss: -26.30147, policy_entropy: -6.08078, alpha: 0.00213, time: 33.02443
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 30.60105, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 0.02140, qf2_loss: 0.02213, policy_loss: -26.24663, policy_entropy: -5.77348, alpha: 0.00212, time: 33.05783
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 30.56176, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 0.02023, qf2_loss: 0.02015, policy_loss: -26.16115, policy_entropy: -6.50471, alpha: 0.00212, time: 33.20256
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 31.12075, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 0.02392, qf2_loss: 0.02439, policy_loss: -26.06661, policy_entropy: -6.80477, alpha: 0.00215, time: 33.29501
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 32.79203, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 0.03424, qf2_loss: 0.03508, policy_loss: -25.99032, policy_entropy: -5.20827, alpha: 0.00217, time: 34.72980
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 36.08964, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 0.01661, qf2_loss: 0.01678, policy_loss: -25.86831, policy_entropy: -5.49065, alpha: 0.00212, time: 33.80084
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 31.73787, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 0.01855, qf2_loss: 0.01861, policy_loss: -25.81111, policy_entropy: -5.55206, alpha: 0.00210, time: 34.92567
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 29.50397, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 0.01785, qf2_loss: 0.01816, policy_loss: -25.74279, policy_entropy: -6.99221, alpha: 0.00212, time: 33.22830
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 43.40788, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 0.01951, qf2_loss: 0.01999, policy_loss: -25.61632, policy_entropy: -5.83616, alpha: 0.00214, time: 33.32958
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 29.60187, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 0.01766, qf2_loss: 0.01793, policy_loss: -25.51933, policy_entropy: -5.91838, alpha: 0.00213, time: 33.07385
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 32.84326, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 0.02491, qf2_loss: 0.02506, policy_loss: -25.48514, policy_entropy: -5.38083, alpha: 0.00212, time: 32.81677
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 64.35364, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 0.02635, qf2_loss: 0.02721, policy_loss: -25.39812, policy_entropy: -5.52079, alpha: 0.00209, time: 32.81545
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 60.60087, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 0.02132, qf2_loss: 0.02139, policy_loss: -25.36260, policy_entropy: -5.75378, alpha: 0.00206, time: 33.47203
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 46.08933, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 0.04772, qf2_loss: 0.04926, policy_loss: -25.25160, policy_entropy: -6.84998, alpha: 0.00208, time: 32.99377
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 54.03255, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 0.01927, qf2_loss: 0.01924, policy_loss: -25.15174, policy_entropy: -7.21943, alpha: 0.00214, time: 32.80511
[CW] eval: return: 55.76152, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 30.70031, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 0.02057, qf2_loss: 0.02029, policy_loss: -25.10005, policy_entropy: -6.81798, alpha: 0.00219, time: 33.37657
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 46.45205, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 0.02150, qf2_loss: 0.02193, policy_loss: -25.02194, policy_entropy: -6.80342, alpha: 0.00224, time: 32.95840
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 44.27001, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 0.02869, qf2_loss: 0.02869, policy_loss: -24.95763, policy_entropy: -6.75924, alpha: 0.00228, time: 32.79963
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 45.05693, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 0.02636, qf2_loss: 0.02749, policy_loss: -24.93409, policy_entropy: -7.08018, alpha: 0.00235, time: 33.10987
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 49.84474, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 0.03200, qf2_loss: 0.03273, policy_loss: -24.85234, policy_entropy: -6.97276, alpha: 0.00242, time: 33.05990
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 3.94893, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 0.02975, qf2_loss: 0.02986, policy_loss: -24.78780, policy_entropy: -5.97987, alpha: 0.00245, time: 32.85074
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 49.88888, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 0.02560, qf2_loss: 0.02588, policy_loss: -24.70639, policy_entropy: -5.77709, alpha: 0.00245, time: 33.11435
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 92.01459, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 0.04150, qf2_loss: 0.04246, policy_loss: -24.66541, policy_entropy: -6.14169, alpha: 0.00244, time: 33.02826
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 46.23709, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 0.02608, qf2_loss: 0.02640, policy_loss: -24.58511, policy_entropy: -6.06118, alpha: 0.00246, time: 32.64289
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 49.88784, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 0.02820, qf2_loss: 0.02854, policy_loss: -24.53623, policy_entropy: -5.85992, alpha: 0.00245, time: 33.08007
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 87.54681, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 0.03023, qf2_loss: 0.03050, policy_loss: -24.41412, policy_entropy: -5.71186, alpha: 0.00244, time: 33.14485
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 78.48301, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 0.03538, qf2_loss: 0.03618, policy_loss: -24.39029, policy_entropy: -6.04834, alpha: 0.00242, time: 33.14884
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 89.47538, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 0.02911, qf2_loss: 0.02904, policy_loss: -24.34665, policy_entropy: -5.68624, alpha: 0.00241, time: 33.02643
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 59.86779, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 0.03208, qf2_loss: 0.03251, policy_loss: -24.31696, policy_entropy: -6.02628, alpha: 0.00240, time: 33.00463
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 84.85493, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 0.04259, qf2_loss: 0.04343, policy_loss: -24.21983, policy_entropy: -5.53190, alpha: 0.00238, time: 33.22557
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 73.63935, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 0.03140, qf2_loss: 0.03154, policy_loss: -24.13353, policy_entropy: -6.16354, alpha: 0.00236, time: 33.14653
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 85.80389, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 0.03072, qf2_loss: 0.03103, policy_loss: -24.10588, policy_entropy: -6.14838, alpha: 0.00238, time: 33.05886
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 85.55569, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 0.04682, qf2_loss: 0.04751, policy_loss: -24.07124, policy_entropy: -6.28788, alpha: 0.00239, time: 33.05180
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 96.63163, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 0.04027, qf2_loss: 0.04081, policy_loss: -23.97932, policy_entropy: -6.60854, alpha: 0.00243, time: 32.87728
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 89.15260, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 0.03760, qf2_loss: 0.03813, policy_loss: -23.93874, policy_entropy: -6.41848, alpha: 0.00250, time: 32.87819
[CW] eval: return: 67.40340, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 86.01088, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 0.03487, qf2_loss: 0.03506, policy_loss: -23.88150, policy_entropy: -5.99815, alpha: 0.00251, time: 33.31554
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 95.34552, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 0.03282, qf2_loss: 0.03298, policy_loss: -23.85667, policy_entropy: -6.23356, alpha: 0.00253, time: 32.77554
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 80.48207, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 0.07988, qf2_loss: 0.08158, policy_loss: -23.80736, policy_entropy: -5.96110, alpha: 0.00255, time: 33.06389
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 57.55835, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 0.10091, qf2_loss: 0.10248, policy_loss: -23.70739, policy_entropy: -5.47404, alpha: 0.00252, time: 32.95936
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 90.92556, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 0.03084, qf2_loss: 0.03091, policy_loss: -23.64735, policy_entropy: -6.70951, alpha: 0.00251, time: 32.96847
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 73.42127, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 0.03131, qf2_loss: 0.03146, policy_loss: -23.63092, policy_entropy: -6.94397, alpha: 0.00261, time: 32.85206
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 70.92658, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 0.03012, qf2_loss: 0.03022, policy_loss: -23.58028, policy_entropy: -6.72490, alpha: 0.00271, time: 33.03184
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 90.58407, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 0.03254, qf2_loss: 0.03239, policy_loss: -23.55201, policy_entropy: -6.48716, alpha: 0.00278, time: 32.77907
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 79.79021, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 0.03596, qf2_loss: 0.03608, policy_loss: -23.52954, policy_entropy: -6.35838, alpha: 0.00284, time: 33.26545
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 84.11745, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 0.03896, qf2_loss: 0.03918, policy_loss: -23.50956, policy_entropy: -6.05529, alpha: 0.00288, time: 33.19069
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 94.89369, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 0.04460, qf2_loss: 0.04512, policy_loss: -23.42366, policy_entropy: -5.99678, alpha: 0.00287, time: 32.83360
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 61.00585, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 0.04832, qf2_loss: 0.04887, policy_loss: -23.38779, policy_entropy: -6.08495, alpha: 0.00287, time: 33.08494
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 82.75637, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 0.05087, qf2_loss: 0.05148, policy_loss: -23.35554, policy_entropy: -6.36687, alpha: 0.00290, time: 32.72622
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 82.22663, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 0.04179, qf2_loss: 0.04154, policy_loss: -23.31743, policy_entropy: -6.51242, alpha: 0.00297, time: 32.80881
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 81.73696, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 0.04429, qf2_loss: 0.04497, policy_loss: -23.28198, policy_entropy: -6.22113, alpha: 0.00304, time: 32.90461
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 100.15421, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 0.03983, qf2_loss: 0.03983, policy_loss: -23.26659, policy_entropy: -6.27824, alpha: 0.00307, time: 33.06657
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 54.44521, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 0.04136, qf2_loss: 0.04137, policy_loss: -23.23616, policy_entropy: -6.04741, alpha: 0.00310, time: 33.02456
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 92.32321, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 0.03655, qf2_loss: 0.03695, policy_loss: -23.17226, policy_entropy: -6.10957, alpha: 0.00312, time: 33.24452
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 58.70021, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 0.06112, qf2_loss: 0.06217, policy_loss: -23.16837, policy_entropy: -6.03678, alpha: 0.00312, time: 33.03709
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 51.58967, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 0.03806, qf2_loss: 0.03812, policy_loss: -23.10961, policy_entropy: -6.63818, alpha: 0.00319, time: 32.78941
[CW] eval: return: 69.57285, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 94.90727, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 0.04363, qf2_loss: 0.04407, policy_loss: -23.11143, policy_entropy: -6.12410, alpha: 0.00327, time: 33.18596
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 106.75629, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 0.04603, qf2_loss: 0.04652, policy_loss: -23.03852, policy_entropy: -6.06946, alpha: 0.00328, time: 33.09848
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 68.29577, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 0.03825, qf2_loss: 0.03836, policy_loss: -23.03350, policy_entropy: -5.97426, alpha: 0.00329, time: 33.15168
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 47.26550, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 0.06403, qf2_loss: 0.06518, policy_loss: -22.97278, policy_entropy: -5.61649, alpha: 0.00327, time: 32.91896
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 65.35030, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 0.03736, qf2_loss: 0.03758, policy_loss: -22.99446, policy_entropy: -5.98164, alpha: 0.00320, time: 32.78964
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 104.17688, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 0.04129, qf2_loss: 0.04173, policy_loss: -22.93654, policy_entropy: -5.92000, alpha: 0.00320, time: 33.16241
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 84.16059, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 0.03853, qf2_loss: 0.03894, policy_loss: -22.92858, policy_entropy: -6.02475, alpha: 0.00318, time: 33.09804
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 90.07949, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 0.03801, qf2_loss: 0.03801, policy_loss: -22.92024, policy_entropy: -6.00406, alpha: 0.00319, time: 33.01342
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 99.36914, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 0.04664, qf2_loss: 0.04750, policy_loss: -22.82944, policy_entropy: -6.03077, alpha: 0.00321, time: 32.87047
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 88.04496, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 0.04730, qf2_loss: 0.04768, policy_loss: -22.83562, policy_entropy: -6.10613, alpha: 0.00321, time: 33.10958
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 111.38949, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 0.04962, qf2_loss: 0.05061, policy_loss: -22.78730, policy_entropy: -6.11560, alpha: 0.00324, time: 32.60982
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 56.12634, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 0.05097, qf2_loss: 0.05173, policy_loss: -22.78201, policy_entropy: -6.31862, alpha: 0.00329, time: 32.83471
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 57.29803, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 0.04733, qf2_loss: 0.04796, policy_loss: -22.76540, policy_entropy: -6.30994, alpha: 0.00334, time: 33.08636
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 87.44297, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 0.03891, qf2_loss: 0.03922, policy_loss: -22.71808, policy_entropy: -6.73972, alpha: 0.00343, time: 33.33498
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 109.47931, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 0.05545, qf2_loss: 0.05584, policy_loss: -22.67306, policy_entropy: -6.71152, alpha: 0.00358, time: 32.94652
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 97.46545, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 0.03575, qf2_loss: 0.03608, policy_loss: -22.68252, policy_entropy: -6.52986, alpha: 0.00372, time: 33.08421
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 59.98143, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 0.03688, qf2_loss: 0.03706, policy_loss: -22.66582, policy_entropy: -6.31639, alpha: 0.00380, time: 33.01296
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 50.36066, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 0.07803, qf2_loss: 0.07985, policy_loss: -22.66393, policy_entropy: -5.96782, alpha: 0.00385, time: 33.07551
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 58.08469, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 0.03609, qf2_loss: 0.03581, policy_loss: -22.63194, policy_entropy: -6.17224, alpha: 0.00386, time: 33.00887
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 32.47272, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 0.04404, qf2_loss: 0.04492, policy_loss: -22.55637, policy_entropy: -5.97537, alpha: 0.00388, time: 32.70199
[CW] eval: return: 77.76871, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 103.69762, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 0.04460, qf2_loss: 0.04514, policy_loss: -22.53487, policy_entropy: -6.13857, alpha: 0.00385, time: 33.34446
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 57.37501, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 0.03757, qf2_loss: 0.03845, policy_loss: -22.51507, policy_entropy: -6.23305, alpha: 0.00391, time: 33.10439
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 87.17695, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 0.03590, qf2_loss: 0.03592, policy_loss: -22.50006, policy_entropy: -6.10407, alpha: 0.00395, time: 32.97427
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 89.42254, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 0.03583, qf2_loss: 0.03597, policy_loss: -22.43215, policy_entropy: -6.01903, alpha: 0.00397, time: 32.98398
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 79.18502, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 0.03931, qf2_loss: 0.03973, policy_loss: -22.44107, policy_entropy: -6.15809, alpha: 0.00397, time: 32.77505
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 58.23108, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 0.05902, qf2_loss: 0.06006, policy_loss: -22.42261, policy_entropy: -6.03238, alpha: 0.00402, time: 33.33632
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 56.13678, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 0.06910, qf2_loss: 0.07055, policy_loss: -22.38622, policy_entropy: -5.80003, alpha: 0.00400, time: 33.01479
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 81.86822, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 0.04742, qf2_loss: 0.04848, policy_loss: -22.34689, policy_entropy: -5.68774, alpha: 0.00394, time: 33.33621
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 62.61379, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 0.03324, qf2_loss: 0.03327, policy_loss: -22.30033, policy_entropy: -5.68391, alpha: 0.00388, time: 33.41761
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 68.64452, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 0.04211, qf2_loss: 0.04260, policy_loss: -22.26015, policy_entropy: -5.49871, alpha: 0.00379, time: 33.40073
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 64.28330, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 0.03671, qf2_loss: 0.03679, policy_loss: -22.25272, policy_entropy: -5.58531, alpha: 0.00370, time: 33.06905
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 53.97124, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 0.03682, qf2_loss: 0.03692, policy_loss: -22.25646, policy_entropy: -5.56346, alpha: 0.00362, time: 33.05522
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 59.55992, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 0.03628, qf2_loss: 0.03681, policy_loss: -22.18298, policy_entropy: -5.63171, alpha: 0.00354, time: 33.14536
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 85.10195, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 0.04485, qf2_loss: 0.04478, policy_loss: -22.22768, policy_entropy: -5.39474, alpha: 0.00347, time: 33.35235
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 74.47276, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 0.09809, qf2_loss: 0.10260, policy_loss: -22.14374, policy_entropy: -4.99233, alpha: 0.00334, time: 33.04217
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 85.86236, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 0.03745, qf2_loss: 0.03750, policy_loss: -22.08577, policy_entropy: -5.72768, alpha: 0.00324, time: 33.52970
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 82.39355, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 0.03415, qf2_loss: 0.03409, policy_loss: -22.06944, policy_entropy: -5.95644, alpha: 0.00322, time: 33.07316
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 72.38911, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 0.03600, qf2_loss: 0.03583, policy_loss: -22.01011, policy_entropy: -6.19840, alpha: 0.00323, time: 33.60862
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 76.43847, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 0.04003, qf2_loss: 0.04023, policy_loss: -22.02399, policy_entropy: -6.00523, alpha: 0.00325, time: 33.18736
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 72.28084, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 0.09876, qf2_loss: 0.10150, policy_loss: -21.91537, policy_entropy: -5.55088, alpha: 0.00322, time: 32.95574
[CW] eval: return: 52.27234, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 32.96625, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 0.05577, qf2_loss: 0.05640, policy_loss: -21.88273, policy_entropy: -5.94416, alpha: 0.00318, time: 33.34074
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 65.40116, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 0.04737, qf2_loss: 0.04711, policy_loss: -21.84894, policy_entropy: -5.78822, alpha: 0.00316, time: 33.05833
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 61.94065, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 0.03345, qf2_loss: 0.03353, policy_loss: -21.80626, policy_entropy: -5.97992, alpha: 0.00314, time: 33.03673
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 60.26601, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 0.03485, qf2_loss: 0.03520, policy_loss: -21.76908, policy_entropy: -5.84313, alpha: 0.00314, time: 33.29583
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 56.70400, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 0.03030, qf2_loss: 0.03027, policy_loss: -21.70509, policy_entropy: -6.00933, alpha: 0.00312, time: 32.97269
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 57.66572, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 0.03078, qf2_loss: 0.03099, policy_loss: -21.67111, policy_entropy: -5.88681, alpha: 0.00311, time: 33.51406
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 76.62003, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 0.03135, qf2_loss: 0.03142, policy_loss: -21.62734, policy_entropy: -5.75654, alpha: 0.00308, time: 33.26078
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 72.43877, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 0.03558, qf2_loss: 0.03563, policy_loss: -21.58401, policy_entropy: -5.84254, alpha: 0.00306, time: 33.28894
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 83.76985, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 0.03076, qf2_loss: 0.03067, policy_loss: -21.58948, policy_entropy: -5.73549, alpha: 0.00302, time: 33.38776
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 93.35759, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 0.03856, qf2_loss: 0.03908, policy_loss: -21.55680, policy_entropy: -6.30621, alpha: 0.00302, time: 33.07264
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 93.08588, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 0.04061, qf2_loss: 0.04172, policy_loss: -21.49304, policy_entropy: -6.49598, alpha: 0.00308, time: 32.85787
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 99.09360, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 0.06163, qf2_loss: 0.06304, policy_loss: -21.46020, policy_entropy: -6.57954, alpha: 0.00315, time: 33.15750
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 83.33978, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 0.03636, qf2_loss: 0.03597, policy_loss: -21.40810, policy_entropy: -6.17435, alpha: 0.00323, time: 37.17537
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 85.39664, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 0.03831, qf2_loss: 0.03880, policy_loss: -21.41759, policy_entropy: -6.32451, alpha: 0.00326, time: 33.22758
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 80.28791, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 0.03742, qf2_loss: 0.03749, policy_loss: -21.38854, policy_entropy: -6.19235, alpha: 0.00330, time: 33.44302
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 106.03743, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 0.04288, qf2_loss: 0.04286, policy_loss: -21.34516, policy_entropy: -6.28008, alpha: 0.00334, time: 33.25148
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 42.87554, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 0.03706, qf2_loss: 0.03770, policy_loss: -21.36884, policy_entropy: -6.23674, alpha: 0.00338, time: 33.18937
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 94.97338, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 0.08203, qf2_loss: 0.08416, policy_loss: -21.23195, policy_entropy: -5.55563, alpha: 0.00339, time: 33.20915
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 109.39776, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 0.04589, qf2_loss: 0.04636, policy_loss: -21.23712, policy_entropy: -6.11076, alpha: 0.00334, time: 33.11835
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 63.83264, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 0.03380, qf2_loss: 0.03403, policy_loss: -21.23011, policy_entropy: -6.33243, alpha: 0.00337, time: 33.17235
[CW] eval: return: 99.15946, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 95.47392, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 0.03336, qf2_loss: 0.03346, policy_loss: -21.18025, policy_entropy: -6.38201, alpha: 0.00343, time: 33.49110
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 122.78599, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 0.03630, qf2_loss: 0.03661, policy_loss: -21.11713, policy_entropy: -6.19178, alpha: 0.00350, time: 33.04479
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 99.58522, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 0.05168, qf2_loss: 0.05241, policy_loss: -21.08791, policy_entropy: -6.33939, alpha: 0.00353, time: 32.96191
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 122.61739, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 0.05135, qf2_loss: 0.05183, policy_loss: -21.04000, policy_entropy: -6.44947, alpha: 0.00359, time: 32.84165
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 100.91715, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 0.08759, qf2_loss: 0.08932, policy_loss: -21.02674, policy_entropy: -5.80316, alpha: 0.00363, time: 33.28366
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 105.27248, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 0.03531, qf2_loss: 0.03560, policy_loss: -20.98837, policy_entropy: -6.50749, alpha: 0.00365, time: 32.80216
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 100.57229, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 0.03295, qf2_loss: 0.03302, policy_loss: -20.90284, policy_entropy: -6.24411, alpha: 0.00372, time: 33.33052
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 126.96195, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 0.03714, qf2_loss: 0.03724, policy_loss: -20.92408, policy_entropy: -6.29568, alpha: 0.00376, time: 33.26555
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 119.98988, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 0.03787, qf2_loss: 0.03808, policy_loss: -20.90154, policy_entropy: -6.34301, alpha: 0.00382, time: 32.88275
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 115.54089, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 0.04521, qf2_loss: 0.04532, policy_loss: -20.84506, policy_entropy: -5.78639, alpha: 0.00385, time: 32.96198
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 119.33495, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 0.03927, qf2_loss: 0.03961, policy_loss: -20.86748, policy_entropy: -6.45481, alpha: 0.00385, time: 32.74184
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 119.97497, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 0.03804, qf2_loss: 0.03844, policy_loss: -20.79520, policy_entropy: -5.69640, alpha: 0.00389, time: 33.05873
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 111.29653, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 0.04523, qf2_loss: 0.04544, policy_loss: -20.81517, policy_entropy: -5.98161, alpha: 0.00383, time: 32.74488
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 118.84430, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 0.05210, qf2_loss: 0.05254, policy_loss: -20.79293, policy_entropy: -5.70061, alpha: 0.00381, time: 32.85573
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 127.73218, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 0.04093, qf2_loss: 0.04117, policy_loss: -20.74157, policy_entropy: -5.78491, alpha: 0.00378, time: 32.81990
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 110.78291, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 0.04032, qf2_loss: 0.04070, policy_loss: -20.67959, policy_entropy: -6.07005, alpha: 0.00375, time: 32.92397
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 107.34101, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 0.05210, qf2_loss: 0.05214, policy_loss: -20.67468, policy_entropy: -6.33494, alpha: 0.00378, time: 33.08383
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 68.68190, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 0.10347, qf2_loss: 0.10671, policy_loss: -20.56629, policy_entropy: -6.28999, alpha: 0.00385, time: 33.21644
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 64.39701, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 0.04586, qf2_loss: 0.04576, policy_loss: -20.60896, policy_entropy: -6.39273, alpha: 0.00391, time: 32.42115
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 88.41602, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 0.04192, qf2_loss: 0.04207, policy_loss: -20.57328, policy_entropy: -6.22488, alpha: 0.00396, time: 32.62059
[CW] eval: return: 92.99182, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 122.14588, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 0.03966, qf2_loss: 0.03982, policy_loss: -20.57951, policy_entropy: -6.22809, alpha: 0.00400, time: 32.70835
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 32.20115, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 0.04181, qf2_loss: 0.04211, policy_loss: -20.52527, policy_entropy: -6.22630, alpha: 0.00405, time: 32.84944
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 31.98258, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 0.04660, qf2_loss: 0.04672, policy_loss: -20.53770, policy_entropy: -5.95931, alpha: 0.00406, time: 32.76090
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 121.37837, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 0.04175, qf2_loss: 0.04194, policy_loss: -20.45299, policy_entropy: -5.94865, alpha: 0.00406, time: 33.04300
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 144.51677, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 0.04648, qf2_loss: 0.04683, policy_loss: -20.44552, policy_entropy: -6.38428, alpha: 0.00408, time: 33.18485
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 31.79878, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 0.05033, qf2_loss: 0.05103, policy_loss: -20.39657, policy_entropy: -5.98540, alpha: 0.00414, time: 32.91234
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 24.68890, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 0.05532, qf2_loss: 0.05593, policy_loss: -20.41788, policy_entropy: -6.11961, alpha: 0.00414, time: 32.68668
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 120.26636, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 0.06467, qf2_loss: 0.06517, policy_loss: -20.34179, policy_entropy: -5.94641, alpha: 0.00414, time: 32.63955
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 131.97946, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 0.04664, qf2_loss: 0.04668, policy_loss: -20.37683, policy_entropy: -6.30750, alpha: 0.00415, time: 32.93202
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 130.50342, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 0.05841, qf2_loss: 0.05933, policy_loss: -20.36694, policy_entropy: -6.23496, alpha: 0.00422, time: 32.73842
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 132.44891, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 0.06235, qf2_loss: 0.06263, policy_loss: -20.33617, policy_entropy: -6.26251, alpha: 0.00426, time: 32.60510
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 30.25010, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 0.06848, qf2_loss: 0.06947, policy_loss: -20.29047, policy_entropy: -6.56627, alpha: 0.00435, time: 32.76730
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 137.89959, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 0.05051, qf2_loss: 0.05079, policy_loss: -20.23511, policy_entropy: -6.45690, alpha: 0.00444, time: 32.68758
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 131.46190, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 0.04964, qf2_loss: 0.05009, policy_loss: -20.21162, policy_entropy: -6.24225, alpha: 0.00453, time: 32.84835
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 128.35398, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 0.05738, qf2_loss: 0.05800, policy_loss: -20.24619, policy_entropy: -6.11135, alpha: 0.00456, time: 32.66395
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 131.07948, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 0.05210, qf2_loss: 0.05268, policy_loss: -20.15691, policy_entropy: -6.09072, alpha: 0.00457, time: 32.81474
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 125.46786, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 0.06300, qf2_loss: 0.06384, policy_loss: -20.17047, policy_entropy: -5.94723, alpha: 0.00459, time: 32.59710
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 136.17271, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 0.06679, qf2_loss: 0.06681, policy_loss: -20.19337, policy_entropy: -6.17519, alpha: 0.00459, time: 33.16105
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 128.67808, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 0.05997, qf2_loss: 0.06006, policy_loss: -20.11537, policy_entropy: -6.24444, alpha: 0.00463, time: 32.43422
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 113.77516, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 0.06028, qf2_loss: 0.06086, policy_loss: -20.16702, policy_entropy: -6.41033, alpha: 0.00470, time: 32.97487
[CW] eval: return: 112.40382, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 100.19906, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 0.05903, qf2_loss: 0.05980, policy_loss: -20.12847, policy_entropy: -6.63249, alpha: 0.00482, time: 33.27504
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 100.33078, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 0.05412, qf2_loss: 0.05439, policy_loss: -20.10785, policy_entropy: -6.43885, alpha: 0.00494, time: 32.66012
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 121.11548, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 0.09946, qf2_loss: 0.10089, policy_loss: -20.06583, policy_entropy: -6.13801, alpha: 0.00498, time: 32.90376
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 78.61295, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 0.07185, qf2_loss: 0.07268, policy_loss: -20.05700, policy_entropy: -6.25852, alpha: 0.00501, time: 32.61941
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 102.89676, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 0.05759, qf2_loss: 0.05765, policy_loss: -20.01735, policy_entropy: -6.41485, alpha: 0.00510, time: 32.86294
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 88.37510, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 0.05336, qf2_loss: 0.05372, policy_loss: -20.02948, policy_entropy: -6.05933, alpha: 0.00517, time: 33.02389
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 125.30480, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 0.07726, qf2_loss: 0.07787, policy_loss: -20.03534, policy_entropy: -5.98968, alpha: 0.00517, time: 33.17518
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 106.77152, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 0.06562, qf2_loss: 0.06593, policy_loss: -19.92028, policy_entropy: -5.89260, alpha: 0.00516, time: 32.81770
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 144.40718, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 0.06080, qf2_loss: 0.06103, policy_loss: -19.93303, policy_entropy: -5.80902, alpha: 0.00511, time: 32.74179
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 118.75790, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 0.06212, qf2_loss: 0.06296, policy_loss: -19.92663, policy_entropy: -6.10134, alpha: 0.00511, time: 32.60851
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 123.54381, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 0.07372, qf2_loss: 0.07424, policy_loss: -19.96680, policy_entropy: -6.13527, alpha: 0.00513, time: 32.76576
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 105.02365, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 0.06240, qf2_loss: 0.06295, policy_loss: -19.90280, policy_entropy: -6.34304, alpha: 0.00518, time: 32.71076
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 122.64327, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 0.07305, qf2_loss: 0.07388, policy_loss: -19.90050, policy_entropy: -6.09226, alpha: 0.00522, time: 33.02903
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 127.86398, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 0.07002, qf2_loss: 0.07066, policy_loss: -19.90002, policy_entropy: -6.03586, alpha: 0.00525, time: 32.78921
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 101.06024, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 0.06432, qf2_loss: 0.06458, policy_loss: -19.88200, policy_entropy: -5.82408, alpha: 0.00524, time: 32.71071
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 93.00432, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 0.06010, qf2_loss: 0.06061, policy_loss: -19.80627, policy_entropy: -5.84019, alpha: 0.00519, time: 32.97502
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 80.87244, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 0.06335, qf2_loss: 0.06388, policy_loss: -19.82999, policy_entropy: -6.14330, alpha: 0.00519, time: 32.69128
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 95.85775, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 0.06321, qf2_loss: 0.06403, policy_loss: -19.75732, policy_entropy: -6.07251, alpha: 0.00521, time: 33.10375
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 31.38125, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 0.07947, qf2_loss: 0.08038, policy_loss: -19.86023, policy_entropy: -5.93295, alpha: 0.00521, time: 32.48547
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 120.24369, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 0.07485, qf2_loss: 0.07470, policy_loss: -19.75661, policy_entropy: -6.00789, alpha: 0.00521, time: 32.99056
[CW] eval: return: 97.36030, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 28.99388, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 0.06212, qf2_loss: 0.06270, policy_loss: -19.74870, policy_entropy: -5.77793, alpha: 0.00520, time: 32.86136
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 123.27714, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 0.07006, qf2_loss: 0.07074, policy_loss: -19.75665, policy_entropy: -5.96997, alpha: 0.00517, time: 32.80039
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 113.55317, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 0.05625, qf2_loss: 0.05664, policy_loss: -19.69945, policy_entropy: -5.68725, alpha: 0.00511, time: 32.57361
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 111.52643, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 0.08353, qf2_loss: 0.08441, policy_loss: -19.75477, policy_entropy: -5.81917, alpha: 0.00508, time: 32.72538
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 110.33917, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 0.08345, qf2_loss: 0.08416, policy_loss: -19.72579, policy_entropy: -5.92648, alpha: 0.00504, time: 32.77648
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 124.12836, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 0.06353, qf2_loss: 0.06384, policy_loss: -19.71871, policy_entropy: -6.09464, alpha: 0.00503, time: 32.79857
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 122.11654, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 0.06609, qf2_loss: 0.06630, policy_loss: -19.70182, policy_entropy: -6.10870, alpha: 0.00506, time: 33.20529
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 110.56089, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 0.06988, qf2_loss: 0.07061, policy_loss: -19.66937, policy_entropy: -5.99880, alpha: 0.00508, time: 33.09088
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 53.88516, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 0.07347, qf2_loss: 0.07402, policy_loss: -19.64581, policy_entropy: -6.00399, alpha: 0.00507, time: 32.74321
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 67.41644, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 0.07351, qf2_loss: 0.07423, policy_loss: -19.67003, policy_entropy: -6.27485, alpha: 0.00510, time: 32.70715
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 62.07190, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 0.07430, qf2_loss: 0.07482, policy_loss: -19.57372, policy_entropy: -6.03713, alpha: 0.00514, time: 32.71264
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 90.61728, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 0.09703, qf2_loss: 0.09776, policy_loss: -19.56193, policy_entropy: -6.04330, alpha: 0.00517, time: 32.76282
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 129.16621, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 0.08446, qf2_loss: 0.08561, policy_loss: -19.52967, policy_entropy: -6.44114, alpha: 0.00521, time: 33.00577
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 113.44923, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 0.07226, qf2_loss: 0.07298, policy_loss: -19.53966, policy_entropy: -5.85384, alpha: 0.00526, time: 33.15044
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 111.59065, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 0.06752, qf2_loss: 0.06756, policy_loss: -19.55504, policy_entropy: -5.89435, alpha: 0.00523, time: 32.82135
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 97.64449, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 0.06471, qf2_loss: 0.06477, policy_loss: -19.51364, policy_entropy: -6.01618, alpha: 0.00520, time: 32.90295
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 113.45287, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 0.06002, qf2_loss: 0.06019, policy_loss: -19.47430, policy_entropy: -5.99425, alpha: 0.00521, time: 33.07256
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 84.38952, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 0.06064, qf2_loss: 0.06098, policy_loss: -19.53181, policy_entropy: -6.07753, alpha: 0.00521, time: 32.94550
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 140.67097, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 0.07486, qf2_loss: 0.07540, policy_loss: -19.47522, policy_entropy: -6.02797, alpha: 0.00522, time: 32.69894
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 110.25572, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 0.07057, qf2_loss: 0.07103, policy_loss: -19.45789, policy_entropy: -6.19158, alpha: 0.00525, time: 32.97781
[CW] eval: return: 103.41304, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 122.27783, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 0.10120, qf2_loss: 0.10208, policy_loss: -19.40609, policy_entropy: -6.19970, alpha: 0.00530, time: 32.75110
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 77.13489, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 0.09848, qf2_loss: 0.09951, policy_loss: -19.43253, policy_entropy: -6.19119, alpha: 0.00535, time: 32.69970
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 114.57462, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 0.06777, qf2_loss: 0.06816, policy_loss: -19.40036, policy_entropy: -6.31229, alpha: 0.00543, time: 32.97536
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 108.77009, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 0.06763, qf2_loss: 0.06788, policy_loss: -19.32172, policy_entropy: -5.99507, alpha: 0.00546, time: 33.23697
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 105.15070, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 0.07095, qf2_loss: 0.07136, policy_loss: -19.42717, policy_entropy: -6.06718, alpha: 0.00548, time: 33.09726
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 117.95197, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 0.06919, qf2_loss: 0.06943, policy_loss: -19.36276, policy_entropy: -6.00091, alpha: 0.00549, time: 32.88213
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 99.34478, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 0.07448, qf2_loss: 0.07513, policy_loss: -19.38970, policy_entropy: -6.27786, alpha: 0.00552, time: 32.83800
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 28.96909, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 0.07783, qf2_loss: 0.07872, policy_loss: -19.33535, policy_entropy: -6.25110, alpha: 0.00560, time: 32.66406
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 103.34043, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 0.07489, qf2_loss: 0.07542, policy_loss: -19.24956, policy_entropy: -6.06488, alpha: 0.00565, time: 33.28994
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 123.28529, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 0.06749, qf2_loss: 0.06781, policy_loss: -19.35588, policy_entropy: -6.04246, alpha: 0.00567, time: 33.52934
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 113.44537, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 0.06999, qf2_loss: 0.07026, policy_loss: -19.24607, policy_entropy: -5.97380, alpha: 0.00567, time: 32.96682
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 73.18389, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 0.06885, qf2_loss: 0.06889, policy_loss: -19.25122, policy_entropy: -5.93879, alpha: 0.00566, time: 34.45301
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 121.52334, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 0.09123, qf2_loss: 0.09194, policy_loss: -19.24652, policy_entropy: -5.89487, alpha: 0.00562, time: 33.21600
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 59.77949, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 0.08209, qf2_loss: 0.08272, policy_loss: -19.17649, policy_entropy: -6.02372, alpha: 0.00561, time: 32.84303
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 98.82323, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 0.06865, qf2_loss: 0.06900, policy_loss: -19.19289, policy_entropy: -5.94520, alpha: 0.00561, time: 32.62954
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 114.04443, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 0.07995, qf2_loss: 0.08038, policy_loss: -19.23676, policy_entropy: -5.96727, alpha: 0.00560, time: 33.05837
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 109.26666, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 0.07568, qf2_loss: 0.07580, policy_loss: -19.16831, policy_entropy: -5.85448, alpha: 0.00558, time: 33.21762
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 110.49427, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 0.07560, qf2_loss: 0.07606, policy_loss: -19.15918, policy_entropy: -5.83209, alpha: 0.00555, time: 35.11105
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 74.30455, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 0.07566, qf2_loss: 0.07597, policy_loss: -19.19093, policy_entropy: -6.01428, alpha: 0.00549, time: 32.68522
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 61.12771, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 0.07717, qf2_loss: 0.07742, policy_loss: -19.15439, policy_entropy: -6.15264, alpha: 0.00553, time: 33.15412
[CW] eval: return: 100.95588, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 90.99111, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 0.06978, qf2_loss: 0.06986, policy_loss: -19.17246, policy_entropy: -6.26769, alpha: 0.00559, time: 33.38034
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 80.92043, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 0.12832, qf2_loss: 0.13055, policy_loss: -19.16945, policy_entropy: -6.36777, alpha: 0.00567, time: 32.73900
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 111.82005, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 0.08334, qf2_loss: 0.08366, policy_loss: -19.11771, policy_entropy: -6.58034, alpha: 0.00580, time: 32.90528
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 96.92653, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 0.07736, qf2_loss: 0.07806, policy_loss: -19.08411, policy_entropy: -6.02382, alpha: 0.00588, time: 32.96733
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 116.92523, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 0.06678, qf2_loss: 0.06703, policy_loss: -19.10459, policy_entropy: -5.84627, alpha: 0.00588, time: 33.25515
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 78.79930, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 0.06457, qf2_loss: 0.06478, policy_loss: -19.05266, policy_entropy: -5.78421, alpha: 0.00582, time: 32.84147
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 117.53259, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 0.06841, qf2_loss: 0.06850, policy_loss: -18.94700, policy_entropy: -5.71117, alpha: 0.00576, time: 33.41649
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 40.59800, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 0.06709, qf2_loss: 0.06762, policy_loss: -19.04210, policy_entropy: -5.85006, alpha: 0.00569, time: 32.61259
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 112.87164, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 0.08100, qf2_loss: 0.08105, policy_loss: -18.97075, policy_entropy: -5.81177, alpha: 0.00564, time: 32.99602
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 131.70582, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 0.07304, qf2_loss: 0.07353, policy_loss: -18.99544, policy_entropy: -6.03038, alpha: 0.00562, time: 33.12530
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 117.48247, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 0.08693, qf2_loss: 0.08743, policy_loss: -19.01761, policy_entropy: -6.24342, alpha: 0.00565, time: 33.40565
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 128.34588, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 0.07902, qf2_loss: 0.07938, policy_loss: -18.96951, policy_entropy: -6.03111, alpha: 0.00569, time: 33.13413
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 138.66529, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 0.08298, qf2_loss: 0.08324, policy_loss: -18.94348, policy_entropy: -6.18300, alpha: 0.00572, time: 33.29452
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 112.27559, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 0.19358, qf2_loss: 0.19664, policy_loss: -18.92783, policy_entropy: -5.92259, alpha: 0.00574, time: 33.04756
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 124.84777, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 0.09647, qf2_loss: 0.09694, policy_loss: -18.94781, policy_entropy: -5.97996, alpha: 0.00572, time: 33.09925
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 143.42975, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 0.06799, qf2_loss: 0.06811, policy_loss: -18.96401, policy_entropy: -6.07096, alpha: 0.00573, time: 32.61331
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 112.54965, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 0.07126, qf2_loss: 0.07152, policy_loss: -18.95231, policy_entropy: -5.99878, alpha: 0.00575, time: 32.59544
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 123.05785, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 0.07249, qf2_loss: 0.07258, policy_loss: -18.91686, policy_entropy: -5.90524, alpha: 0.00572, time: 32.59419
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 131.32275, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 0.06574, qf2_loss: 0.06568, policy_loss: -18.95723, policy_entropy: -5.94450, alpha: 0.00571, time: 32.63325
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 138.33193, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 0.07076, qf2_loss: 0.07073, policy_loss: -18.93315, policy_entropy: -5.86179, alpha: 0.00567, time: 33.02319
[CW] eval: return: 111.92192, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 136.61294, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 0.06531, qf2_loss: 0.06538, policy_loss: -18.93689, policy_entropy: -5.86967, alpha: 0.00564, time: 33.30015
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 123.93869, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 0.07344, qf2_loss: 0.07354, policy_loss: -18.86742, policy_entropy: -5.91255, alpha: 0.00563, time: 32.65760
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 141.66739, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 0.07721, qf2_loss: 0.07767, policy_loss: -18.84768, policy_entropy: -5.91994, alpha: 0.00559, time: 32.94770
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 117.09935, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 0.06845, qf2_loss: 0.06870, policy_loss: -18.93290, policy_entropy: -5.85449, alpha: 0.00557, time: 32.74685
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 134.16316, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 0.07669, qf2_loss: 0.07692, policy_loss: -18.83383, policy_entropy: -5.72350, alpha: 0.00552, time: 33.12066
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 135.54035, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 0.12298, qf2_loss: 0.12357, policy_loss: -18.85089, policy_entropy: -6.18963, alpha: 0.00549, time: 33.06518
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 123.43603, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 0.08527, qf2_loss: 0.08572, policy_loss: -18.89570, policy_entropy: -6.39223, alpha: 0.00556, time: 32.86945
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 131.58415, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 0.09004, qf2_loss: 0.09040, policy_loss: -18.73927, policy_entropy: -6.03763, alpha: 0.00564, time: 32.72721
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 128.69963, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 0.07833, qf2_loss: 0.07877, policy_loss: -18.74511, policy_entropy: -5.97493, alpha: 0.00563, time: 33.46134
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 129.73495, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 0.07655, qf2_loss: 0.07679, policy_loss: -18.83612, policy_entropy: -5.96150, alpha: 0.00562, time: 32.79304
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 140.17322, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 0.06894, qf2_loss: 0.06898, policy_loss: -18.84490, policy_entropy: -6.02704, alpha: 0.00562, time: 33.16902
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 129.79864, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 0.08666, qf2_loss: 0.08715, policy_loss: -18.79955, policy_entropy: -5.94509, alpha: 0.00561, time: 32.83198
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 139.84306, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 0.09456, qf2_loss: 0.09493, policy_loss: -18.80673, policy_entropy: -6.22639, alpha: 0.00564, time: 33.55219
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 133.49014, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 0.10270, qf2_loss: 0.10403, policy_loss: -18.79678, policy_entropy: -6.11977, alpha: 0.00569, time: 32.87982
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 121.38838, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 0.07547, qf2_loss: 0.07575, policy_loss: -18.71027, policy_entropy: -6.10917, alpha: 0.00572, time: 33.47102
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 122.11682, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 0.07938, qf2_loss: 0.07971, policy_loss: -18.77081, policy_entropy: -5.96925, alpha: 0.00573, time: 33.06606
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 105.72838, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 0.08066, qf2_loss: 0.08130, policy_loss: -18.79262, policy_entropy: -6.07874, alpha: 0.00573, time: 32.64009
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 122.77583, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 0.08740, qf2_loss: 0.08779, policy_loss: -18.76428, policy_entropy: -6.14075, alpha: 0.00575, time: 32.94292
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 118.69331, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 0.08364, qf2_loss: 0.08449, policy_loss: -18.81220, policy_entropy: -6.14630, alpha: 0.00579, time: 32.80296
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 116.46789, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 0.09140, qf2_loss: 0.09223, policy_loss: -18.74902, policy_entropy: -6.19796, alpha: 0.00584, time: 33.03106
[CW] eval: return: 116.50284, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 105.99626, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 0.07936, qf2_loss: 0.08005, policy_loss: -18.81445, policy_entropy: -6.38821, alpha: 0.00593, time: 33.20476
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 149.54306, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 0.08576, qf2_loss: 0.08621, policy_loss: -18.75728, policy_entropy: -6.09265, alpha: 0.00598, time: 33.19889
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 146.02786, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 0.07446, qf2_loss: 0.07469, policy_loss: -18.69143, policy_entropy: -6.00719, alpha: 0.00601, time: 33.35106
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 151.02805, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 0.07560, qf2_loss: 0.07597, policy_loss: -18.73152, policy_entropy: -5.88266, alpha: 0.00599, time: 33.28577
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 141.54713, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 0.07785, qf2_loss: 0.07789, policy_loss: -18.80632, policy_entropy: -6.05733, alpha: 0.00598, time: 33.77869
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 117.42270, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 0.09124, qf2_loss: 0.09178, policy_loss: -18.71706, policy_entropy: -6.04955, alpha: 0.00602, time: 33.06637
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 89.45980, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 0.07998, qf2_loss: 0.08030, policy_loss: -18.79848, policy_entropy: -5.96496, alpha: 0.00600, time: 32.86097
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 103.27088, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 0.08828, qf2_loss: 0.08878, policy_loss: -18.64141, policy_entropy: -5.76076, alpha: 0.00597, time: 32.97279
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 105.58162, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 0.10789, qf2_loss: 0.10853, policy_loss: -18.77850, policy_entropy: -6.04670, alpha: 0.00594, time: 33.56848
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 115.82603, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 0.11304, qf2_loss: 0.11350, policy_loss: -18.75818, policy_entropy: -5.96270, alpha: 0.00594, time: 33.25945
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 110.50816, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 0.07875, qf2_loss: 0.07908, policy_loss: -18.72886, policy_entropy: -5.91924, alpha: 0.00594, time: 32.90223
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 107.98946, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 0.07304, qf2_loss: 0.07347, policy_loss: -18.76125, policy_entropy: -5.98270, alpha: 0.00590, time: 32.88967
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 108.00122, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 0.07416, qf2_loss: 0.07437, policy_loss: -18.61228, policy_entropy: -5.84590, alpha: 0.00589, time: 33.00948
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 29.48708, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 0.07399, qf2_loss: 0.07454, policy_loss: -18.68897, policy_entropy: -6.07930, alpha: 0.00586, time: 32.95964
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 131.71964, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 0.07874, qf2_loss: 0.07903, policy_loss: -18.66544, policy_entropy: -5.89759, alpha: 0.00588, time: 35.78934
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 131.06293, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 0.10360, qf2_loss: 0.10464, policy_loss: -18.66877, policy_entropy: -5.96470, alpha: 0.00585, time: 33.10947
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 141.33779, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 0.09566, qf2_loss: 0.09628, policy_loss: -18.60448, policy_entropy: -5.88335, alpha: 0.00583, time: 32.79021
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 141.64867, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 0.08909, qf2_loss: 0.08953, policy_loss: -18.65851, policy_entropy: -5.87778, alpha: 0.00581, time: 33.04777
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 138.64034, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 0.08766, qf2_loss: 0.08863, policy_loss: -18.58326, policy_entropy: -5.76046, alpha: 0.00575, time: 32.86819
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 91.76808, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 0.07660, qf2_loss: 0.07676, policy_loss: -18.57738, policy_entropy: -5.78017, alpha: 0.00568, time: 33.12225
[CW] eval: return: 106.23590, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 65.23482, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 0.08260, qf2_loss: 0.08316, policy_loss: -18.62911, policy_entropy: -5.86088, alpha: 0.00564, time: 32.74591
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 126.81919, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 0.09187, qf2_loss: 0.09226, policy_loss: -18.64557, policy_entropy: -6.03039, alpha: 0.00563, time: 33.00624
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 51.07112, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 0.08763, qf2_loss: 0.08842, policy_loss: -18.65288, policy_entropy: -6.03820, alpha: 0.00566, time: 32.78078
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 143.34374, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 0.11213, qf2_loss: 0.11349, policy_loss: -18.57834, policy_entropy: -5.78645, alpha: 0.00562, time: 32.83486
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 126.96472, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 0.11023, qf2_loss: 0.11112, policy_loss: -18.69380, policy_entropy: -5.96601, alpha: 0.00560, time: 32.90353
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 105.08288, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 0.07841, qf2_loss: 0.07884, policy_loss: -18.64591, policy_entropy: -5.94168, alpha: 0.00557, time: 33.07702
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 132.46142, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 0.07990, qf2_loss: 0.08031, policy_loss: -18.62648, policy_entropy: -5.72604, alpha: 0.00555, time: 33.17523
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 128.00868, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 0.07910, qf2_loss: 0.07935, policy_loss: -18.68507, policy_entropy: -6.03148, alpha: 0.00550, time: 33.19602
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 120.35592, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 0.08044, qf2_loss: 0.08064, policy_loss: -18.62915, policy_entropy: -5.99782, alpha: 0.00552, time: 32.92687
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 143.52038, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 0.07630, qf2_loss: 0.07652, policy_loss: -18.57596, policy_entropy: -5.87748, alpha: 0.00549, time: 33.16542
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 121.54054, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 0.08345, qf2_loss: 0.08407, policy_loss: -18.63014, policy_entropy: -5.89220, alpha: 0.00547, time: 32.83683
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 116.45969, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 0.10145, qf2_loss: 0.10260, policy_loss: -18.57842, policy_entropy: -6.15825, alpha: 0.00547, time: 33.31826
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 132.40890, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 0.10595, qf2_loss: 0.10661, policy_loss: -18.60181, policy_entropy: -6.08979, alpha: 0.00550, time: 33.07332
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 128.63470, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 0.10135, qf2_loss: 0.10176, policy_loss: -18.55384, policy_entropy: -5.87526, alpha: 0.00549, time: 33.04567
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 120.77031, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 0.07606, qf2_loss: 0.07627, policy_loss: -18.64685, policy_entropy: -6.20921, alpha: 0.00549, time: 33.07085
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 140.21429, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 0.08496, qf2_loss: 0.08518, policy_loss: -18.64939, policy_entropy: -6.11464, alpha: 0.00553, time: 32.94414
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 126.54428, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 0.09665, qf2_loss: 0.09759, policy_loss: -18.57383, policy_entropy: -6.01355, alpha: 0.00556, time: 33.12023
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 135.26392, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 0.08770, qf2_loss: 0.08823, policy_loss: -18.50822, policy_entropy: -6.07439, alpha: 0.00556, time: 32.92921
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 26.16321, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 0.07870, qf2_loss: 0.07915, policy_loss: -18.56796, policy_entropy: -6.13405, alpha: 0.00559, time: 32.54737
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 141.69435, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 0.09953, qf2_loss: 0.10009, policy_loss: -18.57114, policy_entropy: -6.24593, alpha: 0.00563, time: 33.36820
[CW] eval: return: 124.28019, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 142.15171, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 0.10069, qf2_loss: 0.10150, policy_loss: -18.57392, policy_entropy: -6.18427, alpha: 0.00567, time: 32.94528
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 118.28587, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 0.08892, qf2_loss: 0.08936, policy_loss: -18.64839, policy_entropy: -6.14281, alpha: 0.00573, time: 32.84959
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 129.18425, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 0.09729, qf2_loss: 0.09788, policy_loss: -18.55682, policy_entropy: -6.08370, alpha: 0.00578, time: 33.31655
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 132.28352, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 0.09833, qf2_loss: 0.09906, policy_loss: -18.51412, policy_entropy: -5.84421, alpha: 0.00575, time: 32.71406
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 128.15467, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 0.08522, qf2_loss: 0.08534, policy_loss: -18.65365, policy_entropy: -5.99939, alpha: 0.00574, time: 33.37129
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 132.90490, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 0.08153, qf2_loss: 0.08182, policy_loss: -18.46956, policy_entropy: -5.82995, alpha: 0.00572, time: 32.86916
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 138.34174, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 0.10256, qf2_loss: 0.10330, policy_loss: -18.61259, policy_entropy: -6.01348, alpha: 0.00569, time: 33.21947
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 135.76618, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 0.25264, qf2_loss: 0.25552, policy_loss: -18.64823, policy_entropy: -6.68503, alpha: 0.00577, time: 32.89371
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 143.63779, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 0.09815, qf2_loss: 0.09831, policy_loss: -18.64205, policy_entropy: -6.04538, alpha: 0.00587, time: 33.65279
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 118.52097, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 0.07972, qf2_loss: 0.07997, policy_loss: -18.51060, policy_entropy: -5.82439, alpha: 0.00588, time: 33.49828
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 37.61760, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 0.08461, qf2_loss: 0.08491, policy_loss: -18.51090, policy_entropy: -5.71438, alpha: 0.00580, time: 33.48792
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 130.95641, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 0.07779, qf2_loss: 0.07813, policy_loss: -18.49475, policy_entropy: -5.86172, alpha: 0.00575, time: 33.42027
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 137.30070, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 0.07802, qf2_loss: 0.07832, policy_loss: -18.44437, policy_entropy: -5.82466, alpha: 0.00570, time: 32.83452
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 125.40211, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 0.07788, qf2_loss: 0.07834, policy_loss: -18.49663, policy_entropy: -6.01855, alpha: 0.00569, time: 33.36392
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 131.85024, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 0.08957, qf2_loss: 0.09006, policy_loss: -18.51138, policy_entropy: -6.07373, alpha: 0.00570, time: 33.64610
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 141.49938, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 0.08543, qf2_loss: 0.08568, policy_loss: -18.51122, policy_entropy: -6.07751, alpha: 0.00573, time: 33.51257
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 112.72351, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 0.09667, qf2_loss: 0.09754, policy_loss: -18.45410, policy_entropy: -6.33648, alpha: 0.00576, time: 33.07728
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 112.98290, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 0.09051, qf2_loss: 0.09081, policy_loss: -18.54869, policy_entropy: -6.16480, alpha: 0.00584, time: 33.06640
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 122.56289, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 0.09269, qf2_loss: 0.09315, policy_loss: -18.46117, policy_entropy: -6.14805, alpha: 0.00588, time: 32.91701
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 105.71688, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 0.08867, qf2_loss: 0.08911, policy_loss: -18.49411, policy_entropy: -5.89128, alpha: 0.00590, time: 33.63403
[CW] eval: return: 128.83614, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 128.59847, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 0.09825, qf2_loss: 0.09880, policy_loss: -18.49557, policy_entropy: -6.18056, alpha: 0.00589, time: 34.62253
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 130.01302, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 0.11293, qf2_loss: 0.11416, policy_loss: -18.42615, policy_entropy: -6.30055, alpha: 0.00595, time: 33.38913
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 129.46445, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 0.08858, qf2_loss: 0.08890, policy_loss: -18.51047, policy_entropy: -6.27168, alpha: 0.00607, time: 32.90362
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 126.58091, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 0.10593, qf2_loss: 0.10666, policy_loss: -18.51067, policy_entropy: -5.96043, alpha: 0.00608, time: 33.26756
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 133.28316, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 0.12561, qf2_loss: 0.12654, policy_loss: -18.49742, policy_entropy: -6.30126, alpha: 0.00612, time: 32.95649
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 130.79295, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 0.09233, qf2_loss: 0.09276, policy_loss: -18.37965, policy_entropy: -5.96743, alpha: 0.00618, time: 33.09968
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 112.99500, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 0.08882, qf2_loss: 0.08901, policy_loss: -18.48726, policy_entropy: -6.07954, alpha: 0.00615, time: 34.50302
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 130.28717, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 0.09782, qf2_loss: 0.09879, policy_loss: -18.51961, policy_entropy: -6.11011, alpha: 0.00621, time: 33.45931
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 124.75703, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 0.10364, qf2_loss: 0.10427, policy_loss: -18.44476, policy_entropy: -6.00323, alpha: 0.00623, time: 33.08860
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 136.75384, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 0.09650, qf2_loss: 0.09686, policy_loss: -18.52037, policy_entropy: -5.96207, alpha: 0.00622, time: 33.33658
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 146.61847, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 0.09817, qf2_loss: 0.09878, policy_loss: -18.54663, policy_entropy: -5.82720, alpha: 0.00619, time: 33.23945
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 140.61449, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 0.11622, qf2_loss: 0.11739, policy_loss: -18.48744, policy_entropy: -6.26930, alpha: 0.00616, time: 32.99866
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 126.62235, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 0.14267, qf2_loss: 0.14407, policy_loss: -18.47119, policy_entropy: -6.57923, alpha: 0.00633, time: 33.03867
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 130.46720, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 0.14815, qf2_loss: 0.14933, policy_loss: -18.53465, policy_entropy: -6.42184, alpha: 0.00652, time: 33.33531
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 111.27792, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 0.09490, qf2_loss: 0.09525, policy_loss: -18.53582, policy_entropy: -6.48127, alpha: 0.00665, time: 33.28465
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 134.75948, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 0.08868, qf2_loss: 0.08905, policy_loss: -18.60168, policy_entropy: -6.08999, alpha: 0.00679, time: 32.94298
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 111.73347, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 0.08784, qf2_loss: 0.08802, policy_loss: -18.53497, policy_entropy: -5.73225, alpha: 0.00673, time: 32.95113
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 138.26158, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 0.08678, qf2_loss: 0.08716, policy_loss: -18.49444, policy_entropy: -5.92122, alpha: 0.00669, time: 33.02512
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 131.65491, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 0.09338, qf2_loss: 0.09383, policy_loss: -18.57265, policy_entropy: -5.84205, alpha: 0.00662, time: 32.78631
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 139.04331, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 0.11193, qf2_loss: 0.11271, policy_loss: -18.55328, policy_entropy: -5.97014, alpha: 0.00660, time: 34.92948
[CW] eval: return: 123.92377, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 127.49333, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 0.11108, qf2_loss: 0.11180, policy_loss: -18.52001, policy_entropy: -6.08647, alpha: 0.00660, time: 33.44351
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 118.16239, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 0.09899, qf2_loss: 0.09942, policy_loss: -18.49675, policy_entropy: -5.91417, alpha: 0.00659, time: 33.08125
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 145.73894, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 0.10290, qf2_loss: 0.10350, policy_loss: -18.67191, policy_entropy: -6.05460, alpha: 0.00659, time: 33.19995
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 127.92035, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 0.09310, qf2_loss: 0.09333, policy_loss: -18.49011, policy_entropy: -5.85504, alpha: 0.00659, time: 32.91077
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 142.89491, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 0.09241, qf2_loss: 0.09277, policy_loss: -18.47690, policy_entropy: -6.18807, alpha: 0.00658, time: 32.95211
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 143.37224, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 0.08585, qf2_loss: 0.08621, policy_loss: -18.54320, policy_entropy: -6.04738, alpha: 0.00664, time: 33.03161
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 141.48853, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 0.09844, qf2_loss: 0.09882, policy_loss: -18.48710, policy_entropy: -5.66513, alpha: 0.00659, time: 33.24657
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 129.31544, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 0.08957, qf2_loss: 0.08986, policy_loss: -18.49340, policy_entropy: -5.85863, alpha: 0.00648, time: 32.78344
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 107.28618, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 0.12124, qf2_loss: 0.12227, policy_loss: -18.60342, policy_entropy: -5.95016, alpha: 0.00644, time: 33.12979
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 131.16430, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 0.10746, qf2_loss: 0.10808, policy_loss: -18.49151, policy_entropy: -5.94875, alpha: 0.00642, time: 33.22052
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 130.70445, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 0.11008, qf2_loss: 0.11039, policy_loss: -18.52414, policy_entropy: -5.92114, alpha: 0.00640, time: 33.03967
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 139.84109, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 0.13524, qf2_loss: 0.13628, policy_loss: -18.49640, policy_entropy: -6.06002, alpha: 0.00639, time: 33.51992
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 123.39456, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 0.11672, qf2_loss: 0.11779, policy_loss: -18.43563, policy_entropy: -5.87800, alpha: 0.00641, time: 32.75371
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 117.07772, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 0.10207, qf2_loss: 0.10236, policy_loss: -18.54417, policy_entropy: -5.68259, alpha: 0.00632, time: 32.77978
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 92.23333, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 0.09716, qf2_loss: 0.09744, policy_loss: -18.58036, policy_entropy: -5.73624, alpha: 0.00619, time: 33.11798
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 155.34794, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 0.09764, qf2_loss: 0.09784, policy_loss: -18.43682, policy_entropy: -5.76794, alpha: 0.00610, time: 33.05498
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 126.89489, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 0.09977, qf2_loss: 0.10029, policy_loss: -18.55060, policy_entropy: -6.13611, alpha: 0.00607, time: 33.00413
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 98.20950, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 0.10446, qf2_loss: 0.10479, policy_loss: -18.55577, policy_entropy: -6.08690, alpha: 0.00614, time: 32.70446
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 120.61599, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 0.12101, qf2_loss: 0.12210, policy_loss: -18.51260, policy_entropy: -6.07721, alpha: 0.00617, time: 32.72569
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 132.42425, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 0.10292, qf2_loss: 0.10328, policy_loss: -18.53570, policy_entropy: -6.03385, alpha: 0.00618, time: 33.54751
[CW] eval: return: 133.78131, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 130.62811, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 0.09533, qf2_loss: 0.09582, policy_loss: -18.52573, policy_entropy: -5.91893, alpha: 0.00617, time: 33.15596
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 154.21488, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 0.10693, qf2_loss: 0.10723, policy_loss: -18.46638, policy_entropy: -6.17157, alpha: 0.00617, time: 33.15604
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 147.28413, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 0.09462, qf2_loss: 0.09501, policy_loss: -18.58050, policy_entropy: -6.23194, alpha: 0.00625, time: 32.74590
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 146.03763, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 0.09913, qf2_loss: 0.09940, policy_loss: -18.60983, policy_entropy: -6.31504, alpha: 0.00635, time: 32.75188
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 137.01993, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 0.10577, qf2_loss: 0.10614, policy_loss: -18.49791, policy_entropy: -5.91160, alpha: 0.00640, time: 32.96162
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 53.39091, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 0.10020, qf2_loss: 0.10054, policy_loss: -18.57995, policy_entropy: -6.08590, alpha: 0.00640, time: 33.21623
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 127.94288, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 0.09739, qf2_loss: 0.09803, policy_loss: -18.53293, policy_entropy: -5.98950, alpha: 0.00641, time: 32.88075
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 45.24336, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 0.10156, qf2_loss: 0.10197, policy_loss: -18.55288, policy_entropy: -5.88004, alpha: 0.00639, time: 33.03659
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 107.21198, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 0.10986, qf2_loss: 0.11067, policy_loss: -18.48739, policy_entropy: -5.84747, alpha: 0.00633, time: 32.91917
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 124.52499, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 0.13785, qf2_loss: 0.13878, policy_loss: -18.55865, policy_entropy: -6.03446, alpha: 0.00633, time: 33.27305
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 127.02562, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 0.13602, qf2_loss: 0.13739, policy_loss: -18.46710, policy_entropy: -5.97315, alpha: 0.00633, time: 32.84406
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 138.94430, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 0.12629, qf2_loss: 0.12730, policy_loss: -18.61575, policy_entropy: -6.07265, alpha: 0.00632, time: 32.73104
[CW] ---------------------------
[CW] ---- Iteration:   673 ----
[CW] collect: return: 102.70234, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 0.10171, qf2_loss: 0.10204, policy_loss: -18.59108, policy_entropy: -5.84743, alpha: 0.00632, time: 32.86894
[CW] ---------------------------
[CW] ---- Iteration:   674 ----
[CW] collect: return: 111.79836, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 0.09202, qf2_loss: 0.09230, policy_loss: -18.63678, policy_entropy: -5.79424, alpha: 0.00626, time: 32.73303
[CW] ---------------------------
[CW] ---- Iteration:   675 ----
[CW] collect: return: 137.19470, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 0.10550, qf2_loss: 0.10609, policy_loss: -18.66462, policy_entropy: -6.00913, alpha: 0.00622, time: 33.20327
[CW] ---------------------------
[CW] ---- Iteration:   676 ----
[CW] collect: return: 130.38964, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 0.11099, qf2_loss: 0.11170, policy_loss: -18.55938, policy_entropy: -5.52066, alpha: 0.00614, time: 33.10816
[CW] ---------------------------
[CW] ---- Iteration:   677 ----
[CW] collect: return: 105.80547, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 0.10629, qf2_loss: 0.10675, policy_loss: -18.59712, policy_entropy: -5.94451, alpha: 0.00606, time: 32.88238
[CW] ---------------------------
[CW] ---- Iteration:   678 ----
[CW] collect: return: 129.44980, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 0.11573, qf2_loss: 0.11640, policy_loss: -18.58545, policy_entropy: -6.04665, alpha: 0.00605, time: 32.68925
[CW] ---------------------------
[CW] ---- Iteration:   679 ----
[CW] collect: return: 128.78190, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 0.11876, qf2_loss: 0.11938, policy_loss: -18.68855, policy_entropy: -6.01801, alpha: 0.00608, time: 32.82558
[CW] ---------------------------
[CW] ---- Iteration:   680 ----
[CW] collect: return: 127.70451, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 0.10343, qf2_loss: 0.10376, policy_loss: -18.54412, policy_entropy: -5.86619, alpha: 0.00606, time: 33.01196
[CW] eval: return: 113.42651, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   681 ----
[CW] collect: return: 108.29221, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 0.12067, qf2_loss: 0.12146, policy_loss: -18.67125, policy_entropy: -5.88861, alpha: 0.00603, time: 33.08077
[CW] ---------------------------
[CW] ---- Iteration:   682 ----
[CW] collect: return: 137.33780, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 0.13577, qf2_loss: 0.13698, policy_loss: -18.62309, policy_entropy: -5.91199, alpha: 0.00598, time: 33.20260
[CW] ---------------------------
[CW] ---- Iteration:   683 ----
[CW] collect: return: 139.72503, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 0.15927, qf2_loss: 0.16069, policy_loss: -18.57825, policy_entropy: -5.81926, alpha: 0.00594, time: 32.83250
[CW] ---------------------------
[CW] ---- Iteration:   684 ----
[CW] collect: return: 146.24383, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 0.13653, qf2_loss: 0.13754, policy_loss: -18.63086, policy_entropy: -6.07359, alpha: 0.00593, time: 33.37824
[CW] ---------------------------
[CW] ---- Iteration:   685 ----
[CW] collect: return: 151.35289, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 0.10873, qf2_loss: 0.10920, policy_loss: -18.77046, policy_entropy: -6.02001, alpha: 0.00594, time: 32.85676
[CW] ---------------------------
[CW] ---- Iteration:   686 ----
[CW] collect: return: 142.78619, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 0.09414, qf2_loss: 0.09430, policy_loss: -18.67566, policy_entropy: -6.19180, alpha: 0.00597, time: 32.89807
[CW] ---------------------------
[CW] ---- Iteration:   687 ----
[CW] collect: return: 144.63057, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 0.08719, qf2_loss: 0.08727, policy_loss: -18.66955, policy_entropy: -6.08154, alpha: 0.00601, time: 33.34537
[CW] ---------------------------
[CW] ---- Iteration:   688 ----
[CW] collect: return: 130.83262, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 0.09141, qf2_loss: 0.09175, policy_loss: -18.70153, policy_entropy: -6.20284, alpha: 0.00606, time: 32.99417
[CW] ---------------------------
[CW] ---- Iteration:   689 ----
[CW] collect: return: 131.33379, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 0.09619, qf2_loss: 0.09672, policy_loss: -18.79916, policy_entropy: -6.13703, alpha: 0.00611, time: 32.76240
[CW] ---------------------------
[CW] ---- Iteration:   690 ----
[CW] collect: return: 132.46372, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 0.09291, qf2_loss: 0.09304, policy_loss: -18.61050, policy_entropy: -5.96716, alpha: 0.00613, time: 33.26280
[CW] ---------------------------
[CW] ---- Iteration:   691 ----
[CW] collect: return: 42.49638, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 0.12059, qf2_loss: 0.12155, policy_loss: -18.74560, policy_entropy: -5.97673, alpha: 0.00612, time: 33.40412
[CW] ---------------------------
[CW] ---- Iteration:   692 ----
[CW] collect: return: 61.59170, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 0.11115, qf2_loss: 0.11159, policy_loss: -18.68838, policy_entropy: -5.96644, alpha: 0.00611, time: 33.26408
[CW] ---------------------------
[CW] ---- Iteration:   693 ----
[CW] collect: return: 120.29861, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 0.11836, qf2_loss: 0.11931, policy_loss: -18.65177, policy_entropy: -6.11328, alpha: 0.00615, time: 32.78357
[CW] ---------------------------
[CW] ---- Iteration:   694 ----
[CW] collect: return: 112.37996, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 0.12822, qf2_loss: 0.12984, policy_loss: -18.76530, policy_entropy: -6.11053, alpha: 0.00615, time: 33.10700
[CW] ---------------------------
[CW] ---- Iteration:   695 ----
[CW] collect: return: 111.85371, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 0.12596, qf2_loss: 0.12622, policy_loss: -18.59864, policy_entropy: -6.23453, alpha: 0.00624, time: 33.01384
[CW] ---------------------------
[CW] ---- Iteration:   696 ----
[CW] collect: return: 125.87478, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 0.13184, qf2_loss: 0.13232, policy_loss: -18.68380, policy_entropy: -5.99041, alpha: 0.00627, time: 32.86843
[CW] ---------------------------
[CW] ---- Iteration:   697 ----
[CW] collect: return: 162.73094, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 0.13571, qf2_loss: 0.13669, policy_loss: -18.68059, policy_entropy: -6.33322, alpha: 0.00632, time: 33.23955
[CW] ---------------------------
[CW] ---- Iteration:   698 ----
[CW] collect: return: 126.94705, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 0.10432, qf2_loss: 0.10464, policy_loss: -18.66910, policy_entropy: -6.26421, alpha: 0.00643, time: 32.78241
[CW] ---------------------------
[CW] ---- Iteration:   699 ----
[CW] collect: return: 147.41936, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 0.09720, qf2_loss: 0.09745, policy_loss: -18.71928, policy_entropy: -6.14440, alpha: 0.00650, time: 32.73370
[CW] ---------------------------
[CW] ---- Iteration:   700 ----
[CW] collect: return: 133.48439, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 0.10539, qf2_loss: 0.10578, policy_loss: -18.74130, policy_entropy: -6.16639, alpha: 0.00655, time: 32.97302
[CW] eval: return: 132.71419, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   701 ----
[CW] collect: return: 125.55735, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 0.14364, qf2_loss: 0.14441, policy_loss: -18.62547, policy_entropy: -5.96092, alpha: 0.00658, time: 32.71615
[CW] ---------------------------
[CW] ---- Iteration:   702 ----
[CW] collect: return: 139.61427, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 0.10591, qf2_loss: 0.10620, policy_loss: -18.77393, policy_entropy: -6.01438, alpha: 0.00659, time: 32.88295
[CW] ---------------------------
[CW] ---- Iteration:   703 ----
[CW] collect: return: 123.33999, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 0.09599, qf2_loss: 0.09632, policy_loss: -18.66829, policy_entropy: -5.93632, alpha: 0.00658, time: 32.75506
[CW] ---------------------------
[CW] ---- Iteration:   704 ----
[CW] collect: return: 107.62222, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 0.09938, qf2_loss: 0.09965, policy_loss: -18.68901, policy_entropy: -5.83723, alpha: 0.00653, time: 33.20628
[CW] ---------------------------
[CW] ---- Iteration:   705 ----
[CW] collect: return: 145.25699, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 0.11928, qf2_loss: 0.11996, policy_loss: -18.87419, policy_entropy: -6.00693, alpha: 0.00650, time: 33.26479
[CW] ---------------------------
[CW] ---- Iteration:   706 ----
[CW] collect: return: 131.68526, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 0.12121, qf2_loss: 0.12224, policy_loss: -18.80544, policy_entropy: -6.14414, alpha: 0.00652, time: 33.02578
[CW] ---------------------------
[CW] ---- Iteration:   707 ----
[CW] collect: return: 137.47654, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 0.10311, qf2_loss: 0.10323, policy_loss: -18.78040, policy_entropy: -6.02486, alpha: 0.00657, time: 33.04372
[CW] ---------------------------
[CW] ---- Iteration:   708 ----
[CW] collect: return: 141.34398, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 0.11003, qf2_loss: 0.11072, policy_loss: -18.75971, policy_entropy: -5.85406, alpha: 0.00655, time: 32.83151
[CW] ---------------------------
[CW] ---- Iteration:   709 ----
[CW] collect: return: 135.18460, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 0.11380, qf2_loss: 0.11422, policy_loss: -18.78352, policy_entropy: -5.83755, alpha: 0.00649, time: 33.12255
[CW] ---------------------------
[CW] ---- Iteration:   710 ----
[CW] collect: return: 121.79940, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 0.10863, qf2_loss: 0.10899, policy_loss: -18.81350, policy_entropy: -5.79291, alpha: 0.00641, time: 32.87904
[CW] ---------------------------
[CW] ---- Iteration:   711 ----
[CW] collect: return: 125.26832, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 0.11199, qf2_loss: 0.11259, policy_loss: -18.90805, policy_entropy: -6.16699, alpha: 0.00640, time: 33.39552
[CW] ---------------------------
[CW] ---- Iteration:   712 ----
[CW] collect: return: 147.38919, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 0.11679, qf2_loss: 0.11700, policy_loss: -18.81037, policy_entropy: -6.10273, alpha: 0.00641, time: 33.39575
[CW] ---------------------------
[CW] ---- Iteration:   713 ----
[CW] collect: return: 97.70976, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 0.11834, qf2_loss: 0.11918, policy_loss: -18.76356, policy_entropy: -6.05307, alpha: 0.00650, time: 33.09420
[CW] ---------------------------
[CW] ---- Iteration:   714 ----
[CW] collect: return: 40.15688, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 0.10372, qf2_loss: 0.10427, policy_loss: -18.80625, policy_entropy: -6.08330, alpha: 0.00651, time: 32.87025
[CW] ---------------------------
[CW] ---- Iteration:   715 ----
[CW] collect: return: 118.06214, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 0.14643, qf2_loss: 0.14771, policy_loss: -18.86723, policy_entropy: -5.90064, alpha: 0.00652, time: 32.91763
[CW] ---------------------------
[CW] ---- Iteration:   716 ----
[CW] collect: return: 136.03012, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 0.11617, qf2_loss: 0.11659, policy_loss: -18.83158, policy_entropy: -5.89004, alpha: 0.00647, time: 33.44645
[CW] ---------------------------
[CW] ---- Iteration:   717 ----
[CW] collect: return: 126.73775, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 0.10624, qf2_loss: 0.10641, policy_loss: -18.79933, policy_entropy: -5.79222, alpha: 0.00640, time: 33.31321
[CW] ---------------------------
[CW] ---- Iteration:   718 ----
[CW] collect: return: 138.90641, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 0.11861, qf2_loss: 0.11914, policy_loss: -18.87369, policy_entropy: -6.16387, alpha: 0.00640, time: 33.00322
[CW] ---------------------------
[CW] ---- Iteration:   719 ----
[CW] collect: return: 112.92927, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 0.10455, qf2_loss: 0.10487, policy_loss: -18.87213, policy_entropy: -6.08344, alpha: 0.00644, time: 33.04616
[CW] ---------------------------
[CW] ---- Iteration:   720 ----
[CW] collect: return: 24.15325, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 0.09454, qf2_loss: 0.09486, policy_loss: -18.79025, policy_entropy: -6.13294, alpha: 0.00647, time: 33.32141
[CW] eval: return: 137.21304, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   721 ----
[CW] collect: return: 139.69167, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 0.09441, qf2_loss: 0.09462, policy_loss: -18.87994, policy_entropy: -6.06936, alpha: 0.00653, time: 33.54574
[CW] ---------------------------
[CW] ---- Iteration:   722 ----
[CW] collect: return: 138.47515, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 0.09368, qf2_loss: 0.09413, policy_loss: -18.91956, policy_entropy: -5.91055, alpha: 0.00651, time: 33.26675
[CW] ---------------------------
[CW] ---- Iteration:   723 ----
[CW] collect: return: 146.49744, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 0.09574, qf2_loss: 0.09599, policy_loss: -18.85487, policy_entropy: -6.03375, alpha: 0.00651, time: 33.45569
[CW] ---------------------------
[CW] ---- Iteration:   724 ----
[CW] collect: return: 135.29328, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 0.09972, qf2_loss: 0.10031, policy_loss: -18.75773, policy_entropy: -6.16331, alpha: 0.00654, time: 33.34667
[CW] ---------------------------
[CW] ---- Iteration:   725 ----
[CW] collect: return: 129.53582, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 0.10897, qf2_loss: 0.10953, policy_loss: -18.81761, policy_entropy: -6.03895, alpha: 0.00659, time: 33.22264
[CW] ---------------------------
[CW] ---- Iteration:   726 ----
[CW] collect: return: 145.04869, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 0.10549, qf2_loss: 0.10585, policy_loss: -18.87020, policy_entropy: -6.14887, alpha: 0.00661, time: 34.62262
[CW] ---------------------------
[CW] ---- Iteration:   727 ----
[CW] collect: return: 154.54401, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 0.11090, qf2_loss: 0.11126, policy_loss: -18.89328, policy_entropy: -5.98044, alpha: 0.00664, time: 33.52405
[CW] ---------------------------
[CW] ---- Iteration:   728 ----
[CW] collect: return: 45.40322, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 0.28975, qf2_loss: 0.29282, policy_loss: -18.78704, policy_entropy: -6.34304, alpha: 0.00667, time: 33.39968
[CW] ---------------------------
[CW] ---- Iteration:   729 ----
[CW] collect: return: 115.99074, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 0.13351, qf2_loss: 0.13389, policy_loss: -18.82743, policy_entropy: -5.91109, alpha: 0.00676, time: 33.43875
[CW] ---------------------------
[CW] ---- Iteration:   730 ----
[CW] collect: return: 145.43089, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 0.09943, qf2_loss: 0.09964, policy_loss: -18.91174, policy_entropy: -6.08913, alpha: 0.00675, time: 33.00689
[CW] ---------------------------
[CW] ---- Iteration:   731 ----
[CW] collect: return: 118.56375, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 0.08757, qf2_loss: 0.08770, policy_loss: -18.88871, policy_entropy: -5.91797, alpha: 0.00675, time: 33.38127
[CW] ---------------------------
[CW] ---- Iteration:   732 ----
[CW] collect: return: 113.96189, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 0.09613, qf2_loss: 0.09658, policy_loss: -18.97837, policy_entropy: -6.15446, alpha: 0.00677, time: 35.55146
[CW] ---------------------------
[CW] ---- Iteration:   733 ----
[CW] collect: return: 142.46437, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 0.09674, qf2_loss: 0.09692, policy_loss: -18.91717, policy_entropy: -6.00168, alpha: 0.00678, time: 33.23816
[CW] ---------------------------
[CW] ---- Iteration:   734 ----
[CW] collect: return: 136.36134, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 0.08740, qf2_loss: 0.08765, policy_loss: -18.92311, policy_entropy: -6.16785, alpha: 0.00683, time: 33.31797
[CW] ---------------------------
[CW] ---- Iteration:   735 ----
[CW] collect: return: 142.61348, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 0.09192, qf2_loss: 0.09213, policy_loss: -18.90531, policy_entropy: -5.93661, alpha: 0.00685, time: 32.83240
[CW] ---------------------------
[CW] ---- Iteration:   736 ----
[CW] collect: return: 115.66949, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 0.10908, qf2_loss: 0.10929, policy_loss: -18.97152, policy_entropy: -6.22479, alpha: 0.00686, time: 33.23614
[CW] ---------------------------
[CW] ---- Iteration:   737 ----
[CW] collect: return: 121.08183, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 0.09996, qf2_loss: 0.10049, policy_loss: -18.95272, policy_entropy: -6.19584, alpha: 0.00695, time: 33.19382
[CW] ---------------------------
[CW] ---- Iteration:   738 ----
[CW] collect: return: 128.18226, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 0.10392, qf2_loss: 0.10443, policy_loss: -18.81110, policy_entropy: -6.05500, alpha: 0.00702, time: 32.87364
[CW] ---------------------------
[CW] ---- Iteration:   739 ----
[CW] collect: return: 83.84488, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 0.10057, qf2_loss: 0.10080, policy_loss: -18.92358, policy_entropy: -5.95527, alpha: 0.00704, time: 32.70764
[CW] ---------------------------
[CW] ---- Iteration:   740 ----
[CW] collect: return: 143.16266, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 0.09481, qf2_loss: 0.09503, policy_loss: -18.94002, policy_entropy: -5.77305, alpha: 0.00696, time: 33.10023
[CW] eval: return: 135.48665, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   741 ----
[CW] collect: return: 135.54030, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 0.09999, qf2_loss: 0.10052, policy_loss: -19.03099, policy_entropy: -5.82076, alpha: 0.00690, time: 33.33593
[CW] ---------------------------
[CW] ---- Iteration:   742 ----
[CW] collect: return: 146.34768, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 0.11340, qf2_loss: 0.11384, policy_loss: -19.00836, policy_entropy: -5.65687, alpha: 0.00680, time: 33.25769
[CW] ---------------------------
[CW] ---- Iteration:   743 ----
[CW] collect: return: 123.25903, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 0.13459, qf2_loss: 0.13524, policy_loss: -19.09365, policy_entropy: -6.14194, alpha: 0.00673, time: 33.26838
[CW] ---------------------------
[CW] ---- Iteration:   744 ----
[CW] collect: return: 135.45622, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 0.17920, qf2_loss: 0.18101, policy_loss: -18.99956, policy_entropy: -6.27158, alpha: 0.00682, time: 34.63958
[CW] ---------------------------
[CW] ---- Iteration:   745 ----
[CW] collect: return: 125.90782, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 0.10640, qf2_loss: 0.10675, policy_loss: -19.01673, policy_entropy: -6.04995, alpha: 0.00687, time: 33.39061
[CW] ---------------------------
[CW] ---- Iteration:   746 ----
[CW] collect: return: 111.24861, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 0.09229, qf2_loss: 0.09257, policy_loss: -18.99547, policy_entropy: -6.25940, alpha: 0.00694, time: 33.27079
[CW] ---------------------------
[CW] ---- Iteration:   747 ----
[CW] collect: return: 136.43699, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 0.08907, qf2_loss: 0.08914, policy_loss: -19.08194, policy_entropy: -6.20481, alpha: 0.00703, time: 33.26020
[CW] ---------------------------
[CW] ---- Iteration:   748 ----
[CW] collect: return: 136.26994, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 0.09947, qf2_loss: 0.09987, policy_loss: -19.06103, policy_entropy: -6.18466, alpha: 0.00711, time: 33.37834
[CW] ---------------------------
[CW] ---- Iteration:   749 ----
[CW] collect: return: 26.51671, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 0.09397, qf2_loss: 0.09421, policy_loss: -19.01959, policy_entropy: -6.30796, alpha: 0.00721, time: 33.36623
[CW] ---------------------------
[CW] ---- Iteration:   750 ----
[CW] collect: return: 111.64774, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 0.10901, qf2_loss: 0.10940, policy_loss: -19.03447, policy_entropy: -6.05596, alpha: 0.00731, time: 32.84811
[CW] ---------------------------
[CW] ---- Iteration:   751 ----
[CW] collect: return: 122.83403, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 0.10053, qf2_loss: 0.10074, policy_loss: -19.05592, policy_entropy: -5.95055, alpha: 0.00730, time: 33.51006
[CW] ---------------------------
[CW] ---- Iteration:   752 ----
[CW] collect: return: 123.21800, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 0.09938, qf2_loss: 0.09994, policy_loss: -19.03128, policy_entropy: -6.01849, alpha: 0.00727, time: 32.91888
[CW] ---------------------------
[CW] ---- Iteration:   753 ----
[CW] collect: return: 141.01965, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 0.09757, qf2_loss: 0.09795, policy_loss: -18.92349, policy_entropy: -5.98879, alpha: 0.00728, time: 32.70523
[CW] ---------------------------
[CW] ---- Iteration:   754 ----
[CW] collect: return: 144.82327, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 0.10478, qf2_loss: 0.10494, policy_loss: -18.92073, policy_entropy: -5.91103, alpha: 0.00726, time: 33.35967
[CW] ---------------------------
[CW] ---- Iteration:   755 ----
[CW] collect: return: 136.86123, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 0.11169, qf2_loss: 0.11216, policy_loss: -19.08072, policy_entropy: -5.92631, alpha: 0.00723, time: 32.94569
[CW] ---------------------------
[CW] ---- Iteration:   756 ----
[CW] collect: return: 117.75558, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 0.10545, qf2_loss: 0.10579, policy_loss: -19.04286, policy_entropy: -6.04586, alpha: 0.00723, time: 33.92400
[CW] ---------------------------
[CW] ---- Iteration:   757 ----
[CW] collect: return: 130.70384, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 0.10626, qf2_loss: 0.10653, policy_loss: -19.17571, policy_entropy: -5.82054, alpha: 0.00720, time: 32.64841
[CW] ---------------------------
[CW] ---- Iteration:   758 ----
[CW] collect: return: 136.43835, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 0.26138, qf2_loss: 0.26259, policy_loss: -19.06631, policy_entropy: -6.09709, alpha: 0.00718, time: 32.73752
[CW] ---------------------------
[CW] ---- Iteration:   759 ----
[CW] collect: return: 93.35489, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 0.16524, qf2_loss: 0.16599, policy_loss: -19.06806, policy_entropy: -5.93371, alpha: 0.00719, time: 33.14667
[CW] ---------------------------
[CW] ---- Iteration:   760 ----
[CW] collect: return: 139.67511, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 0.10738, qf2_loss: 0.10774, policy_loss: -19.02167, policy_entropy: -6.16424, alpha: 0.00718, time: 33.14226
[CW] eval: return: 135.34434, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   761 ----
[CW] collect: return: 140.50938, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 0.08883, qf2_loss: 0.08902, policy_loss: -19.05489, policy_entropy: -6.12391, alpha: 0.00727, time: 33.42271
[CW] ---------------------------
[CW] ---- Iteration:   762 ----
[CW] collect: return: 136.40347, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 0.09052, qf2_loss: 0.09065, policy_loss: -19.13129, policy_entropy: -6.11353, alpha: 0.00732, time: 33.13014
[CW] ---------------------------
[CW] ---- Iteration:   763 ----
[CW] collect: return: 140.54883, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 0.09001, qf2_loss: 0.09035, policy_loss: -19.15993, policy_entropy: -6.03784, alpha: 0.00736, time: 33.39257
[CW] ---------------------------
[CW] ---- Iteration:   764 ----
[CW] collect: return: 134.43601, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 0.08631, qf2_loss: 0.08637, policy_loss: -19.16730, policy_entropy: -6.08313, alpha: 0.00738, time: 33.48206
[CW] ---------------------------
[CW] ---- Iteration:   765 ----
[CW] collect: return: 145.82844, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 0.08713, qf2_loss: 0.08735, policy_loss: -19.17189, policy_entropy: -5.86865, alpha: 0.00738, time: 33.39270
[CW] ---------------------------
[CW] ---- Iteration:   766 ----
[CW] collect: return: 146.55708, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 0.09306, qf2_loss: 0.09335, policy_loss: -19.11924, policy_entropy: -5.99437, alpha: 0.00736, time: 33.50719
[CW] ---------------------------
[CW] ---- Iteration:   767 ----
[CW] collect: return: 102.62883, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 0.09587, qf2_loss: 0.09597, policy_loss: -19.16056, policy_entropy: -6.06375, alpha: 0.00733, time: 32.93614
[CW] ---------------------------
[CW] ---- Iteration:   768 ----
[CW] collect: return: 116.93801, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 0.11711, qf2_loss: 0.11748, policy_loss: -19.06626, policy_entropy: -6.13530, alpha: 0.00738, time: 32.80721
[CW] ---------------------------
[CW] ---- Iteration:   769 ----
[CW] collect: return: 156.11656, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 0.10990, qf2_loss: 0.11014, policy_loss: -19.22739, policy_entropy: -6.17808, alpha: 0.00745, time: 33.15595
[CW] ---------------------------
[CW] ---- Iteration:   770 ----
[CW] collect: return: 145.22005, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 0.10674, qf2_loss: 0.10701, policy_loss: -19.18317, policy_entropy: -6.07105, alpha: 0.00755, time: 33.26008
[CW] ---------------------------
[CW] ---- Iteration:   771 ----
[CW] collect: return: 121.21202, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 0.09888, qf2_loss: 0.09943, policy_loss: -19.21465, policy_entropy: -5.99288, alpha: 0.00755, time: 33.11181
[CW] ---------------------------
[CW] ---- Iteration:   772 ----
[CW] collect: return: 126.96425, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 0.10482, qf2_loss: 0.10540, policy_loss: -19.23701, policy_entropy: -6.01458, alpha: 0.00753, time: 33.05099
[CW] ---------------------------
[CW] ---- Iteration:   773 ----
[CW] collect: return: 140.98554, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 0.09813, qf2_loss: 0.09820, policy_loss: -19.12078, policy_entropy: -5.60153, alpha: 0.00746, time: 33.43913
[CW] ---------------------------
[CW] ---- Iteration:   774 ----
[CW] collect: return: 144.65679, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 0.08568, qf2_loss: 0.08583, policy_loss: -19.13241, policy_entropy: -6.04679, alpha: 0.00735, time: 32.98173
[CW] ---------------------------
[CW] ---- Iteration:   775 ----
[CW] collect: return: 140.26304, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 0.09558, qf2_loss: 0.09591, policy_loss: -19.27465, policy_entropy: -6.08037, alpha: 0.00741, time: 33.23827
[CW] ---------------------------
[CW] ---- Iteration:   776 ----
[CW] collect: return: 117.41266, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 0.10075, qf2_loss: 0.10121, policy_loss: -19.14090, policy_entropy: -6.03466, alpha: 0.00741, time: 33.36406
[CW] ---------------------------
[CW] ---- Iteration:   777 ----
[CW] collect: return: 139.00684, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 0.09963, qf2_loss: 0.09981, policy_loss: -19.45107, policy_entropy: -6.28246, alpha: 0.00747, time: 33.25400
[CW] ---------------------------
[CW] ---- Iteration:   778 ----
[CW] collect: return: 140.92141, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 0.10508, qf2_loss: 0.10537, policy_loss: -19.25909, policy_entropy: -6.22829, alpha: 0.00760, time: 32.93528
[CW] ---------------------------
[CW] ---- Iteration:   779 ----
[CW] collect: return: 146.95173, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 0.09231, qf2_loss: 0.09245, policy_loss: -19.22941, policy_entropy: -6.14513, alpha: 0.00770, time: 33.45836
[CW] ---------------------------
[CW] ---- Iteration:   780 ----
[CW] collect: return: 21.42414, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 0.10269, qf2_loss: 0.10346, policy_loss: -19.25553, policy_entropy: -6.07627, alpha: 0.00775, time: 33.32564
[CW] eval: return: 141.33668, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   781 ----
[CW] collect: return: 149.64112, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 0.16702, qf2_loss: 0.16717, policy_loss: -19.20306, policy_entropy: -5.46181, alpha: 0.00772, time: 33.01138
[CW] ---------------------------
[CW] ---- Iteration:   782 ----
[CW] collect: return: 136.52923, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 0.12512, qf2_loss: 0.12538, policy_loss: -19.20160, policy_entropy: -5.84972, alpha: 0.00750, time: 32.63254
[CW] ---------------------------
[CW] ---- Iteration:   783 ----
[CW] collect: return: 144.03255, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 0.10618, qf2_loss: 0.10650, policy_loss: -19.20683, policy_entropy: -6.03794, alpha: 0.00746, time: 32.86638
[CW] ---------------------------
[CW] ---- Iteration:   784 ----
[CW] collect: return: 138.88581, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 0.09298, qf2_loss: 0.09330, policy_loss: -19.11094, policy_entropy: -5.95708, alpha: 0.00748, time: 33.26362
[CW] ---------------------------
[CW] ---- Iteration:   785 ----
[CW] collect: return: 128.11917, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 0.08256, qf2_loss: 0.08280, policy_loss: -19.22334, policy_entropy: -5.90578, alpha: 0.00745, time: 32.89068
[CW] ---------------------------
[CW] ---- Iteration:   786 ----
[CW] collect: return: 118.77317, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 0.08837, qf2_loss: 0.08872, policy_loss: -19.30331, policy_entropy: -5.84672, alpha: 0.00740, time: 32.81892
[CW] ---------------------------
[CW] ---- Iteration:   787 ----
[CW] collect: return: 137.74786, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 0.09268, qf2_loss: 0.09283, policy_loss: -19.34089, policy_entropy: -6.11172, alpha: 0.00738, time: 33.45111
[CW] ---------------------------
[CW] ---- Iteration:   788 ----
[CW] collect: return: 152.80817, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 0.09713, qf2_loss: 0.09724, policy_loss: -19.37616, policy_entropy: -6.04556, alpha: 0.00742, time: 33.09094
[CW] ---------------------------
[CW] ---- Iteration:   789 ----
[CW] collect: return: 139.85077, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 0.10212, qf2_loss: 0.10231, policy_loss: -19.28820, policy_entropy: -5.87996, alpha: 0.00739, time: 33.40166
[CW] ---------------------------
[CW] ---- Iteration:   790 ----
[CW] collect: return: 148.95309, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 0.13354, qf2_loss: 0.13439, policy_loss: -19.34708, policy_entropy: -5.88482, alpha: 0.00736, time: 33.45036
[CW] ---------------------------
[CW] ---- Iteration:   791 ----
[CW] collect: return: 138.82651, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 0.12247, qf2_loss: 0.12320, policy_loss: -19.25286, policy_entropy: -6.25083, alpha: 0.00738, time: 33.11178
[CW] ---------------------------
[CW] ---- Iteration:   792 ----
[CW] collect: return: 127.47567, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 0.09565, qf2_loss: 0.09600, policy_loss: -19.28516, policy_entropy: -5.98802, alpha: 0.00743, time: 32.90928
[CW] ---------------------------
[CW] ---- Iteration:   793 ----
[CW] collect: return: 140.46980, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 0.09153, qf2_loss: 0.09164, policy_loss: -19.24901, policy_entropy: -6.00319, alpha: 0.00744, time: 33.26450
[CW] ---------------------------
[CW] ---- Iteration:   794 ----
[CW] collect: return: 166.20951, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 0.09037, qf2_loss: 0.09076, policy_loss: -19.36728, policy_entropy: -6.01139, alpha: 0.00743, time: 32.91252
[CW] ---------------------------
[CW] ---- Iteration:   795 ----
[CW] collect: return: 152.92802, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 0.08862, qf2_loss: 0.08885, policy_loss: -19.29311, policy_entropy: -5.99402, alpha: 0.00743, time: 33.34120
[CW] ---------------------------
[CW] ---- Iteration:   796 ----
[CW] collect: return: 138.22785, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 0.10173, qf2_loss: 0.10234, policy_loss: -19.40064, policy_entropy: -6.05004, alpha: 0.00745, time: 33.11727
[CW] ---------------------------
[CW] ---- Iteration:   797 ----
[CW] collect: return: 122.46361, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 0.09854, qf2_loss: 0.09881, policy_loss: -19.34603, policy_entropy: -5.89954, alpha: 0.00743, time: 33.16914
[CW] ---------------------------
[CW] ---- Iteration:   798 ----
[CW] collect: return: 126.20369, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 0.09860, qf2_loss: 0.09883, policy_loss: -19.29294, policy_entropy: -5.96972, alpha: 0.00742, time: 32.71629
[CW] ---------------------------
[CW] ---- Iteration:   799 ----
[CW] collect: return: 136.75610, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 0.12260, qf2_loss: 0.12369, policy_loss: -19.35020, policy_entropy: -6.15776, alpha: 0.00741, time: 32.86032
[CW] ---------------------------
[CW] ---- Iteration:   800 ----
[CW] collect: return: 133.74847, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 0.11324, qf2_loss: 0.11356, policy_loss: -19.44210, policy_entropy: -5.91008, alpha: 0.00746, time: 33.16673
[CW] eval: return: 134.14282, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   801 ----
[CW] collect: return: 136.79627, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 0.09614, qf2_loss: 0.09645, policy_loss: -19.31632, policy_entropy: -5.89312, alpha: 0.00741, time: 32.75012
[CW] ---------------------------
[CW] ---- Iteration:   802 ----
[CW] collect: return: 139.61205, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 0.09257, qf2_loss: 0.09276, policy_loss: -19.46362, policy_entropy: -6.01274, alpha: 0.00741, time: 33.78791
[CW] ---------------------------
[CW] ---- Iteration:   803 ----
[CW] collect: return: 159.62662, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 0.08941, qf2_loss: 0.08968, policy_loss: -19.42530, policy_entropy: -6.00984, alpha: 0.00740, time: 32.80205
[CW] ---------------------------
[CW] ---- Iteration:   804 ----
[CW] collect: return: 139.96625, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 0.10398, qf2_loss: 0.10464, policy_loss: -19.44380, policy_entropy: -6.15790, alpha: 0.00741, time: 32.80546
[CW] ---------------------------
[CW] ---- Iteration:   805 ----
[CW] collect: return: 150.94256, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 0.10569, qf2_loss: 0.10623, policy_loss: -19.42658, policy_entropy: -6.33117, alpha: 0.00752, time: 33.16322
[CW] ---------------------------
[CW] ---- Iteration:   806 ----
[CW] collect: return: 149.93493, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 0.11048, qf2_loss: 0.11121, policy_loss: -19.41718, policy_entropy: -6.09739, alpha: 0.00764, time: 32.75369
[CW] ---------------------------
[CW] ---- Iteration:   807 ----
[CW] collect: return: 121.77493, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 0.10458, qf2_loss: 0.10486, policy_loss: -19.46000, policy_entropy: -5.86540, alpha: 0.00767, time: 32.78415
[CW] ---------------------------
[CW] ---- Iteration:   808 ----
[CW] collect: return: 118.76262, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 0.09222, qf2_loss: 0.09246, policy_loss: -19.52045, policy_entropy: -5.99159, alpha: 0.00760, time: 33.11799
[CW] ---------------------------
[CW] ---- Iteration:   809 ----
[CW] collect: return: 154.11706, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 0.10662, qf2_loss: 0.10668, policy_loss: -19.43620, policy_entropy: -5.92029, alpha: 0.00760, time: 33.07780
[CW] ---------------------------
[CW] ---- Iteration:   810 ----
[CW] collect: return: 147.28145, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 0.10084, qf2_loss: 0.10114, policy_loss: -19.60129, policy_entropy: -6.21082, alpha: 0.00761, time: 33.14964
[CW] ---------------------------
[CW] ---- Iteration:   811 ----
[CW] collect: return: 135.19554, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 0.11921, qf2_loss: 0.11997, policy_loss: -19.47593, policy_entropy: -6.12995, alpha: 0.00768, time: 32.99110
[CW] ---------------------------
[CW] ---- Iteration:   812 ----
[CW] collect: return: 115.77208, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 0.11980, qf2_loss: 0.11992, policy_loss: -19.38477, policy_entropy: -5.96467, alpha: 0.00769, time: 32.73225
[CW] ---------------------------
[CW] ---- Iteration:   813 ----
[CW] collect: return: 127.91578, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 0.11619, qf2_loss: 0.11671, policy_loss: -19.42428, policy_entropy: -6.20719, alpha: 0.00776, time: 32.88811
[CW] ---------------------------
[CW] ---- Iteration:   814 ----
[CW] collect: return: 40.66290, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 0.10168, qf2_loss: 0.10181, policy_loss: -19.44073, policy_entropy: -6.14524, alpha: 0.00783, time: 32.85668
[CW] ---------------------------
[CW] ---- Iteration:   815 ----
[CW] collect: return: 111.27216, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 0.09556, qf2_loss: 0.09592, policy_loss: -19.50529, policy_entropy: -6.14883, alpha: 0.00792, time: 33.13905
[CW] ---------------------------
[CW] ---- Iteration:   816 ----
[CW] collect: return: 28.10659, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 0.11467, qf2_loss: 0.11533, policy_loss: -19.48814, policy_entropy: -5.89923, alpha: 0.00796, time: 33.14751
[CW] ---------------------------
[CW] ---- Iteration:   817 ----
[CW] collect: return: 147.90375, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 0.09995, qf2_loss: 0.10025, policy_loss: -19.47309, policy_entropy: -5.52214, alpha: 0.00779, time: 33.35592
[CW] ---------------------------
[CW] ---- Iteration:   818 ----
[CW] collect: return: 147.59942, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 0.11193, qf2_loss: 0.11231, policy_loss: -19.62287, policy_entropy: -5.96712, alpha: 0.00769, time: 32.91001
[CW] ---------------------------
[CW] ---- Iteration:   819 ----
[CW] collect: return: 144.38137, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 0.10640, qf2_loss: 0.10688, policy_loss: -19.49415, policy_entropy: -5.85490, alpha: 0.00765, time: 33.15667
[CW] ---------------------------
[CW] ---- Iteration:   820 ----
[CW] collect: return: 142.95517, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 0.09525, qf2_loss: 0.09580, policy_loss: -19.57888, policy_entropy: -5.88426, alpha: 0.00758, time: 32.72873
[CW] eval: return: 140.74632, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   821 ----
[CW] collect: return: 132.14840, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 0.14243, qf2_loss: 0.14376, policy_loss: -19.53792, policy_entropy: -6.10911, alpha: 0.00755, time: 33.00766
[CW] ---------------------------
[CW] ---- Iteration:   822 ----
[CW] collect: return: 92.90356, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 0.12724, qf2_loss: 0.12832, policy_loss: -19.49531, policy_entropy: -6.08359, alpha: 0.00760, time: 33.26860
[CW] ---------------------------
[CW] ---- Iteration:   823 ----
[CW] collect: return: 148.99075, steps: 1000.00000, total_steps: 829000.00000
[CW] train: qf1_loss: 0.10227, qf2_loss: 0.10264, policy_loss: -19.53981, policy_entropy: -6.20576, alpha: 0.00767, time: 32.98757
[CW] ---------------------------
[CW] ---- Iteration:   824 ----
[CW] collect: return: 137.56035, steps: 1000.00000, total_steps: 830000.00000
[CW] train: qf1_loss: 0.09123, qf2_loss: 0.09144, policy_loss: -19.51769, policy_entropy: -6.08734, alpha: 0.00775, time: 33.32241
[CW] ---------------------------
[CW] ---- Iteration:   825 ----
[CW] collect: return: 26.27632, steps: 1000.00000, total_steps: 831000.00000
[CW] train: qf1_loss: 0.08244, qf2_loss: 0.08250, policy_loss: -19.48827, policy_entropy: -5.93598, alpha: 0.00776, time: 32.86517
[CW] ---------------------------
[CW] ---- Iteration:   826 ----
[CW] collect: return: 157.65998, steps: 1000.00000, total_steps: 832000.00000
[CW] train: qf1_loss: 0.08728, qf2_loss: 0.08764, policy_loss: -19.58696, policy_entropy: -5.88045, alpha: 0.00773, time: 33.00761
[CW] ---------------------------
[CW] ---- Iteration:   827 ----
[CW] collect: return: 130.12498, steps: 1000.00000, total_steps: 833000.00000
[CW] train: qf1_loss: 0.09666, qf2_loss: 0.09706, policy_loss: -19.61555, policy_entropy: -5.89417, alpha: 0.00766, time: 33.33218
[CW] ---------------------------
[CW] ---- Iteration:   828 ----
[CW] collect: return: 121.28722, steps: 1000.00000, total_steps: 834000.00000
[CW] train: qf1_loss: 0.09294, qf2_loss: 0.09328, policy_loss: -19.61132, policy_entropy: -6.02182, alpha: 0.00762, time: 33.41358
[CW] ---------------------------
[CW] ---- Iteration:   829 ----
[CW] collect: return: 138.74259, steps: 1000.00000, total_steps: 835000.00000
[CW] train: qf1_loss: 0.09160, qf2_loss: 0.09181, policy_loss: -19.57070, policy_entropy: -5.91035, alpha: 0.00763, time: 32.99729
[CW] ---------------------------
[CW] ---- Iteration:   830 ----
[CW] collect: return: 132.10125, steps: 1000.00000, total_steps: 836000.00000
[CW] train: qf1_loss: 0.09014, qf2_loss: 0.09062, policy_loss: -19.44173, policy_entropy: -5.91420, alpha: 0.00758, time: 33.37270
[CW] ---------------------------
[CW] ---- Iteration:   831 ----
[CW] collect: return: 155.25695, steps: 1000.00000, total_steps: 837000.00000
[CW] train: qf1_loss: 0.10467, qf2_loss: 0.10529, policy_loss: -19.54443, policy_entropy: -6.18563, alpha: 0.00759, time: 34.47809
[CW] ---------------------------
[CW] ---- Iteration:   832 ----
[CW] collect: return: 127.87120, steps: 1000.00000, total_steps: 838000.00000
[CW] train: qf1_loss: 0.09567, qf2_loss: 0.09605, policy_loss: -19.44705, policy_entropy: -6.11388, alpha: 0.00767, time: 33.23059
[CW] ---------------------------
[CW] ---- Iteration:   833 ----
[CW] collect: return: 143.71746, steps: 1000.00000, total_steps: 839000.00000
[CW] train: qf1_loss: 0.12371, qf2_loss: 0.12493, policy_loss: -19.48268, policy_entropy: -5.84823, alpha: 0.00768, time: 33.17855
[CW] ---------------------------
[CW] ---- Iteration:   834 ----
[CW] collect: return: 28.28472, steps: 1000.00000, total_steps: 840000.00000
[CW] train: qf1_loss: 0.11466, qf2_loss: 0.11481, policy_loss: -19.68449, policy_entropy: -5.98756, alpha: 0.00761, time: 32.91317
[CW] ---------------------------
[CW] ---- Iteration:   835 ----
[CW] collect: return: 134.56262, steps: 1000.00000, total_steps: 841000.00000
[CW] train: qf1_loss: 0.11810, qf2_loss: 0.11867, policy_loss: -19.58973, policy_entropy: -5.83374, alpha: 0.00759, time: 32.99349
[CW] ---------------------------
[CW] ---- Iteration:   836 ----
[CW] collect: return: 138.70839, steps: 1000.00000, total_steps: 842000.00000
[CW] train: qf1_loss: 0.12677, qf2_loss: 0.12741, policy_loss: -19.51428, policy_entropy: -5.83980, alpha: 0.00752, time: 33.27936
[CW] ---------------------------
[CW] ---- Iteration:   837 ----
[CW] collect: return: 109.94895, steps: 1000.00000, total_steps: 843000.00000
[CW] train: qf1_loss: 0.12169, qf2_loss: 0.12226, policy_loss: -19.68811, policy_entropy: -5.99359, alpha: 0.00748, time: 34.43217
[CW] ---------------------------
[CW] ---- Iteration:   838 ----
[CW] collect: return: 143.82173, steps: 1000.00000, total_steps: 844000.00000
[CW] train: qf1_loss: 0.11688, qf2_loss: 0.11735, policy_loss: -19.53937, policy_entropy: -5.84489, alpha: 0.00747, time: 33.09280
[CW] ---------------------------
[CW] ---- Iteration:   839 ----
[CW] collect: return: 23.14824, steps: 1000.00000, total_steps: 845000.00000
[CW] train: qf1_loss: 0.20088, qf2_loss: 0.20193, policy_loss: -19.64492, policy_entropy: -5.68721, alpha: 0.00737, time: 33.18496
[CW] ---------------------------
[CW] ---- Iteration:   840 ----
[CW] collect: return: 142.97576, steps: 1000.00000, total_steps: 846000.00000
[CW] train: qf1_loss: 0.12574, qf2_loss: 0.12621, policy_loss: -19.60340, policy_entropy: -5.79448, alpha: 0.00725, time: 33.64235
[CW] eval: return: 137.79866, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   841 ----
[CW] collect: return: 141.87520, steps: 1000.00000, total_steps: 847000.00000
[CW] train: qf1_loss: 0.09695, qf2_loss: 0.09708, policy_loss: -19.74426, policy_entropy: -6.01570, alpha: 0.00720, time: 32.95811
[CW] ---------------------------
[CW] ---- Iteration:   842 ----
[CW] collect: return: 43.44750, steps: 1000.00000, total_steps: 848000.00000
[CW] train: qf1_loss: 0.09813, qf2_loss: 0.09823, policy_loss: -19.50786, policy_entropy: -6.11249, alpha: 0.00724, time: 32.95961
[CW] ---------------------------
[CW] ---- Iteration:   843 ----
[CW] collect: return: 108.25528, steps: 1000.00000, total_steps: 849000.00000
[CW] train: qf1_loss: 0.09107, qf2_loss: 0.09147, policy_loss: -19.67893, policy_entropy: -6.08802, alpha: 0.00727, time: 32.96740
[CW] ---------------------------
[CW] ---- Iteration:   844 ----
[CW] collect: return: 138.35930, steps: 1000.00000, total_steps: 850000.00000
[CW] train: qf1_loss: 0.08987, qf2_loss: 0.09014, policy_loss: -19.48912, policy_entropy: -5.95611, alpha: 0.00730, time: 32.70768
[CW] ---------------------------
[CW] ---- Iteration:   845 ----
[CW] collect: return: 141.49434, steps: 1000.00000, total_steps: 851000.00000
[CW] train: qf1_loss: 0.09197, qf2_loss: 0.09240, policy_loss: -19.69212, policy_entropy: -6.05028, alpha: 0.00727, time: 33.13067
[CW] ---------------------------
[CW] ---- Iteration:   846 ----
[CW] collect: return: 149.52328, steps: 1000.00000, total_steps: 852000.00000
[CW] train: qf1_loss: 0.09208, qf2_loss: 0.09238, policy_loss: -19.74346, policy_entropy: -6.12625, alpha: 0.00732, time: 34.94549
[CW] ---------------------------
[CW] ---- Iteration:   847 ----
[CW] collect: return: 135.57507, steps: 1000.00000, total_steps: 853000.00000
[CW] train: qf1_loss: 0.10512, qf2_loss: 0.10546, policy_loss: -19.59567, policy_entropy: -6.17959, alpha: 0.00738, time: 32.93177
[CW] ---------------------------
[CW] ---- Iteration:   848 ----
[CW] collect: return: 159.32237, steps: 1000.00000, total_steps: 854000.00000
[CW] train: qf1_loss: 0.10248, qf2_loss: 0.10288, policy_loss: -19.54928, policy_entropy: -6.03783, alpha: 0.00744, time: 32.91868
[CW] ---------------------------
[CW] ---- Iteration:   849 ----
[CW] collect: return: 143.33934, steps: 1000.00000, total_steps: 855000.00000
[CW] train: qf1_loss: 0.10223, qf2_loss: 0.10258, policy_loss: -19.67795, policy_entropy: -5.93543, alpha: 0.00743, time: 32.90997
[CW] ---------------------------
[CW] ---- Iteration:   850 ----
[CW] collect: return: 152.22590, steps: 1000.00000, total_steps: 856000.00000
[CW] train: qf1_loss: 0.10425, qf2_loss: 0.10456, policy_loss: -19.62067, policy_entropy: -5.80627, alpha: 0.00736, time: 32.97916
[CW] ---------------------------
[CW] ---- Iteration:   851 ----
[CW] collect: return: 135.13351, steps: 1000.00000, total_steps: 857000.00000
[CW] train: qf1_loss: 0.10524, qf2_loss: 0.10581, policy_loss: -19.62959, policy_entropy: -5.76907, alpha: 0.00730, time: 32.97757
[CW] ---------------------------
[CW] ---- Iteration:   852 ----
[CW] collect: return: 139.53617, steps: 1000.00000, total_steps: 858000.00000
[CW] train: qf1_loss: 0.08712, qf2_loss: 0.08738, policy_loss: -19.63601, policy_entropy: -6.04907, alpha: 0.00725, time: 32.85462
[CW] ---------------------------
[CW] ---- Iteration:   853 ----
[CW] collect: return: 151.84514, steps: 1000.00000, total_steps: 859000.00000
[CW] train: qf1_loss: 0.10200, qf2_loss: 0.10251, policy_loss: -19.73383, policy_entropy: -5.93924, alpha: 0.00727, time: 32.67669
[CW] ---------------------------
[CW] ---- Iteration:   854 ----
[CW] collect: return: 139.60043, steps: 1000.00000, total_steps: 860000.00000
[CW] train: qf1_loss: 0.11087, qf2_loss: 0.11182, policy_loss: -19.75763, policy_entropy: -6.07667, alpha: 0.00725, time: 33.35993
[CW] ---------------------------
[CW] ---- Iteration:   855 ----
[CW] collect: return: 134.09331, steps: 1000.00000, total_steps: 861000.00000
[CW] train: qf1_loss: 0.51768, qf2_loss: 0.52100, policy_loss: -19.79274, policy_entropy: -6.22314, alpha: 0.00730, time: 33.21015
[CW] ---------------------------
[CW] ---- Iteration:   856 ----
[CW] collect: return: 153.63368, steps: 1000.00000, total_steps: 862000.00000
[CW] train: qf1_loss: 0.16155, qf2_loss: 0.16156, policy_loss: -19.57245, policy_entropy: -6.06412, alpha: 0.00736, time: 32.87496
[CW] ---------------------------
[CW] ---- Iteration:   857 ----
[CW] collect: return: 124.68817, steps: 1000.00000, total_steps: 863000.00000
[CW] train: qf1_loss: 0.10489, qf2_loss: 0.10501, policy_loss: -19.69703, policy_entropy: -6.24613, alpha: 0.00740, time: 32.65215
[CW] ---------------------------
[CW] ---- Iteration:   858 ----
[CW] collect: return: 146.44459, steps: 1000.00000, total_steps: 864000.00000
[CW] train: qf1_loss: 0.08925, qf2_loss: 0.08939, policy_loss: -19.77610, policy_entropy: -6.21988, alpha: 0.00752, time: 32.66163
[CW] ---------------------------
[CW] ---- Iteration:   859 ----
[CW] collect: return: 150.18139, steps: 1000.00000, total_steps: 865000.00000
[CW] train: qf1_loss: 0.08386, qf2_loss: 0.08398, policy_loss: -19.74982, policy_entropy: -5.95208, alpha: 0.00755, time: 33.10587
[CW] ---------------------------
[CW] ---- Iteration:   860 ----
[CW] collect: return: 130.08529, steps: 1000.00000, total_steps: 866000.00000
[CW] train: qf1_loss: 0.09013, qf2_loss: 0.09039, policy_loss: -19.71170, policy_entropy: -5.74338, alpha: 0.00749, time: 32.98665
[CW] eval: return: 138.57528, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   861 ----
[CW] collect: return: 142.85323, steps: 1000.00000, total_steps: 867000.00000
[CW] train: qf1_loss: 0.08501, qf2_loss: 0.08516, policy_loss: -19.61564, policy_entropy: -5.79710, alpha: 0.00742, time: 32.91174
[CW] ---------------------------
[CW] ---- Iteration:   862 ----
[CW] collect: return: 150.49372, steps: 1000.00000, total_steps: 868000.00000
[CW] train: qf1_loss: 0.08737, qf2_loss: 0.08757, policy_loss: -19.79823, policy_entropy: -6.06421, alpha: 0.00737, time: 32.77803
[CW] ---------------------------
[CW] ---- Iteration:   863 ----
[CW] collect: return: 125.97384, steps: 1000.00000, total_steps: 869000.00000
[CW] train: qf1_loss: 0.08852, qf2_loss: 0.08882, policy_loss: -19.62919, policy_entropy: -6.06414, alpha: 0.00740, time: 33.29044
[CW] ---------------------------
[CW] ---- Iteration:   864 ----
[CW] collect: return: 143.44185, steps: 1000.00000, total_steps: 870000.00000
[CW] train: qf1_loss: 0.08601, qf2_loss: 0.08616, policy_loss: -19.71113, policy_entropy: -5.98606, alpha: 0.00741, time: 33.06199
[CW] ---------------------------
[CW] ---- Iteration:   865 ----
[CW] collect: return: 143.43552, steps: 1000.00000, total_steps: 871000.00000
[CW] train: qf1_loss: 0.09858, qf2_loss: 0.09866, policy_loss: -19.70400, policy_entropy: -5.91647, alpha: 0.00738, time: 33.06181
[CW] ---------------------------
[CW] ---- Iteration:   866 ----
[CW] collect: return: 143.18635, steps: 1000.00000, total_steps: 872000.00000
[CW] train: qf1_loss: 0.09489, qf2_loss: 0.09540, policy_loss: -19.73089, policy_entropy: -5.81621, alpha: 0.00735, time: 33.25683
[CW] ---------------------------
[CW] ---- Iteration:   867 ----
[CW] collect: return: 153.51045, steps: 1000.00000, total_steps: 873000.00000
[CW] train: qf1_loss: 0.10183, qf2_loss: 0.10233, policy_loss: -19.85353, policy_entropy: -5.84944, alpha: 0.00727, time: 32.64323
[CW] ---------------------------
[CW] ---- Iteration:   868 ----
[CW] collect: return: 146.33481, steps: 1000.00000, total_steps: 874000.00000
[CW] train: qf1_loss: 0.10297, qf2_loss: 0.10329, policy_loss: -19.87891, policy_entropy: -5.87848, alpha: 0.00724, time: 32.80215
[CW] ---------------------------
[CW] ---- Iteration:   869 ----
[CW] collect: return: 155.03421, steps: 1000.00000, total_steps: 875000.00000
[CW] train: qf1_loss: 0.10197, qf2_loss: 0.10236, policy_loss: -19.85706, policy_entropy: -5.86743, alpha: 0.00717, time: 32.94560
[CW] ---------------------------
[CW] ---- Iteration:   870 ----
[CW] collect: return: 151.33686, steps: 1000.00000, total_steps: 876000.00000
[CW] train: qf1_loss: 0.11934, qf2_loss: 0.11984, policy_loss: -19.79735, policy_entropy: -5.87729, alpha: 0.00714, time: 32.84396
[CW] ---------------------------
[CW] ---- Iteration:   871 ----
[CW] collect: return: 115.31241, steps: 1000.00000, total_steps: 877000.00000
[CW] train: qf1_loss: 0.12722, qf2_loss: 0.12788, policy_loss: -19.88861, policy_entropy: -6.17938, alpha: 0.00714, time: 33.09995
[CW] ---------------------------
[CW] ---- Iteration:   872 ----
[CW] collect: return: 139.85062, steps: 1000.00000, total_steps: 878000.00000
[CW] train: qf1_loss: 0.10343, qf2_loss: 0.10376, policy_loss: -19.79205, policy_entropy: -6.22660, alpha: 0.00720, time: 32.73912
[CW] ---------------------------
[CW] ---- Iteration:   873 ----
[CW] collect: return: 147.58176, steps: 1000.00000, total_steps: 879000.00000
[CW] train: qf1_loss: 0.09327, qf2_loss: 0.09363, policy_loss: -19.84714, policy_entropy: -6.13239, alpha: 0.00728, time: 32.95919
[CW] ---------------------------
[CW] ---- Iteration:   874 ----
[CW] collect: return: 139.68751, steps: 1000.00000, total_steps: 880000.00000
[CW] train: qf1_loss: 0.10159, qf2_loss: 0.10225, policy_loss: -19.88070, policy_entropy: -6.03121, alpha: 0.00731, time: 32.78691
[CW] ---------------------------
[CW] ---- Iteration:   875 ----
[CW] collect: return: 141.65751, steps: 1000.00000, total_steps: 881000.00000
[CW] train: qf1_loss: 0.10321, qf2_loss: 0.10341, policy_loss: -19.82572, policy_entropy: -6.11184, alpha: 0.00734, time: 33.16847
[CW] ---------------------------
[CW] ---- Iteration:   876 ----
[CW] collect: return: 151.20035, steps: 1000.00000, total_steps: 882000.00000
[CW] train: qf1_loss: 0.12979, qf2_loss: 0.13039, policy_loss: -19.75354, policy_entropy: -5.89720, alpha: 0.00733, time: 32.76255
[CW] ---------------------------
[CW] ---- Iteration:   877 ----
[CW] collect: return: 115.83068, steps: 1000.00000, total_steps: 883000.00000
[CW] train: qf1_loss: 0.13890, qf2_loss: 0.13972, policy_loss: -19.84088, policy_entropy: -6.23204, alpha: 0.00738, time: 32.93257
[CW] ---------------------------
[CW] ---- Iteration:   878 ----
[CW] collect: return: 118.23293, steps: 1000.00000, total_steps: 884000.00000
[CW] train: qf1_loss: 0.10855, qf2_loss: 0.10888, policy_loss: -19.70360, policy_entropy: -5.93272, alpha: 0.00740, time: 32.54026
[CW] ---------------------------
[CW] ---- Iteration:   879 ----
[CW] collect: return: 128.26855, steps: 1000.00000, total_steps: 885000.00000
[CW] train: qf1_loss: 0.10120, qf2_loss: 0.10136, policy_loss: -19.85678, policy_entropy: -6.09783, alpha: 0.00739, time: 32.71980
[CW] ---------------------------
[CW] ---- Iteration:   880 ----
[CW] collect: return: 137.75728, steps: 1000.00000, total_steps: 886000.00000
[CW] train: qf1_loss: 0.09306, qf2_loss: 0.09322, policy_loss: -19.84109, policy_entropy: -6.04897, alpha: 0.00745, time: 32.64305
[CW] eval: return: 132.58596, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   881 ----
[CW] collect: return: 155.10772, steps: 1000.00000, total_steps: 887000.00000
[CW] train: qf1_loss: 0.10123, qf2_loss: 0.10161, policy_loss: -19.76310, policy_entropy: -5.93386, alpha: 0.00745, time: 33.27687
[CW] ---------------------------
[CW] ---- Iteration:   882 ----
[CW] collect: return: 146.53306, steps: 1000.00000, total_steps: 888000.00000
[CW] train: qf1_loss: 0.25472, qf2_loss: 0.25634, policy_loss: -19.85129, policy_entropy: -5.90463, alpha: 0.00742, time: 32.72778
[CW] ---------------------------
[CW] ---- Iteration:   883 ----
[CW] collect: return: 132.92763, steps: 1000.00000, total_steps: 889000.00000
[CW] train: qf1_loss: 0.18680, qf2_loss: 0.18750, policy_loss: -19.82053, policy_entropy: -6.04300, alpha: 0.00738, time: 32.73860
[CW] ---------------------------
[CW] ---- Iteration:   884 ----
[CW] collect: return: 153.79257, steps: 1000.00000, total_steps: 890000.00000
[CW] train: qf1_loss: 0.13459, qf2_loss: 0.13524, policy_loss: -19.92066, policy_entropy: -6.11553, alpha: 0.00742, time: 33.20445
[CW] ---------------------------
[CW] ---- Iteration:   885 ----
[CW] collect: return: 135.59728, steps: 1000.00000, total_steps: 891000.00000
[CW] train: qf1_loss: 0.10508, qf2_loss: 0.10476, policy_loss: -19.95745, policy_entropy: -6.03445, alpha: 0.00746, time: 33.23796
[CW] ---------------------------
[CW] ---- Iteration:   886 ----
[CW] collect: return: 102.91114, steps: 1000.00000, total_steps: 892000.00000
[CW] train: qf1_loss: 0.10967, qf2_loss: 0.10942, policy_loss: -19.98868, policy_entropy: -6.01944, alpha: 0.00748, time: 33.02805
[CW] ---------------------------
[CW] ---- Iteration:   887 ----
[CW] collect: return: 124.53510, steps: 1000.00000, total_steps: 893000.00000
[CW] train: qf1_loss: 0.11521, qf2_loss: 0.11559, policy_loss: -19.87631, policy_entropy: -5.80078, alpha: 0.00744, time: 33.56246
[CW] ---------------------------
[CW] ---- Iteration:   888 ----
[CW] collect: return: 84.14225, steps: 1000.00000, total_steps: 894000.00000
[CW] train: qf1_loss: 0.09822, qf2_loss: 0.09852, policy_loss: -19.88619, policy_entropy: -5.89566, alpha: 0.00735, time: 33.19040
[CW] ---------------------------
[CW] ---- Iteration:   889 ----
[CW] collect: return: 108.32858, steps: 1000.00000, total_steps: 895000.00000
[CW] train: qf1_loss: 0.09531, qf2_loss: 0.09588, policy_loss: -19.91464, policy_entropy: -6.11730, alpha: 0.00736, time: 33.34176
[CW] ---------------------------
[CW] ---- Iteration:   890 ----
[CW] collect: return: 97.10551, steps: 1000.00000, total_steps: 896000.00000
[CW] train: qf1_loss: 0.09541, qf2_loss: 0.09577, policy_loss: -19.87519, policy_entropy: -6.09383, alpha: 0.00741, time: 33.25718
[CW] ---------------------------
[CW] ---- Iteration:   891 ----
[CW] collect: return: 128.77531, steps: 1000.00000, total_steps: 897000.00000
[CW] train: qf1_loss: 0.10673, qf2_loss: 0.10757, policy_loss: -19.87995, policy_entropy: -6.27352, alpha: 0.00747, time: 33.26515
[CW] ---------------------------
[CW] ---- Iteration:   892 ----
[CW] collect: return: 97.03265, steps: 1000.00000, total_steps: 898000.00000
[CW] train: qf1_loss: 0.11000, qf2_loss: 0.11053, policy_loss: -19.94631, policy_entropy: -6.06351, alpha: 0.00755, time: 33.26329
[CW] ---------------------------
[CW] ---- Iteration:   893 ----
[CW] collect: return: 146.10747, steps: 1000.00000, total_steps: 899000.00000
[CW] train: qf1_loss: 0.51375, qf2_loss: 0.51154, policy_loss: -19.94743, policy_entropy: -6.20499, alpha: 0.00763, time: 33.34862
[CW] ---------------------------
[CW] ---- Iteration:   894 ----
[CW] collect: return: 52.21722, steps: 1000.00000, total_steps: 900000.00000
[CW] train: qf1_loss: 0.51600, qf2_loss: 0.51204, policy_loss: -19.86616, policy_entropy: -6.86400, alpha: 0.00779, time: 33.66625
[CW] ---------------------------
[CW] ---- Iteration:   895 ----
[CW] collect: return: 21.63386, steps: 1000.00000, total_steps: 901000.00000
[CW] train: qf1_loss: 0.15980, qf2_loss: 0.16064, policy_loss: -20.04115, policy_entropy: -6.43472, alpha: 0.00803, time: 33.28626
[CW] ---------------------------
[CW] ---- Iteration:   896 ----
[CW] collect: return: 27.83327, steps: 1000.00000, total_steps: 902000.00000
[CW] train: qf1_loss: 0.12466, qf2_loss: 0.12471, policy_loss: -20.13211, policy_entropy: -5.96495, alpha: 0.00809, time: 33.10821
[CW] ---------------------------
[CW] ---- Iteration:   897 ----
[CW] collect: return: 34.32073, steps: 1000.00000, total_steps: 903000.00000
[CW] train: qf1_loss: 0.13897, qf2_loss: 0.13913, policy_loss: -19.96283, policy_entropy: -5.80140, alpha: 0.00808, time: 33.13981
[CW] ---------------------------
[CW] ---- Iteration:   898 ----
[CW] collect: return: 24.64938, steps: 1000.00000, total_steps: 904000.00000
[CW] train: qf1_loss: 0.17819, qf2_loss: 0.17701, policy_loss: -19.97833, policy_entropy: -5.44808, alpha: 0.00795, time: 33.33874
[CW] ---------------------------
[CW] ---- Iteration:   899 ----
[CW] collect: return: 73.42753, steps: 1000.00000, total_steps: 905000.00000
[CW] train: qf1_loss: 0.37763, qf2_loss: 0.37689, policy_loss: -20.08382, policy_entropy: -5.92257, alpha: 0.00782, time: 33.39065
[CW] ---------------------------
[CW] ---- Iteration:   900 ----
[CW] collect: return: 42.74882, steps: 1000.00000, total_steps: 906000.00000
[CW] train: qf1_loss: 0.24879, qf2_loss: 0.24582, policy_loss: -20.02759, policy_entropy: -6.23265, alpha: 0.00783, time: 33.06490
[CW] eval: return: 29.96944, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   901 ----
[CW] collect: return: 18.19065, steps: 1000.00000, total_steps: 907000.00000
[CW] train: qf1_loss: 0.12041, qf2_loss: 0.12082, policy_loss: -19.95906, policy_entropy: -5.93657, alpha: 0.00786, time: 33.21311
[CW] ---------------------------
[CW] ---- Iteration:   902 ----
[CW] collect: return: 54.23171, steps: 1000.00000, total_steps: 908000.00000
[CW] train: qf1_loss: 0.11031, qf2_loss: 0.11088, policy_loss: -20.22003, policy_entropy: -6.04927, alpha: 0.00786, time: 33.18604
[CW] ---------------------------
[CW] ---- Iteration:   903 ----
[CW] collect: return: 118.51436, steps: 1000.00000, total_steps: 909000.00000
[CW] train: qf1_loss: 0.08780, qf2_loss: 0.08796, policy_loss: -20.01675, policy_entropy: -5.86317, alpha: 0.00785, time: 33.46940
[CW] ---------------------------
[CW] ---- Iteration:   904 ----
[CW] collect: return: 139.94821, steps: 1000.00000, total_steps: 910000.00000
[CW] train: qf1_loss: 0.08619, qf2_loss: 0.08643, policy_loss: -19.98672, policy_entropy: -5.89899, alpha: 0.00782, time: 33.23639
[CW] ---------------------------
[CW] ---- Iteration:   905 ----
[CW] collect: return: 115.36842, steps: 1000.00000, total_steps: 911000.00000
[CW] train: qf1_loss: 0.09690, qf2_loss: 0.09707, policy_loss: -20.09619, policy_entropy: -6.09158, alpha: 0.00781, time: 33.00659
[CW] ---------------------------
[CW] ---- Iteration:   906 ----
[CW] collect: return: 138.45771, steps: 1000.00000, total_steps: 912000.00000
[CW] train: qf1_loss: 0.09172, qf2_loss: 0.09206, policy_loss: -20.05824, policy_entropy: -6.08979, alpha: 0.00784, time: 33.55749
[CW] ---------------------------
[CW] ---- Iteration:   907 ----
[CW] collect: return: 143.94013, steps: 1000.00000, total_steps: 913000.00000
[CW] train: qf1_loss: 0.09394, qf2_loss: 0.09426, policy_loss: -20.02520, policy_entropy: -5.96506, alpha: 0.00786, time: 33.86836
[CW] ---------------------------
[CW] ---- Iteration:   908 ----
[CW] collect: return: 127.83842, steps: 1000.00000, total_steps: 914000.00000
[CW] train: qf1_loss: 0.09144, qf2_loss: 0.09153, policy_loss: -20.01614, policy_entropy: -5.82692, alpha: 0.00782, time: 33.45898
[CW] ---------------------------
[CW] ---- Iteration:   909 ----
[CW] collect: return: 31.61697, steps: 1000.00000, total_steps: 915000.00000
[CW] train: qf1_loss: 0.09700, qf2_loss: 0.09724, policy_loss: -20.11163, policy_entropy: -5.89299, alpha: 0.00775, time: 33.24778
[CW] ---------------------------
[CW] ---- Iteration:   910 ----
[CW] collect: return: 130.83070, steps: 1000.00000, total_steps: 916000.00000
[CW] train: qf1_loss: 0.09840, qf2_loss: 0.09866, policy_loss: -20.16906, policy_entropy: -6.07924, alpha: 0.00775, time: 33.12258
[CW] ---------------------------
[CW] ---- Iteration:   911 ----
[CW] collect: return: 130.30173, steps: 1000.00000, total_steps: 917000.00000
[CW] train: qf1_loss: 0.09659, qf2_loss: 0.09676, policy_loss: -19.92183, policy_entropy: -5.98975, alpha: 0.00777, time: 33.15266
[CW] ---------------------------
[CW] ---- Iteration:   912 ----
[CW] collect: return: 158.38685, steps: 1000.00000, total_steps: 918000.00000
[CW] train: qf1_loss: 0.10216, qf2_loss: 0.10235, policy_loss: -20.05599, policy_entropy: -6.00272, alpha: 0.00777, time: 33.15720
[CW] ---------------------------
[CW] ---- Iteration:   913 ----
[CW] collect: return: 74.69856, steps: 1000.00000, total_steps: 919000.00000
[CW] train: qf1_loss: 0.14937, qf2_loss: 0.15064, policy_loss: -20.05801, policy_entropy: -5.92608, alpha: 0.00776, time: 33.22969
[CW] ---------------------------
[CW] ---- Iteration:   914 ----
[CW] collect: return: 135.62292, steps: 1000.00000, total_steps: 920000.00000
[CW] train: qf1_loss: 0.12619, qf2_loss: 0.12654, policy_loss: -20.02880, policy_entropy: -5.95257, alpha: 0.00772, time: 33.44716
[CW] ---------------------------
[CW] ---- Iteration:   915 ----
[CW] collect: return: 134.46269, steps: 1000.00000, total_steps: 921000.00000
[CW] train: qf1_loss: 0.15391, qf2_loss: 0.15446, policy_loss: -20.14707, policy_entropy: -6.03009, alpha: 0.00772, time: 33.32106
[CW] ---------------------------
[CW] ---- Iteration:   916 ----
[CW] collect: return: 29.78411, steps: 1000.00000, total_steps: 922000.00000
[CW] train: qf1_loss: 0.13036, qf2_loss: 0.13061, policy_loss: -20.05122, policy_entropy: -5.91409, alpha: 0.00772, time: 33.51928
[CW] ---------------------------
[CW] ---- Iteration:   917 ----
[CW] collect: return: 151.76497, steps: 1000.00000, total_steps: 923000.00000
[CW] train: qf1_loss: 0.22106, qf2_loss: 0.22272, policy_loss: -20.06900, policy_entropy: -5.90525, alpha: 0.00769, time: 33.00392
[CW] ---------------------------
[CW] ---- Iteration:   918 ----
[CW] collect: return: 151.30340, steps: 1000.00000, total_steps: 924000.00000
[CW] train: qf1_loss: 0.19521, qf2_loss: 0.19518, policy_loss: -20.10596, policy_entropy: -5.93790, alpha: 0.00764, time: 33.08091
[CW] ---------------------------
[CW] ---- Iteration:   919 ----
[CW] collect: return: 145.87690, steps: 1000.00000, total_steps: 925000.00000
[CW] train: qf1_loss: 0.13861, qf2_loss: 0.13883, policy_loss: -19.95070, policy_entropy: -5.76528, alpha: 0.00761, time: 32.98816
[CW] ---------------------------
[CW] ---- Iteration:   920 ----
[CW] collect: return: 126.98660, steps: 1000.00000, total_steps: 926000.00000
[CW] train: qf1_loss: 0.11901, qf2_loss: 0.11902, policy_loss: -19.97057, policy_entropy: -5.76376, alpha: 0.00753, time: 33.16652
[CW] eval: return: 137.19981, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   921 ----
[CW] collect: return: 142.23460, steps: 1000.00000, total_steps: 927000.00000
[CW] train: qf1_loss: 0.11324, qf2_loss: 0.11310, policy_loss: -20.07485, policy_entropy: -6.05616, alpha: 0.00747, time: 33.61596
[CW] ---------------------------
[CW] ---- Iteration:   922 ----
[CW] collect: return: 107.74269, steps: 1000.00000, total_steps: 928000.00000
[CW] train: qf1_loss: 0.10924, qf2_loss: 0.10864, policy_loss: -20.07907, policy_entropy: -6.03475, alpha: 0.00750, time: 33.24208
[CW] ---------------------------
[CW] ---- Iteration:   923 ----
[CW] collect: return: 137.44238, steps: 1000.00000, total_steps: 929000.00000
[CW] train: qf1_loss: 0.20221, qf2_loss: 0.20364, policy_loss: -20.06389, policy_entropy: -6.26711, alpha: 0.00752, time: 33.34142
[CW] ---------------------------
[CW] ---- Iteration:   924 ----
[CW] collect: return: 129.66175, steps: 1000.00000, total_steps: 930000.00000
[CW] train: qf1_loss: 0.12448, qf2_loss: 0.12494, policy_loss: -20.10365, policy_entropy: -5.96618, alpha: 0.00761, time: 33.12744
[CW] ---------------------------
[CW] ---- Iteration:   925 ----
[CW] collect: return: 138.20669, steps: 1000.00000, total_steps: 931000.00000
[CW] train: qf1_loss: 0.10356, qf2_loss: 0.10403, policy_loss: -20.12134, policy_entropy: -5.91307, alpha: 0.00758, time: 33.34237
[CW] ---------------------------
[CW] ---- Iteration:   926 ----
[CW] collect: return: 124.33468, steps: 1000.00000, total_steps: 932000.00000
[CW] train: qf1_loss: 0.10309, qf2_loss: 0.10357, policy_loss: -20.11875, policy_entropy: -5.92183, alpha: 0.00754, time: 32.95287
[CW] ---------------------------
[CW] ---- Iteration:   927 ----
[CW] collect: return: 139.42935, steps: 1000.00000, total_steps: 933000.00000
[CW] train: qf1_loss: 0.10069, qf2_loss: 0.10106, policy_loss: -19.98869, policy_entropy: -6.06560, alpha: 0.00752, time: 33.18663
[CW] ---------------------------
[CW] ---- Iteration:   928 ----
[CW] collect: return: 138.40554, steps: 1000.00000, total_steps: 934000.00000
[CW] train: qf1_loss: 0.10200, qf2_loss: 0.10230, policy_loss: -20.20550, policy_entropy: -6.26150, alpha: 0.00759, time: 33.25888
[CW] ---------------------------
[CW] ---- Iteration:   929 ----
[CW] collect: return: 162.06627, steps: 1000.00000, total_steps: 935000.00000
[CW] train: qf1_loss: 0.57697, qf2_loss: 0.57937, policy_loss: -20.51323, policy_entropy: -6.44574, alpha: 0.00768, time: 33.76020
[CW] ---------------------------
[CW] ---- Iteration:   930 ----
[CW] collect: return: 120.42612, steps: 1000.00000, total_steps: 936000.00000
[CW] train: qf1_loss: 0.43132, qf2_loss: 0.43372, policy_loss: -20.14740, policy_entropy: -6.16859, alpha: 0.00785, time: 33.30082
[CW] ---------------------------
[CW] ---- Iteration:   931 ----
[CW] collect: return: 130.25999, steps: 1000.00000, total_steps: 937000.00000
[CW] train: qf1_loss: 0.14413, qf2_loss: 0.14483, policy_loss: -20.14122, policy_entropy: -6.15983, alpha: 0.00790, time: 33.25361
[CW] ---------------------------
[CW] ---- Iteration:   932 ----
[CW] collect: return: 142.69911, steps: 1000.00000, total_steps: 938000.00000
[CW] train: qf1_loss: 0.11436, qf2_loss: 0.11488, policy_loss: -19.81368, policy_entropy: -5.93452, alpha: 0.00792, time: 33.20262
[CW] ---------------------------
[CW] ---- Iteration:   933 ----
[CW] collect: return: 128.42530, steps: 1000.00000, total_steps: 939000.00000
[CW] train: qf1_loss: 0.11596, qf2_loss: 0.11646, policy_loss: -20.17230, policy_entropy: -6.07935, alpha: 0.00792, time: 32.97595
[CW] ---------------------------
[CW] ---- Iteration:   934 ----
[CW] collect: return: 144.27948, steps: 1000.00000, total_steps: 940000.00000
[CW] train: qf1_loss: 0.10433, qf2_loss: 0.10479, policy_loss: -20.20601, policy_entropy: -5.94997, alpha: 0.00794, time: 33.32292
[CW] ---------------------------
[CW] ---- Iteration:   935 ----
[CW] collect: return: 88.34458, steps: 1000.00000, total_steps: 941000.00000
[CW] train: qf1_loss: 0.10036, qf2_loss: 0.10078, policy_loss: -20.02534, policy_entropy: -5.89402, alpha: 0.00790, time: 33.06495
[CW] ---------------------------
[CW] ---- Iteration:   936 ----
[CW] collect: return: 62.94297, steps: 1000.00000, total_steps: 942000.00000
[CW] train: qf1_loss: 0.10922, qf2_loss: 0.10955, policy_loss: -20.08304, policy_entropy: -5.89330, alpha: 0.00786, time: 33.21620
[CW] ---------------------------
[CW] ---- Iteration:   937 ----
[CW] collect: return: 161.00872, steps: 1000.00000, total_steps: 943000.00000
[CW] train: qf1_loss: 0.13546, qf2_loss: 0.13583, policy_loss: -20.35524, policy_entropy: -5.93894, alpha: 0.00783, time: 32.70302
[CW] ---------------------------
[CW] ---- Iteration:   938 ----
[CW] collect: return: 129.11274, steps: 1000.00000, total_steps: 944000.00000
[CW] train: qf1_loss: 0.13609, qf2_loss: 0.13600, policy_loss: -20.24881, policy_entropy: -6.05757, alpha: 0.00782, time: 33.48539
[CW] ---------------------------
[CW] ---- Iteration:   939 ----
[CW] collect: return: 133.11239, steps: 1000.00000, total_steps: 945000.00000
[CW] train: qf1_loss: 0.11707, qf2_loss: 0.11755, policy_loss: -20.17684, policy_entropy: -5.94080, alpha: 0.00782, time: 33.19140
[CW] ---------------------------
[CW] ---- Iteration:   940 ----
[CW] collect: return: 128.69598, steps: 1000.00000, total_steps: 946000.00000
[CW] train: qf1_loss: 0.11856, qf2_loss: 0.11901, policy_loss: -20.16868, policy_entropy: -5.99355, alpha: 0.00780, time: 33.01994
[CW] eval: return: 139.58466, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   941 ----
[CW] collect: return: 154.75028, steps: 1000.00000, total_steps: 947000.00000
[CW] train: qf1_loss: 0.11493, qf2_loss: 0.11517, policy_loss: -20.06325, policy_entropy: -5.97767, alpha: 0.00780, time: 33.39070
[CW] ---------------------------
[CW] ---- Iteration:   942 ----
[CW] collect: return: 136.84244, steps: 1000.00000, total_steps: 948000.00000
[CW] train: qf1_loss: 0.10524, qf2_loss: 0.10570, policy_loss: -20.15007, policy_entropy: -5.92829, alpha: 0.00779, time: 32.92585
[CW] ---------------------------
[CW] ---- Iteration:   943 ----
[CW] collect: return: 25.40496, steps: 1000.00000, total_steps: 949000.00000
[CW] train: qf1_loss: 0.21279, qf2_loss: 0.21382, policy_loss: -20.20110, policy_entropy: -5.81134, alpha: 0.00775, time: 32.98057
[CW] ---------------------------
[CW] ---- Iteration:   944 ----
[CW] collect: return: 126.81435, steps: 1000.00000, total_steps: 950000.00000
[CW] train: qf1_loss: 1.76783, qf2_loss: 1.76296, policy_loss: -20.06068, policy_entropy: -6.12583, alpha: 0.00773, time: 32.73382
[CW] ---------------------------
[CW] ---- Iteration:   945 ----
[CW] collect: return: 125.71186, steps: 1000.00000, total_steps: 951000.00000
[CW] train: qf1_loss: 1.07072, qf2_loss: 1.07164, policy_loss: -20.15861, policy_entropy: -7.07595, alpha: 0.00789, time: 33.57842
[CW] ---------------------------
[CW] ---- Iteration:   946 ----
[CW] collect: return: 17.16511, steps: 1000.00000, total_steps: 952000.00000
[CW] train: qf1_loss: 0.21598, qf2_loss: 0.21657, policy_loss: -20.12881, policy_entropy: -6.31398, alpha: 0.00820, time: 33.75529
[CW] ---------------------------
[CW] ---- Iteration:   947 ----
[CW] collect: return: 18.47271, steps: 1000.00000, total_steps: 953000.00000
[CW] train: qf1_loss: 0.15867, qf2_loss: 0.15920, policy_loss: -20.12484, policy_entropy: -5.87259, alpha: 0.00823, time: 33.64513
[CW] ---------------------------
[CW] ---- Iteration:   948 ----
[CW] collect: return: 17.29522, steps: 1000.00000, total_steps: 954000.00000
[CW] train: qf1_loss: 0.14349, qf2_loss: 0.14375, policy_loss: -20.07929, policy_entropy: -5.62049, alpha: 0.00812, time: 33.62005
[CW] ---------------------------
[CW] ---- Iteration:   949 ----
[CW] collect: return: 28.86058, steps: 1000.00000, total_steps: 955000.00000
[CW] train: qf1_loss: 0.12436, qf2_loss: 0.12489, policy_loss: -20.23354, policy_entropy: -5.59718, alpha: 0.00799, time: 33.60051
[CW] ---------------------------
[CW] ---- Iteration:   950 ----
[CW] collect: return: 37.30159, steps: 1000.00000, total_steps: 956000.00000
[CW] train: qf1_loss: 0.11228, qf2_loss: 0.11259, policy_loss: -20.01737, policy_entropy: -5.40842, alpha: 0.00783, time: 33.46551
[CW] ---------------------------
[CW] ---- Iteration:   951 ----
[CW] collect: return: 127.28270, steps: 1000.00000, total_steps: 957000.00000
[CW] train: qf1_loss: 0.10147, qf2_loss: 0.10180, policy_loss: -19.98363, policy_entropy: -5.37713, alpha: 0.00764, time: 34.28081
[CW] ---------------------------
[CW] ---- Iteration:   952 ----
[CW] collect: return: 126.59835, steps: 1000.00000, total_steps: 958000.00000
[CW] train: qf1_loss: 0.09547, qf2_loss: 0.09587, policy_loss: -19.98603, policy_entropy: -5.54133, alpha: 0.00746, time: 33.42969
[CW] ---------------------------
[CW] ---- Iteration:   953 ----
[CW] collect: return: 138.90847, steps: 1000.00000, total_steps: 959000.00000
[CW] train: qf1_loss: 0.10091, qf2_loss: 0.10116, policy_loss: -20.23476, policy_entropy: -5.87466, alpha: 0.00737, time: 33.70663
[CW] ---------------------------
[CW] ---- Iteration:   954 ----
[CW] collect: return: 35.23587, steps: 1000.00000, total_steps: 960000.00000
[CW] train: qf1_loss: 0.09328, qf2_loss: 0.09358, policy_loss: -20.07565, policy_entropy: -5.78486, alpha: 0.00732, time: 33.58350
[CW] ---------------------------
[CW] ---- Iteration:   955 ----
[CW] collect: return: 133.92450, steps: 1000.00000, total_steps: 961000.00000
[CW] train: qf1_loss: 0.09282, qf2_loss: 0.09319, policy_loss: -20.10967, policy_entropy: -5.90608, alpha: 0.00727, time: 33.46963
[CW] ---------------------------
[CW] ---- Iteration:   956 ----
[CW] collect: return: 129.04217, steps: 1000.00000, total_steps: 962000.00000
[CW] train: qf1_loss: 0.09318, qf2_loss: 0.09356, policy_loss: -20.29861, policy_entropy: -6.00639, alpha: 0.00726, time: 33.94843
[CW] ---------------------------
[CW] ---- Iteration:   957 ----
[CW] collect: return: 141.04375, steps: 1000.00000, total_steps: 963000.00000
[CW] train: qf1_loss: 0.09651, qf2_loss: 0.09686, policy_loss: -20.04152, policy_entropy: -6.02357, alpha: 0.00727, time: 33.43479
[CW] ---------------------------
[CW] ---- Iteration:   958 ----
[CW] collect: return: 126.72951, steps: 1000.00000, total_steps: 964000.00000
[CW] train: qf1_loss: 0.12905, qf2_loss: 0.12839, policy_loss: -20.13118, policy_entropy: -5.91367, alpha: 0.00726, time: 33.87541
[CW] ---------------------------
[CW] ---- Iteration:   959 ----
[CW] collect: return: 118.52926, steps: 1000.00000, total_steps: 965000.00000
[CW] train: qf1_loss: 0.10715, qf2_loss: 0.10729, policy_loss: -19.99291, policy_entropy: -5.93118, alpha: 0.00723, time: 33.63770
[CW] ---------------------------
[CW] ---- Iteration:   960 ----
[CW] collect: return: 84.82398, steps: 1000.00000, total_steps: 966000.00000
[CW] train: qf1_loss: 0.10058, qf2_loss: 0.10098, policy_loss: -20.23733, policy_entropy: -5.98171, alpha: 0.00722, time: 33.53551
[CW] eval: return: 77.62193, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   961 ----
[CW] collect: return: 94.27645, steps: 1000.00000, total_steps: 967000.00000
[CW] train: qf1_loss: 0.09563, qf2_loss: 0.09574, policy_loss: -20.07119, policy_entropy: -5.94066, alpha: 0.00721, time: 33.85791
[CW] ---------------------------
[CW] ---- Iteration:   962 ----
[CW] collect: return: 127.69599, steps: 1000.00000, total_steps: 968000.00000
[CW] train: qf1_loss: 0.10036, qf2_loss: 0.10070, policy_loss: -20.12026, policy_entropy: -5.86639, alpha: 0.00719, time: 33.73233
[CW] ---------------------------
[CW] ---- Iteration:   963 ----
[CW] collect: return: 133.24778, steps: 1000.00000, total_steps: 969000.00000
[CW] train: qf1_loss: 0.10941, qf2_loss: 0.10965, policy_loss: -20.11587, policy_entropy: -5.76462, alpha: 0.00713, time: 33.76634
[CW] ---------------------------
[CW] ---- Iteration:   964 ----
[CW] collect: return: 65.65251, steps: 1000.00000, total_steps: 970000.00000
[CW] train: qf1_loss: 0.11694, qf2_loss: 0.11693, policy_loss: -20.26463, policy_entropy: -5.84460, alpha: 0.00704, time: 33.43623
[CW] ---------------------------
[CW] ---- Iteration:   965 ----
[CW] collect: return: 109.66139, steps: 1000.00000, total_steps: 971000.00000
[CW] train: qf1_loss: 0.14957, qf2_loss: 0.14941, policy_loss: -20.05617, policy_entropy: -6.00884, alpha: 0.00703, time: 33.77908
[CW] ---------------------------
[CW] ---- Iteration:   966 ----
[CW] collect: return: 137.15027, steps: 1000.00000, total_steps: 972000.00000
[CW] train: qf1_loss: 0.21747, qf2_loss: 0.21915, policy_loss: -20.11739, policy_entropy: -6.14079, alpha: 0.00704, time: 33.73292
[CW] ---------------------------
[CW] ---- Iteration:   967 ----
[CW] collect: return: 129.19235, steps: 1000.00000, total_steps: 973000.00000
[CW] train: qf1_loss: 0.27392, qf2_loss: 0.27314, policy_loss: -20.04077, policy_entropy: -5.84068, alpha: 0.00703, time: 33.68555
[CW] ---------------------------
[CW] ---- Iteration:   968 ----
[CW] collect: return: 107.27051, steps: 1000.00000, total_steps: 974000.00000
[CW] train: qf1_loss: 0.14467, qf2_loss: 0.14498, policy_loss: -20.17544, policy_entropy: -5.97515, alpha: 0.00702, time: 33.87402
[CW] ---------------------------
[CW] ---- Iteration:   969 ----
[CW] collect: return: 122.72085, steps: 1000.00000, total_steps: 975000.00000
[CW] train: qf1_loss: 0.11524, qf2_loss: 0.11562, policy_loss: -20.35014, policy_entropy: -5.99383, alpha: 0.00702, time: 33.83416
[CW] ---------------------------
[CW] ---- Iteration:   970 ----
[CW] collect: return: 120.57798, steps: 1000.00000, total_steps: 976000.00000
[CW] train: qf1_loss: 0.12618, qf2_loss: 0.12627, policy_loss: -20.21332, policy_entropy: -6.08010, alpha: 0.00703, time: 33.70188
[CW] ---------------------------
[CW] ---- Iteration:   971 ----
[CW] collect: return: 5.17556, steps: 1000.00000, total_steps: 977000.00000
[CW] train: qf1_loss: 0.09699, qf2_loss: 0.09718, policy_loss: -20.15108, policy_entropy: -5.99089, alpha: 0.00704, time: 33.81989
[CW] ---------------------------
[CW] ---- Iteration:   972 ----
[CW] collect: return: 125.07851, steps: 1000.00000, total_steps: 978000.00000
[CW] train: qf1_loss: 0.10197, qf2_loss: 0.10217, policy_loss: -20.24546, policy_entropy: -6.10321, alpha: 0.00706, time: 33.73284
[CW] ---------------------------
[CW] ---- Iteration:   973 ----
[CW] collect: return: 109.57544, steps: 1000.00000, total_steps: 979000.00000
[CW] train: qf1_loss: 0.10362, qf2_loss: 0.10371, policy_loss: -20.22591, policy_entropy: -6.00989, alpha: 0.00707, time: 33.46176
[CW] ---------------------------
[CW] ---- Iteration:   974 ----
[CW] collect: return: 145.11774, steps: 1000.00000, total_steps: 980000.00000
[CW] train: qf1_loss: 0.12542, qf2_loss: 0.12553, policy_loss: -20.37021, policy_entropy: -6.02443, alpha: 0.00708, time: 33.45913
[CW] ---------------------------
[CW] ---- Iteration:   975 ----
[CW] collect: return: 88.77780, steps: 1000.00000, total_steps: 981000.00000
[CW] train: qf1_loss: 1.03657, qf2_loss: 1.02793, policy_loss: -20.20693, policy_entropy: -6.02567, alpha: 0.00705, time: 33.90700
[CW] ---------------------------
[CW] ---- Iteration:   976 ----
[CW] collect: return: 118.97187, steps: 1000.00000, total_steps: 982000.00000
[CW] train: qf1_loss: 0.77002, qf2_loss: 0.77621, policy_loss: -20.01726, policy_entropy: -7.18675, alpha: 0.00727, time: 33.62860
[CW] ---------------------------
[CW] ---- Iteration:   977 ----
[CW] collect: return: 143.88150, steps: 1000.00000, total_steps: 983000.00000
[CW] train: qf1_loss: 0.18675, qf2_loss: 0.18764, policy_loss: -20.32616, policy_entropy: -6.32293, alpha: 0.00752, time: 33.39971
[CW] ---------------------------
[CW] ---- Iteration:   978 ----
[CW] collect: return: 115.97360, steps: 1000.00000, total_steps: 984000.00000
[CW] train: qf1_loss: 0.13187, qf2_loss: 0.13270, policy_loss: -20.29855, policy_entropy: -6.10559, alpha: 0.00760, time: 33.92989
[CW] ---------------------------
[CW] ---- Iteration:   979 ----
[CW] collect: return: 124.66379, steps: 1000.00000, total_steps: 985000.00000
[CW] train: qf1_loss: 0.11003, qf2_loss: 0.11047, policy_loss: -20.17207, policy_entropy: -5.91392, alpha: 0.00760, time: 33.67059
[CW] ---------------------------
[CW] ---- Iteration:   980 ----
[CW] collect: return: 119.52179, steps: 1000.00000, total_steps: 986000.00000
[CW] train: qf1_loss: 0.10379, qf2_loss: 0.10417, policy_loss: -20.33516, policy_entropy: -5.79820, alpha: 0.00754, time: 33.93337
[CW] eval: return: 122.44271, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   981 ----
[CW] collect: return: 126.83064, steps: 1000.00000, total_steps: 987000.00000
[CW] train: qf1_loss: 0.09885, qf2_loss: 0.09920, policy_loss: -20.11437, policy_entropy: -5.86803, alpha: 0.00748, time: 33.41509
[CW] ---------------------------
[CW] ---- Iteration:   982 ----
[CW] collect: return: 141.14522, steps: 1000.00000, total_steps: 988000.00000
[CW] train: qf1_loss: 0.11601, qf2_loss: 0.11643, policy_loss: -20.38452, policy_entropy: -5.89054, alpha: 0.00746, time: 33.51729
[CW] ---------------------------
[CW] ---- Iteration:   983 ----
[CW] collect: return: 82.24272, steps: 1000.00000, total_steps: 989000.00000
[CW] train: qf1_loss: 0.09354, qf2_loss: 0.09390, policy_loss: -20.08305, policy_entropy: -5.81121, alpha: 0.00740, time: 33.88733
[CW] ---------------------------
[CW] ---- Iteration:   984 ----
[CW] collect: return: 132.19167, steps: 1000.00000, total_steps: 990000.00000
[CW] train: qf1_loss: 0.09637, qf2_loss: 0.09670, policy_loss: -20.20133, policy_entropy: -5.83666, alpha: 0.00735, time: 33.56258
[CW] ---------------------------
[CW] ---- Iteration:   985 ----
[CW] collect: return: 51.71395, steps: 1000.00000, total_steps: 991000.00000
[CW] train: qf1_loss: 0.09007, qf2_loss: 0.09044, policy_loss: -20.21732, policy_entropy: -5.82163, alpha: 0.00728, time: 33.37406
[CW] ---------------------------
[CW] ---- Iteration:   986 ----
[CW] collect: return: 133.31499, steps: 1000.00000, total_steps: 992000.00000
[CW] train: qf1_loss: 0.09161, qf2_loss: 0.09187, policy_loss: -20.32783, policy_entropy: -5.93989, alpha: 0.00725, time: 33.97979
[CW] ---------------------------
[CW] ---- Iteration:   987 ----
[CW] collect: return: 46.46469, steps: 1000.00000, total_steps: 993000.00000
[CW] train: qf1_loss: 0.08863, qf2_loss: 0.08883, policy_loss: -20.22942, policy_entropy: -5.96841, alpha: 0.00721, time: 33.77153
[CW] ---------------------------
[CW] ---- Iteration:   988 ----
[CW] collect: return: 130.93529, steps: 1000.00000, total_steps: 994000.00000
[CW] train: qf1_loss: 0.09495, qf2_loss: 0.09492, policy_loss: -20.18152, policy_entropy: -5.83892, alpha: 0.00718, time: 33.49617
[CW] ---------------------------
[CW] ---- Iteration:   989 ----
[CW] collect: return: 154.30438, steps: 1000.00000, total_steps: 995000.00000
[CW] train: qf1_loss: 0.09804, qf2_loss: 0.09790, policy_loss: -20.38134, policy_entropy: -6.10112, alpha: 0.00718, time: 34.16101
[CW] ---------------------------
[CW] ---- Iteration:   990 ----
[CW] collect: return: 77.58027, steps: 1000.00000, total_steps: 996000.00000
[CW] train: qf1_loss: 0.09286, qf2_loss: 0.09297, policy_loss: -20.26535, policy_entropy: -5.94602, alpha: 0.00717, time: 33.94447
[CW] ---------------------------
[CW] ---- Iteration:   991 ----
[CW] collect: return: 149.87060, steps: 1000.00000, total_steps: 997000.00000
[CW] train: qf1_loss: 0.09756, qf2_loss: 0.09748, policy_loss: -20.23469, policy_entropy: -6.00426, alpha: 0.00718, time: 33.65446
[CW] ---------------------------
[CW] ---- Iteration:   992 ----
[CW] collect: return: 137.54492, steps: 1000.00000, total_steps: 998000.00000
[CW] train: qf1_loss: 0.11131, qf2_loss: 0.11087, policy_loss: -20.22872, policy_entropy: -5.83538, alpha: 0.00714, time: 33.75929
[CW] ---------------------------
[CW] ---- Iteration:   993 ----
[CW] collect: return: 132.76176, steps: 1000.00000, total_steps: 999000.00000
[CW] train: qf1_loss: 0.10762, qf2_loss: 0.10770, policy_loss: -20.33669, policy_entropy: -6.13121, alpha: 0.00713, time: 33.86964
[CW] ---------------------------
[CW] ---- Iteration:   994 ----
[CW] collect: return: 97.37077, steps: 1000.00000, total_steps: 1000000.00000
[CW] train: qf1_loss: 0.09796, qf2_loss: 0.09796, policy_loss: -20.21103, policy_entropy: -5.88157, alpha: 0.00717, time: 33.81845
[CW] ---------------------------
[CW] ---- Iteration:   995 ----
[CW] collect: return: 126.86498, steps: 1000.00000, total_steps: 1001000.00000
[CW] train: qf1_loss: 0.10157, qf2_loss: 0.10161, policy_loss: -20.24051, policy_entropy: -6.00624, alpha: 0.00711, time: 33.76685
[CW] ---------------------------
[CW] ---- Iteration:   996 ----
[CW] collect: return: 100.64649, steps: 1000.00000, total_steps: 1002000.00000
[CW] train: qf1_loss: 0.11447, qf2_loss: 0.11439, policy_loss: -20.38201, policy_entropy: -6.09113, alpha: 0.00716, time: 33.93231
[CW] ---------------------------
[CW] ---- Iteration:   997 ----
[CW] collect: return: 122.08025, steps: 1000.00000, total_steps: 1003000.00000
[CW] train: qf1_loss: 0.09755, qf2_loss: 0.09760, policy_loss: -20.28126, policy_entropy: -5.91898, alpha: 0.00712, time: 33.35815
[CW] ---------------------------
[CW] ---- Iteration:   998 ----
[CW] collect: return: 78.65172, steps: 1000.00000, total_steps: 1004000.00000
[CW] train: qf1_loss: 0.11487, qf2_loss: 0.11443, policy_loss: -20.28016, policy_entropy: -6.19350, alpha: 0.00716, time: 33.48024
[CW] ---------------------------
[CW] ---- Iteration:   999 ----
[CW] collect: return: 144.00980, steps: 1000.00000, total_steps: 1005000.00000
[CW] train: qf1_loss: 0.11352, qf2_loss: 0.11379, policy_loss: -20.17315, policy_entropy: -6.00797, alpha: 0.00719, time: 33.35224
[CW] ---------------------------
[CW] ---- Iteration:  1000 ----
[CW] collect: return: 113.10788, steps: 1000.00000, total_steps: 1006000.00000
[CW] train: qf1_loss: 0.10315, qf2_loss: 0.10316, policy_loss: -20.39578, policy_entropy: -6.08532, alpha: 0.00723, time: 33.34437
[CW] eval: return: 111.73065, steps: 1000.00000
[CW] ---------------------------
