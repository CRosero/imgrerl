{"collect/return": 552.0043583717779, "collect/steps": 1000.0, "collect/total_steps": 416000.0, "train/qf1_loss": 55.922191848754885, "train/qf2_loss": 55.86088851928711, "train/policy_loss": -389.4102200317383, "train/policy_entropy": -1.0051800608634949, "train/alpha": 0.6847751075029374, "train/time": 67.74219226837158, "eval/return": 843.022067647813, "eval/steps": 1000.0, "_timestamp": 1678336144.7888854, "_runtime": 28657.271431446075, "_step": 410}