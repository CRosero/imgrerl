{"collect/return": 164.24776240438223, "collect/steps": 1000.0, "collect/total_steps": 776000.0, "train/qf1_loss": 0.093828125, "train/qf2_loss": 0.09325770694762468, "train/policy_loss": -26.442749824523926, "train/policy_entropy": -5.909469623565673, "train/alpha": 0.005624068733304739, "train/time": 51.512917041778564, "eval/return": 165.60366326359798, "eval/steps": 1000.0, "_timestamp": 1678754980.6923203, "_runtime": 43144.35123133659, "_step": 770}