{"collect/return": 353.55700333829736, "collect/steps": 1000.0, "collect/total_steps": 1006000.0, "train/qf1_loss": 9.160550899505616, "train/qf2_loss": 9.164132604598999, "train/policy_loss": -112.84163581848145, "train/policy_entropy": -6.211680335998535, "train/alpha": 0.028836863525211812, "train/time": 33.877912759780884, "eval/return": 271.97767763388606, "eval/steps": 1000.0, "_timestamp": 1678768399.4565353, "_runtime": 35774.57160139084, "_step": 1000, "_wandb": {"runtime": 35774}}