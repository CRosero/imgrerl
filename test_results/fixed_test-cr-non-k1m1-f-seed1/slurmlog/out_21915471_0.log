Hostname: uc2n903.localdomain
Found slurm config: SLURM
Seeting default slurm config
/home/kit/stud/uprnr/imgrerl/test_results/fixed_test-cr-non-k1m1-f-seed1/fixed_test-cr-non-k1m1-f-seed1/fixed_test-cr-non-k1m1-f-seed1__env.echeetah-run/log/rep_00
[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
False
params: 
 {'env': {'env': 'cheetah-run'}} 

additionalVars: 
 {'seed': 1, 'agent': {'image_augmentation_K': 1, 'image_augmentation_M': 1, 'image_augmentation_type': <AugmentationType.NONE: 1>, 'image_augmentation_actor_critic_same_aug': False}}
conf_dict: 
 --------Config-------- 
seed: 1
cuda_id: 0
Subconfig: env
	env: cheetah-run
	obs_type: image
Subconfig: agent
	algo_name: sac
	encoder: lstm
	action_embedding_size: 32
	observ_embedding_size: 0
	reward_embedding_size: 32
	rnn_hidden_size: 512
	dqn_layers: [512, 512, 512]
	policy_layers: [512, 512, 512]
	lr: 0.0003
	gamma: 0.99
	tau: 0.005
	entropy_alpha: 1.0
	image_augmentation_type: AugmentationType.NONE
	image_augmentation_K: 1
	image_augmentation_M: 1
	image_augmentation_actor_critic_same_aug: False
Subconfig: rl
	sampled_seq_len: 64
	buffer_size: 1000000.0
	batch_size: 32
	rollouts_per_iter: 1
	num_updates_per_iter: 100
	num_init_rollouts: 5
Subconfig: eval
	interval: 20
	num_rollouts: 20
---------------------- 
 
Init feature extractor:  6 ;  32 ;  <function relu at 0x14d0aed6e7a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x14d0aed6e7a0>
Init feature extractor:  6 ;  164 ;  <function relu at 0x14d0aed6e7a0>
Critic: 
Critic_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (current_shortcut_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=164, bias=True)
  )
  (qf1): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
  (qf2): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
)
Init feature extractor:  6 ;  32 ;  <function relu at 0x14d0aed6e7a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x14d0aed6e7a0>
Actor: 
Actor_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (policy): TanhGaussianPolicy(
    (fc0): Linear(in_features=612, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=6, bias=True)
    (last_fc_log_std): Linear(in_features=512, out_features=6, bias=True)
  )
)
buffer RAM usage: 11.48 GB
Collecting train stats
[CW] ---- Iteration:     0 ----
[CW] collect: return: 15.02984, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.68390, qf2_loss: 1.69428, policy_loss: -7.85231, policy_entropy: 4.09760, alpha: 0.98504, time: 35.96562
[CW] eval: return: 14.20394, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     1 ----
[CW] collect: return: 17.24708, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.08984, qf2_loss: 0.08991, policy_loss: -8.53314, policy_entropy: 4.10143, alpha: 0.95626, time: 33.15126
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     2 ----
[CW] collect: return: 23.19494, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.08383, qf2_loss: 0.08392, policy_loss: -9.22647, policy_entropy: 4.10106, alpha: 0.92870, time: 33.12598
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     3 ----
[CW] collect: return: 17.50664, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.07638, qf2_loss: 0.07635, policy_loss: -10.13332, policy_entropy: 4.09959, alpha: 0.90231, time: 33.24593
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     4 ----
[CW] collect: return: 12.48061, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.07009, qf2_loss: 0.06991, policy_loss: -11.14917, policy_entropy: 4.10061, alpha: 0.87699, time: 33.38128
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     5 ----
[CW] collect: return: 13.97025, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06761, qf2_loss: 0.06707, policy_loss: -12.24988, policy_entropy: 4.10269, alpha: 0.85267, time: 33.41323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     6 ----
[CW] collect: return: 9.60880, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.08089, qf2_loss: 0.08016, policy_loss: -13.40069, policy_entropy: 4.10132, alpha: 0.82930, time: 33.38996
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     7 ----
[CW] collect: return: 15.88890, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.07823, qf2_loss: 0.07818, policy_loss: -14.60267, policy_entropy: 4.09957, alpha: 0.80683, time: 33.39118
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     8 ----
[CW] collect: return: 13.62137, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.08995, qf2_loss: 0.09035, policy_loss: -15.81942, policy_entropy: 4.10125, alpha: 0.78520, time: 33.14280
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     9 ----
[CW] collect: return: 23.50705, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.08953, qf2_loss: 0.08999, policy_loss: -17.03065, policy_entropy: 4.10012, alpha: 0.76436, time: 33.16804
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    10 ----
[CW] collect: return: 17.56778, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.09077, qf2_loss: 0.09123, policy_loss: -18.22529, policy_entropy: 4.10007, alpha: 0.74427, time: 33.35407
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    11 ----
[CW] collect: return: 7.68542, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.08308, qf2_loss: 0.08339, policy_loss: -19.39814, policy_entropy: 4.09982, alpha: 0.72488, time: 33.36092
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    12 ----
[CW] collect: return: 10.61273, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.08201, qf2_loss: 0.08238, policy_loss: -20.54830, policy_entropy: 4.10055, alpha: 0.70617, time: 33.40566
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    13 ----
[CW] collect: return: 15.17396, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.12059, qf2_loss: 0.12173, policy_loss: -21.66821, policy_entropy: 4.09992, alpha: 0.68810, time: 33.36174
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    14 ----
[CW] collect: return: 17.51702, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.10477, qf2_loss: 0.10536, policy_loss: -22.76646, policy_entropy: 4.10001, alpha: 0.67063, time: 33.32051
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    15 ----
[CW] collect: return: 12.57689, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.08847, qf2_loss: 0.08892, policy_loss: -23.83342, policy_entropy: 4.09850, alpha: 0.65373, time: 33.20725
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    16 ----
[CW] collect: return: 19.66157, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.09246, qf2_loss: 0.09296, policy_loss: -24.87086, policy_entropy: 4.09844, alpha: 0.63737, time: 33.35067
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    17 ----
[CW] collect: return: 12.60742, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.09872, qf2_loss: 0.09931, policy_loss: -25.88886, policy_entropy: 4.09909, alpha: 0.62154, time: 33.32502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    18 ----
[CW] collect: return: 20.96126, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.11352, qf2_loss: 0.11425, policy_loss: -26.87845, policy_entropy: 4.09855, alpha: 0.60620, time: 33.29171
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    19 ----
[CW] collect: return: 15.20709, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.11600, qf2_loss: 0.11664, policy_loss: -27.83987, policy_entropy: 4.09797, alpha: 0.59133, time: 33.34757
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    20 ----
[CW] collect: return: 16.24636, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.11105, qf2_loss: 0.11161, policy_loss: -28.77348, policy_entropy: 4.09847, alpha: 0.57692, time: 33.31220
[CW] eval: return: 13.18920, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    21 ----
[CW] collect: return: 18.57914, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.11684, qf2_loss: 0.11745, policy_loss: -29.68189, policy_entropy: 4.09655, alpha: 0.56293, time: 33.23421
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    22 ----
[CW] collect: return: 13.04686, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.11911, qf2_loss: 0.11974, policy_loss: -30.55666, policy_entropy: 4.09490, alpha: 0.54936, time: 33.38382
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    23 ----
[CW] collect: return: 17.85867, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.12480, qf2_loss: 0.12537, policy_loss: -31.41706, policy_entropy: 4.09465, alpha: 0.53618, time: 33.35686
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    24 ----
[CW] collect: return: 6.41741, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.12402, qf2_loss: 0.12466, policy_loss: -32.23630, policy_entropy: 4.09541, alpha: 0.52338, time: 33.40428
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    25 ----
[CW] collect: return: 17.56635, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.14003, qf2_loss: 0.14073, policy_loss: -33.04933, policy_entropy: 4.09428, alpha: 0.51094, time: 33.45636
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    26 ----
[CW] collect: return: 8.82714, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.13766, qf2_loss: 0.13831, policy_loss: -33.81617, policy_entropy: 4.09405, alpha: 0.49885, time: 33.38580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    27 ----
[CW] collect: return: 18.46225, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.16135, qf2_loss: 0.16196, policy_loss: -34.57002, policy_entropy: 4.09248, alpha: 0.48709, time: 33.09260
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    28 ----
[CW] collect: return: 12.24477, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.10842, qf2_loss: 0.10872, policy_loss: -35.30558, policy_entropy: 4.09214, alpha: 0.47566, time: 33.27198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    29 ----
[CW] collect: return: 28.13196, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.14120, qf2_loss: 0.14186, policy_loss: -36.01541, policy_entropy: 4.09065, alpha: 0.46453, time: 33.54948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    30 ----
[CW] collect: return: 16.07629, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.16545, qf2_loss: 0.16635, policy_loss: -36.70718, policy_entropy: 4.09203, alpha: 0.45370, time: 33.48630
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    31 ----
[CW] collect: return: 11.97510, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.13088, qf2_loss: 0.13146, policy_loss: -37.36313, policy_entropy: 4.09020, alpha: 0.44316, time: 33.60212
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    32 ----
[CW] collect: return: 8.83255, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.16878, qf2_loss: 0.16966, policy_loss: -38.00932, policy_entropy: 4.08968, alpha: 0.43290, time: 33.56950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    33 ----
[CW] collect: return: 13.41426, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.10614, qf2_loss: 0.10686, policy_loss: -38.61152, policy_entropy: 4.09022, alpha: 0.42290, time: 33.47360
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    34 ----
[CW] collect: return: 14.24712, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.17809, qf2_loss: 0.17967, policy_loss: -39.22046, policy_entropy: 4.09005, alpha: 0.41316, time: 33.44126
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    35 ----
[CW] collect: return: 11.56362, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.16662, qf2_loss: 0.16830, policy_loss: -39.79784, policy_entropy: 4.09053, alpha: 0.40367, time: 33.51354
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    36 ----
[CW] collect: return: 33.12532, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.18679, qf2_loss: 0.18891, policy_loss: -40.35932, policy_entropy: 4.08875, alpha: 0.39442, time: 33.56291
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    37 ----
[CW] collect: return: 6.14811, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.12435, qf2_loss: 0.12676, policy_loss: -40.89409, policy_entropy: 4.08936, alpha: 0.38540, time: 33.52678
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    38 ----
[CW] collect: return: 13.87408, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.15007, qf2_loss: 0.15228, policy_loss: -41.39908, policy_entropy: 4.09051, alpha: 0.37660, time: 33.47672
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    39 ----
[CW] collect: return: 17.19044, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.16778, qf2_loss: 0.17074, policy_loss: -41.91121, policy_entropy: 4.09137, alpha: 0.36803, time: 33.43023
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    40 ----
[CW] collect: return: 8.16542, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.14097, qf2_loss: 0.14393, policy_loss: -42.39247, policy_entropy: 4.08962, alpha: 0.35966, time: 33.27224
[CW] eval: return: 14.48891, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    41 ----
[CW] collect: return: 17.81638, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.18675, qf2_loss: 0.18965, policy_loss: -42.85756, policy_entropy: 4.09236, alpha: 0.35150, time: 33.28909
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    42 ----
[CW] collect: return: 16.49854, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.16970, qf2_loss: 0.17227, policy_loss: -43.31144, policy_entropy: 4.09256, alpha: 0.34353, time: 33.34354
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    43 ----
[CW] collect: return: 10.45248, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.15999, qf2_loss: 0.16195, policy_loss: -43.73289, policy_entropy: 4.09150, alpha: 0.33576, time: 33.35715
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    44 ----
[CW] collect: return: 17.13204, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.19101, qf2_loss: 0.19306, policy_loss: -44.17257, policy_entropy: 4.09215, alpha: 0.32818, time: 33.43419
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    45 ----
[CW] collect: return: 14.44822, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.12101, qf2_loss: 0.12204, policy_loss: -44.57795, policy_entropy: 4.08795, alpha: 0.32078, time: 33.48699
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    46 ----
[CW] collect: return: 13.34913, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.16603, qf2_loss: 0.16741, policy_loss: -44.95869, policy_entropy: 4.08794, alpha: 0.31355, time: 33.39935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    47 ----
[CW] collect: return: 18.59158, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.16959, qf2_loss: 0.17060, policy_loss: -45.35767, policy_entropy: 4.08494, alpha: 0.30650, time: 33.54100
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    48 ----
[CW] collect: return: 11.12973, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.17258, qf2_loss: 0.17359, policy_loss: -45.70629, policy_entropy: 4.08330, alpha: 0.29962, time: 33.48324
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    49 ----
[CW] collect: return: 19.01967, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.18738, qf2_loss: 0.18842, policy_loss: -46.04155, policy_entropy: 4.08165, alpha: 0.29290, time: 33.54266
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    50 ----
[CW] collect: return: 9.26905, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.15566, qf2_loss: 0.15613, policy_loss: -46.39236, policy_entropy: 4.07737, alpha: 0.28633, time: 33.41049
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    51 ----
[CW] collect: return: 10.70372, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.18670, qf2_loss: 0.18736, policy_loss: -46.71360, policy_entropy: 4.07429, alpha: 0.27993, time: 33.55149
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    52 ----
[CW] collect: return: 18.73223, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.19570, qf2_loss: 0.19676, policy_loss: -47.02180, policy_entropy: 4.07171, alpha: 0.27367, time: 33.31065
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    53 ----
[CW] collect: return: 15.09234, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.13967, qf2_loss: 0.13963, policy_loss: -47.29989, policy_entropy: 4.07039, alpha: 0.26756, time: 33.41335
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    54 ----
[CW] collect: return: 28.54086, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.14100, qf2_loss: 0.14119, policy_loss: -47.58541, policy_entropy: 4.06220, alpha: 0.26159, time: 33.49404
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    55 ----
[CW] collect: return: 12.27154, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.18686, qf2_loss: 0.18736, policy_loss: -47.85304, policy_entropy: 4.06062, alpha: 0.25576, time: 33.50990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    56 ----
[CW] collect: return: 9.58774, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.15605, qf2_loss: 0.15630, policy_loss: -48.11478, policy_entropy: 4.05617, alpha: 0.25006, time: 33.38842
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    57 ----
[CW] collect: return: 17.82743, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.19225, qf2_loss: 0.19277, policy_loss: -48.34778, policy_entropy: 4.05718, alpha: 0.24450, time: 33.49224
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    58 ----
[CW] collect: return: 10.31271, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.15527, qf2_loss: 0.15584, policy_loss: -48.59307, policy_entropy: 4.04691, alpha: 0.23906, time: 33.18584
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    59 ----
[CW] collect: return: 10.82717, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.20651, qf2_loss: 0.20757, policy_loss: -48.81575, policy_entropy: 4.04902, alpha: 0.23375, time: 33.41641
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    60 ----
[CW] collect: return: 17.06350, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.15281, qf2_loss: 0.15295, policy_loss: -49.02174, policy_entropy: 4.04047, alpha: 0.22856, time: 33.45549
[CW] eval: return: 18.96780, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    61 ----
[CW] collect: return: 15.06369, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.18828, qf2_loss: 0.18898, policy_loss: -49.22546, policy_entropy: 4.03601, alpha: 0.22349, time: 33.37673
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    62 ----
[CW] collect: return: 19.34480, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.18866, qf2_loss: 0.18943, policy_loss: -49.42383, policy_entropy: 4.03593, alpha: 0.21853, time: 33.40245
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    63 ----
[CW] collect: return: 9.75358, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.14297, qf2_loss: 0.14379, policy_loss: -49.59278, policy_entropy: 4.02192, alpha: 0.21369, time: 33.42978
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    64 ----
[CW] collect: return: 26.14375, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.15412, qf2_loss: 0.15505, policy_loss: -49.77688, policy_entropy: 4.01750, alpha: 0.20896, time: 33.14369
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    65 ----
[CW] collect: return: 21.91608, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.20455, qf2_loss: 0.20585, policy_loss: -49.95067, policy_entropy: 4.01235, alpha: 0.20434, time: 33.37518
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    66 ----
[CW] collect: return: 32.57291, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.17739, qf2_loss: 0.17871, policy_loss: -50.10736, policy_entropy: 4.00687, alpha: 0.19982, time: 33.28933
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    67 ----
[CW] collect: return: 19.35285, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.19139, qf2_loss: 0.19284, policy_loss: -50.25188, policy_entropy: 3.99503, alpha: 0.19540, time: 33.41762
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    68 ----
[CW] collect: return: 23.25249, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.14293, qf2_loss: 0.14440, policy_loss: -50.37190, policy_entropy: 3.99448, alpha: 0.19109, time: 33.40027
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    69 ----
[CW] collect: return: 23.84649, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.24698, qf2_loss: 0.24873, policy_loss: -50.50958, policy_entropy: 3.98459, alpha: 0.18687, time: 33.43496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    70 ----
[CW] collect: return: 25.33433, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.15226, qf2_loss: 0.15382, policy_loss: -50.63267, policy_entropy: 3.96864, alpha: 0.18275, time: 33.28913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    71 ----
[CW] collect: return: 19.94066, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.20126, qf2_loss: 0.20314, policy_loss: -50.75496, policy_entropy: 3.94398, alpha: 0.17873, time: 33.18916
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    72 ----
[CW] collect: return: 20.05892, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.22634, qf2_loss: 0.22769, policy_loss: -50.85782, policy_entropy: 3.93268, alpha: 0.17480, time: 33.35816
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    73 ----
[CW] collect: return: 28.82617, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.14717, qf2_loss: 0.14905, policy_loss: -50.96324, policy_entropy: 3.89689, alpha: 0.17096, time: 33.38198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    74 ----
[CW] collect: return: 24.98412, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.22871, qf2_loss: 0.23075, policy_loss: -51.04566, policy_entropy: 3.87937, alpha: 0.16722, time: 33.29843
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    75 ----
[CW] collect: return: 31.72316, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.18786, qf2_loss: 0.19078, policy_loss: -51.13821, policy_entropy: 3.84323, alpha: 0.16356, time: 33.19000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    76 ----
[CW] collect: return: 49.72797, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.21663, qf2_loss: 0.21879, policy_loss: -51.23200, policy_entropy: 3.81996, alpha: 0.15999, time: 33.25913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    77 ----
[CW] collect: return: 44.63911, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.22288, qf2_loss: 0.22560, policy_loss: -51.33013, policy_entropy: 3.77551, alpha: 0.15651, time: 33.07324
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    78 ----
[CW] collect: return: 34.91244, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.21101, qf2_loss: 0.21321, policy_loss: -51.39065, policy_entropy: 3.74842, alpha: 0.15311, time: 33.30457
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    79 ----
[CW] collect: return: 51.60632, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.30913, qf2_loss: 0.31071, policy_loss: -51.49682, policy_entropy: 3.67454, alpha: 0.14979, time: 33.26038
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    80 ----
[CW] collect: return: 48.21220, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.22489, qf2_loss: 0.22685, policy_loss: -51.57575, policy_entropy: 3.61857, alpha: 0.14657, time: 33.36642
[CW] eval: return: 45.93898, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    81 ----
[CW] collect: return: 44.69881, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.21976, qf2_loss: 0.22241, policy_loss: -51.63237, policy_entropy: 3.50036, alpha: 0.14343, time: 33.25541
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    82 ----
[CW] collect: return: 75.45736, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.32865, qf2_loss: 0.33027, policy_loss: -51.74695, policy_entropy: 3.41572, alpha: 0.14038, time: 33.36674
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    83 ----
[CW] collect: return: 68.32986, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.28241, qf2_loss: 0.28508, policy_loss: -51.82759, policy_entropy: 3.16233, alpha: 0.13744, time: 33.20014
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    84 ----
[CW] collect: return: 50.41147, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.22866, qf2_loss: 0.23112, policy_loss: -51.88869, policy_entropy: 2.90095, alpha: 0.13462, time: 33.39034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    85 ----
[CW] collect: return: 71.31626, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.27431, qf2_loss: 0.27732, policy_loss: -52.02040, policy_entropy: 2.61777, alpha: 0.13193, time: 33.27871
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    86 ----
[CW] collect: return: 39.66946, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.29068, qf2_loss: 0.29242, policy_loss: -52.09846, policy_entropy: 2.49723, alpha: 0.12933, time: 33.30251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    87 ----
[CW] collect: return: 94.35428, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.32737, qf2_loss: 0.32940, policy_loss: -52.23983, policy_entropy: 2.18934, alpha: 0.12682, time: 33.31173
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    88 ----
[CW] collect: return: 93.27504, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.31206, qf2_loss: 0.31505, policy_loss: -52.41060, policy_entropy: 1.94801, alpha: 0.12442, time: 33.31253
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    89 ----
[CW] collect: return: 77.18320, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.33325, qf2_loss: 0.33629, policy_loss: -52.57233, policy_entropy: 1.67388, alpha: 0.12211, time: 33.15500
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    90 ----
[CW] collect: return: 74.94957, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.33939, qf2_loss: 0.34276, policy_loss: -52.73503, policy_entropy: 1.46100, alpha: 0.11989, time: 33.30193
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    91 ----
[CW] collect: return: 90.87020, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.32300, qf2_loss: 0.32665, policy_loss: -52.98641, policy_entropy: 1.33176, alpha: 0.11773, time: 33.38561
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    92 ----
[CW] collect: return: 46.46546, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.36011, qf2_loss: 0.36440, policy_loss: -53.09604, policy_entropy: 1.17990, alpha: 0.11563, time: 33.33292
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    93 ----
[CW] collect: return: 112.93110, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.33822, qf2_loss: 0.34225, policy_loss: -53.31415, policy_entropy: 1.07198, alpha: 0.11356, time: 33.44605
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    94 ----
[CW] collect: return: 74.70419, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.33686, qf2_loss: 0.34046, policy_loss: -53.56815, policy_entropy: 0.88294, alpha: 0.11155, time: 33.33274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    95 ----
[CW] collect: return: 56.11567, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.34555, qf2_loss: 0.34826, policy_loss: -53.74948, policy_entropy: 0.65113, alpha: 0.10960, time: 33.18948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    96 ----
[CW] collect: return: 62.92002, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.38286, qf2_loss: 0.38510, policy_loss: -53.99407, policy_entropy: 0.48232, alpha: 0.10771, time: 33.29161
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    97 ----
[CW] collect: return: 28.38245, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.33612, qf2_loss: 0.33900, policy_loss: -54.20890, policy_entropy: 0.34503, alpha: 0.10588, time: 33.31395
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    98 ----
[CW] collect: return: 64.07577, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.36330, qf2_loss: 0.36668, policy_loss: -54.43970, policy_entropy: 0.22503, alpha: 0.10408, time: 33.33544
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    99 ----
[CW] collect: return: 46.27473, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.33409, qf2_loss: 0.33750, policy_loss: -54.67582, policy_entropy: 0.13095, alpha: 0.10231, time: 33.29268
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   100 ----
[CW] collect: return: 59.65492, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.36266, qf2_loss: 0.36355, policy_loss: -54.89417, policy_entropy: 0.04737, alpha: 0.10057, time: 33.25993
[CW] eval: return: 65.84914, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   101 ----
[CW] collect: return: 57.52506, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.34837, qf2_loss: 0.35002, policy_loss: -55.09808, policy_entropy: -0.04525, alpha: 0.09885, time: 33.02799
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   102 ----
[CW] collect: return: 28.94485, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.33518, qf2_loss: 0.33593, policy_loss: -55.24064, policy_entropy: -0.08575, alpha: 0.09715, time: 33.15468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   103 ----
[CW] collect: return: 62.78897, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.29435, qf2_loss: 0.29632, policy_loss: -55.47337, policy_entropy: -0.22839, alpha: 0.09548, time: 33.26617
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   104 ----
[CW] collect: return: 34.30776, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.30161, qf2_loss: 0.30389, policy_loss: -55.62451, policy_entropy: -0.37821, alpha: 0.09386, time: 33.16027
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   105 ----
[CW] collect: return: 68.60957, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.32270, qf2_loss: 0.32467, policy_loss: -55.72240, policy_entropy: -0.23270, alpha: 0.09225, time: 33.15921
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   106 ----
[CW] collect: return: 30.50931, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.28366, qf2_loss: 0.28513, policy_loss: -55.93260, policy_entropy: -0.11968, alpha: 0.09059, time: 33.30431
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   107 ----
[CW] collect: return: 50.14336, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.27616, qf2_loss: 0.28055, policy_loss: -55.99287, policy_entropy: -0.23543, alpha: 0.08895, time: 33.35314
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   108 ----
[CW] collect: return: 81.63845, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.33695, qf2_loss: 0.34000, policy_loss: -56.02349, policy_entropy: 0.14640, alpha: 0.08729, time: 33.65643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   109 ----
[CW] collect: return: 51.65945, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.28131, qf2_loss: 0.28332, policy_loss: -56.13027, policy_entropy: 0.20423, alpha: 0.08557, time: 38.47269
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   110 ----
[CW] collect: return: 88.64688, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.26850, qf2_loss: 0.27114, policy_loss: -56.21542, policy_entropy: 0.23633, alpha: 0.08387, time: 33.87110
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   111 ----
[CW] collect: return: 40.18918, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.37079, qf2_loss: 0.37110, policy_loss: -56.21515, policy_entropy: 0.46010, alpha: 0.08215, time: 33.75245
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   112 ----
[CW] collect: return: 57.11889, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.25708, qf2_loss: 0.25883, policy_loss: -56.32548, policy_entropy: 0.39507, alpha: 0.08044, time: 33.83102
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   113 ----
[CW] collect: return: 62.66263, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.22595, qf2_loss: 0.22677, policy_loss: -56.28540, policy_entropy: 0.47319, alpha: 0.07875, time: 33.73138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   114 ----
[CW] collect: return: 38.49775, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.28763, qf2_loss: 0.29016, policy_loss: -56.36911, policy_entropy: 0.44290, alpha: 0.07709, time: 33.65058
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   115 ----
[CW] collect: return: 39.82874, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.31579, qf2_loss: 0.31860, policy_loss: -56.31685, policy_entropy: 0.42491, alpha: 0.07547, time: 33.78075
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   116 ----
[CW] collect: return: 30.25255, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.24011, qf2_loss: 0.24089, policy_loss: -56.35438, policy_entropy: 0.39053, alpha: 0.07387, time: 33.70178
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   117 ----
[CW] collect: return: 44.39986, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.29426, qf2_loss: 0.29590, policy_loss: -56.28962, policy_entropy: 0.32279, alpha: 0.07232, time: 33.97075
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   118 ----
[CW] collect: return: 132.96687, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.26060, qf2_loss: 0.26144, policy_loss: -56.45748, policy_entropy: 0.28737, alpha: 0.07081, time: 33.78298
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   119 ----
[CW] collect: return: 100.90844, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.38964, qf2_loss: 0.39109, policy_loss: -56.51838, policy_entropy: 0.17834, alpha: 0.06933, time: 33.80398
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   120 ----
[CW] collect: return: 123.76548, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.29014, qf2_loss: 0.28976, policy_loss: -56.51308, policy_entropy: 0.18924, alpha: 0.06789, time: 33.53930
[CW] eval: return: 77.93466, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   121 ----
[CW] collect: return: 139.15166, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.33343, qf2_loss: 0.33342, policy_loss: -56.57127, policy_entropy: 0.03395, alpha: 0.06650, time: 33.74524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   122 ----
[CW] collect: return: 114.80581, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.49606, qf2_loss: 0.49716, policy_loss: -56.80167, policy_entropy: -0.10863, alpha: 0.06515, time: 33.78172
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   123 ----
[CW] collect: return: 66.61378, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.35312, qf2_loss: 0.35200, policy_loss: -56.76730, policy_entropy: -0.22964, alpha: 0.06385, time: 33.70080
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   124 ----
[CW] collect: return: 64.86240, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.27394, qf2_loss: 0.27247, policy_loss: -56.80205, policy_entropy: -0.35090, alpha: 0.06260, time: 33.81437
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   125 ----
[CW] collect: return: 62.04717, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.32382, qf2_loss: 0.32275, policy_loss: -56.76639, policy_entropy: -0.32743, alpha: 0.06137, time: 33.75944
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   126 ----
[CW] collect: return: 123.07838, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.31871, qf2_loss: 0.31761, policy_loss: -57.02000, policy_entropy: -0.55312, alpha: 0.06017, time: 33.81032
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   127 ----
[CW] collect: return: 84.74044, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.28982, qf2_loss: 0.28844, policy_loss: -56.97905, policy_entropy: -0.58207, alpha: 0.05902, time: 33.71572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   128 ----
[CW] collect: return: 56.59618, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.30581, qf2_loss: 0.30415, policy_loss: -57.12171, policy_entropy: -0.68792, alpha: 0.05789, time: 33.85905
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   129 ----
[CW] collect: return: 105.14331, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.36601, qf2_loss: 0.36409, policy_loss: -57.16347, policy_entropy: -0.74714, alpha: 0.05679, time: 33.75272
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   130 ----
[CW] collect: return: 135.01767, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.35039, qf2_loss: 0.34795, policy_loss: -57.20665, policy_entropy: -0.81168, alpha: 0.05571, time: 33.83821
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   131 ----
[CW] collect: return: 52.78951, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.33972, qf2_loss: 0.33751, policy_loss: -57.18346, policy_entropy: -0.81036, alpha: 0.05464, time: 33.72063
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   132 ----
[CW] collect: return: 62.94232, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.42242, qf2_loss: 0.42267, policy_loss: -57.38791, policy_entropy: -0.93862, alpha: 0.05361, time: 33.55198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   133 ----
[CW] collect: return: 74.99878, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.38384, qf2_loss: 0.38133, policy_loss: -57.38136, policy_entropy: -0.86849, alpha: 0.05258, time: 33.86753
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   134 ----
[CW] collect: return: 53.83895, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.35067, qf2_loss: 0.34969, policy_loss: -57.44397, policy_entropy: -1.02312, alpha: 0.05157, time: 33.73637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   135 ----
[CW] collect: return: 156.98279, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.36705, qf2_loss: 0.36396, policy_loss: -57.59953, policy_entropy: -0.98904, alpha: 0.05059, time: 33.86274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   136 ----
[CW] collect: return: 115.01144, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.43650, qf2_loss: 0.43616, policy_loss: -57.60800, policy_entropy: -0.92318, alpha: 0.04960, time: 33.80074
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   137 ----
[CW] collect: return: 133.48633, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.57914, qf2_loss: 0.58262, policy_loss: -57.64376, policy_entropy: -0.79518, alpha: 0.04861, time: 33.82879
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   138 ----
[CW] collect: return: 142.74013, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.39549, qf2_loss: 0.39524, policy_loss: -57.60097, policy_entropy: -0.90939, alpha: 0.04762, time: 33.50659
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   139 ----
[CW] collect: return: 177.34642, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.33960, qf2_loss: 0.33818, policy_loss: -57.79146, policy_entropy: -0.99674, alpha: 0.04668, time: 33.67072
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   140 ----
[CW] collect: return: 76.23948, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.33421, qf2_loss: 0.33194, policy_loss: -57.91397, policy_entropy: -1.04014, alpha: 0.04575, time: 33.81356
[CW] eval: return: 108.61786, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   141 ----
[CW] collect: return: 127.61642, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.38047, qf2_loss: 0.38016, policy_loss: -58.00722, policy_entropy: -1.04186, alpha: 0.04484, time: 33.79376
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   142 ----
[CW] collect: return: 66.75081, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.44811, qf2_loss: 0.44756, policy_loss: -57.94690, policy_entropy: -0.98834, alpha: 0.04394, time: 33.84852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   143 ----
[CW] collect: return: 135.54824, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.40488, qf2_loss: 0.40338, policy_loss: -58.08890, policy_entropy: -1.08490, alpha: 0.04305, time: 33.82643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   144 ----
[CW] collect: return: 137.77548, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.43289, qf2_loss: 0.43289, policy_loss: -58.12469, policy_entropy: -1.18901, alpha: 0.04219, time: 33.65332
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   145 ----
[CW] collect: return: 112.65492, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.36845, qf2_loss: 0.36732, policy_loss: -58.18176, policy_entropy: -1.19924, alpha: 0.04136, time: 33.82918
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   146 ----
[CW] collect: return: 43.38023, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.37142, qf2_loss: 0.37185, policy_loss: -58.20165, policy_entropy: -1.24982, alpha: 0.04054, time: 33.92830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   147 ----
[CW] collect: return: 114.60388, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.57503, qf2_loss: 0.57516, policy_loss: -58.14852, policy_entropy: -1.16912, alpha: 0.03973, time: 33.83650
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   148 ----
[CW] collect: return: 66.49315, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.40559, qf2_loss: 0.40557, policy_loss: -58.17405, policy_entropy: -1.34757, alpha: 0.03893, time: 33.88284
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   149 ----
[CW] collect: return: 228.12532, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.37319, qf2_loss: 0.37266, policy_loss: -58.35551, policy_entropy: -1.49927, alpha: 0.03817, time: 33.82338
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   150 ----
[CW] collect: return: 109.65926, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.44410, qf2_loss: 0.44374, policy_loss: -58.49308, policy_entropy: -1.71644, alpha: 0.03745, time: 33.66663
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   151 ----
[CW] collect: return: 64.52409, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.39371, qf2_loss: 0.39344, policy_loss: -58.30612, policy_entropy: -1.72157, alpha: 0.03676, time: 33.84581
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   152 ----
[CW] collect: return: 56.58887, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.43096, qf2_loss: 0.43105, policy_loss: -58.52746, policy_entropy: -1.93719, alpha: 0.03608, time: 33.74690
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   153 ----
[CW] collect: return: 78.89064, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.47766, qf2_loss: 0.47934, policy_loss: -58.35897, policy_entropy: -2.05143, alpha: 0.03543, time: 33.87415
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   154 ----
[CW] collect: return: 204.66632, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.43467, qf2_loss: 0.43354, policy_loss: -58.79668, policy_entropy: -2.43695, alpha: 0.03482, time: 33.78310
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   155 ----
[CW] collect: return: 80.05467, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.43830, qf2_loss: 0.43714, policy_loss: -58.59423, policy_entropy: -2.50980, alpha: 0.03426, time: 33.84145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   156 ----
[CW] collect: return: 48.14396, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.42746, qf2_loss: 0.42647, policy_loss: -58.61513, policy_entropy: -2.64881, alpha: 0.03370, time: 33.68272
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   157 ----
[CW] collect: return: 47.47842, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.47357, qf2_loss: 0.47387, policy_loss: -58.73513, policy_entropy: -2.88768, alpha: 0.03318, time: 33.62619
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   158 ----
[CW] collect: return: 66.64930, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.46502, qf2_loss: 0.46598, policy_loss: -58.85592, policy_entropy: -2.96875, alpha: 0.03268, time: 33.78684
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   159 ----
[CW] collect: return: 20.08956, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.46806, qf2_loss: 0.47002, policy_loss: -58.84546, policy_entropy: -3.10409, alpha: 0.03220, time: 33.70356
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   160 ----
[CW] collect: return: 33.32087, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.47683, qf2_loss: 0.47680, policy_loss: -58.82085, policy_entropy: -3.12633, alpha: 0.03172, time: 33.93500
[CW] eval: return: 103.94822, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   161 ----
[CW] collect: return: 71.14657, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.41083, qf2_loss: 0.41085, policy_loss: -58.88557, policy_entropy: -3.25647, alpha: 0.03125, time: 33.72504
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   162 ----
[CW] collect: return: 127.22453, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.58995, qf2_loss: 0.58952, policy_loss: -59.51150, policy_entropy: -3.38463, alpha: 0.03080, time: 33.59908
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   163 ----
[CW] collect: return: 78.11307, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.45379, qf2_loss: 0.45220, policy_loss: -59.05168, policy_entropy: -3.14868, alpha: 0.03035, time: 33.71423
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   164 ----
[CW] collect: return: 61.58767, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.53107, qf2_loss: 0.53138, policy_loss: -59.34772, policy_entropy: -3.20422, alpha: 0.02987, time: 33.70342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   165 ----
[CW] collect: return: 67.38297, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.48545, qf2_loss: 0.48671, policy_loss: -59.32094, policy_entropy: -3.28197, alpha: 0.02939, time: 33.78913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   166 ----
[CW] collect: return: 295.04180, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.53200, qf2_loss: 0.53167, policy_loss: -59.45948, policy_entropy: -3.33699, alpha: 0.02894, time: 33.72465
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   167 ----
[CW] collect: return: 140.82962, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.51408, qf2_loss: 0.51309, policy_loss: -59.38745, policy_entropy: -3.35513, alpha: 0.02848, time: 33.84325
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   168 ----
[CW] collect: return: 56.20745, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.53281, qf2_loss: 0.53416, policy_loss: -59.42212, policy_entropy: -3.36853, alpha: 0.02803, time: 33.61658
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   169 ----
[CW] collect: return: 26.61454, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.58114, qf2_loss: 0.58448, policy_loss: -59.49463, policy_entropy: -3.48315, alpha: 0.02758, time: 33.81933
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   170 ----
[CW] collect: return: 260.44836, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.63570, qf2_loss: 0.63378, policy_loss: -59.95292, policy_entropy: -3.62204, alpha: 0.02715, time: 33.86546
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   171 ----
[CW] collect: return: 114.91623, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.47657, qf2_loss: 0.47652, policy_loss: -59.79442, policy_entropy: -3.64023, alpha: 0.02674, time: 33.77606
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   172 ----
[CW] collect: return: 218.17728, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.50317, qf2_loss: 0.50163, policy_loss: -59.96308, policy_entropy: -3.83335, alpha: 0.02634, time: 33.74583
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   173 ----
[CW] collect: return: 218.06966, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.53805, qf2_loss: 0.53905, policy_loss: -60.33569, policy_entropy: -4.15814, alpha: 0.02598, time: 33.78401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   174 ----
[CW] collect: return: 90.23836, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.56161, qf2_loss: 0.56119, policy_loss: -60.22404, policy_entropy: -4.23666, alpha: 0.02566, time: 33.93613
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   175 ----
[CW] collect: return: 124.88279, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.64691, qf2_loss: 0.64943, policy_loss: -60.24484, policy_entropy: -4.18027, alpha: 0.02533, time: 33.51883
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   176 ----
[CW] collect: return: 247.07064, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.68258, qf2_loss: 0.68636, policy_loss: -60.47987, policy_entropy: -4.13050, alpha: 0.02499, time: 33.85002
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   177 ----
[CW] collect: return: 210.94328, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.59105, qf2_loss: 0.58873, policy_loss: -60.81796, policy_entropy: -4.16692, alpha: 0.02464, time: 33.85140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   178 ----
[CW] collect: return: 245.25700, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.58441, qf2_loss: 0.58258, policy_loss: -61.00566, policy_entropy: -4.22245, alpha: 0.02430, time: 33.64744
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   179 ----
[CW] collect: return: 56.57002, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.58915, qf2_loss: 0.59200, policy_loss: -60.69102, policy_entropy: -4.09682, alpha: 0.02395, time: 33.81001
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   180 ----
[CW] collect: return: 119.91389, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.98793, qf2_loss: 0.99208, policy_loss: -61.30168, policy_entropy: -4.14409, alpha: 0.02359, time: 33.73070
[CW] eval: return: 137.21109, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   181 ----
[CW] collect: return: 149.59884, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.53739, qf2_loss: 0.53868, policy_loss: -61.16660, policy_entropy: -4.22367, alpha: 0.02323, time: 33.69514
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   182 ----
[CW] collect: return: 269.84744, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.57830, qf2_loss: 0.57882, policy_loss: -61.37867, policy_entropy: -4.64355, alpha: 0.02291, time: 33.57881
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   183 ----
[CW] collect: return: 243.27222, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.56287, qf2_loss: 0.56254, policy_loss: -61.83565, policy_entropy: -5.18585, alpha: 0.02270, time: 33.83073
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   184 ----
[CW] collect: return: 100.65941, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.78909, qf2_loss: 0.79178, policy_loss: -62.25925, policy_entropy: -5.19529, alpha: 0.02254, time: 33.74939
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   185 ----
[CW] collect: return: 173.35087, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.69094, qf2_loss: 0.68757, policy_loss: -62.30993, policy_entropy: -5.07138, alpha: 0.02236, time: 33.72567
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   186 ----
[CW] collect: return: 76.81748, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.61791, qf2_loss: 0.61551, policy_loss: -62.02074, policy_entropy: -4.93740, alpha: 0.02215, time: 33.80364
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   187 ----
[CW] collect: return: 71.42225, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.59969, qf2_loss: 0.59989, policy_loss: -62.32888, policy_entropy: -5.08440, alpha: 0.02192, time: 33.59120
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   188 ----
[CW] collect: return: 88.71617, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.66804, qf2_loss: 0.66912, policy_loss: -61.87278, policy_entropy: -4.88015, alpha: 0.02169, time: 33.90660
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   189 ----
[CW] collect: return: 197.13645, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.69894, qf2_loss: 0.69887, policy_loss: -62.40684, policy_entropy: -4.94878, alpha: 0.02144, time: 33.73084
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   190 ----
[CW] collect: return: 246.90337, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.60890, qf2_loss: 0.60673, policy_loss: -62.34060, policy_entropy: -4.72523, alpha: 0.02116, time: 33.77941
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   191 ----
[CW] collect: return: 217.38985, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 1.13117, qf2_loss: 1.13443, policy_loss: -62.65922, policy_entropy: -5.03886, alpha: 0.02090, time: 33.77815
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   192 ----
[CW] collect: return: 211.80972, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.67719, qf2_loss: 0.68419, policy_loss: -62.89684, policy_entropy: -5.16049, alpha: 0.02066, time: 33.81380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   193 ----
[CW] collect: return: 257.93912, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.58436, qf2_loss: 0.58718, policy_loss: -62.80850, policy_entropy: -5.39263, alpha: 0.02050, time: 33.76406
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   194 ----
[CW] collect: return: 113.10908, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.54990, qf2_loss: 0.55170, policy_loss: -62.87474, policy_entropy: -5.47562, alpha: 0.02035, time: 33.73467
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   195 ----
[CW] collect: return: 108.11600, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.65072, qf2_loss: 0.65076, policy_loss: -63.26914, policy_entropy: -5.34758, alpha: 0.02021, time: 33.78958
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   196 ----
[CW] collect: return: 161.48020, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.91326, qf2_loss: 0.91830, policy_loss: -62.90664, policy_entropy: -5.14547, alpha: 0.02001, time: 33.62544
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   197 ----
[CW] collect: return: 54.76102, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.71798, qf2_loss: 0.71935, policy_loss: -63.22689, policy_entropy: -5.33663, alpha: 0.01977, time: 33.76125
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   198 ----
[CW] collect: return: 100.12200, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.68319, qf2_loss: 0.68671, policy_loss: -63.50006, policy_entropy: -5.48169, alpha: 0.01961, time: 33.70653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   199 ----
[CW] collect: return: 231.02992, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.56471, qf2_loss: 0.56499, policy_loss: -63.35071, policy_entropy: -5.43199, alpha: 0.01947, time: 33.73935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   200 ----
[CW] collect: return: 201.30834, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.67777, qf2_loss: 0.67912, policy_loss: -63.45797, policy_entropy: -5.49471, alpha: 0.01929, time: 33.84313
[CW] eval: return: 159.40488, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   201 ----
[CW] collect: return: 261.90631, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.70391, qf2_loss: 0.70127, policy_loss: -63.73034, policy_entropy: -5.72685, alpha: 0.01915, time: 33.72535
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   202 ----
[CW] collect: return: 46.85084, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.71959, qf2_loss: 0.72097, policy_loss: -63.71854, policy_entropy: -5.86748, alpha: 0.01910, time: 33.70403
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   203 ----
[CW] collect: return: 256.58172, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.77078, qf2_loss: 0.77748, policy_loss: -64.29767, policy_entropy: -5.69180, alpha: 0.01904, time: 33.85720
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   204 ----
[CW] collect: return: 73.65575, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.60632, qf2_loss: 0.60618, policy_loss: -64.24469, policy_entropy: -5.82265, alpha: 0.01894, time: 33.88468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   205 ----
[CW] collect: return: 265.49590, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.76554, qf2_loss: 0.76926, policy_loss: -64.35261, policy_entropy: -5.65541, alpha: 0.01886, time: 33.59036
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   206 ----
[CW] collect: return: 204.05098, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.70124, qf2_loss: 0.69966, policy_loss: -64.45267, policy_entropy: -5.61796, alpha: 0.01871, time: 33.88585
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   207 ----
[CW] collect: return: 260.93078, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.86684, qf2_loss: 0.87255, policy_loss: -64.89974, policy_entropy: -5.54311, alpha: 0.01856, time: 33.81775
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   208 ----
[CW] collect: return: 279.46154, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.72313, qf2_loss: 0.72562, policy_loss: -64.45780, policy_entropy: -5.38555, alpha: 0.01834, time: 33.86925
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   209 ----
[CW] collect: return: 249.36346, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.69734, qf2_loss: 0.69741, policy_loss: -65.29835, policy_entropy: -5.83055, alpha: 0.01819, time: 33.74159
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   210 ----
[CW] collect: return: 65.26946, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 1.01260, qf2_loss: 1.01992, policy_loss: -64.96532, policy_entropy: -5.56313, alpha: 0.01807, time: 33.76185
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   211 ----
[CW] collect: return: 280.07665, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.68778, qf2_loss: 0.68878, policy_loss: -65.33856, policy_entropy: -5.70134, alpha: 0.01790, time: 33.78481
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   212 ----
[CW] collect: return: 108.60516, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.65512, qf2_loss: 0.65848, policy_loss: -65.27139, policy_entropy: -5.68938, alpha: 0.01779, time: 33.75385
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   213 ----
[CW] collect: return: 254.41377, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.78212, qf2_loss: 0.78488, policy_loss: -65.72963, policy_entropy: -5.90525, alpha: 0.01770, time: 33.78652
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   214 ----
[CW] collect: return: 147.12137, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 1.03583, qf2_loss: 1.03164, policy_loss: -66.04412, policy_entropy: -5.86496, alpha: 0.01765, time: 33.78266
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   215 ----
[CW] collect: return: 39.12393, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.84082, qf2_loss: 0.84481, policy_loss: -65.80534, policy_entropy: -6.15177, alpha: 0.01760, time: 33.86166
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   216 ----
[CW] collect: return: 178.78735, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.71138, qf2_loss: 0.71413, policy_loss: -66.58772, policy_entropy: -6.22710, alpha: 0.01773, time: 33.79523
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   217 ----
[CW] collect: return: 279.11302, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.78971, qf2_loss: 0.79286, policy_loss: -66.30204, policy_entropy: -6.32619, alpha: 0.01786, time: 33.87725
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   218 ----
[CW] collect: return: 206.33257, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.87097, qf2_loss: 0.87211, policy_loss: -66.12042, policy_entropy: -6.15867, alpha: 0.01802, time: 38.07153
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   219 ----
[CW] collect: return: 227.61852, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.94405, qf2_loss: 0.94312, policy_loss: -66.82659, policy_entropy: -6.14416, alpha: 0.01810, time: 33.97988
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   220 ----
[CW] collect: return: 106.09760, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.69036, qf2_loss: 0.69092, policy_loss: -66.65022, policy_entropy: -6.20548, alpha: 0.01819, time: 33.90877
[CW] eval: return: 170.98920, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   221 ----
[CW] collect: return: 100.29153, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.68021, qf2_loss: 0.68222, policy_loss: -66.94525, policy_entropy: -6.16502, alpha: 0.01832, time: 33.68859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   222 ----
[CW] collect: return: 297.31662, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.94449, qf2_loss: 0.94485, policy_loss: -66.85416, policy_entropy: -5.87166, alpha: 0.01833, time: 33.86955
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   223 ----
[CW] collect: return: 252.88263, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.85930, qf2_loss: 0.86345, policy_loss: -66.87403, policy_entropy: -5.97002, alpha: 0.01826, time: 33.62576
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   224 ----
[CW] collect: return: 151.58480, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.72748, qf2_loss: 0.72580, policy_loss: -67.42673, policy_entropy: -6.10430, alpha: 0.01832, time: 33.92149
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   225 ----
[CW] collect: return: 91.29869, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.90319, qf2_loss: 0.90958, policy_loss: -67.30185, policy_entropy: -5.86904, alpha: 0.01834, time: 33.72287
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   226 ----
[CW] collect: return: 107.79017, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.78766, qf2_loss: 0.78710, policy_loss: -67.16833, policy_entropy: -5.94231, alpha: 0.01822, time: 33.91492
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   227 ----
[CW] collect: return: 285.76541, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.72545, qf2_loss: 0.72435, policy_loss: -67.75957, policy_entropy: -5.74902, alpha: 0.01816, time: 33.83131
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   228 ----
[CW] collect: return: 303.43939, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 1.10678, qf2_loss: 1.11090, policy_loss: -67.68134, policy_entropy: -5.41835, alpha: 0.01784, time: 33.81821
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   229 ----
[CW] collect: return: 80.28388, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.82912, qf2_loss: 0.82905, policy_loss: -67.74012, policy_entropy: -5.65634, alpha: 0.01738, time: 33.92491
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   230 ----
[CW] collect: return: 306.87220, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.81350, qf2_loss: 0.81532, policy_loss: -67.48835, policy_entropy: -6.83091, alpha: 0.01757, time: 33.55809
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   231 ----
[CW] collect: return: 88.35382, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 0.80479, qf2_loss: 0.80578, policy_loss: -67.70234, policy_entropy: -6.62030, alpha: 0.01811, time: 33.94610
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   232 ----
[CW] collect: return: 205.74903, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.99768, qf2_loss: 1.00176, policy_loss: -67.83267, policy_entropy: -6.39140, alpha: 0.01852, time: 33.77605
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   233 ----
[CW] collect: return: 290.44479, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.85756, qf2_loss: 0.85739, policy_loss: -68.12848, policy_entropy: -6.50864, alpha: 0.01890, time: 33.90822
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   234 ----
[CW] collect: return: 76.95848, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.90887, qf2_loss: 0.90986, policy_loss: -68.47224, policy_entropy: -6.20203, alpha: 0.01924, time: 33.83555
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   235 ----
[CW] collect: return: 145.13851, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.90165, qf2_loss: 0.89946, policy_loss: -68.07391, policy_entropy: -5.89456, alpha: 0.01933, time: 33.70866
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   236 ----
[CW] collect: return: 273.20463, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.88313, qf2_loss: 0.88803, policy_loss: -68.25986, policy_entropy: -5.89876, alpha: 0.01914, time: 33.72443
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   237 ----
[CW] collect: return: 294.08096, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 1.32067, qf2_loss: 1.31773, policy_loss: -68.65892, policy_entropy: -6.27131, alpha: 0.01925, time: 33.81578
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   238 ----
[CW] collect: return: 117.77956, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 1.50831, qf2_loss: 1.51195, policy_loss: -68.61665, policy_entropy: -6.09250, alpha: 0.01946, time: 33.90610
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   239 ----
[CW] collect: return: 183.29744, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 0.96239, qf2_loss: 0.96356, policy_loss: -69.40162, policy_entropy: -6.16393, alpha: 0.01956, time: 33.70577
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   240 ----
[CW] collect: return: 172.65365, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.89846, qf2_loss: 0.89156, policy_loss: -68.62724, policy_entropy: -6.09300, alpha: 0.01971, time: 33.89574
[CW] eval: return: 174.13034, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   241 ----
[CW] collect: return: 124.13768, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 1.22255, qf2_loss: 1.22684, policy_loss: -68.85766, policy_entropy: -6.07987, alpha: 0.01979, time: 33.75321
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   242 ----
[CW] collect: return: 318.97628, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 0.97768, qf2_loss: 0.97217, policy_loss: -69.45379, policy_entropy: -6.25733, alpha: 0.01999, time: 33.57196
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   243 ----
[CW] collect: return: 85.27129, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 0.93579, qf2_loss: 0.93576, policy_loss: -69.85352, policy_entropy: -6.26000, alpha: 0.02030, time: 33.65328
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   244 ----
[CW] collect: return: 254.93516, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 0.90364, qf2_loss: 0.89786, policy_loss: -69.62562, policy_entropy: -6.25714, alpha: 0.02062, time: 33.76547
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   245 ----
[CW] collect: return: 301.25461, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 0.99985, qf2_loss: 0.99853, policy_loss: -69.93129, policy_entropy: -6.24364, alpha: 0.02093, time: 33.81841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   246 ----
[CW] collect: return: 124.82517, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 1.15341, qf2_loss: 1.15160, policy_loss: -70.15890, policy_entropy: -6.18739, alpha: 0.02126, time: 33.81905
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   247 ----
[CW] collect: return: 151.89220, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 1.23316, qf2_loss: 1.23530, policy_loss: -69.94335, policy_entropy: -6.04372, alpha: 0.02147, time: 33.82136
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   248 ----
[CW] collect: return: 284.75097, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 1.69944, qf2_loss: 1.70508, policy_loss: -70.35497, policy_entropy: -5.99990, alpha: 0.02152, time: 33.47993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   249 ----
[CW] collect: return: 86.43078, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 0.95258, qf2_loss: 0.95669, policy_loss: -70.73860, policy_entropy: -6.10389, alpha: 0.02152, time: 33.85509
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   250 ----
[CW] collect: return: 212.19315, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 0.93270, qf2_loss: 0.92757, policy_loss: -70.84418, policy_entropy: -6.11913, alpha: 0.02169, time: 33.69838
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   251 ----
[CW] collect: return: 290.32809, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 1.57949, qf2_loss: 1.58489, policy_loss: -70.06606, policy_entropy: -5.95065, alpha: 0.02183, time: 33.77889
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   252 ----
[CW] collect: return: 110.57163, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 1.06768, qf2_loss: 1.06761, policy_loss: -70.89285, policy_entropy: -5.92514, alpha: 0.02176, time: 33.74394
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   253 ----
[CW] collect: return: 272.68061, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 0.99951, qf2_loss: 1.00656, policy_loss: -70.86887, policy_entropy: -5.84050, alpha: 0.02155, time: 33.95899
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   254 ----
[CW] collect: return: 276.98398, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 1.21250, qf2_loss: 1.22610, policy_loss: -70.81976, policy_entropy: -5.82886, alpha: 0.02131, time: 33.66994
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   255 ----
[CW] collect: return: 85.08379, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 1.18585, qf2_loss: 1.18638, policy_loss: -70.81430, policy_entropy: -5.85696, alpha: 0.02109, time: 33.82537
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   256 ----
[CW] collect: return: 306.99199, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 0.98253, qf2_loss: 0.98814, policy_loss: -71.62817, policy_entropy: -5.50384, alpha: 0.02066, time: 33.95797
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   257 ----
[CW] collect: return: 352.70429, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 1.25675, qf2_loss: 1.25814, policy_loss: -71.89987, policy_entropy: -5.54430, alpha: 0.02002, time: 33.87773
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   258 ----
[CW] collect: return: 243.03697, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 1.08897, qf2_loss: 1.09328, policy_loss: -71.02976, policy_entropy: -5.85718, alpha: 0.01963, time: 33.97338
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   259 ----
[CW] collect: return: 299.94446, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 1.15599, qf2_loss: 1.15447, policy_loss: -71.73431, policy_entropy: -6.33199, alpha: 0.01970, time: 33.82875
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   260 ----
[CW] collect: return: 269.81822, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 1.11427, qf2_loss: 1.11342, policy_loss: -71.93780, policy_entropy: -5.80096, alpha: 0.01982, time: 33.61832
[CW] eval: return: 217.87339, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   261 ----
[CW] collect: return: 301.30392, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 1.12500, qf2_loss: 1.12209, policy_loss: -72.23213, policy_entropy: -6.19246, alpha: 0.01981, time: 33.90668
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   262 ----
[CW] collect: return: 204.28006, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 1.50558, qf2_loss: 1.51097, policy_loss: -72.46965, policy_entropy: -6.21999, alpha: 0.02003, time: 33.89068
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   263 ----
[CW] collect: return: 49.54572, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 1.06951, qf2_loss: 1.06720, policy_loss: -71.96485, policy_entropy: -5.99344, alpha: 0.02017, time: 33.91258
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   264 ----
[CW] collect: return: 287.04507, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 1.43124, qf2_loss: 1.43553, policy_loss: -72.36867, policy_entropy: -5.86788, alpha: 0.02018, time: 33.78644
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   265 ----
[CW] collect: return: 284.40712, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 1.30045, qf2_loss: 1.29960, policy_loss: -72.55100, policy_entropy: -5.86645, alpha: 0.02001, time: 34.23303
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   266 ----
[CW] collect: return: 101.06903, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 1.29381, qf2_loss: 1.30054, policy_loss: -72.75789, policy_entropy: -5.80775, alpha: 0.01978, time: 33.53994
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   267 ----
[CW] collect: return: 212.93246, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 1.33997, qf2_loss: 1.34989, policy_loss: -73.14768, policy_entropy: -5.70095, alpha: 0.01944, time: 33.82770
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   268 ----
[CW] collect: return: 54.35823, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 1.09566, qf2_loss: 1.09705, policy_loss: -71.97439, policy_entropy: -6.38630, alpha: 0.01947, time: 33.76418
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   269 ----
[CW] collect: return: 126.73448, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 1.22223, qf2_loss: 1.22862, policy_loss: -73.26700, policy_entropy: -6.43650, alpha: 0.01994, time: 35.72812
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   270 ----
[CW] collect: return: 125.47976, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 1.24156, qf2_loss: 1.24175, policy_loss: -72.91556, policy_entropy: -6.43634, alpha: 0.02050, time: 33.99420
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   271 ----
[CW] collect: return: 185.67604, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 1.18392, qf2_loss: 1.18248, policy_loss: -73.07064, policy_entropy: -6.15361, alpha: 0.02083, time: 33.82693
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   272 ----
[CW] collect: return: 108.56330, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 1.12518, qf2_loss: 1.12364, policy_loss: -73.18379, policy_entropy: -6.15364, alpha: 0.02107, time: 33.81195
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   273 ----
[CW] collect: return: 174.47134, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 1.26292, qf2_loss: 1.26694, policy_loss: -72.84126, policy_entropy: -5.77156, alpha: 0.02103, time: 33.67870
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   274 ----
[CW] collect: return: 60.13472, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 1.76422, qf2_loss: 1.76388, policy_loss: -73.42758, policy_entropy: -6.02739, alpha: 0.02082, time: 33.92855
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   275 ----
[CW] collect: return: 295.11389, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 1.19064, qf2_loss: 1.18821, policy_loss: -73.25754, policy_entropy: -5.78105, alpha: 0.02076, time: 33.93366
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   276 ----
[CW] collect: return: 48.17919, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 1.01078, qf2_loss: 1.01227, policy_loss: -73.35415, policy_entropy: -5.55092, alpha: 0.02043, time: 33.82450
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   277 ----
[CW] collect: return: 338.80471, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 1.14562, qf2_loss: 1.14197, policy_loss: -74.48060, policy_entropy: -6.20243, alpha: 0.02022, time: 33.86159
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   278 ----
[CW] collect: return: 66.43865, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 1.21472, qf2_loss: 1.22334, policy_loss: -73.72093, policy_entropy: -6.03239, alpha: 0.02040, time: 33.76405
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   279 ----
[CW] collect: return: 98.97586, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 1.55540, qf2_loss: 1.55465, policy_loss: -73.93276, policy_entropy: -5.75379, alpha: 0.02028, time: 33.68725
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   280 ----
[CW] collect: return: 98.49455, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 1.17242, qf2_loss: 1.17151, policy_loss: -73.46726, policy_entropy: -5.44653, alpha: 0.01983, time: 33.76432
[CW] eval: return: 215.43368, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   281 ----
[CW] collect: return: 170.35224, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 1.17694, qf2_loss: 1.18028, policy_loss: -73.74691, policy_entropy: -5.80437, alpha: 0.01944, time: 33.87292
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   282 ----
[CW] collect: return: 121.70011, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 1.32483, qf2_loss: 1.33365, policy_loss: -74.03050, policy_entropy: -5.87072, alpha: 0.01925, time: 33.69525
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   283 ----
[CW] collect: return: 274.57383, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 1.20659, qf2_loss: 1.21328, policy_loss: -74.32660, policy_entropy: -6.00706, alpha: 0.01922, time: 33.85365
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   284 ----
[CW] collect: return: 41.01985, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 1.43701, qf2_loss: 1.44809, policy_loss: -73.26701, policy_entropy: -5.62336, alpha: 0.01905, time: 33.67014
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   285 ----
[CW] collect: return: 67.98275, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 1.14066, qf2_loss: 1.13952, policy_loss: -73.80873, policy_entropy: -5.93889, alpha: 0.01880, time: 33.69371
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   286 ----
[CW] collect: return: 336.06234, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 1.16504, qf2_loss: 1.16848, policy_loss: -73.10581, policy_entropy: -5.78403, alpha: 0.01871, time: 33.92288
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   287 ----
[CW] collect: return: 112.10548, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 1.43005, qf2_loss: 1.43276, policy_loss: -73.62387, policy_entropy: -5.88809, alpha: 0.01855, time: 33.77616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   288 ----
[CW] collect: return: 55.88823, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 1.36451, qf2_loss: 1.37136, policy_loss: -73.72119, policy_entropy: -5.82192, alpha: 0.01841, time: 33.93853
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   289 ----
[CW] collect: return: 101.24246, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 1.79524, qf2_loss: 1.80696, policy_loss: -73.83675, policy_entropy: -5.94210, alpha: 0.01829, time: 33.78322
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   290 ----
[CW] collect: return: 87.12909, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 1.33586, qf2_loss: 1.34021, policy_loss: -74.99179, policy_entropy: -6.03572, alpha: 0.01835, time: 33.92285
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   291 ----
[CW] collect: return: 319.97383, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 1.23851, qf2_loss: 1.24385, policy_loss: -74.26060, policy_entropy: -5.86297, alpha: 0.01832, time: 33.76825
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   292 ----
[CW] collect: return: 28.22237, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 1.38042, qf2_loss: 1.38648, policy_loss: -74.82943, policy_entropy: -5.83656, alpha: 0.01818, time: 33.84199
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   293 ----
[CW] collect: return: 80.84477, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 1.49949, qf2_loss: 1.50610, policy_loss: -74.72524, policy_entropy: -6.13707, alpha: 0.01803, time: 34.00663
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   294 ----
[CW] collect: return: 275.69760, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 1.67276, qf2_loss: 1.68902, policy_loss: -74.83147, policy_entropy: -6.60985, alpha: 0.01838, time: 33.83711
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   295 ----
[CW] collect: return: 308.36224, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 1.84235, qf2_loss: 1.85767, policy_loss: -75.24126, policy_entropy: -6.58370, alpha: 0.01886, time: 34.01342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   296 ----
[CW] collect: return: 204.47240, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 1.61028, qf2_loss: 1.61440, policy_loss: -74.99589, policy_entropy: -6.64319, alpha: 0.01943, time: 33.86581
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   297 ----
[CW] collect: return: 56.98067, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 1.45279, qf2_loss: 1.45860, policy_loss: -75.31831, policy_entropy: -6.41026, alpha: 0.01988, time: 33.57782
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   298 ----
[CW] collect: return: 40.39171, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 1.69484, qf2_loss: 1.70350, policy_loss: -75.05052, policy_entropy: -6.32568, alpha: 0.02019, time: 33.89696
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   299 ----
[CW] collect: return: 346.81408, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 1.80399, qf2_loss: 1.81006, policy_loss: -74.87558, policy_entropy: -6.16656, alpha: 0.02041, time: 33.74723
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   300 ----
[CW] collect: return: 240.71468, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 1.96899, qf2_loss: 1.98905, policy_loss: -75.25707, policy_entropy: -5.91625, alpha: 0.02049, time: 33.95211
[CW] eval: return: 238.56896, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   301 ----
[CW] collect: return: 121.42775, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 1.42113, qf2_loss: 1.43184, policy_loss: -75.66774, policy_entropy: -6.31469, alpha: 0.02062, time: 33.74464
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   302 ----
[CW] collect: return: 164.92052, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 1.56289, qf2_loss: 1.57439, policy_loss: -75.67378, policy_entropy: -6.07835, alpha: 0.02081, time: 33.89346
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   303 ----
[CW] collect: return: 267.25388, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 1.56527, qf2_loss: 1.56869, policy_loss: -75.80122, policy_entropy: -6.10455, alpha: 0.02087, time: 33.51902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   304 ----
[CW] collect: return: 318.01371, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 1.67547, qf2_loss: 1.68608, policy_loss: -75.15746, policy_entropy: -5.79992, alpha: 0.02083, time: 33.87488
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   305 ----
[CW] collect: return: 276.22688, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 1.63717, qf2_loss: 1.65141, policy_loss: -76.05936, policy_entropy: -6.18475, alpha: 0.02078, time: 33.83838
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   306 ----
[CW] collect: return: 291.64927, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 1.76551, qf2_loss: 1.77334, policy_loss: -76.70665, policy_entropy: -6.10546, alpha: 0.02097, time: 33.71574
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   307 ----
[CW] collect: return: 298.89583, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 1.50686, qf2_loss: 1.51506, policy_loss: -76.09716, policy_entropy: -5.93820, alpha: 0.02094, time: 33.96552
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   308 ----
[CW] collect: return: 303.07626, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 1.50323, qf2_loss: 1.51303, policy_loss: -77.88033, policy_entropy: -6.43706, alpha: 0.02117, time: 33.78097
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   309 ----
[CW] collect: return: 299.08829, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 1.56927, qf2_loss: 1.56769, policy_loss: -77.42854, policy_entropy: -6.24812, alpha: 0.02155, time: 33.82420
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   310 ----
[CW] collect: return: 208.72651, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 2.16620, qf2_loss: 2.16920, policy_loss: -76.99185, policy_entropy: -6.13061, alpha: 0.02175, time: 33.76291
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   311 ----
[CW] collect: return: 87.89066, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 1.93950, qf2_loss: 1.93870, policy_loss: -76.45814, policy_entropy: -6.21278, alpha: 0.02195, time: 33.93653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   312 ----
[CW] collect: return: 78.52364, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 1.57437, qf2_loss: 1.58520, policy_loss: -77.21539, policy_entropy: -6.06884, alpha: 0.02205, time: 33.78740
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   313 ----
[CW] collect: return: 334.94399, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 1.62409, qf2_loss: 1.64134, policy_loss: -78.22654, policy_entropy: -6.17918, alpha: 0.02222, time: 33.86942
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   314 ----
[CW] collect: return: 299.54650, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 1.55904, qf2_loss: 1.57693, policy_loss: -78.00558, policy_entropy: -5.83113, alpha: 0.02226, time: 33.90181
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   315 ----
[CW] collect: return: 326.32157, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 1.55669, qf2_loss: 1.56917, policy_loss: -77.88808, policy_entropy: -5.89811, alpha: 0.02210, time: 33.67492
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   316 ----
[CW] collect: return: 339.39406, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 2.02270, qf2_loss: 2.03445, policy_loss: -79.11383, policy_entropy: -5.81775, alpha: 0.02198, time: 33.90502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   317 ----
[CW] collect: return: 299.95660, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 1.57294, qf2_loss: 1.59571, policy_loss: -77.58472, policy_entropy: -5.72154, alpha: 0.02166, time: 33.75196
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   318 ----
[CW] collect: return: 329.01818, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 1.57434, qf2_loss: 1.59240, policy_loss: -78.04719, policy_entropy: -5.87843, alpha: 0.02143, time: 33.95930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   319 ----
[CW] collect: return: 54.32972, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 1.90838, qf2_loss: 1.91401, policy_loss: -78.41129, policy_entropy: -6.06088, alpha: 0.02141, time: 33.85717
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   320 ----
[CW] collect: return: 204.50448, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 1.43717, qf2_loss: 1.44980, policy_loss: -78.33973, policy_entropy: -5.90534, alpha: 0.02139, time: 33.87291
[CW] eval: return: 182.08734, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   321 ----
[CW] collect: return: 157.22537, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 1.83562, qf2_loss: 1.86377, policy_loss: -78.55063, policy_entropy: -6.07533, alpha: 0.02136, time: 33.52923
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   322 ----
[CW] collect: return: 226.27816, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 1.56067, qf2_loss: 1.58889, policy_loss: -79.09025, policy_entropy: -6.18565, alpha: 0.02146, time: 33.87664
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   323 ----
[CW] collect: return: 265.33749, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 1.64096, qf2_loss: 1.66138, policy_loss: -78.74285, policy_entropy: -5.99878, alpha: 0.02164, time: 33.76703
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   324 ----
[CW] collect: return: 130.49318, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 1.51225, qf2_loss: 1.53490, policy_loss: -79.15014, policy_entropy: -6.20164, alpha: 0.02163, time: 33.74810
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   325 ----
[CW] collect: return: 286.65940, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 1.92712, qf2_loss: 1.94104, policy_loss: -79.12516, policy_entropy: -6.44020, alpha: 0.02203, time: 33.85257
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   326 ----
[CW] collect: return: 274.60722, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 2.15297, qf2_loss: 2.17359, policy_loss: -78.01272, policy_entropy: -6.27359, alpha: 0.02245, time: 34.29449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   327 ----
[CW] collect: return: 65.27757, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 1.44726, qf2_loss: 1.46696, policy_loss: -78.30344, policy_entropy: -6.28557, alpha: 0.02283, time: 35.87013
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   328 ----
[CW] collect: return: 56.09247, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 1.48684, qf2_loss: 1.49936, policy_loss: -78.94837, policy_entropy: -6.01616, alpha: 0.02311, time: 33.76220
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   329 ----
[CW] collect: return: 261.59984, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 1.41748, qf2_loss: 1.43332, policy_loss: -79.56291, policy_entropy: -5.95616, alpha: 0.02310, time: 34.00602
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   330 ----
[CW] collect: return: 280.74897, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 1.44715, qf2_loss: 1.45534, policy_loss: -78.90518, policy_entropy: -5.79559, alpha: 0.02296, time: 33.82625
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   331 ----
[CW] collect: return: 309.56969, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 1.68846, qf2_loss: 1.69565, policy_loss: -78.29275, policy_entropy: -5.68441, alpha: 0.02258, time: 33.80495
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   332 ----
[CW] collect: return: 180.96684, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 1.43571, qf2_loss: 1.45640, policy_loss: -79.23932, policy_entropy: -5.70834, alpha: 0.02219, time: 33.83576
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   333 ----
[CW] collect: return: 352.80656, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 1.89765, qf2_loss: 1.90488, policy_loss: -79.29316, policy_entropy: -5.94286, alpha: 0.02193, time: 33.70650
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   334 ----
[CW] collect: return: 307.81520, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 1.43779, qf2_loss: 1.45579, policy_loss: -79.96177, policy_entropy: -6.16725, alpha: 0.02209, time: 33.78481
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   335 ----
[CW] collect: return: 269.83468, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 1.33074, qf2_loss: 1.35069, policy_loss: -79.30707, policy_entropy: -6.09341, alpha: 0.02223, time: 33.88755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   336 ----
[CW] collect: return: 99.24128, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 1.51223, qf2_loss: 1.51329, policy_loss: -78.96350, policy_entropy: -6.04858, alpha: 0.02230, time: 34.04209
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   337 ----
[CW] collect: return: 132.62310, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 1.61427, qf2_loss: 1.64224, policy_loss: -78.53482, policy_entropy: -5.90651, alpha: 0.02227, time: 33.95187
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   338 ----
[CW] collect: return: 128.69996, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 1.67219, qf2_loss: 1.68480, policy_loss: -79.67792, policy_entropy: -5.97307, alpha: 0.02221, time: 33.88261
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   339 ----
[CW] collect: return: 95.71645, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 1.51619, qf2_loss: 1.51982, policy_loss: -78.96323, policy_entropy: -6.05531, alpha: 0.02220, time: 33.83674
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   340 ----
[CW] collect: return: 58.64695, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 1.58419, qf2_loss: 1.61320, policy_loss: -79.33869, policy_entropy: -6.15055, alpha: 0.02235, time: 33.72338
[CW] eval: return: 224.78906, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   341 ----
[CW] collect: return: 199.00806, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 1.49832, qf2_loss: 1.51089, policy_loss: -78.96907, policy_entropy: -5.84353, alpha: 0.02240, time: 33.84683
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   342 ----
[CW] collect: return: 114.51380, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 1.79583, qf2_loss: 1.81410, policy_loss: -80.16030, policy_entropy: -5.99773, alpha: 0.02225, time: 33.80852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   343 ----
[CW] collect: return: 167.75092, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 2.05050, qf2_loss: 2.06001, policy_loss: -78.63451, policy_entropy: -5.74630, alpha: 0.02206, time: 33.90245
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   344 ----
[CW] collect: return: 279.08260, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 1.75041, qf2_loss: 1.76502, policy_loss: -79.87795, policy_entropy: -5.59129, alpha: 0.02171, time: 33.89484
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   345 ----
[CW] collect: return: 112.25996, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 1.43475, qf2_loss: 1.45488, policy_loss: -80.18693, policy_entropy: -5.84873, alpha: 0.02127, time: 34.05451
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   346 ----
[CW] collect: return: 122.92160, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 1.59913, qf2_loss: 1.62508, policy_loss: -79.93369, policy_entropy: -5.89502, alpha: 0.02113, time: 33.63105
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   347 ----
[CW] collect: return: 124.29582, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 1.47489, qf2_loss: 1.48307, policy_loss: -80.13250, policy_entropy: -5.87432, alpha: 0.02098, time: 33.93855
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   348 ----
[CW] collect: return: 66.17124, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 1.61247, qf2_loss: 1.63190, policy_loss: -78.74441, policy_entropy: -5.72310, alpha: 0.02081, time: 33.89300
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   349 ----
[CW] collect: return: 234.26369, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 1.68435, qf2_loss: 1.70273, policy_loss: -80.68400, policy_entropy: -6.07808, alpha: 0.02064, time: 33.95153
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   350 ----
[CW] collect: return: 154.58777, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 2.27347, qf2_loss: 2.30525, policy_loss: -79.21890, policy_entropy: -6.04473, alpha: 0.02078, time: 33.92371
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   351 ----
[CW] collect: return: 125.18671, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 1.56431, qf2_loss: 1.58379, policy_loss: -80.13361, policy_entropy: -5.91014, alpha: 0.02072, time: 33.85498
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   352 ----
[CW] collect: return: 82.41353, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 1.71741, qf2_loss: 1.73421, policy_loss: -80.40780, policy_entropy: -5.84070, alpha: 0.02052, time: 33.83785
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   353 ----
[CW] collect: return: 80.12863, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 1.56963, qf2_loss: 1.59680, policy_loss: -79.76836, policy_entropy: -5.94997, alpha: 0.02047, time: 33.88703
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   354 ----
[CW] collect: return: 251.31624, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 1.52422, qf2_loss: 1.53478, policy_loss: -79.49689, policy_entropy: -5.76709, alpha: 0.02028, time: 34.02537
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   355 ----
[CW] collect: return: 289.24300, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 1.50135, qf2_loss: 1.50413, policy_loss: -80.17521, policy_entropy: -5.92935, alpha: 0.02014, time: 33.83407
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   356 ----
[CW] collect: return: 271.04267, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 1.84085, qf2_loss: 1.85664, policy_loss: -80.43180, policy_entropy: -5.72069, alpha: 0.01998, time: 33.89900
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   357 ----
[CW] collect: return: 32.12781, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 2.33259, qf2_loss: 2.34242, policy_loss: -80.62505, policy_entropy: -5.74679, alpha: 0.01973, time: 33.95609
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   358 ----
[CW] collect: return: 323.59316, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 1.63035, qf2_loss: 1.63848, policy_loss: -79.75586, policy_entropy: -5.59463, alpha: 0.01936, time: 33.55523
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   359 ----
[CW] collect: return: 243.48177, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 1.58376, qf2_loss: 1.59389, policy_loss: -80.25224, policy_entropy: -5.85709, alpha: 0.01910, time: 33.94140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   360 ----
[CW] collect: return: 247.59763, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 1.51405, qf2_loss: 1.53485, policy_loss: -79.99339, policy_entropy: -6.08159, alpha: 0.01909, time: 33.76818
[CW] eval: return: 255.61502, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   361 ----
[CW] collect: return: 82.83926, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 1.69155, qf2_loss: 1.70337, policy_loss: -79.98558, policy_entropy: -5.94520, alpha: 0.01910, time: 33.85350
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   362 ----
[CW] collect: return: 315.96112, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 1.81751, qf2_loss: 1.84452, policy_loss: -80.04593, policy_entropy: -5.85609, alpha: 0.01905, time: 33.87889
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   363 ----
[CW] collect: return: 208.36066, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 1.93018, qf2_loss: 1.93883, policy_loss: -81.34595, policy_entropy: -5.95604, alpha: 0.01893, time: 33.88826
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   364 ----
[CW] collect: return: 246.20585, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 1.84221, qf2_loss: 1.86946, policy_loss: -81.04659, policy_entropy: -5.96883, alpha: 0.01892, time: 33.97395
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   365 ----
[CW] collect: return: 169.74030, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 1.68004, qf2_loss: 1.68831, policy_loss: -80.91806, policy_entropy: -5.89243, alpha: 0.01884, time: 33.85346
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   366 ----
[CW] collect: return: 69.02662, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 1.52772, qf2_loss: 1.55432, policy_loss: -80.64372, policy_entropy: -5.92896, alpha: 0.01871, time: 33.86472
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   367 ----
[CW] collect: return: 299.20022, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 1.62158, qf2_loss: 1.62991, policy_loss: -80.08836, policy_entropy: -6.09829, alpha: 0.01879, time: 33.77668
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   368 ----
[CW] collect: return: 359.41714, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 1.96966, qf2_loss: 2.00651, policy_loss: -80.33815, policy_entropy: -6.21411, alpha: 0.01891, time: 33.94545
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   369 ----
[CW] collect: return: 95.28521, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 1.77125, qf2_loss: 1.79312, policy_loss: -80.50535, policy_entropy: -6.05275, alpha: 0.01900, time: 33.86867
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   370 ----
[CW] collect: return: 192.59959, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 1.64289, qf2_loss: 1.65750, policy_loss: -81.89719, policy_entropy: -6.18708, alpha: 0.01909, time: 33.91037
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   371 ----
[CW] collect: return: 321.91221, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 1.71706, qf2_loss: 1.73559, policy_loss: -81.16217, policy_entropy: -6.07369, alpha: 0.01928, time: 33.90023
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   372 ----
[CW] collect: return: 181.37526, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 1.89331, qf2_loss: 1.91446, policy_loss: -81.18956, policy_entropy: -6.03408, alpha: 0.01931, time: 33.94963
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   373 ----
[CW] collect: return: 244.55301, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 2.77777, qf2_loss: 2.75312, policy_loss: -81.38283, policy_entropy: -5.97502, alpha: 0.01930, time: 34.75005
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   374 ----
[CW] collect: return: 256.75768, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 2.15357, qf2_loss: 2.16886, policy_loss: -80.73616, policy_entropy: -5.76032, alpha: 0.01919, time: 34.07436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   375 ----
[CW] collect: return: 402.16288, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 1.91941, qf2_loss: 1.93254, policy_loss: -81.22539, policy_entropy: -5.90255, alpha: 0.01901, time: 33.99507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   376 ----
[CW] collect: return: 283.42890, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 1.75595, qf2_loss: 1.76572, policy_loss: -82.27577, policy_entropy: -6.00621, alpha: 0.01896, time: 33.68833
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   377 ----
[CW] collect: return: 378.25399, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 1.87063, qf2_loss: 1.88595, policy_loss: -80.96935, policy_entropy: -5.94522, alpha: 0.01895, time: 33.94018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   378 ----
[CW] collect: return: 358.37235, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 2.29898, qf2_loss: 2.31522, policy_loss: -82.19329, policy_entropy: -6.10176, alpha: 0.01894, time: 35.95533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   379 ----
[CW] collect: return: 268.18903, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 1.96549, qf2_loss: 1.98678, policy_loss: -81.73596, policy_entropy: -6.09872, alpha: 0.01905, time: 33.98405
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   380 ----
[CW] collect: return: 327.21213, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 1.85491, qf2_loss: 1.87129, policy_loss: -81.56631, policy_entropy: -5.94425, alpha: 0.01906, time: 33.91088
[CW] eval: return: 240.70934, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   381 ----
[CW] collect: return: 246.65895, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 1.95173, qf2_loss: 1.97204, policy_loss: -82.22706, policy_entropy: -5.98380, alpha: 0.01906, time: 33.99490
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   382 ----
[CW] collect: return: 337.45559, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 2.96107, qf2_loss: 2.99638, policy_loss: -81.37226, policy_entropy: -5.95455, alpha: 0.01903, time: 33.55899
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   383 ----
[CW] collect: return: 367.57264, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 2.58405, qf2_loss: 2.60009, policy_loss: -80.55966, policy_entropy: -5.66044, alpha: 0.01886, time: 33.88767
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   384 ----
[CW] collect: return: 348.15076, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 2.01605, qf2_loss: 2.03035, policy_loss: -81.82202, policy_entropy: -5.89554, alpha: 0.01868, time: 33.89637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   385 ----
[CW] collect: return: 408.11379, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 2.18659, qf2_loss: 2.19938, policy_loss: -82.74839, policy_entropy: -6.16250, alpha: 0.01869, time: 33.77649
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   386 ----
[CW] collect: return: 377.15059, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 1.81496, qf2_loss: 1.82050, policy_loss: -82.36066, policy_entropy: -6.23359, alpha: 0.01882, time: 33.95050
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   387 ----
[CW] collect: return: 366.25294, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 1.88939, qf2_loss: 1.91749, policy_loss: -81.65246, policy_entropy: -6.14497, alpha: 0.01901, time: 33.86105
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   388 ----
[CW] collect: return: 133.38149, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 2.08955, qf2_loss: 2.11834, policy_loss: -82.50746, policy_entropy: -6.30972, alpha: 0.01918, time: 33.82181
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   389 ----
[CW] collect: return: 159.55774, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 2.26066, qf2_loss: 2.27273, policy_loss: -82.85075, policy_entropy: -6.40205, alpha: 0.01953, time: 33.73206
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   390 ----
[CW] collect: return: 312.27733, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 3.04103, qf2_loss: 3.05115, policy_loss: -82.65270, policy_entropy: -6.37610, alpha: 0.01985, time: 33.93497
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   391 ----
[CW] collect: return: 115.64514, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 2.28241, qf2_loss: 2.30235, policy_loss: -83.03823, policy_entropy: -6.14506, alpha: 0.02015, time: 33.93477
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   392 ----
[CW] collect: return: 241.07964, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 2.17246, qf2_loss: 2.18807, policy_loss: -82.08183, policy_entropy: -5.88349, alpha: 0.02015, time: 33.78237
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   393 ----
[CW] collect: return: 40.47512, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 2.15569, qf2_loss: 2.16758, policy_loss: -84.00517, policy_entropy: -6.29801, alpha: 0.02019, time: 33.99191
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   394 ----
[CW] collect: return: 147.14676, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 2.18230, qf2_loss: 2.20021, policy_loss: -83.41363, policy_entropy: -6.10193, alpha: 0.02045, time: 33.88209
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   395 ----
[CW] collect: return: 353.72109, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 2.26191, qf2_loss: 2.26303, policy_loss: -83.49075, policy_entropy: -6.02172, alpha: 0.02049, time: 33.81956
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   396 ----
[CW] collect: return: 388.80171, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 2.15875, qf2_loss: 2.16329, policy_loss: -82.89489, policy_entropy: -5.79901, alpha: 0.02044, time: 33.90147
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   397 ----
[CW] collect: return: 260.46817, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 2.40797, qf2_loss: 2.43694, policy_loss: -82.36594, policy_entropy: -5.96386, alpha: 0.02031, time: 33.77542
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   398 ----
[CW] collect: return: 380.44020, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 2.13657, qf2_loss: 2.14810, policy_loss: -82.50062, policy_entropy: -5.82155, alpha: 0.02020, time: 34.03790
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   399 ----
[CW] collect: return: 370.13253, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 2.24336, qf2_loss: 2.25668, policy_loss: -83.62871, policy_entropy: -6.04016, alpha: 0.02012, time: 33.70951
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   400 ----
[CW] collect: return: 192.46174, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 2.56192, qf2_loss: 2.55840, policy_loss: -84.15284, policy_entropy: -6.04441, alpha: 0.02021, time: 33.90075
[CW] eval: return: 304.93109, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   401 ----
[CW] collect: return: 179.87425, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 2.76830, qf2_loss: 2.80074, policy_loss: -85.33762, policy_entropy: -6.18543, alpha: 0.02029, time: 33.78216
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   402 ----
[CW] collect: return: 340.36932, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 3.20309, qf2_loss: 3.24630, policy_loss: -82.90269, policy_entropy: -5.82995, alpha: 0.02031, time: 33.97200
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   403 ----
[CW] collect: return: 238.25988, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 2.44250, qf2_loss: 2.44242, policy_loss: -83.57125, policy_entropy: -5.96734, alpha: 0.02022, time: 33.68267
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   404 ----
[CW] collect: return: 297.83416, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 2.54388, qf2_loss: 2.53852, policy_loss: -83.89699, policy_entropy: -6.21289, alpha: 0.02025, time: 33.96346
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   405 ----
[CW] collect: return: 224.90606, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 2.98006, qf2_loss: 2.97728, policy_loss: -84.21301, policy_entropy: -6.14004, alpha: 0.02046, time: 33.69709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   406 ----
[CW] collect: return: 414.92890, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 2.82934, qf2_loss: 2.85541, policy_loss: -83.60205, policy_entropy: -6.01469, alpha: 0.02052, time: 33.85655
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   407 ----
[CW] collect: return: 320.58187, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 2.66692, qf2_loss: 2.68976, policy_loss: -83.06514, policy_entropy: -5.97083, alpha: 0.02051, time: 33.67670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   408 ----
[CW] collect: return: 316.29949, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 2.32629, qf2_loss: 2.34194, policy_loss: -84.83973, policy_entropy: -6.20495, alpha: 0.02062, time: 33.83093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   409 ----
[CW] collect: return: 200.60749, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 2.70837, qf2_loss: 2.72018, policy_loss: -83.99404, policy_entropy: -5.80555, alpha: 0.02070, time: 33.87677
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   410 ----
[CW] collect: return: 167.46966, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 2.40997, qf2_loss: 2.41810, policy_loss: -84.23085, policy_entropy: -6.00535, alpha: 0.02050, time: 33.85270
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   411 ----
[CW] collect: return: 188.34466, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 2.50674, qf2_loss: 2.51294, policy_loss: -84.36124, policy_entropy: -6.05197, alpha: 0.02054, time: 33.98260
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   412 ----
[CW] collect: return: 388.18183, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 3.18888, qf2_loss: 3.18165, policy_loss: -84.95590, policy_entropy: -5.91878, alpha: 0.02058, time: 33.75137
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   413 ----
[CW] collect: return: 368.34951, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 2.92836, qf2_loss: 2.96306, policy_loss: -84.11205, policy_entropy: -5.83752, alpha: 0.02041, time: 33.73947
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   414 ----
[CW] collect: return: 378.56112, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 2.51047, qf2_loss: 2.52092, policy_loss: -84.17098, policy_entropy: -5.82401, alpha: 0.02022, time: 33.70709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   415 ----
[CW] collect: return: 231.86784, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 2.25180, qf2_loss: 2.26948, policy_loss: -83.31128, policy_entropy: -5.80793, alpha: 0.02010, time: 33.78274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   416 ----
[CW] collect: return: 333.43347, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 2.45771, qf2_loss: 2.47354, policy_loss: -84.08410, policy_entropy: -5.92083, alpha: 0.02001, time: 33.75018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   417 ----
[CW] collect: return: 257.57846, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 2.80186, qf2_loss: 2.82959, policy_loss: -84.21426, policy_entropy: -5.96818, alpha: 0.01993, time: 33.79640
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   418 ----
[CW] collect: return: 163.52362, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 3.09067, qf2_loss: 3.10897, policy_loss: -84.93645, policy_entropy: -6.08908, alpha: 0.01997, time: 33.87400
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   419 ----
[CW] collect: return: 342.15607, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 3.32661, qf2_loss: 3.32587, policy_loss: -85.60690, policy_entropy: -6.15726, alpha: 0.02004, time: 33.63449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   420 ----
[CW] collect: return: 109.80477, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 2.46420, qf2_loss: 2.47861, policy_loss: -85.19792, policy_entropy: -5.96447, alpha: 0.02012, time: 33.94294
[CW] eval: return: 277.63029, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   421 ----
[CW] collect: return: 380.98529, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 2.58566, qf2_loss: 2.58035, policy_loss: -85.00706, policy_entropy: -6.12755, alpha: 0.02015, time: 33.80402
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   422 ----
[CW] collect: return: 407.27023, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 2.66863, qf2_loss: 2.70348, policy_loss: -85.93797, policy_entropy: -6.09181, alpha: 0.02024, time: 33.68111
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   423 ----
[CW] collect: return: 147.15328, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 2.92879, qf2_loss: 2.92200, policy_loss: -84.73637, policy_entropy: -6.23310, alpha: 0.02035, time: 33.56970
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   424 ----
[CW] collect: return: 366.70400, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 2.74369, qf2_loss: 2.75681, policy_loss: -85.61525, policy_entropy: -6.20892, alpha: 0.02057, time: 33.78743
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   425 ----
[CW] collect: return: 264.51516, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 2.54185, qf2_loss: 2.56802, policy_loss: -84.67349, policy_entropy: -6.13238, alpha: 0.02070, time: 33.53670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   426 ----
[CW] collect: return: 340.10423, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 2.50921, qf2_loss: 2.52297, policy_loss: -85.98356, policy_entropy: -6.38112, alpha: 0.02090, time: 33.87659
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   427 ----
[CW] collect: return: 404.29397, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 2.53488, qf2_loss: 2.56512, policy_loss: -86.19657, policy_entropy: -6.30644, alpha: 0.02124, time: 33.64689
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   428 ----
[CW] collect: return: 327.39688, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 2.49924, qf2_loss: 2.52659, policy_loss: -86.87198, policy_entropy: -6.38611, alpha: 0.02152, time: 33.77435
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   429 ----
[CW] collect: return: 251.79796, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 2.65865, qf2_loss: 2.68577, policy_loss: -87.06904, policy_entropy: -6.35538, alpha: 0.02187, time: 33.69807
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   430 ----
[CW] collect: return: 299.91136, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 2.85976, qf2_loss: 2.88610, policy_loss: -87.17321, policy_entropy: -6.35899, alpha: 0.02221, time: 33.55526
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   431 ----
[CW] collect: return: 395.74659, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 2.87564, qf2_loss: 2.89973, policy_loss: -85.15271, policy_entropy: -6.11881, alpha: 0.02246, time: 33.53139
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   432 ----
[CW] collect: return: 73.65780, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 2.81301, qf2_loss: 2.81562, policy_loss: -86.16480, policy_entropy: -6.03033, alpha: 0.02259, time: 33.58404
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   433 ----
[CW] collect: return: 374.99963, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 2.85487, qf2_loss: 2.88195, policy_loss: -85.87255, policy_entropy: -5.94102, alpha: 0.02252, time: 33.68022
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   434 ----
[CW] collect: return: 258.89038, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 2.52204, qf2_loss: 2.55527, policy_loss: -86.79094, policy_entropy: -6.12686, alpha: 0.02255, time: 33.78451
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   435 ----
[CW] collect: return: 189.32840, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 3.10355, qf2_loss: 3.16230, policy_loss: -85.55349, policy_entropy: -6.01146, alpha: 0.02263, time: 34.67224
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   436 ----
[CW] collect: return: 213.36969, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 2.82858, qf2_loss: 2.82973, policy_loss: -86.26155, policy_entropy: -5.98946, alpha: 0.02267, time: 35.46485
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   437 ----
[CW] collect: return: 243.58483, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 3.27197, qf2_loss: 3.25853, policy_loss: -87.73286, policy_entropy: -6.12144, alpha: 0.02271, time: 33.50681
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   438 ----
[CW] collect: return: 247.36332, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 3.17370, qf2_loss: 3.20057, policy_loss: -86.67975, policy_entropy: -5.67129, alpha: 0.02261, time: 33.48734
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   439 ----
[CW] collect: return: 208.90461, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 2.55291, qf2_loss: 2.57169, policy_loss: -86.86683, policy_entropy: -5.74542, alpha: 0.02230, time: 33.43615
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   440 ----
[CW] collect: return: 319.31257, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 2.71767, qf2_loss: 2.73514, policy_loss: -88.30688, policy_entropy: -5.94413, alpha: 0.02216, time: 33.50313
[CW] eval: return: 197.02567, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   441 ----
[CW] collect: return: 293.35256, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 2.73165, qf2_loss: 2.76201, policy_loss: -86.92250, policy_entropy: -5.86579, alpha: 0.02208, time: 33.46146
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   442 ----
[CW] collect: return: 393.59338, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 2.78480, qf2_loss: 2.81354, policy_loss: -86.11897, policy_entropy: -5.86031, alpha: 0.02193, time: 33.71917
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   443 ----
[CW] collect: return: 239.01487, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 2.93610, qf2_loss: 2.95621, policy_loss: -87.55120, policy_entropy: -6.00007, alpha: 0.02190, time: 33.41712
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   444 ----
[CW] collect: return: 67.84056, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 4.59330, qf2_loss: 4.58943, policy_loss: -88.54456, policy_entropy: -6.08329, alpha: 0.02192, time: 33.38502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   445 ----
[CW] collect: return: 107.49950, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 3.07483, qf2_loss: 3.08819, policy_loss: -88.63244, policy_entropy: -6.06897, alpha: 0.02200, time: 33.50126
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   446 ----
[CW] collect: return: 365.20672, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 2.45776, qf2_loss: 2.46569, policy_loss: -87.20630, policy_entropy: -5.91320, alpha: 0.02200, time: 33.42985
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   447 ----
[CW] collect: return: 330.52097, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 2.63329, qf2_loss: 2.63541, policy_loss: -88.40422, policy_entropy: -6.00062, alpha: 0.02198, time: 33.51888
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   448 ----
[CW] collect: return: 366.22857, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 2.93253, qf2_loss: 2.94662, policy_loss: -85.98896, policy_entropy: -5.85087, alpha: 0.02189, time: 33.47832
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   449 ----
[CW] collect: return: 60.06337, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 2.73611, qf2_loss: 2.75349, policy_loss: -86.09347, policy_entropy: -6.10514, alpha: 0.02184, time: 33.55580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   450 ----
[CW] collect: return: 318.51402, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 2.74200, qf2_loss: 2.73867, policy_loss: -87.98518, policy_entropy: -6.13416, alpha: 0.02201, time: 33.28889
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   451 ----
[CW] collect: return: 226.80045, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 2.54651, qf2_loss: 2.54174, policy_loss: -88.87669, policy_entropy: -6.20063, alpha: 0.02215, time: 33.35566
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   452 ----
[CW] collect: return: 326.50438, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 3.39765, qf2_loss: 3.40659, policy_loss: -88.10276, policy_entropy: -5.95151, alpha: 0.02219, time: 33.63070
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   453 ----
[CW] collect: return: 212.00870, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 3.46712, qf2_loss: 3.49403, policy_loss: -86.48779, policy_entropy: -5.98834, alpha: 0.02214, time: 33.45165
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   454 ----
[CW] collect: return: 336.92214, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 3.05378, qf2_loss: 3.05343, policy_loss: -86.80583, policy_entropy: -6.17572, alpha: 0.02225, time: 33.69260
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   455 ----
[CW] collect: return: 340.37797, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 2.94760, qf2_loss: 2.95213, policy_loss: -88.36723, policy_entropy: -6.10860, alpha: 0.02239, time: 33.41462
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   456 ----
[CW] collect: return: 55.37933, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 2.83597, qf2_loss: 2.84793, policy_loss: -88.71437, policy_entropy: -6.09629, alpha: 0.02244, time: 33.40728
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   457 ----
[CW] collect: return: 387.66738, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 2.99223, qf2_loss: 3.00571, policy_loss: -87.50266, policy_entropy: -5.91040, alpha: 0.02249, time: 33.51729
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   458 ----
[CW] collect: return: 317.43657, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 2.60148, qf2_loss: 2.61771, policy_loss: -87.49656, policy_entropy: -5.80127, alpha: 0.02235, time: 33.50938
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   459 ----
[CW] collect: return: 375.14819, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 2.60457, qf2_loss: 2.63056, policy_loss: -88.55901, policy_entropy: -5.86251, alpha: 0.02217, time: 33.61616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   460 ----
[CW] collect: return: 347.04088, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 3.45323, qf2_loss: 3.44438, policy_loss: -88.87699, policy_entropy: -5.83259, alpha: 0.02207, time: 33.36537
[CW] eval: return: 216.97599, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   461 ----
[CW] collect: return: 296.99289, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 3.90506, qf2_loss: 3.88483, policy_loss: -89.60847, policy_entropy: -6.00956, alpha: 0.02200, time: 33.53727
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   462 ----
[CW] collect: return: 350.58352, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 3.10661, qf2_loss: 3.11284, policy_loss: -88.47810, policy_entropy: -5.79798, alpha: 0.02190, time: 33.33875
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   463 ----
[CW] collect: return: 357.08035, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 2.88609, qf2_loss: 2.88685, policy_loss: -88.72365, policy_entropy: -5.73647, alpha: 0.02167, time: 33.55479
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   464 ----
[CW] collect: return: 121.28630, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 3.22711, qf2_loss: 3.23649, policy_loss: -88.25360, policy_entropy: -5.87808, alpha: 0.02148, time: 33.31649
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   465 ----
[CW] collect: return: 236.08150, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 3.01711, qf2_loss: 3.05401, policy_loss: -88.87016, policy_entropy: -5.98649, alpha: 0.02144, time: 33.47553
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   466 ----
[CW] collect: return: 318.41724, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 2.84756, qf2_loss: 2.86254, policy_loss: -90.05435, policy_entropy: -6.11544, alpha: 0.02146, time: 33.38671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   467 ----
[CW] collect: return: 21.43014, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 3.27747, qf2_loss: 3.25342, policy_loss: -88.87046, policy_entropy: -5.97411, alpha: 0.02152, time: 33.36993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   468 ----
[CW] collect: return: 96.62443, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 3.57924, qf2_loss: 3.59019, policy_loss: -88.89306, policy_entropy: -5.94281, alpha: 0.02148, time: 33.34482
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   469 ----
[CW] collect: return: 56.91180, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 2.94308, qf2_loss: 2.95356, policy_loss: -87.61710, policy_entropy: -5.81494, alpha: 0.02140, time: 33.45148
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   470 ----
[CW] collect: return: 327.56474, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 3.24824, qf2_loss: 3.24983, policy_loss: -88.57484, policy_entropy: -6.06691, alpha: 0.02135, time: 33.74292
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   471 ----
[CW] collect: return: 348.94655, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 2.85890, qf2_loss: 2.87523, policy_loss: -89.14895, policy_entropy: -6.12156, alpha: 0.02142, time: 34.98368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   472 ----
[CW] collect: return: 367.11700, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 2.74667, qf2_loss: 2.76868, policy_loss: -90.08112, policy_entropy: -6.10118, alpha: 0.02152, time: 33.74693
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   473 ----
[CW] collect: return: 310.86145, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 3.24251, qf2_loss: 3.23582, policy_loss: -88.88588, policy_entropy: -5.97832, alpha: 0.02155, time: 33.45251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   474 ----
[CW] collect: return: 406.50542, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 3.08398, qf2_loss: 3.09472, policy_loss: -89.08373, policy_entropy: -5.96550, alpha: 0.02153, time: 33.37739
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   475 ----
[CW] collect: return: 411.98280, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 3.22615, qf2_loss: 3.23400, policy_loss: -89.34089, policy_entropy: -6.09081, alpha: 0.02152, time: 33.43902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   476 ----
[CW] collect: return: 106.22320, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 3.47819, qf2_loss: 3.49672, policy_loss: -89.41288, policy_entropy: -6.01844, alpha: 0.02159, time: 36.34823
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   477 ----
[CW] collect: return: 118.05297, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 4.53013, qf2_loss: 4.50181, policy_loss: -88.37535, policy_entropy: -5.94188, alpha: 0.02159, time: 33.59844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   478 ----
[CW] collect: return: 62.02983, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 3.28999, qf2_loss: 3.30299, policy_loss: -89.62571, policy_entropy: -6.04149, alpha: 0.02155, time: 33.42530
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   479 ----
[CW] collect: return: 339.54859, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 3.09898, qf2_loss: 3.12428, policy_loss: -89.45683, policy_entropy: -6.07297, alpha: 0.02159, time: 33.55762
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   480 ----
[CW] collect: return: 220.77276, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 3.09995, qf2_loss: 3.13029, policy_loss: -88.86834, policy_entropy: -5.90024, alpha: 0.02161, time: 33.39499
[CW] eval: return: 276.21354, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   481 ----
[CW] collect: return: 381.18351, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 3.11531, qf2_loss: 3.14828, policy_loss: -89.35685, policy_entropy: -5.87453, alpha: 0.02156, time: 33.65572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   482 ----
[CW] collect: return: 320.14358, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 3.60977, qf2_loss: 3.62504, policy_loss: -90.15362, policy_entropy: -5.83131, alpha: 0.02137, time: 33.97176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   483 ----
[CW] collect: return: 57.12221, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 3.23753, qf2_loss: 3.25515, policy_loss: -89.51606, policy_entropy: -6.03749, alpha: 0.02125, time: 33.66499
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   484 ----
[CW] collect: return: 383.66585, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 3.67495, qf2_loss: 3.66662, policy_loss: -88.98866, policy_entropy: -6.06740, alpha: 0.02138, time: 33.59902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   485 ----
[CW] collect: return: 154.74939, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 3.95323, qf2_loss: 3.96557, policy_loss: -89.90427, policy_entropy: -6.06571, alpha: 0.02137, time: 33.51490
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   486 ----
[CW] collect: return: 221.12222, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 3.58708, qf2_loss: 3.57180, policy_loss: -87.28945, policy_entropy: -5.85050, alpha: 0.02137, time: 33.43626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   487 ----
[CW] collect: return: 76.02509, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 3.40789, qf2_loss: 3.43575, policy_loss: -89.35225, policy_entropy: -6.26806, alpha: 0.02140, time: 33.41311
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   488 ----
[CW] collect: return: 204.10723, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 3.65417, qf2_loss: 3.69301, policy_loss: -87.72268, policy_entropy: -6.22296, alpha: 0.02166, time: 33.73625
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   489 ----
[CW] collect: return: 142.54398, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 4.19974, qf2_loss: 4.18614, policy_loss: -89.40358, policy_entropy: -6.16794, alpha: 0.02181, time: 33.49179
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   490 ----
[CW] collect: return: 427.73861, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 3.29676, qf2_loss: 3.32982, policy_loss: -90.03453, policy_entropy: -6.14730, alpha: 0.02192, time: 34.47824
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   491 ----
[CW] collect: return: 189.79407, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 3.20819, qf2_loss: 3.22686, policy_loss: -89.75554, policy_entropy: -6.08297, alpha: 0.02204, time: 33.40551
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   492 ----
[CW] collect: return: 273.38597, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 3.40369, qf2_loss: 3.43039, policy_loss: -89.61544, policy_entropy: -6.05931, alpha: 0.02218, time: 33.55840
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   493 ----
[CW] collect: return: 80.81984, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 3.71565, qf2_loss: 3.72527, policy_loss: -88.75976, policy_entropy: -5.87731, alpha: 0.02219, time: 33.24906
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   494 ----
[CW] collect: return: 162.15737, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 4.03612, qf2_loss: 4.05817, policy_loss: -89.92002, policy_entropy: -5.96074, alpha: 0.02205, time: 33.65885
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   495 ----
[CW] collect: return: 329.37951, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 3.21350, qf2_loss: 3.25377, policy_loss: -89.35624, policy_entropy: -5.79562, alpha: 0.02191, time: 33.57559
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   496 ----
[CW] collect: return: 383.02359, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 3.30885, qf2_loss: 3.32315, policy_loss: -89.06727, policy_entropy: -5.83210, alpha: 0.02176, time: 33.44709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   497 ----
[CW] collect: return: 77.36619, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 3.52648, qf2_loss: 3.54847, policy_loss: -89.13637, policy_entropy: -5.93085, alpha: 0.02168, time: 33.61960
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   498 ----
[CW] collect: return: 235.52350, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 3.45008, qf2_loss: 3.45868, policy_loss: -89.26705, policy_entropy: -5.95474, alpha: 0.02163, time: 33.50023
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   499 ----
[CW] collect: return: 49.97799, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 3.38406, qf2_loss: 3.38245, policy_loss: -90.82157, policy_entropy: -6.06400, alpha: 0.02161, time: 33.54907
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   500 ----
[CW] collect: return: 334.93200, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 3.71766, qf2_loss: 3.75031, policy_loss: -90.33152, policy_entropy: -5.99496, alpha: 0.02162, time: 33.42580
[CW] eval: return: 197.31572, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   501 ----
[CW] collect: return: 396.22588, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 3.94177, qf2_loss: 3.95373, policy_loss: -89.20189, policy_entropy: -5.79261, alpha: 0.02157, time: 33.63966
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   502 ----
[CW] collect: return: 232.15488, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 3.64909, qf2_loss: 3.67037, policy_loss: -88.72669, policy_entropy: -5.91130, alpha: 0.02140, time: 33.43711
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   503 ----
[CW] collect: return: 55.35569, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 3.91676, qf2_loss: 3.93519, policy_loss: -91.38484, policy_entropy: -6.19921, alpha: 0.02149, time: 33.64540
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   504 ----
[CW] collect: return: 232.76646, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 3.93957, qf2_loss: 3.90871, policy_loss: -90.71970, policy_entropy: -6.06671, alpha: 0.02156, time: 33.41502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   505 ----
[CW] collect: return: 144.27896, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 4.14014, qf2_loss: 4.17105, policy_loss: -89.76988, policy_entropy: -5.88566, alpha: 0.02155, time: 33.49297
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   506 ----
[CW] collect: return: 371.75509, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 3.62481, qf2_loss: 3.66463, policy_loss: -90.35877, policy_entropy: -6.01251, alpha: 0.02154, time: 33.53968
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   507 ----
[CW] collect: return: 447.58165, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 3.70199, qf2_loss: 3.72645, policy_loss: -90.60518, policy_entropy: -6.12543, alpha: 0.02156, time: 33.51452
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   508 ----
[CW] collect: return: 374.19161, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 3.63061, qf2_loss: 3.65129, policy_loss: -90.62120, policy_entropy: -6.09081, alpha: 0.02167, time: 33.50919
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   509 ----
[CW] collect: return: 175.05714, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 3.66994, qf2_loss: 3.66152, policy_loss: -89.82350, policy_entropy: -5.95888, alpha: 0.02172, time: 33.54629
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   510 ----
[CW] collect: return: 424.35407, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 3.33807, qf2_loss: 3.33216, policy_loss: -89.44539, policy_entropy: -6.03956, alpha: 0.02168, time: 33.45505
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   511 ----
[CW] collect: return: 182.92140, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 4.12896, qf2_loss: 4.13396, policy_loss: -89.65383, policy_entropy: -5.90387, alpha: 0.02168, time: 33.26230
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   512 ----
[CW] collect: return: 375.98849, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 4.41885, qf2_loss: 4.41773, policy_loss: -90.04020, policy_entropy: -6.00482, alpha: 0.02161, time: 33.44038
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   513 ----
[CW] collect: return: 442.02447, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 4.43647, qf2_loss: 4.44966, policy_loss: -90.77359, policy_entropy: -6.23927, alpha: 0.02171, time: 33.37705
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   514 ----
[CW] collect: return: 88.88062, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 4.52354, qf2_loss: 4.52576, policy_loss: -91.92146, policy_entropy: -6.23678, alpha: 0.02197, time: 33.53585
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   515 ----
[CW] collect: return: 355.82287, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 4.71380, qf2_loss: 4.73985, policy_loss: -90.03334, policy_entropy: -6.03382, alpha: 0.02207, time: 33.31947
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   516 ----
[CW] collect: return: 432.04280, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 4.03374, qf2_loss: 4.05560, policy_loss: -91.10473, policy_entropy: -6.16338, alpha: 0.02211, time: 33.57430
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   517 ----
[CW] collect: return: 274.39819, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 3.96962, qf2_loss: 3.93565, policy_loss: -90.54773, policy_entropy: -6.18610, alpha: 0.02231, time: 33.33154
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   518 ----
[CW] collect: return: 334.91646, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 4.01969, qf2_loss: 4.01245, policy_loss: -91.35474, policy_entropy: -6.06414, alpha: 0.02242, time: 33.37105
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   519 ----
[CW] collect: return: 201.98743, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 3.77617, qf2_loss: 3.75177, policy_loss: -93.04875, policy_entropy: -6.25837, alpha: 0.02259, time: 33.33786
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   520 ----
[CW] collect: return: 457.94383, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 3.89582, qf2_loss: 3.93409, policy_loss: -92.60940, policy_entropy: -6.00919, alpha: 0.02265, time: 33.53151
[CW] eval: return: 269.28004, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   521 ----
[CW] collect: return: 402.75934, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 3.99310, qf2_loss: 4.00065, policy_loss: -92.05649, policy_entropy: -5.91402, alpha: 0.02263, time: 33.45343
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   522 ----
[CW] collect: return: 388.81605, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 3.46220, qf2_loss: 3.46212, policy_loss: -91.21722, policy_entropy: -5.78060, alpha: 0.02250, time: 33.48419
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   523 ----
[CW] collect: return: 114.30291, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 3.88116, qf2_loss: 3.86893, policy_loss: -92.55221, policy_entropy: -6.07176, alpha: 0.02246, time: 33.28537
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   524 ----
[CW] collect: return: 379.93114, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 4.10591, qf2_loss: 4.08247, policy_loss: -92.97850, policy_entropy: -6.02509, alpha: 0.02251, time: 33.51210
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   525 ----
[CW] collect: return: 423.71902, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 4.08933, qf2_loss: 4.10562, policy_loss: -91.15657, policy_entropy: -5.89098, alpha: 0.02244, time: 33.57377
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   526 ----
[CW] collect: return: 133.73708, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 3.82114, qf2_loss: 3.82133, policy_loss: -91.17845, policy_entropy: -5.97266, alpha: 0.02238, time: 33.38636
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   527 ----
[CW] collect: return: 235.17086, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 4.90550, qf2_loss: 4.88493, policy_loss: -91.85450, policy_entropy: -5.89973, alpha: 0.02235, time: 33.63542
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   528 ----
[CW] collect: return: 378.60460, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 4.56429, qf2_loss: 4.56267, policy_loss: -94.10154, policy_entropy: -6.34621, alpha: 0.02241, time: 33.46071
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   529 ----
[CW] collect: return: 293.86775, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 4.42898, qf2_loss: 4.42327, policy_loss: -92.38628, policy_entropy: -6.19528, alpha: 0.02271, time: 33.51058
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   530 ----
[CW] collect: return: 210.31972, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 3.92737, qf2_loss: 3.93886, policy_loss: -91.81156, policy_entropy: -6.10868, alpha: 0.02287, time: 33.19430
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   531 ----
[CW] collect: return: 237.40822, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 5.13311, qf2_loss: 5.07791, policy_loss: -91.52480, policy_entropy: -6.05827, alpha: 0.02293, time: 33.50479
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   532 ----
[CW] collect: return: 67.39653, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 3.83574, qf2_loss: 3.82729, policy_loss: -91.71025, policy_entropy: -6.01720, alpha: 0.02295, time: 33.42748
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   533 ----
[CW] collect: return: 159.15073, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 4.31319, qf2_loss: 4.29861, policy_loss: -91.85353, policy_entropy: -6.02905, alpha: 0.02297, time: 33.53298
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   534 ----
[CW] collect: return: 296.47998, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 4.35684, qf2_loss: 4.36991, policy_loss: -92.46843, policy_entropy: -6.07473, alpha: 0.02302, time: 33.45622
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   535 ----
[CW] collect: return: 427.88427, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 4.17720, qf2_loss: 4.19142, policy_loss: -93.77527, policy_entropy: -6.09588, alpha: 0.02310, time: 33.48119
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   536 ----
[CW] collect: return: 355.87317, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 3.98207, qf2_loss: 3.99665, policy_loss: -91.63993, policy_entropy: -6.02265, alpha: 0.02318, time: 33.47987
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   537 ----
[CW] collect: return: 59.79256, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 4.05028, qf2_loss: 4.03627, policy_loss: -93.19034, policy_entropy: -5.94303, alpha: 0.02315, time: 33.38646
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   538 ----
[CW] collect: return: 346.69576, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 4.13227, qf2_loss: 4.14080, policy_loss: -92.71435, policy_entropy: -6.00236, alpha: 0.02310, time: 33.57039
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   539 ----
[CW] collect: return: 388.92959, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 5.09333, qf2_loss: 5.11231, policy_loss: -92.64593, policy_entropy: -5.92759, alpha: 0.02313, time: 33.52170
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   540 ----
[CW] collect: return: 451.10932, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 3.78092, qf2_loss: 3.75216, policy_loss: -92.92341, policy_entropy: -5.83888, alpha: 0.02299, time: 33.68039
[CW] eval: return: 253.52237, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   541 ----
[CW] collect: return: 297.72805, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 3.96343, qf2_loss: 3.96282, policy_loss: -91.09322, policy_entropy: -5.77437, alpha: 0.02287, time: 33.47243
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   542 ----
[CW] collect: return: 192.46565, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 4.25694, qf2_loss: 4.24804, policy_loss: -92.75820, policy_entropy: -6.00229, alpha: 0.02269, time: 33.31455
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   543 ----
[CW] collect: return: 400.15629, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 3.84994, qf2_loss: 3.85814, policy_loss: -91.99839, policy_entropy: -5.86088, alpha: 0.02266, time: 33.27265
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   544 ----
[CW] collect: return: 375.49759, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 3.89555, qf2_loss: 3.89195, policy_loss: -93.21551, policy_entropy: -6.02031, alpha: 0.02259, time: 33.42741
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   545 ----
[CW] collect: return: 124.94317, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 5.79076, qf2_loss: 5.77393, policy_loss: -92.91299, policy_entropy: -5.94135, alpha: 0.02257, time: 33.37259
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   546 ----
[CW] collect: return: 166.21774, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 4.60680, qf2_loss: 4.59702, policy_loss: -92.85331, policy_entropy: -5.99965, alpha: 0.02253, time: 35.50402
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   547 ----
[CW] collect: return: 162.09120, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 4.12751, qf2_loss: 4.15317, policy_loss: -93.78727, policy_entropy: -5.96537, alpha: 0.02253, time: 33.45442
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   548 ----
[CW] collect: return: 228.20883, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 3.72724, qf2_loss: 3.73479, policy_loss: -94.30033, policy_entropy: -6.05800, alpha: 0.02252, time: 33.38444
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   549 ----
[CW] collect: return: 425.58503, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 3.63073, qf2_loss: 3.63366, policy_loss: -93.04334, policy_entropy: -5.86154, alpha: 0.02251, time: 33.52083
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   550 ----
[CW] collect: return: 212.36265, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 4.20200, qf2_loss: 4.19263, policy_loss: -92.08132, policy_entropy: -5.81051, alpha: 0.02239, time: 33.42879
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   551 ----
[CW] collect: return: 63.08221, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 4.09633, qf2_loss: 4.10487, policy_loss: -93.48012, policy_entropy: -5.91754, alpha: 0.02228, time: 33.61155
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   552 ----
[CW] collect: return: 91.91936, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 3.77620, qf2_loss: 3.76827, policy_loss: -93.16834, policy_entropy: -5.81181, alpha: 0.02216, time: 33.52707
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   553 ----
[CW] collect: return: 214.64541, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 4.87245, qf2_loss: 4.87046, policy_loss: -92.09517, policy_entropy: -5.79176, alpha: 0.02201, time: 33.68583
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   554 ----
[CW] collect: return: 424.82546, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 4.72068, qf2_loss: 4.69303, policy_loss: -93.37745, policy_entropy: -5.81890, alpha: 0.02181, time: 33.27733
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   555 ----
[CW] collect: return: 211.98648, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 4.02526, qf2_loss: 4.02039, policy_loss: -92.95793, policy_entropy: -5.81380, alpha: 0.02169, time: 33.53209
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   556 ----
[CW] collect: return: 473.25690, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 4.41108, qf2_loss: 4.36010, policy_loss: -92.46316, policy_entropy: -5.75234, alpha: 0.02151, time: 33.44040
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   557 ----
[CW] collect: return: 284.25373, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 3.85726, qf2_loss: 3.89596, policy_loss: -94.03279, policy_entropy: -6.12414, alpha: 0.02144, time: 33.45609
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   558 ----
[CW] collect: return: 66.94553, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 3.77585, qf2_loss: 3.79086, policy_loss: -94.18588, policy_entropy: -6.37776, alpha: 0.02159, time: 33.58650
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   559 ----
[CW] collect: return: 391.83411, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 4.20925, qf2_loss: 4.18647, policy_loss: -92.70994, policy_entropy: -6.32695, alpha: 0.02192, time: 33.47954
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   560 ----
[CW] collect: return: 410.69635, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 4.43932, qf2_loss: 4.42669, policy_loss: -92.66334, policy_entropy: -5.98893, alpha: 0.02204, time: 33.68878
[CW] eval: return: 249.55496, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   561 ----
[CW] collect: return: 348.84335, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 4.35353, qf2_loss: 4.34216, policy_loss: -95.22466, policy_entropy: -6.30838, alpha: 0.02215, time: 33.40812
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   562 ----
[CW] collect: return: 182.66961, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 4.53241, qf2_loss: 4.48525, policy_loss: -93.05744, policy_entropy: -6.12491, alpha: 0.02235, time: 33.74866
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   563 ----
[CW] collect: return: 304.39746, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 4.13861, qf2_loss: 4.14256, policy_loss: -93.60919, policy_entropy: -6.18041, alpha: 0.02245, time: 33.47572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   564 ----
[CW] collect: return: 71.65356, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 4.17303, qf2_loss: 4.16626, policy_loss: -92.57466, policy_entropy: -5.94286, alpha: 0.02254, time: 33.61831
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   565 ----
[CW] collect: return: 263.90645, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 4.70487, qf2_loss: 4.66113, policy_loss: -95.12042, policy_entropy: -6.09129, alpha: 0.02254, time: 33.48576
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   566 ----
[CW] collect: return: 441.32695, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 4.52220, qf2_loss: 4.52971, policy_loss: -92.82812, policy_entropy: -5.99914, alpha: 0.02256, time: 33.40533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   567 ----
[CW] collect: return: 453.17260, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 4.62624, qf2_loss: 4.61242, policy_loss: -93.51052, policy_entropy: -6.04850, alpha: 0.02255, time: 33.40897
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   568 ----
[CW] collect: return: 437.74583, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 4.28421, qf2_loss: 4.29135, policy_loss: -93.84219, policy_entropy: -6.17983, alpha: 0.02267, time: 33.56093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   569 ----
[CW] collect: return: 124.90376, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 4.38218, qf2_loss: 4.38622, policy_loss: -94.16071, policy_entropy: -6.02577, alpha: 0.02279, time: 33.60515
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   570 ----
[CW] collect: return: 427.55077, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 4.05309, qf2_loss: 4.04653, policy_loss: -94.22442, policy_entropy: -5.89923, alpha: 0.02276, time: 33.35391
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   571 ----
[CW] collect: return: 97.05008, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 4.81911, qf2_loss: 4.84223, policy_loss: -94.53728, policy_entropy: -6.01013, alpha: 0.02271, time: 33.66748
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   572 ----
[CW] collect: return: 178.66017, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 4.92524, qf2_loss: 4.82563, policy_loss: -93.95029, policy_entropy: -5.90264, alpha: 0.02268, time: 33.54846
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   573 ----
[CW] collect: return: 136.26307, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 4.72608, qf2_loss: 4.69318, policy_loss: -94.72173, policy_entropy: -6.02701, alpha: 0.02268, time: 33.52251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   574 ----
[CW] collect: return: 314.29233, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 4.07596, qf2_loss: 4.07616, policy_loss: -93.73420, policy_entropy: -5.88837, alpha: 0.02261, time: 33.59194
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   575 ----
[CW] collect: return: 144.99886, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 4.20923, qf2_loss: 4.21199, policy_loss: -94.41761, policy_entropy: -5.86150, alpha: 0.02251, time: 33.62176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   576 ----
[CW] collect: return: 438.41456, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 4.01569, qf2_loss: 3.99783, policy_loss: -94.07371, policy_entropy: -6.07568, alpha: 0.02244, time: 33.61463
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   577 ----
[CW] collect: return: 91.63213, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 4.80642, qf2_loss: 4.80933, policy_loss: -94.90236, policy_entropy: -6.16125, alpha: 0.02259, time: 33.71066
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   578 ----
[CW] collect: return: 392.98212, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 4.26631, qf2_loss: 4.26483, policy_loss: -94.05978, policy_entropy: -5.94502, alpha: 0.02265, time: 33.64868
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   579 ----
[CW] collect: return: 436.76066, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 3.83359, qf2_loss: 3.83766, policy_loss: -94.23244, policy_entropy: -5.99253, alpha: 0.02261, time: 33.42902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   580 ----
[CW] collect: return: 97.40139, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 4.23169, qf2_loss: 4.19734, policy_loss: -94.15089, policy_entropy: -5.97955, alpha: 0.02258, time: 33.70324
[CW] eval: return: 249.46725, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   581 ----
[CW] collect: return: 122.07572, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 5.23233, qf2_loss: 5.13398, policy_loss: -93.41037, policy_entropy: -5.87456, alpha: 0.02253, time: 33.61033
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   582 ----
[CW] collect: return: 447.11671, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 5.12944, qf2_loss: 5.10571, policy_loss: -94.65609, policy_entropy: -6.05314, alpha: 0.02249, time: 33.55174
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   583 ----
[CW] collect: return: 385.49703, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 4.79135, qf2_loss: 4.78027, policy_loss: -94.95501, policy_entropy: -6.08104, alpha: 0.02253, time: 33.62081
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   584 ----
[CW] collect: return: 116.52386, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 5.20761, qf2_loss: 5.15054, policy_loss: -93.83948, policy_entropy: -5.86279, alpha: 0.02254, time: 33.53946
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   585 ----
[CW] collect: return: 111.16530, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 4.59552, qf2_loss: 4.62322, policy_loss: -94.98044, policy_entropy: -5.94791, alpha: 0.02250, time: 35.18437
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   586 ----
[CW] collect: return: 269.30638, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 5.07818, qf2_loss: 5.07702, policy_loss: -96.17298, policy_entropy: -6.07203, alpha: 0.02243, time: 33.58853
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   587 ----
[CW] collect: return: 239.24155, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 4.63657, qf2_loss: 4.59732, policy_loss: -95.89009, policy_entropy: -5.93396, alpha: 0.02249, time: 33.56417
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   588 ----
[CW] collect: return: 304.12763, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 4.12058, qf2_loss: 4.10623, policy_loss: -96.67431, policy_entropy: -5.94264, alpha: 0.02241, time: 33.61686
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   589 ----
[CW] collect: return: 117.36691, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 4.60396, qf2_loss: 4.60027, policy_loss: -94.65683, policy_entropy: -5.70650, alpha: 0.02230, time: 33.42838
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   590 ----
[CW] collect: return: 124.53871, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 4.85198, qf2_loss: 4.85819, policy_loss: -95.19146, policy_entropy: -5.90139, alpha: 0.02211, time: 33.64906
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   591 ----
[CW] collect: return: 47.02504, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 5.29118, qf2_loss: 5.27183, policy_loss: -95.73111, policy_entropy: -5.98115, alpha: 0.02202, time: 33.36178
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   592 ----
[CW] collect: return: 293.98648, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 5.17710, qf2_loss: 5.11767, policy_loss: -94.65613, policy_entropy: -5.70416, alpha: 0.02195, time: 33.63841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   593 ----
[CW] collect: return: 421.20920, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 5.36981, qf2_loss: 5.37279, policy_loss: -96.56912, policy_entropy: -6.04498, alpha: 0.02184, time: 33.47428
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   594 ----
[CW] collect: return: 379.97261, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 4.88331, qf2_loss: 4.83894, policy_loss: -96.16091, policy_entropy: -6.15708, alpha: 0.02189, time: 34.42585
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   595 ----
[CW] collect: return: 65.79157, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 5.15238, qf2_loss: 5.19266, policy_loss: -95.65110, policy_entropy: -6.19770, alpha: 0.02205, time: 33.59187
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   596 ----
[CW] collect: return: 378.55783, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 4.30570, qf2_loss: 4.27602, policy_loss: -96.21292, policy_entropy: -6.22554, alpha: 0.02224, time: 33.69144
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   597 ----
[CW] collect: return: 402.18974, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 4.42720, qf2_loss: 4.39737, policy_loss: -94.78138, policy_entropy: -5.90834, alpha: 0.02230, time: 33.57910
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   598 ----
[CW] collect: return: 419.97435, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 4.51072, qf2_loss: 4.53285, policy_loss: -93.94454, policy_entropy: -5.83019, alpha: 0.02221, time: 33.56976
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   599 ----
[CW] collect: return: 153.75657, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 4.26022, qf2_loss: 4.25165, policy_loss: -95.99953, policy_entropy: -5.96796, alpha: 0.02214, time: 33.55253
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   600 ----
[CW] collect: return: 98.68801, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 4.41567, qf2_loss: 4.40282, policy_loss: -96.28221, policy_entropy: -6.01396, alpha: 0.02208, time: 33.55934
[CW] eval: return: 167.99435, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   601 ----
[CW] collect: return: 127.16354, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 5.80773, qf2_loss: 5.78020, policy_loss: -95.50105, policy_entropy: -5.96722, alpha: 0.02208, time: 33.41398
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   602 ----
[CW] collect: return: 86.07825, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 5.41541, qf2_loss: 5.38746, policy_loss: -96.10036, policy_entropy: -5.92288, alpha: 0.02205, time: 33.55623
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   603 ----
[CW] collect: return: 487.75773, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 5.12367, qf2_loss: 5.14609, policy_loss: -97.47115, policy_entropy: -5.96222, alpha: 0.02198, time: 33.33464
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   604 ----
[CW] collect: return: 30.79103, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 4.59780, qf2_loss: 4.61557, policy_loss: -96.08426, policy_entropy: -5.83160, alpha: 0.02191, time: 33.60757
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   605 ----
[CW] collect: return: 185.23097, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 5.57265, qf2_loss: 5.56004, policy_loss: -95.15840, policy_entropy: -5.79341, alpha: 0.02179, time: 33.49682
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   606 ----
[CW] collect: return: 336.77393, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 4.34239, qf2_loss: 4.33040, policy_loss: -96.37117, policy_entropy: -5.93064, alpha: 0.02168, time: 33.55413
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   607 ----
[CW] collect: return: 140.02492, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 4.41203, qf2_loss: 4.40864, policy_loss: -95.28435, policy_entropy: -5.74373, alpha: 0.02156, time: 33.53176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   608 ----
[CW] collect: return: 174.62492, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 4.46718, qf2_loss: 4.48842, policy_loss: -96.53190, policy_entropy: -5.97491, alpha: 0.02140, time: 33.49851
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   609 ----
[CW] collect: return: 341.04439, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 5.33847, qf2_loss: 5.29841, policy_loss: -96.02986, policy_entropy: -5.84747, alpha: 0.02140, time: 33.59804
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   610 ----
[CW] collect: return: 84.31206, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 4.97060, qf2_loss: 5.02609, policy_loss: -94.10314, policy_entropy: -5.71810, alpha: 0.02121, time: 33.38775
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   611 ----
[CW] collect: return: 148.38155, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 4.98044, qf2_loss: 4.94144, policy_loss: -96.30651, policy_entropy: -5.86369, alpha: 0.02103, time: 33.68670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   612 ----
[CW] collect: return: 398.05316, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 4.86518, qf2_loss: 4.85436, policy_loss: -97.59739, policy_entropy: -6.15776, alpha: 0.02101, time: 33.45295
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   613 ----
[CW] collect: return: 131.66594, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 4.88524, qf2_loss: 4.88572, policy_loss: -96.19154, policy_entropy: -5.95813, alpha: 0.02108, time: 33.58380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   614 ----
[CW] collect: return: 119.60280, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 4.22707, qf2_loss: 4.22972, policy_loss: -94.87580, policy_entropy: -5.83710, alpha: 0.02102, time: 33.43332
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   615 ----
[CW] collect: return: 103.60096, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 4.98237, qf2_loss: 4.92634, policy_loss: -95.00680, policy_entropy: -5.93620, alpha: 0.02094, time: 33.60321
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   616 ----
[CW] collect: return: 301.56956, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 5.13904, qf2_loss: 5.08655, policy_loss: -97.32717, policy_entropy: -6.12628, alpha: 0.02093, time: 33.34188
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   617 ----
[CW] collect: return: 68.10116, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 4.84119, qf2_loss: 4.86225, policy_loss: -96.87614, policy_entropy: -6.08499, alpha: 0.02105, time: 33.61414
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   618 ----
[CW] collect: return: 273.09264, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 4.32911, qf2_loss: 4.36828, policy_loss: -95.10484, policy_entropy: -5.80795, alpha: 0.02101, time: 33.59845
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   619 ----
[CW] collect: return: 169.14761, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 4.84792, qf2_loss: 4.86427, policy_loss: -96.01116, policy_entropy: -5.96459, alpha: 0.02094, time: 34.15312
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   620 ----
[CW] collect: return: 279.31204, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 5.43171, qf2_loss: 5.43717, policy_loss: -96.65685, policy_entropy: -6.08132, alpha: 0.02092, time: 33.63819
[CW] eval: return: 290.57107, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   621 ----
[CW] collect: return: 54.55375, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 5.73021, qf2_loss: 5.65120, policy_loss: -94.84030, policy_entropy: -5.76834, alpha: 0.02089, time: 33.57564
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   622 ----
[CW] collect: return: 55.25643, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 5.13468, qf2_loss: 5.16846, policy_loss: -96.79150, policy_entropy: -6.12651, alpha: 0.02083, time: 33.31474
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   623 ----
[CW] collect: return: 200.48490, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 5.00417, qf2_loss: 4.97980, policy_loss: -96.58788, policy_entropy: -5.99885, alpha: 0.02090, time: 33.61362
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   624 ----
[CW] collect: return: 330.78103, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 4.90099, qf2_loss: 4.86652, policy_loss: -96.56438, policy_entropy: -5.93745, alpha: 0.02092, time: 33.57993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   625 ----
[CW] collect: return: 468.71036, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 5.27186, qf2_loss: 5.27455, policy_loss: -95.70441, policy_entropy: -5.93679, alpha: 0.02081, time: 33.47052
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   626 ----
[CW] collect: return: 164.36296, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 5.25791, qf2_loss: 5.24799, policy_loss: -94.70142, policy_entropy: -5.91050, alpha: 0.02076, time: 33.59857
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   627 ----
[CW] collect: return: 125.17865, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 4.84912, qf2_loss: 4.86960, policy_loss: -97.54944, policy_entropy: -6.02053, alpha: 0.02071, time: 33.39786
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   628 ----
[CW] collect: return: 133.75449, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 5.90704, qf2_loss: 5.89552, policy_loss: -97.59134, policy_entropy: -6.25788, alpha: 0.02085, time: 33.52095
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   629 ----
[CW] collect: return: 285.36329, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 4.95547, qf2_loss: 4.96396, policy_loss: -95.80130, policy_entropy: -6.19589, alpha: 0.02099, time: 33.54465
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   630 ----
[CW] collect: return: 379.62589, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 5.13000, qf2_loss: 5.11801, policy_loss: -97.93028, policy_entropy: -6.10530, alpha: 0.02113, time: 33.51795
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   631 ----
[CW] collect: return: 70.60591, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 4.94226, qf2_loss: 4.94671, policy_loss: -99.37881, policy_entropy: -6.23236, alpha: 0.02120, time: 33.51661
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   632 ----
[CW] collect: return: 324.70191, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 5.12386, qf2_loss: 5.11205, policy_loss: -96.40413, policy_entropy: -6.01274, alpha: 0.02133, time: 33.64689
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   633 ----
[CW] collect: return: 293.25230, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 5.25194, qf2_loss: 5.23042, policy_loss: -97.43011, policy_entropy: -6.09821, alpha: 0.02137, time: 33.53605
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   634 ----
[CW] collect: return: 233.01027, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 5.84487, qf2_loss: 5.77621, policy_loss: -96.23203, policy_entropy: -6.02049, alpha: 0.02140, time: 33.49324
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   635 ----
[CW] collect: return: 420.33490, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 4.85281, qf2_loss: 4.87074, policy_loss: -97.71442, policy_entropy: -6.09734, alpha: 0.02149, time: 33.36009
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   636 ----
[CW] collect: return: 448.04432, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 4.73689, qf2_loss: 4.72641, policy_loss: -96.72183, policy_entropy: -5.98358, alpha: 0.02146, time: 33.47371
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   637 ----
[CW] collect: return: 193.46541, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 4.89291, qf2_loss: 4.91715, policy_loss: -97.71193, policy_entropy: -6.09001, alpha: 0.02149, time: 33.61769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   638 ----
[CW] collect: return: 150.73659, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 5.25441, qf2_loss: 5.28075, policy_loss: -96.88109, policy_entropy: -6.11175, alpha: 0.02157, time: 33.47138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   639 ----
[CW] collect: return: 448.95465, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 4.88332, qf2_loss: 4.87083, policy_loss: -98.13399, policy_entropy: -6.12516, alpha: 0.02165, time: 33.61975
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   640 ----
[CW] collect: return: 91.61113, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 4.67782, qf2_loss: 4.68772, policy_loss: -98.88378, policy_entropy: -6.15221, alpha: 0.02173, time: 33.43163
[CW] eval: return: 305.90008, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   641 ----
[CW] collect: return: 468.23029, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 4.77245, qf2_loss: 4.76929, policy_loss: -98.17851, policy_entropy: -5.96393, alpha: 0.02181, time: 33.56569
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   642 ----
[CW] collect: return: 55.61206, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 5.15750, qf2_loss: 5.15310, policy_loss: -98.55083, policy_entropy: -6.03555, alpha: 0.02180, time: 33.28536
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   643 ----
[CW] collect: return: 439.95594, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 5.15463, qf2_loss: 5.16476, policy_loss: -98.08393, policy_entropy: -5.98689, alpha: 0.02184, time: 33.44528
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   644 ----
[CW] collect: return: 59.30524, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 5.05428, qf2_loss: 5.01507, policy_loss: -97.36746, policy_entropy: -5.96000, alpha: 0.02184, time: 33.46447
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   645 ----
[CW] collect: return: 241.73337, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 5.08782, qf2_loss: 5.06262, policy_loss: -99.28695, policy_entropy: -6.02361, alpha: 0.02182, time: 33.53138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   646 ----
[CW] collect: return: 241.36687, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 5.27708, qf2_loss: 5.22343, policy_loss: -97.97414, policy_entropy: -5.83579, alpha: 0.02180, time: 33.31610
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   647 ----
[CW] collect: return: 406.22420, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 5.38030, qf2_loss: 5.33858, policy_loss: -98.04204, policy_entropy: -5.94000, alpha: 0.02166, time: 33.23680
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   648 ----
[CW] collect: return: 51.97247, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 4.91402, qf2_loss: 4.91388, policy_loss: -97.70530, policy_entropy: -6.11318, alpha: 0.02169, time: 33.31825
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   649 ----
[CW] collect: return: 83.48035, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 5.70669, qf2_loss: 5.65071, policy_loss: -97.77260, policy_entropy: -6.03487, alpha: 0.02179, time: 33.42164
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   650 ----
[CW] collect: return: 379.38460, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 4.93599, qf2_loss: 4.94853, policy_loss: -98.63361, policy_entropy: -5.92764, alpha: 0.02174, time: 33.40769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   651 ----
[CW] collect: return: 57.42282, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 4.64381, qf2_loss: 4.63083, policy_loss: -99.36253, policy_entropy: -5.98349, alpha: 0.02171, time: 33.35380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   652 ----
[CW] collect: return: 401.27453, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 4.42266, qf2_loss: 4.46367, policy_loss: -98.45015, policy_entropy: -5.80861, alpha: 0.02166, time: 33.47162
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   653 ----
[CW] collect: return: 236.70547, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 4.46945, qf2_loss: 4.44797, policy_loss: -97.58087, policy_entropy: -5.78714, alpha: 0.02151, time: 33.08750
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   654 ----
[CW] collect: return: 289.39942, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 4.47665, qf2_loss: 4.49777, policy_loss: -98.56374, policy_entropy: -5.79029, alpha: 0.02131, time: 33.46770
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   655 ----
[CW] collect: return: 426.68383, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 5.20256, qf2_loss: 5.16981, policy_loss: -97.60099, policy_entropy: -5.89635, alpha: 0.02121, time: 34.73333
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   656 ----
[CW] collect: return: 426.24884, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 5.02774, qf2_loss: 5.04239, policy_loss: -99.72417, policy_entropy: -6.02447, alpha: 0.02120, time: 33.38688
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   657 ----
[CW] collect: return: 392.68804, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 4.88616, qf2_loss: 4.88068, policy_loss: -97.91443, policy_entropy: -5.94349, alpha: 0.02120, time: 33.38881
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   658 ----
[CW] collect: return: 437.39117, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 4.75356, qf2_loss: 4.74543, policy_loss: -100.26664, policy_entropy: -6.16227, alpha: 0.02120, time: 33.49936
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   659 ----
[CW] collect: return: 182.61885, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 4.59813, qf2_loss: 4.57304, policy_loss: -97.83503, policy_entropy: -6.01164, alpha: 0.02127, time: 33.49333
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   660 ----
[CW] collect: return: 155.83955, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 5.23019, qf2_loss: 5.24150, policy_loss: -97.43675, policy_entropy: -5.98963, alpha: 0.02127, time: 33.21245
[CW] eval: return: 383.78381, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   661 ----
[CW] collect: return: 276.80255, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 5.69489, qf2_loss: 5.64054, policy_loss: -99.05763, policy_entropy: -6.03806, alpha: 0.02125, time: 33.38195
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   662 ----
[CW] collect: return: 398.09188, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 4.83813, qf2_loss: 4.83194, policy_loss: -98.31448, policy_entropy: -5.92297, alpha: 0.02127, time: 33.41118
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   663 ----
[CW] collect: return: 256.00372, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 5.13496, qf2_loss: 5.14475, policy_loss: -97.88717, policy_entropy: -6.01980, alpha: 0.02126, time: 33.24018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   664 ----
[CW] collect: return: 450.26715, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 5.19287, qf2_loss: 5.23055, policy_loss: -99.06432, policy_entropy: -6.17434, alpha: 0.02135, time: 33.44692
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   665 ----
[CW] collect: return: 414.99517, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 5.90115, qf2_loss: 5.83366, policy_loss: -100.28268, policy_entropy: -6.22465, alpha: 0.02143, time: 33.32808
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   666 ----
[CW] collect: return: 376.94423, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 4.94168, qf2_loss: 4.98252, policy_loss: -100.04322, policy_entropy: -6.25349, alpha: 0.02166, time: 33.34830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   667 ----
[CW] collect: return: 435.80709, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 4.48818, qf2_loss: 4.50250, policy_loss: -100.81149, policy_entropy: -6.24495, alpha: 0.02182, time: 33.42562
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   668 ----
[CW] collect: return: 475.91185, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 4.73484, qf2_loss: 4.75960, policy_loss: -98.33069, policy_entropy: -5.91851, alpha: 0.02192, time: 33.39998
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   669 ----
[CW] collect: return: 97.06495, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 6.14471, qf2_loss: 6.07656, policy_loss: -101.49428, policy_entropy: -6.11961, alpha: 0.02191, time: 33.50084
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   670 ----
[CW] collect: return: 460.37945, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 6.15245, qf2_loss: 6.18396, policy_loss: -98.73709, policy_entropy: -5.86825, alpha: 0.02193, time: 33.31799
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   671 ----
[CW] collect: return: 222.35450, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 4.86766, qf2_loss: 4.86188, policy_loss: -101.48381, policy_entropy: -6.11848, alpha: 0.02192, time: 33.43264
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   672 ----
[CW] collect: return: 77.53845, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 4.97212, qf2_loss: 4.95393, policy_loss: -99.70858, policy_entropy: -5.91258, alpha: 0.02191, time: 33.12871
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   673 ----
[CW] collect: return: 437.35745, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 4.91407, qf2_loss: 4.93507, policy_loss: -99.21787, policy_entropy: -5.92458, alpha: 0.02183, time: 33.52307
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   674 ----
[CW] collect: return: 231.42837, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 4.84082, qf2_loss: 4.78282, policy_loss: -99.87699, policy_entropy: -6.00297, alpha: 0.02181, time: 33.17540
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   675 ----
[CW] collect: return: 396.07405, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 4.77959, qf2_loss: 4.75901, policy_loss: -99.95735, policy_entropy: -6.08403, alpha: 0.02189, time: 33.75264
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   676 ----
[CW] collect: return: 445.67203, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 5.83882, qf2_loss: 5.84474, policy_loss: -100.14502, policy_entropy: -5.99506, alpha: 0.02192, time: 33.22406
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   677 ----
[CW] collect: return: 207.89945, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 5.30898, qf2_loss: 5.30432, policy_loss: -99.81971, policy_entropy: -6.07199, alpha: 0.02192, time: 33.43803
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   678 ----
[CW] collect: return: 255.56765, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 5.15604, qf2_loss: 5.14874, policy_loss: -101.02938, policy_entropy: -6.13788, alpha: 0.02199, time: 33.18207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   679 ----
[CW] collect: return: 103.23489, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 5.38881, qf2_loss: 5.30799, policy_loss: -101.34249, policy_entropy: -6.12852, alpha: 0.02207, time: 33.47993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   680 ----
[CW] collect: return: 431.84171, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 4.72553, qf2_loss: 4.73843, policy_loss: -100.00355, policy_entropy: -5.98903, alpha: 0.02216, time: 33.55383
[CW] eval: return: 259.82365, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   681 ----
[CW] collect: return: 51.18754, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 5.29977, qf2_loss: 5.30778, policy_loss: -100.32600, policy_entropy: -5.97597, alpha: 0.02211, time: 33.38634
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   682 ----
[CW] collect: return: 395.73561, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 5.13801, qf2_loss: 5.14812, policy_loss: -100.69879, policy_entropy: -6.11221, alpha: 0.02213, time: 33.24715
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   683 ----
[CW] collect: return: 411.29850, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 5.09502, qf2_loss: 5.10213, policy_loss: -99.77054, policy_entropy: -6.01778, alpha: 0.02223, time: 33.40766
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   684 ----
[CW] collect: return: 177.77111, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 5.41370, qf2_loss: 5.41598, policy_loss: -100.30778, policy_entropy: -6.10323, alpha: 0.02226, time: 33.10161
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   685 ----
[CW] collect: return: 291.01325, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 5.76304, qf2_loss: 5.74250, policy_loss: -102.03745, policy_entropy: -6.27914, alpha: 0.02235, time: 33.41286
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   686 ----
[CW] collect: return: 304.45650, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 5.19883, qf2_loss: 5.28981, policy_loss: -100.96968, policy_entropy: -6.27412, alpha: 0.02261, time: 33.17145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   687 ----
[CW] collect: return: 417.00659, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 5.40201, qf2_loss: 5.33417, policy_loss: -100.76859, policy_entropy: -6.14205, alpha: 0.02275, time: 33.38890
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   688 ----
[CW] collect: return: 429.49261, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 5.29129, qf2_loss: 5.30795, policy_loss: -101.68420, policy_entropy: -6.17199, alpha: 0.02286, time: 33.17974
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   689 ----
[CW] collect: return: 28.36190, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 5.59413, qf2_loss: 5.55424, policy_loss: -101.33534, policy_entropy: -6.12937, alpha: 0.02304, time: 33.31517
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   690 ----
[CW] collect: return: 99.10421, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 6.38878, qf2_loss: 6.38968, policy_loss: -102.02669, policy_entropy: -6.16369, alpha: 0.02313, time: 33.24254
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   691 ----
[CW] collect: return: 404.84643, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 5.74002, qf2_loss: 5.70285, policy_loss: -100.71049, policy_entropy: -6.04767, alpha: 0.02323, time: 33.15418
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   692 ----
[CW] collect: return: 376.79910, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 5.40096, qf2_loss: 5.36957, policy_loss: -101.21858, policy_entropy: -5.99732, alpha: 0.02324, time: 33.24983
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   693 ----
[CW] collect: return: 288.62236, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 4.93899, qf2_loss: 4.95954, policy_loss: -100.40937, policy_entropy: -5.86578, alpha: 0.02319, time: 33.25609
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   694 ----
[CW] collect: return: 446.17090, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 4.96170, qf2_loss: 4.97859, policy_loss: -101.07653, policy_entropy: -5.92693, alpha: 0.02310, time: 33.35629
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   695 ----
[CW] collect: return: 479.81596, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 5.29363, qf2_loss: 5.29062, policy_loss: -101.91782, policy_entropy: -5.95463, alpha: 0.02305, time: 34.92282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   696 ----
[CW] collect: return: 420.42023, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 5.23340, qf2_loss: 5.20561, policy_loss: -100.54647, policy_entropy: -5.93479, alpha: 0.02298, time: 33.43865
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   697 ----
[CW] collect: return: 274.19556, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 5.60785, qf2_loss: 5.59892, policy_loss: -101.42261, policy_entropy: -5.89658, alpha: 0.02294, time: 33.09717
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   698 ----
[CW] collect: return: 46.48666, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 5.70931, qf2_loss: 5.73153, policy_loss: -100.45545, policy_entropy: -5.84289, alpha: 0.02285, time: 33.38810
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   699 ----
[CW] collect: return: 487.33724, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 5.76543, qf2_loss: 5.78971, policy_loss: -100.83177, policy_entropy: -5.99864, alpha: 0.02277, time: 33.39099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   700 ----
[CW] collect: return: 426.90555, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 7.45821, qf2_loss: 7.40071, policy_loss: -102.14568, policy_entropy: -6.19372, alpha: 0.02283, time: 34.00547
[CW] eval: return: 337.41405, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   701 ----
[CW] collect: return: 415.45508, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 6.36621, qf2_loss: 6.35550, policy_loss: -102.67295, policy_entropy: -6.22159, alpha: 0.02299, time: 33.42997
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   702 ----
[CW] collect: return: 425.79714, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 5.19801, qf2_loss: 5.23574, policy_loss: -102.11558, policy_entropy: -6.04149, alpha: 0.02314, time: 33.29683
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   703 ----
[CW] collect: return: 392.70638, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 5.45198, qf2_loss: 5.48901, policy_loss: -103.14823, policy_entropy: -6.20592, alpha: 0.02315, time: 33.27991
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   704 ----
[CW] collect: return: 406.19875, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 5.13737, qf2_loss: 5.11162, policy_loss: -103.40991, policy_entropy: -6.04916, alpha: 0.02337, time: 33.34232
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   705 ----
[CW] collect: return: 127.34678, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 5.02793, qf2_loss: 5.05195, policy_loss: -102.93044, policy_entropy: -5.98884, alpha: 0.02338, time: 33.35328
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   706 ----
[CW] collect: return: 170.40074, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 4.49462, qf2_loss: 4.48016, policy_loss: -102.32346, policy_entropy: -5.99995, alpha: 0.02334, time: 33.28764
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   707 ----
[CW] collect: return: 323.35304, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 5.62231, qf2_loss: 5.60947, policy_loss: -103.97440, policy_entropy: -6.11893, alpha: 0.02337, time: 33.38285
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   708 ----
[CW] collect: return: 483.75232, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 5.51632, qf2_loss: 5.52451, policy_loss: -101.91350, policy_entropy: -5.98599, alpha: 0.02344, time: 33.29835
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   709 ----
[CW] collect: return: 334.79576, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 5.89241, qf2_loss: 5.84787, policy_loss: -101.81985, policy_entropy: -5.78527, alpha: 0.02337, time: 33.17885
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   710 ----
[CW] collect: return: 426.31481, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 5.25388, qf2_loss: 5.22768, policy_loss: -101.01250, policy_entropy: -5.76834, alpha: 0.02318, time: 33.16338
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   711 ----
[CW] collect: return: 399.10197, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 5.44761, qf2_loss: 5.47156, policy_loss: -102.36485, policy_entropy: -5.86546, alpha: 0.02302, time: 33.34188
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   712 ----
[CW] collect: return: 95.27886, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 6.40819, qf2_loss: 6.40519, policy_loss: -102.58797, policy_entropy: -5.96473, alpha: 0.02294, time: 33.26369
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   713 ----
[CW] collect: return: 296.49235, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 6.42632, qf2_loss: 6.40777, policy_loss: -102.94701, policy_entropy: -6.04867, alpha: 0.02294, time: 33.57258
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   714 ----
[CW] collect: return: 211.43976, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 5.69330, qf2_loss: 5.70314, policy_loss: -102.92030, policy_entropy: -6.01793, alpha: 0.02301, time: 33.33934
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   715 ----
[CW] collect: return: 430.52577, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 6.79581, qf2_loss: 6.69651, policy_loss: -102.74125, policy_entropy: -6.06625, alpha: 0.02303, time: 33.22935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   716 ----
[CW] collect: return: 485.40199, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 5.78564, qf2_loss: 5.82152, policy_loss: -102.16400, policy_entropy: -6.00615, alpha: 0.02303, time: 33.23019
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   717 ----
[CW] collect: return: 426.27778, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 4.98116, qf2_loss: 4.97335, policy_loss: -102.65650, policy_entropy: -6.03859, alpha: 0.02302, time: 33.25336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   718 ----
[CW] collect: return: 477.42217, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 5.08608, qf2_loss: 5.05472, policy_loss: -101.99842, policy_entropy: -5.97029, alpha: 0.02308, time: 33.29655
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   719 ----
[CW] collect: return: 59.03948, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 5.24406, qf2_loss: 5.23872, policy_loss: -101.93577, policy_entropy: -5.87340, alpha: 0.02302, time: 33.33829
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   720 ----
[CW] collect: return: 434.28420, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 5.30319, qf2_loss: 5.29609, policy_loss: -103.82356, policy_entropy: -6.08749, alpha: 0.02298, time: 33.49360
[CW] eval: return: 344.98780, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   721 ----
[CW] collect: return: 442.43230, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 5.20437, qf2_loss: 5.21974, policy_loss: -102.23724, policy_entropy: -6.01221, alpha: 0.02301, time: 33.14913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   722 ----
[CW] collect: return: 340.32695, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 5.29493, qf2_loss: 5.32957, policy_loss: -103.51521, policy_entropy: -6.10005, alpha: 0.02308, time: 33.41517
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   723 ----
[CW] collect: return: 426.73652, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 5.40797, qf2_loss: 5.39363, policy_loss: -103.40590, policy_entropy: -6.01265, alpha: 0.02309, time: 33.31152
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   724 ----
[CW] collect: return: 464.59113, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 5.68023, qf2_loss: 5.61450, policy_loss: -101.47876, policy_entropy: -5.84493, alpha: 0.02305, time: 33.33898
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   725 ----
[CW] collect: return: 145.97967, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 6.10236, qf2_loss: 6.05775, policy_loss: -103.96649, policy_entropy: -6.21754, alpha: 0.02304, time: 33.14842
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   726 ----
[CW] collect: return: 484.76277, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 5.81001, qf2_loss: 5.80093, policy_loss: -103.22242, policy_entropy: -6.08191, alpha: 0.02317, time: 33.39494
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   727 ----
[CW] collect: return: 437.17904, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 5.89023, qf2_loss: 5.81799, policy_loss: -101.28639, policy_entropy: -5.80364, alpha: 0.02312, time: 33.06104
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   728 ----
[CW] collect: return: 395.45811, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 4.92919, qf2_loss: 4.93539, policy_loss: -104.51875, policy_entropy: -6.04026, alpha: 0.02309, time: 34.01955
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   729 ----
[CW] collect: return: 265.69843, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 5.36869, qf2_loss: 5.37136, policy_loss: -104.28719, policy_entropy: -6.15967, alpha: 0.02313, time: 34.53782
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   730 ----
[CW] collect: return: 410.39528, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 5.58833, qf2_loss: 5.60060, policy_loss: -103.56394, policy_entropy: -6.05904, alpha: 0.02322, time: 33.59278
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   731 ----
[CW] collect: return: 444.83158, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 6.20833, qf2_loss: 6.18110, policy_loss: -104.01816, policy_entropy: -6.17586, alpha: 0.02334, time: 33.41048
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   732 ----
[CW] collect: return: 513.62678, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 5.65887, qf2_loss: 5.69156, policy_loss: -105.17245, policy_entropy: -6.09373, alpha: 0.02343, time: 33.38555
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   733 ----
[CW] collect: return: 365.81006, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 5.85446, qf2_loss: 5.87884, policy_loss: -103.85128, policy_entropy: -6.05088, alpha: 0.02348, time: 33.38341
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   734 ----
[CW] collect: return: 393.16066, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 5.61896, qf2_loss: 5.62559, policy_loss: -104.74947, policy_entropy: -6.15600, alpha: 0.02362, time: 33.19009
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   735 ----
[CW] collect: return: 488.86874, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 6.76965, qf2_loss: 6.68730, policy_loss: -104.66783, policy_entropy: -6.43930, alpha: 0.02381, time: 33.37778
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   736 ----
[CW] collect: return: 332.80650, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 10.02733, qf2_loss: 9.97075, policy_loss: -104.51986, policy_entropy: -6.34885, alpha: 0.02413, time: 33.26371
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   737 ----
[CW] collect: return: 440.81070, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 6.94986, qf2_loss: 6.99055, policy_loss: -104.66394, policy_entropy: -6.12909, alpha: 0.02435, time: 33.48348
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   738 ----
[CW] collect: return: 435.79143, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 5.59782, qf2_loss: 5.58528, policy_loss: -103.05739, policy_entropy: -6.04497, alpha: 0.02443, time: 33.34532
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   739 ----
[CW] collect: return: 282.76731, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 5.84412, qf2_loss: 5.86432, policy_loss: -104.47814, policy_entropy: -6.25985, alpha: 0.02453, time: 33.39011
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   740 ----
[CW] collect: return: 234.41053, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 5.40253, qf2_loss: 5.38328, policy_loss: -107.54983, policy_entropy: -6.35723, alpha: 0.02483, time: 33.18944
[CW] eval: return: 328.77963, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   741 ----
[CW] collect: return: 74.77635, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 5.33411, qf2_loss: 5.31876, policy_loss: -105.88052, policy_entropy: -6.20403, alpha: 0.02507, time: 33.29235
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   742 ----
[CW] collect: return: 302.28532, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 5.25017, qf2_loss: 5.22895, policy_loss: -103.37484, policy_entropy: -5.91342, alpha: 0.02513, time: 33.09590
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   743 ----
[CW] collect: return: 407.74279, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 5.59644, qf2_loss: 5.50297, policy_loss: -105.30178, policy_entropy: -5.91632, alpha: 0.02508, time: 33.45686
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   744 ----
[CW] collect: return: 75.59912, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 5.40144, qf2_loss: 5.40836, policy_loss: -104.66555, policy_entropy: -5.83074, alpha: 0.02497, time: 33.15775
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   745 ----
[CW] collect: return: 477.83367, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 5.29694, qf2_loss: 5.35227, policy_loss: -105.39224, policy_entropy: -6.05019, alpha: 0.02486, time: 33.30917
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   746 ----
[CW] collect: return: 67.68296, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 5.76346, qf2_loss: 5.66930, policy_loss: -105.32082, policy_entropy: -6.06990, alpha: 0.02494, time: 33.14808
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   747 ----
[CW] collect: return: 450.84193, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 5.68986, qf2_loss: 5.60727, policy_loss: -103.08820, policy_entropy: -5.90919, alpha: 0.02497, time: 33.37989
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   748 ----
[CW] collect: return: 250.50435, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 5.34559, qf2_loss: 5.31264, policy_loss: -104.88994, policy_entropy: -6.04451, alpha: 0.02491, time: 33.28945
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   749 ----
[CW] collect: return: 136.38297, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 6.04202, qf2_loss: 6.03893, policy_loss: -104.60981, policy_entropy: -5.91623, alpha: 0.02487, time: 33.23507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   750 ----
[CW] collect: return: 145.89989, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 5.60400, qf2_loss: 5.60904, policy_loss: -104.03318, policy_entropy: -5.87675, alpha: 0.02483, time: 33.23822
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   751 ----
[CW] collect: return: 356.86892, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 7.14144, qf2_loss: 7.06490, policy_loss: -104.86252, policy_entropy: -5.89328, alpha: 0.02471, time: 33.04569
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   752 ----
[CW] collect: return: 514.14347, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 6.22234, qf2_loss: 6.16874, policy_loss: -104.26956, policy_entropy: -5.83475, alpha: 0.02463, time: 33.18798
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   753 ----
[CW] collect: return: 171.36105, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 6.12647, qf2_loss: 6.15968, policy_loss: -106.03298, policy_entropy: -5.95481, alpha: 0.02449, time: 33.21183
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   754 ----
[CW] collect: return: 482.41376, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 5.33402, qf2_loss: 5.28994, policy_loss: -103.82051, policy_entropy: -5.74138, alpha: 0.02440, time: 33.32939
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   755 ----
[CW] collect: return: 219.32169, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 6.32722, qf2_loss: 6.27145, policy_loss: -104.86917, policy_entropy: -5.92275, alpha: 0.02424, time: 33.14923
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   756 ----
[CW] collect: return: 159.60287, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 6.81790, qf2_loss: 6.77223, policy_loss: -106.37965, policy_entropy: -5.98518, alpha: 0.02424, time: 33.27648
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   757 ----
[CW] collect: return: 321.06963, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 9.01654, qf2_loss: 8.92745, policy_loss: -105.13118, policy_entropy: -6.04816, alpha: 0.02419, time: 33.17770
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   758 ----
[CW] collect: return: 165.16206, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 7.02520, qf2_loss: 7.03293, policy_loss: -105.68104, policy_entropy: -6.28516, alpha: 0.02434, time: 33.27711
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   759 ----
[CW] collect: return: 444.60369, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 5.97539, qf2_loss: 5.92634, policy_loss: -105.34570, policy_entropy: -6.25351, alpha: 0.02457, time: 33.04836
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   760 ----
[CW] collect: return: 61.13801, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 5.15178, qf2_loss: 5.13195, policy_loss: -106.44132, policy_entropy: -6.14828, alpha: 0.02477, time: 33.40178
[CW] eval: return: 275.13728, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   761 ----
[CW] collect: return: 426.46695, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 5.32147, qf2_loss: 5.31336, policy_loss: -105.71324, policy_entropy: -5.98264, alpha: 0.02484, time: 33.28211
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   762 ----
[CW] collect: return: 458.63756, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 6.22782, qf2_loss: 6.23666, policy_loss: -104.98871, policy_entropy: -6.01374, alpha: 0.02481, time: 33.36807
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   763 ----
[CW] collect: return: 407.15096, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 6.07820, qf2_loss: 6.10338, policy_loss: -106.84242, policy_entropy: -6.06255, alpha: 0.02480, time: 33.12950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   764 ----
[CW] collect: return: 271.58294, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 5.88570, qf2_loss: 5.88522, policy_loss: -105.05728, policy_entropy: -5.97991, alpha: 0.02486, time: 33.24698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   765 ----
[CW] collect: return: 469.11551, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 5.68337, qf2_loss: 5.64662, policy_loss: -106.30882, policy_entropy: -6.12885, alpha: 0.02496, time: 33.15273
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   766 ----
[CW] collect: return: 522.77925, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 7.06452, qf2_loss: 7.01921, policy_loss: -104.75029, policy_entropy: -6.00996, alpha: 0.02495, time: 33.33069
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   767 ----
[CW] collect: return: 210.46323, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 6.70806, qf2_loss: 6.60250, policy_loss: -106.65564, policy_entropy: -6.11422, alpha: 0.02502, time: 33.36650
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   768 ----
[CW] collect: return: 298.94465, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 6.59066, qf2_loss: 6.49532, policy_loss: -105.51596, policy_entropy: -5.91983, alpha: 0.02501, time: 34.31430
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   769 ----
[CW] collect: return: 446.17766, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 5.65176, qf2_loss: 5.64245, policy_loss: -107.42361, policy_entropy: -6.05520, alpha: 0.02502, time: 33.80838
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   770 ----
[CW] collect: return: 281.56668, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 5.95231, qf2_loss: 5.88794, policy_loss: -105.04989, policy_entropy: -5.94514, alpha: 0.02502, time: 33.70934
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   771 ----
[CW] collect: return: 294.19797, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 6.36209, qf2_loss: 6.34163, policy_loss: -107.42172, policy_entropy: -6.04870, alpha: 0.02505, time: 33.65503
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   772 ----
[CW] collect: return: 435.20790, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 6.26865, qf2_loss: 6.27223, policy_loss: -106.95695, policy_entropy: -5.93371, alpha: 0.02499, time: 33.75627
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   773 ----
[CW] collect: return: 524.09225, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 5.82894, qf2_loss: 5.78041, policy_loss: -107.95238, policy_entropy: -6.13319, alpha: 0.02504, time: 33.64697
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   774 ----
[CW] collect: return: 140.76209, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 5.36183, qf2_loss: 5.39710, policy_loss: -106.77197, policy_entropy: -5.95892, alpha: 0.02508, time: 33.51704
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   775 ----
[CW] collect: return: 384.37802, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 7.67116, qf2_loss: 7.60045, policy_loss: -106.55169, policy_entropy: -6.03416, alpha: 0.02501, time: 33.61538
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   776 ----
[CW] collect: return: 456.98502, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 7.23781, qf2_loss: 7.22501, policy_loss: -107.28372, policy_entropy: -6.07239, alpha: 0.02513, time: 33.40790
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   777 ----
[CW] collect: return: 475.33802, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 6.02395, qf2_loss: 6.01238, policy_loss: -106.31713, policy_entropy: -5.92506, alpha: 0.02510, time: 33.34830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   778 ----
[CW] collect: return: 385.00297, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 6.65637, qf2_loss: 6.48444, policy_loss: -107.89921, policy_entropy: -6.07975, alpha: 0.02513, time: 33.52123
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   779 ----
[CW] collect: return: 485.33061, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 5.88485, qf2_loss: 5.91290, policy_loss: -108.21140, policy_entropy: -6.10983, alpha: 0.02518, time: 33.59709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   780 ----
[CW] collect: return: 373.37159, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 5.74658, qf2_loss: 5.76984, policy_loss: -105.93180, policy_entropy: -6.00135, alpha: 0.02527, time: 33.47684
[CW] eval: return: 277.06652, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   781 ----
[CW] collect: return: 481.18801, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 5.36684, qf2_loss: 5.40943, policy_loss: -108.65833, policy_entropy: -6.11327, alpha: 0.02533, time: 33.55332
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   782 ----
[CW] collect: return: 216.57572, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 6.97842, qf2_loss: 6.96438, policy_loss: -106.20355, policy_entropy: -5.96825, alpha: 0.02530, time: 33.37025
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   783 ----
[CW] collect: return: 116.21964, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 6.42458, qf2_loss: 6.41270, policy_loss: -105.41200, policy_entropy: -5.89307, alpha: 0.02529, time: 33.46374
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   784 ----
[CW] collect: return: 459.00897, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 7.04161, qf2_loss: 7.04423, policy_loss: -107.21026, policy_entropy: -6.02953, alpha: 0.02522, time: 33.54332
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   785 ----
[CW] collect: return: 425.04893, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 6.30972, qf2_loss: 6.22082, policy_loss: -106.10795, policy_entropy: -5.93533, alpha: 0.02525, time: 33.60106
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   786 ----
[CW] collect: return: 264.98289, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 5.87291, qf2_loss: 5.89213, policy_loss: -105.77679, policy_entropy: -5.94265, alpha: 0.02516, time: 33.45376
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   787 ----
[CW] collect: return: 232.54926, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 6.53670, qf2_loss: 6.56943, policy_loss: -106.10056, policy_entropy: -5.99113, alpha: 0.02513, time: 33.42205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   788 ----
[CW] collect: return: 415.88978, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 7.60469, qf2_loss: 7.59294, policy_loss: -108.14894, policy_entropy: -6.14793, alpha: 0.02519, time: 33.52100
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   789 ----
[CW] collect: return: 98.16251, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 10.69815, qf2_loss: 10.63711, policy_loss: -107.08916, policy_entropy: -6.15901, alpha: 0.02536, time: 33.24344
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   790 ----
[CW] collect: return: 123.13288, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 6.58996, qf2_loss: 6.59586, policy_loss: -107.35081, policy_entropy: -6.20868, alpha: 0.02550, time: 33.50588
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   791 ----
[CW] collect: return: 441.92205, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 6.50624, qf2_loss: 6.54046, policy_loss: -106.30897, policy_entropy: -5.92120, alpha: 0.02561, time: 34.18061
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   792 ----
[CW] collect: return: 408.56949, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 8.59806, qf2_loss: 8.42715, policy_loss: -107.27523, policy_entropy: -6.10276, alpha: 0.02553, time: 33.48066
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   793 ----
[CW] collect: return: 421.28525, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 8.53995, qf2_loss: 8.52275, policy_loss: -108.08349, policy_entropy: -6.13111, alpha: 0.02564, time: 33.27686
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   794 ----
[CW] collect: return: 464.79743, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 6.54533, qf2_loss: 6.52455, policy_loss: -107.08247, policy_entropy: -5.92513, alpha: 0.02575, time: 33.44009
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   795 ----
[CW] collect: return: 478.50054, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 6.23248, qf2_loss: 6.22223, policy_loss: -105.00252, policy_entropy: -5.71082, alpha: 0.02557, time: 33.25833
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   796 ----
[CW] collect: return: 131.11286, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 6.56117, qf2_loss: 6.46952, policy_loss: -109.40173, policy_entropy: -5.96492, alpha: 0.02543, time: 33.23093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   797 ----
[CW] collect: return: 425.75458, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 6.06393, qf2_loss: 6.08235, policy_loss: -109.50032, policy_entropy: -6.06418, alpha: 0.02542, time: 33.31807
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   798 ----
[CW] collect: return: 275.32534, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 5.85488, qf2_loss: 5.79780, policy_loss: -108.19047, policy_entropy: -5.95559, alpha: 0.02538, time: 33.36837
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   799 ----
[CW] collect: return: 473.35635, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 6.23321, qf2_loss: 6.18862, policy_loss: -109.22047, policy_entropy: -6.03010, alpha: 0.02542, time: 33.34230
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   800 ----
[CW] collect: return: 187.81188, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 6.66968, qf2_loss: 6.64657, policy_loss: -106.95258, policy_entropy: -5.91148, alpha: 0.02538, time: 33.28769
[CW] eval: return: 335.21097, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   801 ----
[CW] collect: return: 507.55157, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 5.80857, qf2_loss: 5.74571, policy_loss: -108.12957, policy_entropy: -5.87146, alpha: 0.02532, time: 33.40099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   802 ----
[CW] collect: return: 38.20199, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 5.64743, qf2_loss: 5.64335, policy_loss: -107.95281, policy_entropy: -6.00533, alpha: 0.02523, time: 33.26722
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   803 ----
[CW] collect: return: 519.43502, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 6.65482, qf2_loss: 6.55543, policy_loss: -106.48044, policy_entropy: -5.97966, alpha: 0.02526, time: 33.19617
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   804 ----
[CW] collect: return: 345.92904, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 6.14252, qf2_loss: 6.17584, policy_loss: -107.78092, policy_entropy: -6.06373, alpha: 0.02524, time: 33.45844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   805 ----
[CW] collect: return: 101.99950, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 6.49170, qf2_loss: 6.43414, policy_loss: -108.26323, policy_entropy: -6.11243, alpha: 0.02533, time: 33.87866
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   806 ----
[CW] collect: return: 215.99919, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 6.54381, qf2_loss: 6.57993, policy_loss: -107.88078, policy_entropy: -5.95813, alpha: 0.02537, time: 33.86006
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   807 ----
[CW] collect: return: 487.84768, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 6.38402, qf2_loss: 6.33925, policy_loss: -108.08624, policy_entropy: -6.05648, alpha: 0.02538, time: 33.84039
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   808 ----
[CW] collect: return: 134.89463, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 6.56189, qf2_loss: 6.54509, policy_loss: -109.20407, policy_entropy: -6.22895, alpha: 0.02548, time: 35.24840
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   809 ----
[CW] collect: return: 169.74555, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 14.53798, qf2_loss: 14.33070, policy_loss: -107.40754, policy_entropy: -6.18645, alpha: 0.02563, time: 33.88882
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   810 ----
[CW] collect: return: 442.64714, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 9.17788, qf2_loss: 9.02884, policy_loss: -109.23461, policy_entropy: -6.16353, alpha: 0.02585, time: 33.76859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   811 ----
[CW] collect: return: 475.14425, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 6.57103, qf2_loss: 6.58594, policy_loss: -109.17958, policy_entropy: -6.15775, alpha: 0.02600, time: 33.94179
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   812 ----
[CW] collect: return: 261.98161, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 6.08702, qf2_loss: 6.09740, policy_loss: -106.57113, policy_entropy: -5.88147, alpha: 0.02602, time: 33.76521
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   813 ----
[CW] collect: return: 460.36541, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 6.14253, qf2_loss: 6.07277, policy_loss: -108.25865, policy_entropy: -5.95155, alpha: 0.02594, time: 34.03969
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   814 ----
[CW] collect: return: 474.40760, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 6.06310, qf2_loss: 6.01452, policy_loss: -108.55308, policy_entropy: -5.95050, alpha: 0.02593, time: 33.69461
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   815 ----
[CW] collect: return: 504.56697, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 6.51641, qf2_loss: 6.49211, policy_loss: -108.21970, policy_entropy: -6.00906, alpha: 0.02586, time: 33.90248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   816 ----
[CW] collect: return: 441.04923, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 6.67884, qf2_loss: 6.66832, policy_loss: -109.74462, policy_entropy: -6.16682, alpha: 0.02595, time: 33.89159
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   817 ----
[CW] collect: return: 452.10743, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 6.47039, qf2_loss: 6.35235, policy_loss: -109.72471, policy_entropy: -6.24688, alpha: 0.02612, time: 34.01565
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   818 ----
[CW] collect: return: 315.80942, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 5.73713, qf2_loss: 5.71405, policy_loss: -109.89945, policy_entropy: -6.17710, alpha: 0.02631, time: 33.89279
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   819 ----
[CW] collect: return: 263.90306, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 6.40886, qf2_loss: 6.43358, policy_loss: -109.53905, policy_entropy: -6.07514, alpha: 0.02639, time: 33.85053
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   820 ----
[CW] collect: return: 201.31438, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 6.55829, qf2_loss: 6.51393, policy_loss: -108.50712, policy_entropy: -5.92726, alpha: 0.02639, time: 33.87895
[CW] eval: return: 280.50867, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   821 ----
[CW] collect: return: 180.30943, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 6.37572, qf2_loss: 6.36597, policy_loss: -110.22770, policy_entropy: -6.12984, alpha: 0.02645, time: 33.90070
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   822 ----
[CW] collect: return: 53.52920, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 6.14057, qf2_loss: 6.06380, policy_loss: -109.22679, policy_entropy: -6.04794, alpha: 0.02654, time: 33.73198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   823 ----
[CW] collect: return: 428.82146, steps: 1000.00000, total_steps: 829000.00000
[CW] train: qf1_loss: 5.91172, qf2_loss: 5.83066, policy_loss: -109.02234, policy_entropy: -5.93430, alpha: 0.02652, time: 33.83129
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   824 ----
[CW] collect: return: 470.98791, steps: 1000.00000, total_steps: 830000.00000
[CW] train: qf1_loss: 5.84415, qf2_loss: 5.84820, policy_loss: -108.79190, policy_entropy: -5.99214, alpha: 0.02646, time: 33.76020
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   825 ----
[CW] collect: return: 140.74237, steps: 1000.00000, total_steps: 831000.00000
[CW] train: qf1_loss: 6.44799, qf2_loss: 6.47358, policy_loss: -109.48279, policy_entropy: -6.01379, alpha: 0.02650, time: 33.83997
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   826 ----
[CW] collect: return: 312.52577, steps: 1000.00000, total_steps: 832000.00000
[CW] train: qf1_loss: 6.17316, qf2_loss: 6.05458, policy_loss: -108.19629, policy_entropy: -5.93617, alpha: 0.02650, time: 33.95501
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   827 ----
[CW] collect: return: 491.98556, steps: 1000.00000, total_steps: 833000.00000
[CW] train: qf1_loss: 6.14619, qf2_loss: 6.08429, policy_loss: -108.79488, policy_entropy: -5.99966, alpha: 0.02645, time: 33.63996
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   828 ----
[CW] collect: return: 315.36842, steps: 1000.00000, total_steps: 834000.00000
[CW] train: qf1_loss: 7.30802, qf2_loss: 7.25572, policy_loss: -108.93677, policy_entropy: -5.90405, alpha: 0.02639, time: 34.58224
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   829 ----
[CW] collect: return: 223.86351, steps: 1000.00000, total_steps: 835000.00000
[CW] train: qf1_loss: 6.18478, qf2_loss: 6.19955, policy_loss: -110.57748, policy_entropy: -6.13617, alpha: 0.02638, time: 33.79564
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   830 ----
[CW] collect: return: 291.26277, steps: 1000.00000, total_steps: 836000.00000
[CW] train: qf1_loss: 6.71707, qf2_loss: 6.52116, policy_loss: -108.48483, policy_entropy: -5.94957, alpha: 0.02643, time: 34.04855
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   831 ----
[CW] collect: return: 348.83612, steps: 1000.00000, total_steps: 837000.00000
[CW] train: qf1_loss: 8.17922, qf2_loss: 8.19256, policy_loss: -110.36782, policy_entropy: -6.17693, alpha: 0.02652, time: 33.79115
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   832 ----
[CW] collect: return: 93.54398, steps: 1000.00000, total_steps: 838000.00000
[CW] train: qf1_loss: 7.02484, qf2_loss: 6.97810, policy_loss: -107.12232, policy_entropy: -5.81024, alpha: 0.02650, time: 33.99830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   833 ----
[CW] collect: return: 298.51264, steps: 1000.00000, total_steps: 839000.00000
[CW] train: qf1_loss: 6.18007, qf2_loss: 6.13160, policy_loss: -111.13999, policy_entropy: -6.16789, alpha: 0.02650, time: 33.59860
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   834 ----
[CW] collect: return: 200.54510, steps: 1000.00000, total_steps: 840000.00000
[CW] train: qf1_loss: 6.91253, qf2_loss: 6.78308, policy_loss: -108.51450, policy_entropy: -5.95656, alpha: 0.02656, time: 33.92930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   835 ----
[CW] collect: return: 192.16375, steps: 1000.00000, total_steps: 841000.00000
[CW] train: qf1_loss: 6.49392, qf2_loss: 6.51458, policy_loss: -110.70535, policy_entropy: -6.06664, alpha: 0.02657, time: 33.75899
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   836 ----
[CW] collect: return: 365.17238, steps: 1000.00000, total_steps: 842000.00000
[CW] train: qf1_loss: 7.77697, qf2_loss: 7.74460, policy_loss: -108.72010, policy_entropy: -6.00296, alpha: 0.02663, time: 33.94890
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   837 ----
[CW] collect: return: 483.36766, steps: 1000.00000, total_steps: 843000.00000
[CW] train: qf1_loss: 6.71105, qf2_loss: 6.61906, policy_loss: -109.16783, policy_entropy: -6.08254, alpha: 0.02667, time: 33.75589
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   838 ----
[CW] collect: return: 450.77385, steps: 1000.00000, total_steps: 844000.00000
[CW] train: qf1_loss: 6.30761, qf2_loss: 6.22544, policy_loss: -109.97710, policy_entropy: -6.02570, alpha: 0.02666, time: 33.98668
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   839 ----
[CW] collect: return: 447.06817, steps: 1000.00000, total_steps: 845000.00000
[CW] train: qf1_loss: 6.81717, qf2_loss: 6.68545, policy_loss: -110.12210, policy_entropy: -6.08649, alpha: 0.02670, time: 35.61703
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   840 ----
[CW] collect: return: 464.47569, steps: 1000.00000, total_steps: 846000.00000
[CW] train: qf1_loss: 6.04450, qf2_loss: 5.97264, policy_loss: -109.86181, policy_entropy: -6.09048, alpha: 0.02678, time: 33.90794
[CW] eval: return: 231.72495, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   841 ----
[CW] collect: return: 500.26032, steps: 1000.00000, total_steps: 847000.00000
[CW] train: qf1_loss: 7.66728, qf2_loss: 7.44286, policy_loss: -110.85224, policy_entropy: -6.23776, alpha: 0.02690, time: 33.83366
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   842 ----
[CW] collect: return: 494.26408, steps: 1000.00000, total_steps: 848000.00000
[CW] train: qf1_loss: 6.58172, qf2_loss: 6.54111, policy_loss: -109.49775, policy_entropy: -6.10660, alpha: 0.02714, time: 34.05051
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   843 ----
[CW] collect: return: 336.04214, steps: 1000.00000, total_steps: 849000.00000
[CW] train: qf1_loss: 5.85854, qf2_loss: 5.87254, policy_loss: -110.04245, policy_entropy: -5.97859, alpha: 0.02720, time: 33.72255
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   844 ----
[CW] collect: return: 463.98174, steps: 1000.00000, total_steps: 850000.00000
[CW] train: qf1_loss: 6.54473, qf2_loss: 6.51470, policy_loss: -110.87406, policy_entropy: -5.97491, alpha: 0.02717, time: 34.01856
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   845 ----
[CW] collect: return: 188.46957, steps: 1000.00000, total_steps: 851000.00000
[CW] train: qf1_loss: 6.41335, qf2_loss: 6.39679, policy_loss: -110.50588, policy_entropy: -6.07277, alpha: 0.02720, time: 33.69211
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   846 ----
[CW] collect: return: 473.13504, steps: 1000.00000, total_steps: 852000.00000
[CW] train: qf1_loss: 6.40703, qf2_loss: 6.30739, policy_loss: -111.65697, policy_entropy: -6.21045, alpha: 0.02729, time: 33.88715
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   847 ----
[CW] collect: return: 135.70987, steps: 1000.00000, total_steps: 853000.00000
[CW] train: qf1_loss: 5.96436, qf2_loss: 5.91995, policy_loss: -110.54553, policy_entropy: -5.94512, alpha: 0.02740, time: 33.77583
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   848 ----
[CW] collect: return: 52.80766, steps: 1000.00000, total_steps: 854000.00000
[CW] train: qf1_loss: 5.94073, qf2_loss: 5.92347, policy_loss: -109.02952, policy_entropy: -5.90065, alpha: 0.02730, time: 34.00666
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   849 ----
[CW] collect: return: 257.92774, steps: 1000.00000, total_steps: 855000.00000
[CW] train: qf1_loss: 5.81000, qf2_loss: 5.80065, policy_loss: -111.25824, policy_entropy: -6.12874, alpha: 0.02735, time: 33.86755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   850 ----
[CW] collect: return: 61.55503, steps: 1000.00000, total_steps: 856000.00000
[CW] train: qf1_loss: 6.61104, qf2_loss: 6.50090, policy_loss: -110.10019, policy_entropy: -5.93488, alpha: 0.02735, time: 33.87392
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   851 ----
[CW] collect: return: 496.53101, steps: 1000.00000, total_steps: 857000.00000
[CW] train: qf1_loss: 5.93705, qf2_loss: 5.86179, policy_loss: -107.74622, policy_entropy: -5.69414, alpha: 0.02717, time: 33.93080
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   852 ----
[CW] collect: return: 491.03944, steps: 1000.00000, total_steps: 858000.00000
[CW] train: qf1_loss: 6.54849, qf2_loss: 6.45919, policy_loss: -108.87822, policy_entropy: -5.83104, alpha: 0.02698, time: 33.67032
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   853 ----
[CW] collect: return: 341.73706, steps: 1000.00000, total_steps: 859000.00000
[CW] train: qf1_loss: 6.63745, qf2_loss: 6.56443, policy_loss: -110.03930, policy_entropy: -6.10826, alpha: 0.02689, time: 34.25741
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   854 ----
[CW] collect: return: 253.74659, steps: 1000.00000, total_steps: 860000.00000
[CW] train: qf1_loss: 8.22566, qf2_loss: 8.24884, policy_loss: -108.04240, policy_entropy: -5.86804, alpha: 0.02693, time: 33.78270
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   855 ----
[CW] collect: return: 432.87625, steps: 1000.00000, total_steps: 861000.00000
[CW] train: qf1_loss: 8.37731, qf2_loss: 8.32059, policy_loss: -111.38961, policy_entropy: -6.18265, alpha: 0.02695, time: 33.83915
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   856 ----
[CW] collect: return: 208.30224, steps: 1000.00000, total_steps: 862000.00000
[CW] train: qf1_loss: 8.94791, qf2_loss: 8.93056, policy_loss: -109.17933, policy_entropy: -5.88182, alpha: 0.02706, time: 33.79431
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   857 ----
[CW] collect: return: 134.35795, steps: 1000.00000, total_steps: 863000.00000
[CW] train: qf1_loss: 10.71715, qf2_loss: 10.63185, policy_loss: -109.03586, policy_entropy: -5.89053, alpha: 0.02687, time: 33.90803
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   858 ----
[CW] collect: return: 78.78301, steps: 1000.00000, total_steps: 864000.00000
[CW] train: qf1_loss: 9.37234, qf2_loss: 9.18340, policy_loss: -108.06867, policy_entropy: -5.92538, alpha: 0.02683, time: 33.51592
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   859 ----
[CW] collect: return: 226.68874, steps: 1000.00000, total_steps: 865000.00000
[CW] train: qf1_loss: 7.89204, qf2_loss: 7.85158, policy_loss: -109.14203, policy_entropy: -5.98090, alpha: 0.02675, time: 33.91209
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   860 ----
[CW] collect: return: 466.66270, steps: 1000.00000, total_steps: 866000.00000
[CW] train: qf1_loss: 6.86299, qf2_loss: 6.85068, policy_loss: -109.83278, policy_entropy: -6.01861, alpha: 0.02675, time: 33.63163
[CW] eval: return: 266.59212, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   861 ----
[CW] collect: return: 213.27125, steps: 1000.00000, total_steps: 867000.00000
[CW] train: qf1_loss: 8.42792, qf2_loss: 8.37343, policy_loss: -110.49928, policy_entropy: -5.95915, alpha: 0.02675, time: 33.80263
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   862 ----
[CW] collect: return: 60.22295, steps: 1000.00000, total_steps: 868000.00000
[CW] train: qf1_loss: 7.13207, qf2_loss: 7.05100, policy_loss: -110.60049, policy_entropy: -6.18737, alpha: 0.02679, time: 33.70624
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   863 ----
[CW] collect: return: 122.08018, steps: 1000.00000, total_steps: 869000.00000
[CW] train: qf1_loss: 7.01810, qf2_loss: 6.95877, policy_loss: -109.09340, policy_entropy: -6.08663, alpha: 0.02695, time: 33.77985
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   864 ----
[CW] collect: return: 443.22501, steps: 1000.00000, total_steps: 870000.00000
[CW] train: qf1_loss: 6.98389, qf2_loss: 6.89197, policy_loss: -110.44271, policy_entropy: -6.01975, alpha: 0.02697, time: 33.48850
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   865 ----
[CW] collect: return: 130.40186, steps: 1000.00000, total_steps: 871000.00000
[CW] train: qf1_loss: 7.29174, qf2_loss: 7.17928, policy_loss: -109.89815, policy_entropy: -6.03955, alpha: 0.02700, time: 33.86922
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   866 ----
[CW] collect: return: 301.02434, steps: 1000.00000, total_steps: 872000.00000
[CW] train: qf1_loss: 9.02995, qf2_loss: 9.00056, policy_loss: -110.59474, policy_entropy: -6.14994, alpha: 0.02708, time: 33.66528
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   867 ----
[CW] collect: return: 281.88284, steps: 1000.00000, total_steps: 873000.00000
[CW] train: qf1_loss: 14.30393, qf2_loss: 14.07806, policy_loss: -109.92105, policy_entropy: -6.31156, alpha: 0.02727, time: 33.86544
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   868 ----
[CW] collect: return: 432.40614, steps: 1000.00000, total_steps: 874000.00000
[CW] train: qf1_loss: 9.40567, qf2_loss: 9.36006, policy_loss: -109.13489, policy_entropy: -6.03381, alpha: 0.02748, time: 33.77211
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   869 ----
[CW] collect: return: 307.35758, steps: 1000.00000, total_steps: 875000.00000
[CW] train: qf1_loss: 7.13863, qf2_loss: 7.02576, policy_loss: -108.57280, policy_entropy: -5.88043, alpha: 0.02742, time: 33.87601
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   870 ----
[CW] collect: return: 475.49489, steps: 1000.00000, total_steps: 876000.00000
[CW] train: qf1_loss: 7.04099, qf2_loss: 7.10959, policy_loss: -109.78554, policy_entropy: -6.00894, alpha: 0.02736, time: 33.60843
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   871 ----
[CW] collect: return: 484.75268, steps: 1000.00000, total_steps: 877000.00000
[CW] train: qf1_loss: 7.05270, qf2_loss: 7.04623, policy_loss: -109.19661, policy_entropy: -5.91543, alpha: 0.02732, time: 33.87353
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   872 ----
[CW] collect: return: 346.45364, steps: 1000.00000, total_steps: 878000.00000
[CW] train: qf1_loss: 6.38760, qf2_loss: 6.38767, policy_loss: -109.59467, policy_entropy: -5.93583, alpha: 0.02730, time: 33.77074
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   873 ----
[CW] collect: return: 157.78959, steps: 1000.00000, total_steps: 879000.00000
[CW] train: qf1_loss: 7.40403, qf2_loss: 7.34927, policy_loss: -107.88419, policy_entropy: -5.72784, alpha: 0.02714, time: 33.75199
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   874 ----
[CW] collect: return: 175.46441, steps: 1000.00000, total_steps: 880000.00000
[CW] train: qf1_loss: 7.79956, qf2_loss: 7.80760, policy_loss: -108.64588, policy_entropy: -5.91239, alpha: 0.02691, time: 33.69339
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   875 ----
[CW] collect: return: 422.09755, steps: 1000.00000, total_steps: 881000.00000
[CW] train: qf1_loss: 8.11874, qf2_loss: 8.06935, policy_loss: -108.19469, policy_entropy: -5.82360, alpha: 0.02687, time: 33.69104
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   876 ----
[CW] collect: return: 338.41313, steps: 1000.00000, total_steps: 882000.00000
[CW] train: qf1_loss: 7.73763, qf2_loss: 7.77075, policy_loss: -109.12810, policy_entropy: -5.89840, alpha: 0.02674, time: 33.71997
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   877 ----
[CW] collect: return: 473.70844, steps: 1000.00000, total_steps: 883000.00000
[CW] train: qf1_loss: 7.24683, qf2_loss: 7.22612, policy_loss: -110.99132, policy_entropy: -6.05622, alpha: 0.02664, time: 33.53092
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   878 ----
[CW] collect: return: 310.87267, steps: 1000.00000, total_steps: 884000.00000
[CW] train: qf1_loss: 7.63062, qf2_loss: 7.59916, policy_loss: -109.94722, policy_entropy: -5.91485, alpha: 0.02661, time: 36.47544
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   879 ----
[CW] collect: return: 338.95279, steps: 1000.00000, total_steps: 885000.00000
[CW] train: qf1_loss: 9.19406, qf2_loss: 8.99884, policy_loss: -109.37509, policy_entropy: -5.99497, alpha: 0.02664, time: 33.80364
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   880 ----
[CW] collect: return: 446.20255, steps: 1000.00000, total_steps: 886000.00000
[CW] train: qf1_loss: 8.45713, qf2_loss: 8.50069, policy_loss: -109.07248, policy_entropy: -5.87181, alpha: 0.02658, time: 33.88509
[CW] eval: return: 235.80795, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   881 ----
[CW] collect: return: 428.52484, steps: 1000.00000, total_steps: 887000.00000
[CW] train: qf1_loss: 11.85265, qf2_loss: 11.67054, policy_loss: -110.65316, policy_entropy: -6.14668, alpha: 0.02655, time: 33.88556
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   882 ----
[CW] collect: return: 180.68106, steps: 1000.00000, total_steps: 888000.00000
[CW] train: qf1_loss: 9.15639, qf2_loss: 9.06618, policy_loss: -109.57183, policy_entropy: -6.06139, alpha: 0.02666, time: 33.78366
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   883 ----
[CW] collect: return: 226.00265, steps: 1000.00000, total_steps: 889000.00000
[CW] train: qf1_loss: 7.44758, qf2_loss: 7.43013, policy_loss: -108.76674, policy_entropy: -5.79713, alpha: 0.02664, time: 33.68607
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   884 ----
[CW] collect: return: 188.58261, steps: 1000.00000, total_steps: 890000.00000
[CW] train: qf1_loss: 7.65113, qf2_loss: 7.51207, policy_loss: -108.79629, policy_entropy: -5.96576, alpha: 0.02650, time: 33.90077
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   885 ----
[CW] collect: return: 319.09877, steps: 1000.00000, total_steps: 891000.00000
[CW] train: qf1_loss: 8.27674, qf2_loss: 8.19344, policy_loss: -109.47675, policy_entropy: -5.98021, alpha: 0.02649, time: 33.77274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   886 ----
[CW] collect: return: 467.98851, steps: 1000.00000, total_steps: 892000.00000
[CW] train: qf1_loss: 7.64354, qf2_loss: 7.61288, policy_loss: -111.50485, policy_entropy: -6.14314, alpha: 0.02652, time: 33.95041
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   887 ----
[CW] collect: return: 489.20604, steps: 1000.00000, total_steps: 893000.00000
[CW] train: qf1_loss: 7.91885, qf2_loss: 7.90774, policy_loss: -111.12817, policy_entropy: -6.09292, alpha: 0.02663, time: 33.77755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   888 ----
[CW] collect: return: 464.88690, steps: 1000.00000, total_steps: 894000.00000
[CW] train: qf1_loss: 6.99502, qf2_loss: 6.89338, policy_loss: -110.47776, policy_entropy: -5.96892, alpha: 0.02662, time: 33.90441
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   889 ----
[CW] collect: return: 481.33020, steps: 1000.00000, total_steps: 895000.00000
[CW] train: qf1_loss: 6.92234, qf2_loss: 6.85264, policy_loss: -110.72145, policy_entropy: -6.11362, alpha: 0.02671, time: 33.60635
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   890 ----
[CW] collect: return: 190.21668, steps: 1000.00000, total_steps: 896000.00000
[CW] train: qf1_loss: 8.24270, qf2_loss: 8.17047, policy_loss: -111.15222, policy_entropy: -6.10017, alpha: 0.02680, time: 33.90650
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   891 ----
[CW] collect: return: 496.58866, steps: 1000.00000, total_steps: 897000.00000
[CW] train: qf1_loss: 7.61793, qf2_loss: 7.59206, policy_loss: -109.88220, policy_entropy: -5.94649, alpha: 0.02678, time: 33.64669
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   892 ----
[CW] collect: return: 115.78642, steps: 1000.00000, total_steps: 898000.00000
[CW] train: qf1_loss: 9.20684, qf2_loss: 9.04599, policy_loss: -110.50535, policy_entropy: -5.99657, alpha: 0.02679, time: 33.89929
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   893 ----
[CW] collect: return: 372.19696, steps: 1000.00000, total_steps: 899000.00000
[CW] train: qf1_loss: 7.46264, qf2_loss: 7.45014, policy_loss: -110.64719, policy_entropy: -6.09488, alpha: 0.02679, time: 33.75322
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   894 ----
[CW] collect: return: 42.22705, steps: 1000.00000, total_steps: 900000.00000
[CW] train: qf1_loss: 7.33583, qf2_loss: 7.31731, policy_loss: -109.28457, policy_entropy: -5.93929, alpha: 0.02685, time: 33.85490
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   895 ----
[CW] collect: return: 341.00483, steps: 1000.00000, total_steps: 901000.00000
[CW] train: qf1_loss: 7.50337, qf2_loss: 7.52582, policy_loss: -111.64757, policy_entropy: -6.02635, alpha: 0.02684, time: 33.65415
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   896 ----
[CW] collect: return: 90.97972, steps: 1000.00000, total_steps: 902000.00000
[CW] train: qf1_loss: 8.24367, qf2_loss: 8.04866, policy_loss: -109.78801, policy_entropy: -6.04295, alpha: 0.02687, time: 33.74637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   897 ----
[CW] collect: return: 153.79376, steps: 1000.00000, total_steps: 903000.00000
[CW] train: qf1_loss: 6.96609, qf2_loss: 6.83506, policy_loss: -111.42666, policy_entropy: -5.97699, alpha: 0.02690, time: 33.76110
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   898 ----
[CW] collect: return: 412.39307, steps: 1000.00000, total_steps: 904000.00000
[CW] train: qf1_loss: 7.64471, qf2_loss: 7.66529, policy_loss: -110.75026, policy_entropy: -6.11026, alpha: 0.02688, time: 33.81538
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   899 ----
[CW] collect: return: 321.45442, steps: 1000.00000, total_steps: 905000.00000
[CW] train: qf1_loss: 10.49456, qf2_loss: 10.30161, policy_loss: -109.19989, policy_entropy: -5.92909, alpha: 0.02694, time: 33.85080
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   900 ----
[CW] collect: return: 62.91594, steps: 1000.00000, total_steps: 906000.00000
[CW] train: qf1_loss: 8.15182, qf2_loss: 8.02753, policy_loss: -110.38026, policy_entropy: -5.92150, alpha: 0.02686, time: 34.23778
[CW] eval: return: 307.30308, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   901 ----
[CW] collect: return: 87.82562, steps: 1000.00000, total_steps: 907000.00000
[CW] train: qf1_loss: 7.76325, qf2_loss: 7.74128, policy_loss: -111.84159, policy_entropy: -6.17748, alpha: 0.02689, time: 33.59321
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   902 ----
[CW] collect: return: 442.92999, steps: 1000.00000, total_steps: 908000.00000
[CW] train: qf1_loss: 8.56155, qf2_loss: 8.51930, policy_loss: -109.64667, policy_entropy: -5.87796, alpha: 0.02697, time: 33.87384
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   903 ----
[CW] collect: return: 279.40679, steps: 1000.00000, total_steps: 909000.00000
[CW] train: qf1_loss: 8.31539, qf2_loss: 8.33764, policy_loss: -110.61527, policy_entropy: -5.99851, alpha: 0.02694, time: 33.77296
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   904 ----
[CW] collect: return: 274.27483, steps: 1000.00000, total_steps: 910000.00000
[CW] train: qf1_loss: 8.78412, qf2_loss: 8.76267, policy_loss: -110.44251, policy_entropy: -6.06777, alpha: 0.02693, time: 33.89336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   905 ----
[CW] collect: return: 290.18860, steps: 1000.00000, total_steps: 911000.00000
[CW] train: qf1_loss: 9.18138, qf2_loss: 9.19174, policy_loss: -110.57320, policy_entropy: -5.91333, alpha: 0.02692, time: 33.72398
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   906 ----
[CW] collect: return: 439.79276, steps: 1000.00000, total_steps: 912000.00000
[CW] train: qf1_loss: 7.28093, qf2_loss: 7.19127, policy_loss: -111.43348, policy_entropy: -6.07927, alpha: 0.02693, time: 33.70441
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   907 ----
[CW] collect: return: 226.44938, steps: 1000.00000, total_steps: 913000.00000
[CW] train: qf1_loss: 8.44796, qf2_loss: 8.42578, policy_loss: -111.07253, policy_entropy: -6.08169, alpha: 0.02699, time: 33.74362
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   908 ----
[CW] collect: return: 314.70632, steps: 1000.00000, total_steps: 914000.00000
[CW] train: qf1_loss: 9.35876, qf2_loss: 9.09492, policy_loss: -108.97967, policy_entropy: -5.99986, alpha: 0.02702, time: 33.57491
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   909 ----
[CW] collect: return: 209.04262, steps: 1000.00000, total_steps: 915000.00000
[CW] train: qf1_loss: 8.44963, qf2_loss: 8.45462, policy_loss: -110.33826, policy_entropy: -6.05879, alpha: 0.02706, time: 33.93036
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   910 ----
[CW] collect: return: 126.85183, steps: 1000.00000, total_steps: 916000.00000
[CW] train: qf1_loss: 8.17182, qf2_loss: 8.10949, policy_loss: -109.70285, policy_entropy: -5.95432, alpha: 0.02711, time: 33.66753
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   911 ----
[CW] collect: return: 293.27512, steps: 1000.00000, total_steps: 917000.00000
[CW] train: qf1_loss: 7.85894, qf2_loss: 7.76720, policy_loss: -110.98161, policy_entropy: -6.22340, alpha: 0.02708, time: 33.79248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   912 ----
[CW] collect: return: 291.30365, steps: 1000.00000, total_steps: 918000.00000
[CW] train: qf1_loss: 7.81751, qf2_loss: 7.75188, policy_loss: -111.20314, policy_entropy: -6.14859, alpha: 0.02727, time: 33.71581
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   913 ----
[CW] collect: return: 452.98256, steps: 1000.00000, total_steps: 919000.00000
[CW] train: qf1_loss: 8.84482, qf2_loss: 8.80701, policy_loss: -110.95759, policy_entropy: -6.19149, alpha: 0.02742, time: 33.90796
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   914 ----
[CW] collect: return: 448.64921, steps: 1000.00000, total_steps: 920000.00000
[CW] train: qf1_loss: 8.36225, qf2_loss: 8.34304, policy_loss: -111.02953, policy_entropy: -6.13804, alpha: 0.02767, time: 33.55382
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   915 ----
[CW] collect: return: 330.51705, steps: 1000.00000, total_steps: 921000.00000
[CW] train: qf1_loss: 8.58059, qf2_loss: 8.46840, policy_loss: -109.40953, policy_entropy: -5.99555, alpha: 0.02775, time: 33.89151
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   916 ----
[CW] collect: return: 497.84395, steps: 1000.00000, total_steps: 922000.00000
[CW] train: qf1_loss: 9.19633, qf2_loss: 9.16173, policy_loss: -111.77497, policy_entropy: -6.06450, alpha: 0.02772, time: 33.67902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   917 ----
[CW] collect: return: 419.80723, steps: 1000.00000, total_steps: 923000.00000
[CW] train: qf1_loss: 7.51494, qf2_loss: 7.54660, policy_loss: -109.63558, policy_entropy: -5.97417, alpha: 0.02776, time: 34.99318
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   918 ----
[CW] collect: return: 441.94866, steps: 1000.00000, total_steps: 924000.00000
[CW] train: qf1_loss: 7.59115, qf2_loss: 7.54011, policy_loss: -112.69787, policy_entropy: -6.21706, alpha: 0.02787, time: 33.69212
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   919 ----
[CW] collect: return: 222.09368, steps: 1000.00000, total_steps: 925000.00000
[CW] train: qf1_loss: 7.57089, qf2_loss: 7.61017, policy_loss: -110.08187, policy_entropy: -5.98540, alpha: 0.02798, time: 33.72155
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   920 ----
[CW] collect: return: 180.80835, steps: 1000.00000, total_steps: 926000.00000
[CW] train: qf1_loss: 8.51216, qf2_loss: 8.58797, policy_loss: -110.55177, policy_entropy: -6.14174, alpha: 0.02801, time: 33.58152
[CW] eval: return: 328.72215, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   921 ----
[CW] collect: return: 373.92251, steps: 1000.00000, total_steps: 927000.00000
[CW] train: qf1_loss: 8.35799, qf2_loss: 8.32743, policy_loss: -109.93987, policy_entropy: -6.03826, alpha: 0.02815, time: 33.89971
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   922 ----
[CW] collect: return: 162.37431, steps: 1000.00000, total_steps: 928000.00000
[CW] train: qf1_loss: 10.08095, qf2_loss: 10.15514, policy_loss: -109.35353, policy_entropy: -6.07709, alpha: 0.02821, time: 33.68993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   923 ----
[CW] collect: return: 397.38017, steps: 1000.00000, total_steps: 929000.00000
[CW] train: qf1_loss: 9.10962, qf2_loss: 9.03631, policy_loss: -110.00046, policy_entropy: -5.88162, alpha: 0.02819, time: 33.77435
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   924 ----
[CW] collect: return: 345.46905, steps: 1000.00000, total_steps: 930000.00000
[CW] train: qf1_loss: 16.01496, qf2_loss: 15.68899, policy_loss: -111.91865, policy_entropy: -6.13162, alpha: 0.02814, time: 33.63561
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   925 ----
[CW] collect: return: 465.29378, steps: 1000.00000, total_steps: 931000.00000
[CW] train: qf1_loss: 10.60225, qf2_loss: 10.54220, policy_loss: -109.35503, policy_entropy: -5.92006, alpha: 0.02826, time: 33.86639
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   926 ----
[CW] collect: return: 201.24590, steps: 1000.00000, total_steps: 932000.00000
[CW] train: qf1_loss: 8.06159, qf2_loss: 8.02201, policy_loss: -110.25424, policy_entropy: -6.03403, alpha: 0.02819, time: 33.77879
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   927 ----
[CW] collect: return: 277.02686, steps: 1000.00000, total_steps: 933000.00000
[CW] train: qf1_loss: 7.48254, qf2_loss: 7.43287, policy_loss: -109.76721, policy_entropy: -5.88407, alpha: 0.02814, time: 33.90242
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   928 ----
[CW] collect: return: 423.25049, steps: 1000.00000, total_steps: 934000.00000
[CW] train: qf1_loss: 7.70171, qf2_loss: 7.62580, policy_loss: -110.63704, policy_entropy: -6.02042, alpha: 0.02808, time: 33.78363
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   929 ----
[CW] collect: return: 489.07242, steps: 1000.00000, total_steps: 935000.00000
[CW] train: qf1_loss: 7.69070, qf2_loss: 7.64596, policy_loss: -110.11386, policy_entropy: -6.13047, alpha: 0.02815, time: 33.94152
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   930 ----
[CW] collect: return: 186.27293, steps: 1000.00000, total_steps: 936000.00000
[CW] train: qf1_loss: 7.99092, qf2_loss: 7.98545, policy_loss: -111.94976, policy_entropy: -6.03063, alpha: 0.02821, time: 33.77816
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   931 ----
[CW] collect: return: 499.98335, steps: 1000.00000, total_steps: 937000.00000
[CW] train: qf1_loss: 9.45997, qf2_loss: 9.40772, policy_loss: -111.29193, policy_entropy: -6.04576, alpha: 0.02826, time: 33.88359
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   932 ----
[CW] collect: return: 479.83312, steps: 1000.00000, total_steps: 938000.00000
[CW] train: qf1_loss: 8.01880, qf2_loss: 8.00667, policy_loss: -112.11612, policy_entropy: -6.05799, alpha: 0.02837, time: 33.79120
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   933 ----
[CW] collect: return: 334.91597, steps: 1000.00000, total_steps: 939000.00000
[CW] train: qf1_loss: 9.24486, qf2_loss: 9.16263, policy_loss: -110.19776, policy_entropy: -5.90185, alpha: 0.02837, time: 33.79681
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   934 ----
[CW] collect: return: 454.57451, steps: 1000.00000, total_steps: 940000.00000
[CW] train: qf1_loss: 8.80413, qf2_loss: 8.74643, policy_loss: -110.88371, policy_entropy: -5.83135, alpha: 0.02819, time: 33.70091
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   935 ----
[CW] collect: return: 407.58935, steps: 1000.00000, total_steps: 941000.00000
[CW] train: qf1_loss: 7.49889, qf2_loss: 7.46103, policy_loss: -113.83250, policy_entropy: -6.05635, alpha: 0.02806, time: 33.75917
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   936 ----
[CW] collect: return: 280.46595, steps: 1000.00000, total_steps: 942000.00000
[CW] train: qf1_loss: 9.35986, qf2_loss: 9.45146, policy_loss: -111.14262, policy_entropy: -5.84198, alpha: 0.02809, time: 33.76106
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   937 ----
[CW] collect: return: 521.90810, steps: 1000.00000, total_steps: 943000.00000
[CW] train: qf1_loss: 55.12307, qf2_loss: 52.89361, policy_loss: -110.75975, policy_entropy: -6.40688, alpha: 0.02813, time: 34.25374
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   938 ----
[CW] collect: return: 454.93692, steps: 1000.00000, total_steps: 944000.00000
[CW] train: qf1_loss: 9.75857, qf2_loss: 9.70229, policy_loss: -110.13216, policy_entropy: -6.48515, alpha: 0.02859, time: 33.79836
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   939 ----
[CW] collect: return: 439.26817, steps: 1000.00000, total_steps: 945000.00000
[CW] train: qf1_loss: 7.50251, qf2_loss: 7.52489, policy_loss: -110.72346, policy_entropy: -6.27167, alpha: 0.02909, time: 33.52192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   940 ----
[CW] collect: return: 55.88446, steps: 1000.00000, total_steps: 946000.00000
[CW] train: qf1_loss: 7.34023, qf2_loss: 7.27951, policy_loss: -112.02849, policy_entropy: -6.24596, alpha: 0.02935, time: 33.85536
[CW] eval: return: 348.66671, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   941 ----
[CW] collect: return: 460.86539, steps: 1000.00000, total_steps: 947000.00000
[CW] train: qf1_loss: 7.16098, qf2_loss: 7.07029, policy_loss: -111.75833, policy_entropy: -6.10804, alpha: 0.02957, time: 33.96198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   942 ----
[CW] collect: return: 294.16071, steps: 1000.00000, total_steps: 948000.00000
[CW] train: qf1_loss: 6.89363, qf2_loss: 6.82855, policy_loss: -111.55311, policy_entropy: -5.99704, alpha: 0.02967, time: 33.67926
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   943 ----
[CW] collect: return: 67.79932, steps: 1000.00000, total_steps: 949000.00000
[CW] train: qf1_loss: 6.98307, qf2_loss: 6.95364, policy_loss: -110.45456, policy_entropy: -5.96502, alpha: 0.02963, time: 33.99181
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   944 ----
[CW] collect: return: 496.92228, steps: 1000.00000, total_steps: 950000.00000
[CW] train: qf1_loss: 6.52712, qf2_loss: 6.52819, policy_loss: -112.63973, policy_entropy: -5.97892, alpha: 0.02956, time: 33.72533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   945 ----
[CW] collect: return: 409.17206, steps: 1000.00000, total_steps: 951000.00000
[CW] train: qf1_loss: 9.72968, qf2_loss: 9.58588, policy_loss: -111.57771, policy_entropy: -5.97178, alpha: 0.02951, time: 33.64769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   946 ----
[CW] collect: return: 430.36206, steps: 1000.00000, total_steps: 952000.00000
[CW] train: qf1_loss: 9.44883, qf2_loss: 9.47713, policy_loss: -110.30995, policy_entropy: -5.87875, alpha: 0.02945, time: 33.69643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   947 ----
[CW] collect: return: 527.02687, steps: 1000.00000, total_steps: 953000.00000
[CW] train: qf1_loss: 7.46395, qf2_loss: 7.41626, policy_loss: -110.95422, policy_entropy: -5.92257, alpha: 0.02933, time: 36.22262
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   948 ----
[CW] collect: return: 496.03205, steps: 1000.00000, total_steps: 954000.00000
[CW] train: qf1_loss: 7.10386, qf2_loss: 7.02403, policy_loss: -113.22280, policy_entropy: -6.09139, alpha: 0.02932, time: 33.82917
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   949 ----
[CW] collect: return: 116.57107, steps: 1000.00000, total_steps: 955000.00000
[CW] train: qf1_loss: 7.40979, qf2_loss: 7.36548, policy_loss: -111.43061, policy_entropy: -5.94268, alpha: 0.02940, time: 33.91333
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   950 ----
[CW] collect: return: 211.89964, steps: 1000.00000, total_steps: 956000.00000
[CW] train: qf1_loss: 6.97839, qf2_loss: 6.96182, policy_loss: -112.78805, policy_entropy: -5.99472, alpha: 0.02934, time: 33.79506
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   951 ----
[CW] collect: return: 454.67948, steps: 1000.00000, total_steps: 957000.00000
[CW] train: qf1_loss: 6.97477, qf2_loss: 6.89382, policy_loss: -111.30783, policy_entropy: -5.88672, alpha: 0.02926, time: 33.61253
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   952 ----
[CW] collect: return: 393.08825, steps: 1000.00000, total_steps: 958000.00000
[CW] train: qf1_loss: 7.39416, qf2_loss: 7.34435, policy_loss: -112.73360, policy_entropy: -5.96086, alpha: 0.02917, time: 33.85111
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   953 ----
[CW] collect: return: 205.88723, steps: 1000.00000, total_steps: 959000.00000
[CW] train: qf1_loss: 8.22796, qf2_loss: 8.17742, policy_loss: -109.38134, policy_entropy: -5.79935, alpha: 0.02907, time: 33.72827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   954 ----
[CW] collect: return: 221.46947, steps: 1000.00000, total_steps: 960000.00000
[CW] train: qf1_loss: 7.68098, qf2_loss: 7.67195, policy_loss: -110.65089, policy_entropy: -6.05308, alpha: 0.02898, time: 33.80141
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   955 ----
[CW] collect: return: 153.07724, steps: 1000.00000, total_steps: 961000.00000
[CW] train: qf1_loss: 7.66473, qf2_loss: 7.58962, policy_loss: -110.92988, policy_entropy: -5.90272, alpha: 0.02896, time: 33.69040
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   956 ----
[CW] collect: return: 453.87960, steps: 1000.00000, total_steps: 962000.00000
[CW] train: qf1_loss: 7.88630, qf2_loss: 7.87296, policy_loss: -111.99081, policy_entropy: -5.98839, alpha: 0.02887, time: 33.91727
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   957 ----
[CW] collect: return: 218.84161, steps: 1000.00000, total_steps: 963000.00000
[CW] train: qf1_loss: 7.62169, qf2_loss: 7.61507, policy_loss: -112.88998, policy_entropy: -6.08991, alpha: 0.02893, time: 33.79248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   958 ----
[CW] collect: return: 462.21390, steps: 1000.00000, total_steps: 964000.00000
[CW] train: qf1_loss: 7.63047, qf2_loss: 7.57379, policy_loss: -112.28624, policy_entropy: -6.00700, alpha: 0.02897, time: 33.75723
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   959 ----
[CW] collect: return: 168.26996, steps: 1000.00000, total_steps: 965000.00000
[CW] train: qf1_loss: 7.87747, qf2_loss: 7.84572, policy_loss: -110.07265, policy_entropy: -5.81623, alpha: 0.02889, time: 33.81059
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   960 ----
[CW] collect: return: 324.50843, steps: 1000.00000, total_steps: 966000.00000
[CW] train: qf1_loss: 7.35853, qf2_loss: 7.36940, policy_loss: -111.85001, policy_entropy: -5.95215, alpha: 0.02877, time: 33.91361
[CW] eval: return: 253.88440, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   961 ----
[CW] collect: return: 39.07154, steps: 1000.00000, total_steps: 967000.00000
[CW] train: qf1_loss: 7.45002, qf2_loss: 7.34989, policy_loss: -109.96564, policy_entropy: -5.76078, alpha: 0.02865, time: 33.80209
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   962 ----
[CW] collect: return: 432.52718, steps: 1000.00000, total_steps: 968000.00000
[CW] train: qf1_loss: 8.21890, qf2_loss: 8.12671, policy_loss: -113.00018, policy_entropy: -6.00187, alpha: 0.02844, time: 34.32365
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   963 ----
[CW] collect: return: 186.43610, steps: 1000.00000, total_steps: 969000.00000
[CW] train: qf1_loss: 7.79515, qf2_loss: 7.75072, policy_loss: -109.99149, policy_entropy: -5.75281, alpha: 0.02842, time: 33.67273
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   964 ----
[CW] collect: return: 449.41353, steps: 1000.00000, total_steps: 970000.00000
[CW] train: qf1_loss: 8.84791, qf2_loss: 8.82056, policy_loss: -109.73173, policy_entropy: -5.89201, alpha: 0.02817, time: 33.98007
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   965 ----
[CW] collect: return: 463.42768, steps: 1000.00000, total_steps: 971000.00000
[CW] train: qf1_loss: 8.10616, qf2_loss: 8.10353, policy_loss: -111.65953, policy_entropy: -5.79759, alpha: 0.02804, time: 33.78769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   966 ----
[CW] collect: return: 386.06321, steps: 1000.00000, total_steps: 972000.00000
[CW] train: qf1_loss: 7.42364, qf2_loss: 7.42465, policy_loss: -109.08544, policy_entropy: -5.68040, alpha: 0.02777, time: 33.95412
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   967 ----
[CW] collect: return: 476.71619, steps: 1000.00000, total_steps: 973000.00000
[CW] train: qf1_loss: 7.46906, qf2_loss: 7.43108, policy_loss: -111.51301, policy_entropy: -5.93064, alpha: 0.02760, time: 33.76321
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   968 ----
[CW] collect: return: 387.34714, steps: 1000.00000, total_steps: 974000.00000
[CW] train: qf1_loss: 7.48452, qf2_loss: 7.39882, policy_loss: -110.30703, policy_entropy: -5.83841, alpha: 0.02746, time: 33.96702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   969 ----
[CW] collect: return: 314.57884, steps: 1000.00000, total_steps: 975000.00000
[CW] train: qf1_loss: 7.61462, qf2_loss: 7.60870, policy_loss: -112.28626, policy_entropy: -6.25098, alpha: 0.02749, time: 33.81698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   970 ----
[CW] collect: return: 518.99628, steps: 1000.00000, total_steps: 976000.00000
[CW] train: qf1_loss: 7.73413, qf2_loss: 7.77235, policy_loss: -110.89365, policy_entropy: -6.06766, alpha: 0.02769, time: 33.66728
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   971 ----
[CW] collect: return: 301.35168, steps: 1000.00000, total_steps: 977000.00000
[CW] train: qf1_loss: 9.79313, qf2_loss: 9.71880, policy_loss: -111.78629, policy_entropy: -6.25514, alpha: 0.02777, time: 33.83023
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   972 ----
[CW] collect: return: 390.29853, steps: 1000.00000, total_steps: 978000.00000
[CW] train: qf1_loss: 8.18261, qf2_loss: 8.16634, policy_loss: -110.62450, policy_entropy: -6.02605, alpha: 0.02796, time: 33.92000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   973 ----
[CW] collect: return: 247.43218, steps: 1000.00000, total_steps: 979000.00000
[CW] train: qf1_loss: 8.14769, qf2_loss: 8.20935, policy_loss: -110.57891, policy_entropy: -6.12150, alpha: 0.02801, time: 33.81142
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   974 ----
[CW] collect: return: 471.81596, steps: 1000.00000, total_steps: 980000.00000
[CW] train: qf1_loss: 7.12667, qf2_loss: 7.02613, policy_loss: -112.07067, policy_entropy: -6.28888, alpha: 0.02824, time: 33.79117
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   975 ----
[CW] collect: return: 490.41887, steps: 1000.00000, total_steps: 981000.00000
[CW] train: qf1_loss: 7.40425, qf2_loss: 7.38525, policy_loss: -111.49350, policy_entropy: -6.16809, alpha: 0.02845, time: 33.76351
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   976 ----
[CW] collect: return: 485.57896, steps: 1000.00000, total_steps: 982000.00000
[CW] train: qf1_loss: 8.00025, qf2_loss: 7.90550, policy_loss: -111.62875, policy_entropy: -6.14425, alpha: 0.02865, time: 33.68933
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   977 ----
[CW] collect: return: 442.69069, steps: 1000.00000, total_steps: 983000.00000
[CW] train: qf1_loss: 9.00578, qf2_loss: 8.89683, policy_loss: -110.42656, policy_entropy: -5.90952, alpha: 0.02866, time: 33.89483
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   978 ----
[CW] collect: return: 452.32061, steps: 1000.00000, total_steps: 984000.00000
[CW] train: qf1_loss: 8.49142, qf2_loss: 8.44451, policy_loss: -111.00570, policy_entropy: -5.77320, alpha: 0.02852, time: 33.87740
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   979 ----
[CW] collect: return: 424.91034, steps: 1000.00000, total_steps: 985000.00000
[CW] train: qf1_loss: 8.13624, qf2_loss: 8.09439, policy_loss: -111.15853, policy_entropy: -5.84201, alpha: 0.02832, time: 33.86020
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   980 ----
[CW] collect: return: 355.98615, steps: 1000.00000, total_steps: 986000.00000
[CW] train: qf1_loss: 7.63908, qf2_loss: 7.60377, policy_loss: -112.82582, policy_entropy: -6.02942, alpha: 0.02820, time: 33.77276
[CW] eval: return: 289.88275, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   981 ----
[CW] collect: return: 77.14936, steps: 1000.00000, total_steps: 987000.00000
[CW] train: qf1_loss: 7.95024, qf2_loss: 7.97086, policy_loss: -109.81097, policy_entropy: -5.94705, alpha: 0.02824, time: 33.95061
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   982 ----
[CW] collect: return: 444.28331, steps: 1000.00000, total_steps: 988000.00000
[CW] train: qf1_loss: 9.79456, qf2_loss: 9.62080, policy_loss: -110.52357, policy_entropy: -5.98936, alpha: 0.02820, time: 33.74669
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   983 ----
[CW] collect: return: 480.68116, steps: 1000.00000, total_steps: 989000.00000
[CW] train: qf1_loss: 8.41178, qf2_loss: 8.40022, policy_loss: -111.09238, policy_entropy: -5.98266, alpha: 0.02819, time: 33.76965
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   984 ----
[CW] collect: return: 499.07920, steps: 1000.00000, total_steps: 990000.00000
[CW] train: qf1_loss: 7.83017, qf2_loss: 7.77492, policy_loss: -112.15893, policy_entropy: -6.10575, alpha: 0.02825, time: 33.83111
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   985 ----
[CW] collect: return: 210.00832, steps: 1000.00000, total_steps: 991000.00000
[CW] train: qf1_loss: 7.56894, qf2_loss: 7.55601, policy_loss: -110.93773, policy_entropy: -5.82454, alpha: 0.02818, time: 33.83410
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   986 ----
[CW] collect: return: 414.21342, steps: 1000.00000, total_steps: 992000.00000
[CW] train: qf1_loss: 7.84378, qf2_loss: 7.86097, policy_loss: -111.65260, policy_entropy: -5.91339, alpha: 0.02807, time: 36.77124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   987 ----
[CW] collect: return: 481.90712, steps: 1000.00000, total_steps: 993000.00000
[CW] train: qf1_loss: 8.93110, qf2_loss: 8.88010, policy_loss: -111.47568, policy_entropy: -6.07979, alpha: 0.02805, time: 33.91962
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   988 ----
[CW] collect: return: 491.30636, steps: 1000.00000, total_steps: 994000.00000
[CW] train: qf1_loss: 8.09001, qf2_loss: 7.92598, policy_loss: -111.22470, policy_entropy: -5.92326, alpha: 0.02805, time: 33.58070
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   989 ----
[CW] collect: return: 423.10809, steps: 1000.00000, total_steps: 995000.00000
[CW] train: qf1_loss: 7.56448, qf2_loss: 7.57327, policy_loss: -110.87487, policy_entropy: -5.93947, alpha: 0.02800, time: 33.99472
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   990 ----
[CW] collect: return: 149.88661, steps: 1000.00000, total_steps: 996000.00000
[CW] train: qf1_loss: 7.83450, qf2_loss: 7.72016, policy_loss: -110.37083, policy_entropy: -5.92439, alpha: 0.02794, time: 33.76289
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   991 ----
[CW] collect: return: 494.00623, steps: 1000.00000, total_steps: 997000.00000
[CW] train: qf1_loss: 7.70932, qf2_loss: 7.68511, policy_loss: -111.96247, policy_entropy: -6.09289, alpha: 0.02787, time: 33.93145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   992 ----
[CW] collect: return: 463.93845, steps: 1000.00000, total_steps: 998000.00000
[CW] train: qf1_loss: 11.35645, qf2_loss: 11.34474, policy_loss: -112.44420, policy_entropy: -6.05928, alpha: 0.02797, time: 33.70267
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   993 ----
[CW] collect: return: 458.66115, steps: 1000.00000, total_steps: 999000.00000
[CW] train: qf1_loss: 10.21132, qf2_loss: 10.05795, policy_loss: -113.08029, policy_entropy: -6.26066, alpha: 0.02819, time: 33.81516
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   994 ----
[CW] collect: return: 262.85624, steps: 1000.00000, total_steps: 1000000.00000
[CW] train: qf1_loss: 7.45307, qf2_loss: 7.46014, policy_loss: -112.11782, policy_entropy: -6.07861, alpha: 0.02834, time: 33.75933
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   995 ----
[CW] collect: return: 207.10732, steps: 1000.00000, total_steps: 1001000.00000
[CW] train: qf1_loss: 7.48283, qf2_loss: 7.42699, policy_loss: -109.91170, policy_entropy: -5.93510, alpha: 0.02834, time: 33.77466
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   996 ----
[CW] collect: return: 433.24221, steps: 1000.00000, total_steps: 1002000.00000
[CW] train: qf1_loss: 8.07231, qf2_loss: 8.06118, policy_loss: -112.84255, policy_entropy: -6.10983, alpha: 0.02838, time: 33.82906
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   997 ----
[CW] collect: return: 423.32887, steps: 1000.00000, total_steps: 1003000.00000
[CW] train: qf1_loss: 8.61872, qf2_loss: 8.52246, policy_loss: -111.72598, policy_entropy: -6.09447, alpha: 0.02852, time: 33.87105
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   998 ----
[CW] collect: return: 325.54913, steps: 1000.00000, total_steps: 1004000.00000
[CW] train: qf1_loss: 9.00796, qf2_loss: 9.03470, policy_loss: -114.22346, policy_entropy: -6.16691, alpha: 0.02859, time: 33.86571
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   999 ----
[CW] collect: return: 91.49707, steps: 1000.00000, total_steps: 1005000.00000
[CW] train: qf1_loss: 7.69370, qf2_loss: 7.67115, policy_loss: -111.23668, policy_entropy: -6.06616, alpha: 0.02871, time: 33.81198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:  1000 ----
[CW] collect: return: 353.55700, steps: 1000.00000, total_steps: 1006000.00000
[CW] train: qf1_loss: 9.16055, qf2_loss: 9.16413, policy_loss: -112.84164, policy_entropy: -6.21168, alpha: 0.02884, time: 33.87791
[CW] eval: return: 271.97768, steps: 1000.00000
[CW] ---------------------------

============================= JOB FEEDBACK =============================

NodeName=uc2n903
Job ID: 21915471
Array Job ID: 21915471_0
Cluster: uc2
User/Group: uprnr/stud
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 4
CPU Utilized: 10:00:06
CPU Efficiency: 25.12% of 1-15:48:48 core-walltime
Job Wall-clock time: 09:57:12
Memory Utilized: 6.64 GB
Memory Efficiency: 11.33% of 58.59 GB
