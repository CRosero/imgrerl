{"collect/return": 851.75591019477, "collect/steps": 1000.0, "collect/total_steps": 410000.0, "train/qf1_loss": 58.104291114807125, "train/qf2_loss": 57.58621782302856, "train/policy_loss": -422.5176086425781, "train/policy_entropy": -0.9959925282001495, "train/alpha": 0.708686431646347, "train/time": 68.53092408180237, "eval/return": 781.5290624588597, "eval/steps": 1000.0, "_timestamp": 1678745063.9026685, "_runtime": 28652.354784488678, "_step": 404}