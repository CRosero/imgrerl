[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 11.49952, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.78825, qf2_loss: 1.79155, policy_loss: -7.80998, policy_entropy: 4.09887, alpha: 0.98504, time: 57.29203
[CW] eval: return: 12.28194, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 18.48224, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.09340, qf2_loss: 0.09373, policy_loss: -8.51772, policy_entropy: 4.10044, alpha: 0.95626, time: 50.91263
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 6.92120, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.08433, qf2_loss: 0.08440, policy_loss: -9.20583, policy_entropy: 4.10118, alpha: 0.92871, time: 51.01111
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 13.82490, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.07624, qf2_loss: 0.07628, policy_loss: -10.12519, policy_entropy: 4.10086, alpha: 0.90231, time: 51.10613
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 12.39158, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06952, qf2_loss: 0.06949, policy_loss: -11.16581, policy_entropy: 4.10094, alpha: 0.87699, time: 51.26190
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 20.47489, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06315, qf2_loss: 0.06305, policy_loss: -12.27534, policy_entropy: 4.10062, alpha: 0.85267, time: 51.20240
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 11.66557, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.08150, qf2_loss: 0.08081, policy_loss: -13.43184, policy_entropy: 4.10106, alpha: 0.82931, time: 51.36297
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 10.41323, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.06830, qf2_loss: 0.06822, policy_loss: -14.62543, policy_entropy: 4.10060, alpha: 0.80683, time: 51.39868
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 8.87854, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.08013, qf2_loss: 0.08066, policy_loss: -15.83764, policy_entropy: 4.10097, alpha: 0.78520, time: 51.29995
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 7.93478, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.08928, qf2_loss: 0.09047, policy_loss: -17.04305, policy_entropy: 4.10060, alpha: 0.76436, time: 51.23130
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 9.16832, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.07947, qf2_loss: 0.08048, policy_loss: -18.23618, policy_entropy: 4.10032, alpha: 0.74427, time: 51.24910
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 7.00456, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.08503, qf2_loss: 0.08636, policy_loss: -19.40619, policy_entropy: 4.10021, alpha: 0.72489, time: 51.37686
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 14.18966, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.08159, qf2_loss: 0.08275, policy_loss: -20.55735, policy_entropy: 4.09981, alpha: 0.70618, time: 51.36860
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 9.69165, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.10477, qf2_loss: 0.10683, policy_loss: -21.67509, policy_entropy: 4.09944, alpha: 0.68810, time: 51.12140
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 16.05890, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.09061, qf2_loss: 0.09211, policy_loss: -22.77463, policy_entropy: 4.10049, alpha: 0.67063, time: 51.05773
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 16.22161, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.11053, qf2_loss: 0.11264, policy_loss: -23.84809, policy_entropy: 4.09875, alpha: 0.65373, time: 51.53016
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 15.43824, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.08422, qf2_loss: 0.08527, policy_loss: -24.89665, policy_entropy: 4.09872, alpha: 0.63738, time: 51.30088
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 18.60997, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.10867, qf2_loss: 0.11052, policy_loss: -25.90837, policy_entropy: 4.09731, alpha: 0.62155, time: 51.36577
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 12.21924, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.11787, qf2_loss: 0.11978, policy_loss: -26.89932, policy_entropy: 4.09835, alpha: 0.60621, time: 51.38122
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 24.32066, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.10260, qf2_loss: 0.10353, policy_loss: -27.86738, policy_entropy: 4.09823, alpha: 0.59134, time: 51.30116
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 24.70749, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.10772, qf2_loss: 0.10946, policy_loss: -28.81033, policy_entropy: 4.09522, alpha: 0.57692, time: 51.13830
[CW] eval: return: 14.99743, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 21.62061, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.13911, qf2_loss: 0.14134, policy_loss: -29.71564, policy_entropy: 4.09522, alpha: 0.56294, time: 51.36889
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 17.78806, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.11993, qf2_loss: 0.12178, policy_loss: -30.60739, policy_entropy: 4.09612, alpha: 0.54937, time: 51.29079
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 21.63572, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.12082, qf2_loss: 0.12247, policy_loss: -31.46121, policy_entropy: 4.09487, alpha: 0.53619, time: 51.38212
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 21.92158, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.13219, qf2_loss: 0.13416, policy_loss: -32.29587, policy_entropy: 4.09368, alpha: 0.52339, time: 51.29611
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 13.45761, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.15001, qf2_loss: 0.15243, policy_loss: -33.10257, policy_entropy: 4.09227, alpha: 0.51095, time: 51.32938
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 20.45586, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.13167, qf2_loss: 0.13329, policy_loss: -33.89684, policy_entropy: 4.09253, alpha: 0.49886, time: 51.26779
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 16.82683, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.12601, qf2_loss: 0.12736, policy_loss: -34.65760, policy_entropy: 4.09101, alpha: 0.48710, time: 51.78291
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 21.85717, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.18919, qf2_loss: 0.19297, policy_loss: -35.39120, policy_entropy: 4.09080, alpha: 0.47567, time: 53.09126
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 7.03614, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.15038, qf2_loss: 0.15231, policy_loss: -36.09046, policy_entropy: 4.08970, alpha: 0.46455, time: 51.59030
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 12.07176, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.13825, qf2_loss: 0.13970, policy_loss: -36.78138, policy_entropy: 4.09036, alpha: 0.45372, time: 51.52320
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 15.65179, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.15451, qf2_loss: 0.15623, policy_loss: -37.43543, policy_entropy: 4.08790, alpha: 0.44318, time: 51.58989
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 23.00633, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.17154, qf2_loss: 0.17429, policy_loss: -38.09942, policy_entropy: 4.08725, alpha: 0.43291, time: 51.43230
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 10.23855, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.15287, qf2_loss: 0.15572, policy_loss: -38.70743, policy_entropy: 4.08722, alpha: 0.42292, time: 51.38790
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 14.33633, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.18150, qf2_loss: 0.18602, policy_loss: -39.29562, policy_entropy: 4.08509, alpha: 0.41318, time: 51.54989
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 11.52818, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.14606, qf2_loss: 0.15015, policy_loss: -39.88058, policy_entropy: 4.08414, alpha: 0.40369, time: 51.25562
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 12.03576, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.17731, qf2_loss: 0.18279, policy_loss: -40.42888, policy_entropy: 4.08384, alpha: 0.39444, time: 51.30200
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 15.83063, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.16697, qf2_loss: 0.17276, policy_loss: -40.96457, policy_entropy: 4.08410, alpha: 0.38542, time: 51.23355
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 21.67019, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.16584, qf2_loss: 0.17183, policy_loss: -41.49243, policy_entropy: 4.08596, alpha: 0.37663, time: 51.34077
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 19.92202, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.15119, qf2_loss: 0.15572, policy_loss: -41.99634, policy_entropy: 4.08554, alpha: 0.36806, time: 51.41898
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 22.17744, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.16705, qf2_loss: 0.17155, policy_loss: -42.50090, policy_entropy: 4.08272, alpha: 0.35969, time: 51.36422
[CW] eval: return: 17.57839, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 10.92791, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.17456, qf2_loss: 0.17801, policy_loss: -42.97673, policy_entropy: 4.08139, alpha: 0.35153, time: 51.21036
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 29.37543, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.15466, qf2_loss: 0.15757, policy_loss: -43.41368, policy_entropy: 4.07841, alpha: 0.34357, time: 51.23340
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 12.87048, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.19721, qf2_loss: 0.20014, policy_loss: -43.85928, policy_entropy: 4.07547, alpha: 0.33581, time: 51.34820
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 20.58660, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.12192, qf2_loss: 0.12346, policy_loss: -44.28468, policy_entropy: 4.07243, alpha: 0.32823, time: 51.33283
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 10.80681, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.18681, qf2_loss: 0.18988, policy_loss: -44.68277, policy_entropy: 4.07154, alpha: 0.32084, time: 51.23228
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 29.57437, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.16256, qf2_loss: 0.16532, policy_loss: -45.06354, policy_entropy: 4.07114, alpha: 0.31362, time: 51.83358
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 29.15025, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.16346, qf2_loss: 0.16600, policy_loss: -45.46552, policy_entropy: 4.06660, alpha: 0.30658, time: 51.24837
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 9.33768, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.16014, qf2_loss: 0.16246, policy_loss: -45.82491, policy_entropy: 4.06438, alpha: 0.29970, time: 51.30406
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 21.42857, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.19527, qf2_loss: 0.19844, policy_loss: -46.17776, policy_entropy: 4.06071, alpha: 0.29298, time: 51.52335
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 27.48177, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.17819, qf2_loss: 0.18124, policy_loss: -46.51336, policy_entropy: 4.05547, alpha: 0.28642, time: 51.03191
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 20.14577, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.15220, qf2_loss: 0.15439, policy_loss: -46.84075, policy_entropy: 4.04853, alpha: 0.28002, time: 51.28859
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 12.30999, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.20607, qf2_loss: 0.20981, policy_loss: -47.15774, policy_entropy: 4.04173, alpha: 0.27377, time: 51.33377
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 12.85119, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.17202, qf2_loss: 0.17459, policy_loss: -47.45369, policy_entropy: 4.03651, alpha: 0.26767, time: 51.79368
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 22.79628, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.15646, qf2_loss: 0.15910, policy_loss: -47.74082, policy_entropy: 4.03085, alpha: 0.26170, time: 51.08812
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 35.27745, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.25900, qf2_loss: 0.26455, policy_loss: -48.02895, policy_entropy: 4.02247, alpha: 0.25588, time: 50.99098
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 20.33239, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.14704, qf2_loss: 0.14938, policy_loss: -48.29387, policy_entropy: 4.01323, alpha: 0.25020, time: 51.03642
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 23.91893, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.18754, qf2_loss: 0.19064, policy_loss: -48.54196, policy_entropy: 4.00091, alpha: 0.24464, time: 51.13899
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 22.43626, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.18168, qf2_loss: 0.18472, policy_loss: -48.79600, policy_entropy: 3.99123, alpha: 0.23922, time: 51.32712
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 23.02321, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.16489, qf2_loss: 0.16834, policy_loss: -49.00239, policy_entropy: 3.98030, alpha: 0.23393, time: 51.34211
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 32.16344, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.18941, qf2_loss: 0.19339, policy_loss: -49.24564, policy_entropy: 3.96929, alpha: 0.22875, time: 51.67466
[CW] eval: return: 36.40141, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 31.83216, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.19945, qf2_loss: 0.20429, policy_loss: -49.46026, policy_entropy: 3.94527, alpha: 0.22370, time: 51.59163
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 31.05609, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.21469, qf2_loss: 0.22061, policy_loss: -49.66589, policy_entropy: 3.93175, alpha: 0.21877, time: 51.61200
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 40.66264, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.16792, qf2_loss: 0.17285, policy_loss: -49.87085, policy_entropy: 3.89470, alpha: 0.21396, time: 51.69868
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 40.44395, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.23946, qf2_loss: 0.24701, policy_loss: -50.05607, policy_entropy: 3.86782, alpha: 0.20926, time: 51.67123
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 45.68886, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.20144, qf2_loss: 0.20791, policy_loss: -50.25432, policy_entropy: 3.80012, alpha: 0.20469, time: 51.65221
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 58.40641, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.23469, qf2_loss: 0.24174, policy_loss: -50.45322, policy_entropy: 3.75648, alpha: 0.20023, time: 51.63090
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 46.95089, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.22898, qf2_loss: 0.23617, policy_loss: -50.63809, policy_entropy: 3.68907, alpha: 0.19589, time: 51.55380
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 67.43108, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.20953, qf2_loss: 0.21716, policy_loss: -50.82528, policy_entropy: 3.59157, alpha: 0.19167, time: 51.60345
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 93.20977, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.25978, qf2_loss: 0.26729, policy_loss: -51.04127, policy_entropy: 3.44718, alpha: 0.18759, time: 51.51995
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 110.07635, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.26566, qf2_loss: 0.27440, policy_loss: -51.26577, policy_entropy: 3.28020, alpha: 0.18364, time: 51.58531
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 62.16627, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.27604, qf2_loss: 0.28482, policy_loss: -51.45157, policy_entropy: 3.05552, alpha: 0.17983, time: 52.28082
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 104.73355, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.31302, qf2_loss: 0.32274, policy_loss: -51.74675, policy_entropy: 2.91098, alpha: 0.17616, time: 51.39800
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 97.68877, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.29629, qf2_loss: 0.30201, policy_loss: -51.97710, policy_entropy: 2.58134, alpha: 0.17264, time: 51.97987
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 37.92376, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.31956, qf2_loss: 0.32614, policy_loss: -52.22068, policy_entropy: 2.44104, alpha: 0.16924, time: 51.31422
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 90.96470, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.30736, qf2_loss: 0.31332, policy_loss: -52.50909, policy_entropy: 2.34038, alpha: 0.16595, time: 51.54484
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 107.11197, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.25998, qf2_loss: 0.26606, policy_loss: -52.72738, policy_entropy: 2.24304, alpha: 0.16272, time: 51.49461
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 102.39261, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.34751, qf2_loss: 0.35786, policy_loss: -52.96420, policy_entropy: 2.11871, alpha: 0.15957, time: 51.56052
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 61.41083, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.31472, qf2_loss: 0.32100, policy_loss: -53.24535, policy_entropy: 2.11802, alpha: 0.15647, time: 51.43266
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 84.64961, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.31468, qf2_loss: 0.32068, policy_loss: -53.43677, policy_entropy: 2.02353, alpha: 0.15343, time: 51.48666
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 46.45336, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.29442, qf2_loss: 0.30168, policy_loss: -53.66751, policy_entropy: 1.94445, alpha: 0.15045, time: 51.39710
[CW] eval: return: 81.45091, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 49.49849, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.31689, qf2_loss: 0.32232, policy_loss: -53.89454, policy_entropy: 2.00774, alpha: 0.14751, time: 51.32627
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 84.54032, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.32465, qf2_loss: 0.33077, policy_loss: -54.09336, policy_entropy: 1.95652, alpha: 0.14459, time: 51.06690
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 62.98889, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.31031, qf2_loss: 0.31713, policy_loss: -54.29426, policy_entropy: 1.91155, alpha: 0.14174, time: 51.25968
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 91.17572, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.28694, qf2_loss: 0.29315, policy_loss: -54.50723, policy_entropy: 1.74522, alpha: 0.13894, time: 51.29894
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 110.76435, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.29677, qf2_loss: 0.30381, policy_loss: -54.73146, policy_entropy: 1.55819, alpha: 0.13624, time: 51.28938
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 75.07798, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.29551, qf2_loss: 0.30125, policy_loss: -54.97172, policy_entropy: 1.40220, alpha: 0.13363, time: 51.18720
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 74.12320, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.35658, qf2_loss: 0.36445, policy_loss: -55.21768, policy_entropy: 1.31115, alpha: 0.13108, time: 51.39470
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 100.56748, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.32600, qf2_loss: 0.33164, policy_loss: -55.45425, policy_entropy: 1.20667, alpha: 0.12859, time: 51.23148
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 84.35774, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.32603, qf2_loss: 0.33172, policy_loss: -55.70246, policy_entropy: 1.09412, alpha: 0.12616, time: 51.24478
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 64.79516, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.34010, qf2_loss: 0.34609, policy_loss: -55.97217, policy_entropy: 1.01956, alpha: 0.12378, time: 51.38645
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 103.25158, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.36702, qf2_loss: 0.37403, policy_loss: -56.20268, policy_entropy: 0.99933, alpha: 0.12144, time: 51.89641
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 58.06498, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.36090, qf2_loss: 0.36807, policy_loss: -56.46995, policy_entropy: 0.87756, alpha: 0.11913, time: 51.20972
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 93.70461, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.37678, qf2_loss: 0.38599, policy_loss: -56.67776, policy_entropy: 0.70946, alpha: 0.11690, time: 51.33183
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 50.69221, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.35522, qf2_loss: 0.36298, policy_loss: -56.92622, policy_entropy: 0.72029, alpha: 0.11471, time: 51.40096
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 30.96948, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.41896, qf2_loss: 0.42847, policy_loss: -57.15037, policy_entropy: 0.72987, alpha: 0.11253, time: 51.21310
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 114.45467, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.44987, qf2_loss: 0.45642, policy_loss: -57.43801, policy_entropy: 0.66149, alpha: 0.11038, time: 51.07117
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 93.40248, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.36992, qf2_loss: 0.37887, policy_loss: -57.70955, policy_entropy: 0.57226, alpha: 0.10828, time: 51.20775
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 45.10583, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.36532, qf2_loss: 0.37459, policy_loss: -57.92986, policy_entropy: 0.61924, alpha: 0.10620, time: 52.66491
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 58.40225, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.42538, qf2_loss: 0.43757, policy_loss: -58.05677, policy_entropy: 0.68871, alpha: 0.10414, time: 51.14776
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 49.04968, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.39304, qf2_loss: 0.40217, policy_loss: -58.24119, policy_entropy: 0.67446, alpha: 0.10207, time: 51.31768
[CW] eval: return: 80.08220, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 35.30575, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.46077, qf2_loss: 0.46913, policy_loss: -58.42039, policy_entropy: 0.70912, alpha: 0.10003, time: 51.10757
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 135.88744, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.39151, qf2_loss: 0.39936, policy_loss: -58.60725, policy_entropy: 0.65141, alpha: 0.09803, time: 51.17792
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 45.09090, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.38720, qf2_loss: 0.39135, policy_loss: -58.79367, policy_entropy: 0.61860, alpha: 0.09606, time: 51.39943
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 44.06958, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.35642, qf2_loss: 0.36115, policy_loss: -58.85578, policy_entropy: 0.61966, alpha: 0.09413, time: 51.30836
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 109.09926, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.41312, qf2_loss: 0.41731, policy_loss: -59.06652, policy_entropy: 0.58592, alpha: 0.09222, time: 51.38213
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 82.80988, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.39074, qf2_loss: 0.39548, policy_loss: -59.18815, policy_entropy: 0.56199, alpha: 0.09035, time: 51.29001
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 104.10741, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.39020, qf2_loss: 0.39438, policy_loss: -59.35123, policy_entropy: 0.52931, alpha: 0.08851, time: 51.30433
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 101.45467, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.37915, qf2_loss: 0.38580, policy_loss: -59.47869, policy_entropy: 0.50399, alpha: 0.08671, time: 51.30582
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 182.71213, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.41443, qf2_loss: 0.41831, policy_loss: -59.62379, policy_entropy: 0.45592, alpha: 0.08495, time: 51.16850
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 77.50025, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.39257, qf2_loss: 0.39538, policy_loss: -59.78976, policy_entropy: 0.40042, alpha: 0.08322, time: 51.28143
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 84.74929, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.43184, qf2_loss: 0.43611, policy_loss: -59.95758, policy_entropy: 0.35181, alpha: 0.08153, time: 51.25106
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 98.98264, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.48697, qf2_loss: 0.49335, policy_loss: -60.06149, policy_entropy: 0.28196, alpha: 0.07988, time: 51.14633
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 119.04270, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.36835, qf2_loss: 0.37179, policy_loss: -60.17980, policy_entropy: 0.21462, alpha: 0.07827, time: 51.16034
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 56.81057, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.41035, qf2_loss: 0.41661, policy_loss: -60.16480, policy_entropy: 0.32322, alpha: 0.07668, time: 51.35121
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 93.73122, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.40418, qf2_loss: 0.40920, policy_loss: -60.38417, policy_entropy: 0.15520, alpha: 0.07512, time: 51.41840
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 84.26982, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.39968, qf2_loss: 0.40379, policy_loss: -60.51938, policy_entropy: 0.13264, alpha: 0.07360, time: 51.26814
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 79.30704, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.40334, qf2_loss: 0.40735, policy_loss: -60.50683, policy_entropy: 0.08528, alpha: 0.07212, time: 51.18668
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 61.98837, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.42338, qf2_loss: 0.42902, policy_loss: -60.48477, policy_entropy: 0.08744, alpha: 0.07066, time: 51.26856
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 36.20196, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.39330, qf2_loss: 0.39922, policy_loss: -60.59007, policy_entropy: 0.02948, alpha: 0.06922, time: 51.58837
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 109.32751, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.41708, qf2_loss: 0.42226, policy_loss: -60.63556, policy_entropy: -0.01676, alpha: 0.06782, time: 51.41245
[CW] eval: return: 101.76600, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 74.03808, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.44392, qf2_loss: 0.44953, policy_loss: -60.64441, policy_entropy: -0.02317, alpha: 0.06645, time: 51.23580
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 40.26592, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.37710, qf2_loss: 0.38327, policy_loss: -60.71678, policy_entropy: -0.11858, alpha: 0.06510, time: 51.41735
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 68.02252, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.42926, qf2_loss: 0.43523, policy_loss: -60.78975, policy_entropy: -0.19132, alpha: 0.06379, time: 51.34256
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 108.60006, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.41132, qf2_loss: 0.41771, policy_loss: -60.87485, policy_entropy: -0.19934, alpha: 0.06251, time: 51.51622
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 109.12608, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.39081, qf2_loss: 0.39615, policy_loss: -61.03100, policy_entropy: -0.30982, alpha: 0.06126, time: 51.25726
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 44.70698, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.43175, qf2_loss: 0.43802, policy_loss: -60.86449, policy_entropy: -0.31621, alpha: 0.06004, time: 51.22127
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 169.92259, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.44057, qf2_loss: 0.44514, policy_loss: -61.13764, policy_entropy: -0.37997, alpha: 0.05884, time: 51.49553
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 135.79799, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.46041, qf2_loss: 0.46611, policy_loss: -61.28899, policy_entropy: -0.42839, alpha: 0.05767, time: 51.54034
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 60.64073, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.47395, qf2_loss: 0.48246, policy_loss: -61.30401, policy_entropy: -0.49657, alpha: 0.05652, time: 51.47659
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 46.48350, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.42066, qf2_loss: 0.42759, policy_loss: -61.39355, policy_entropy: -0.56644, alpha: 0.05540, time: 51.36746
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 135.39066, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.41589, qf2_loss: 0.41984, policy_loss: -61.37327, policy_entropy: -0.62849, alpha: 0.05431, time: 51.37420
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 38.02832, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.42356, qf2_loss: 0.42803, policy_loss: -61.48962, policy_entropy: -0.59120, alpha: 0.05323, time: 51.29257
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 114.47635, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.43102, qf2_loss: 0.43320, policy_loss: -61.53383, policy_entropy: -0.67868, alpha: 0.05217, time: 51.47946
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 39.00404, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.42371, qf2_loss: 0.42784, policy_loss: -61.64636, policy_entropy: -0.76305, alpha: 0.05115, time: 51.67946
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 39.02063, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.53468, qf2_loss: 0.54401, policy_loss: -61.49654, policy_entropy: -0.70209, alpha: 0.05013, time: 51.30233
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 125.99662, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.43553, qf2_loss: 0.44103, policy_loss: -61.86440, policy_entropy: -0.93371, alpha: 0.04914, time: 51.26264
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 102.84372, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.45452, qf2_loss: 0.46032, policy_loss: -61.90889, policy_entropy: -1.03475, alpha: 0.04820, time: 51.34137
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 119.25741, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.51824, qf2_loss: 0.52516, policy_loss: -61.90774, policy_entropy: -1.04244, alpha: 0.04728, time: 51.19539
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 162.41345, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.51197, qf2_loss: 0.51939, policy_loss: -61.79714, policy_entropy: -1.03010, alpha: 0.04636, time: 51.97581
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 64.51834, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.55474, qf2_loss: 0.56089, policy_loss: -61.87127, policy_entropy: -1.11317, alpha: 0.04546, time: 51.35149
[CW] eval: return: 76.15007, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 115.28285, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.49726, qf2_loss: 0.50415, policy_loss: -61.97571, policy_entropy: -1.21998, alpha: 0.04459, time: 51.32977
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 60.21526, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.46897, qf2_loss: 0.47662, policy_loss: -62.14422, policy_entropy: -1.31382, alpha: 0.04374, time: 51.41815
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 134.43152, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.51448, qf2_loss: 0.52009, policy_loss: -62.30202, policy_entropy: -1.36986, alpha: 0.04291, time: 51.34126
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 114.46707, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.61999, qf2_loss: 0.62555, policy_loss: -62.44872, policy_entropy: -1.46250, alpha: 0.04211, time: 51.39662
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 114.93908, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.54672, qf2_loss: 0.55412, policy_loss: -62.42161, policy_entropy: -1.55614, alpha: 0.04132, time: 52.48500
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 168.98769, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.55713, qf2_loss: 0.56212, policy_loss: -62.38448, policy_entropy: -1.67222, alpha: 0.04055, time: 51.27095
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 68.15752, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.58198, qf2_loss: 0.59058, policy_loss: -62.59844, policy_entropy: -1.70537, alpha: 0.03981, time: 51.36941
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 80.25778, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.56823, qf2_loss: 0.57378, policy_loss: -62.18038, policy_entropy: -1.61270, alpha: 0.03907, time: 51.28366
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 44.60338, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.81258, qf2_loss: 0.83242, policy_loss: -62.28658, policy_entropy: -1.81616, alpha: 0.03834, time: 51.24337
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 13.36175, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.50892, qf2_loss: 0.51171, policy_loss: -62.55647, policy_entropy: -2.01298, alpha: 0.03765, time: 51.28776
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 83.79094, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.59856, qf2_loss: 0.60309, policy_loss: -62.47137, policy_entropy: -1.97553, alpha: 0.03697, time: 51.14088
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 125.56251, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.56238, qf2_loss: 0.57121, policy_loss: -62.69277, policy_entropy: -2.16191, alpha: 0.03632, time: 51.21245
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 126.46408, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.71908, qf2_loss: 0.72839, policy_loss: -62.65079, policy_entropy: -2.19410, alpha: 0.03568, time: 51.22552
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 52.28783, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.56268, qf2_loss: 0.56454, policy_loss: -62.74141, policy_entropy: -2.33616, alpha: 0.03505, time: 51.09003
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 95.28692, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.58338, qf2_loss: 0.59191, policy_loss: -62.71439, policy_entropy: -2.36038, alpha: 0.03445, time: 51.02365
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 108.43645, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.56848, qf2_loss: 0.57204, policy_loss: -62.83849, policy_entropy: -2.43267, alpha: 0.03385, time: 51.05531
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 158.25269, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.78398, qf2_loss: 0.79113, policy_loss: -62.90491, policy_entropy: -2.38340, alpha: 0.03326, time: 51.22241
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 138.31718, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.64361, qf2_loss: 0.65281, policy_loss: -63.32175, policy_entropy: -2.81940, alpha: 0.03269, time: 51.01860
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 161.10736, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.79271, qf2_loss: 0.80471, policy_loss: -63.47318, policy_entropy: -2.86688, alpha: 0.03217, time: 51.29910
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 89.84144, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.65215, qf2_loss: 0.66001, policy_loss: -63.47005, policy_entropy: -3.03310, alpha: 0.03166, time: 51.19415
[CW] eval: return: 107.73390, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 67.93705, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.64199, qf2_loss: 0.64707, policy_loss: -63.51648, policy_entropy: -3.00457, alpha: 0.03117, time: 51.30257
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 132.69752, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.67388, qf2_loss: 0.67779, policy_loss: -63.53381, policy_entropy: -3.03616, alpha: 0.03067, time: 51.19252
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 138.50542, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.69488, qf2_loss: 0.70788, policy_loss: -63.49781, policy_entropy: -3.20271, alpha: 0.03019, time: 51.28311
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 109.07186, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.67862, qf2_loss: 0.68174, policy_loss: -64.02078, policy_entropy: -3.35675, alpha: 0.02973, time: 51.38109
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 93.58243, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.68420, qf2_loss: 0.69115, policy_loss: -63.97297, policy_entropy: -3.38468, alpha: 0.02928, time: 51.30649
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 53.86156, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.62514, qf2_loss: 0.63241, policy_loss: -63.78790, policy_entropy: -3.44295, alpha: 0.02884, time: 51.48251
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 90.82492, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.70017, qf2_loss: 0.70673, policy_loss: -64.18265, policy_entropy: -3.68154, alpha: 0.02842, time: 51.13997
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 124.40697, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.78176, qf2_loss: 0.79286, policy_loss: -64.24299, policy_entropy: -3.80367, alpha: 0.02802, time: 51.01619
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 62.04160, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.74851, qf2_loss: 0.75856, policy_loss: -64.57052, policy_entropy: -3.94766, alpha: 0.02765, time: 51.11707
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 185.26148, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.73370, qf2_loss: 0.73717, policy_loss: -64.30966, policy_entropy: -4.09449, alpha: 0.02730, time: 51.52286
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 157.18332, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.73429, qf2_loss: 0.74251, policy_loss: -64.68511, policy_entropy: -4.16217, alpha: 0.02696, time: 51.15784
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 122.77645, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.69656, qf2_loss: 0.70626, policy_loss: -64.69722, policy_entropy: -4.03288, alpha: 0.02661, time: 52.59218
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 56.79016, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.91534, qf2_loss: 0.92969, policy_loss: -64.66164, policy_entropy: -4.20294, alpha: 0.02626, time: 51.16507
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 194.88637, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.72027, qf2_loss: 0.72322, policy_loss: -64.97975, policy_entropy: -3.82930, alpha: 0.02589, time: 51.12832
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 60.65988, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.69850, qf2_loss: 0.70492, policy_loss: -64.80836, policy_entropy: -3.78723, alpha: 0.02547, time: 51.18533
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 91.50142, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.70008, qf2_loss: 0.70960, policy_loss: -65.27913, policy_entropy: -3.99475, alpha: 0.02507, time: 51.13206
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 222.01019, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.92937, qf2_loss: 0.93755, policy_loss: -65.49319, policy_entropy: -4.06602, alpha: 0.02469, time: 51.19383
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 147.73171, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.77351, qf2_loss: 0.78202, policy_loss: -65.42421, policy_entropy: -4.06151, alpha: 0.02432, time: 51.03673
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 64.71909, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.74637, qf2_loss: 0.75114, policy_loss: -65.39148, policy_entropy: -4.14918, alpha: 0.02395, time: 51.19599
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 230.86817, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.66613, qf2_loss: 0.67188, policy_loss: -65.79100, policy_entropy: -4.27276, alpha: 0.02360, time: 51.15040
[CW] eval: return: 153.54916, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 133.91667, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.69229, qf2_loss: 0.69531, policy_loss: -65.76127, policy_entropy: -4.10310, alpha: 0.02325, time: 50.89281
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 100.02662, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.72428, qf2_loss: 0.72974, policy_loss: -66.16772, policy_entropy: -4.25311, alpha: 0.02289, time: 51.00777
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 132.63702, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.77074, qf2_loss: 0.77375, policy_loss: -65.90455, policy_entropy: -4.18660, alpha: 0.02254, time: 50.97528
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 29.90152, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.75945, qf2_loss: 0.77103, policy_loss: -65.74866, policy_entropy: -4.21646, alpha: 0.02218, time: 50.81267
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 196.44872, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.76276, qf2_loss: 0.76764, policy_loss: -65.98043, policy_entropy: -4.27237, alpha: 0.02183, time: 50.70250
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 176.36964, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.76140, qf2_loss: 0.76755, policy_loss: -66.17975, policy_entropy: -4.33733, alpha: 0.02149, time: 51.17035
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 132.67676, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.75557, qf2_loss: 0.76476, policy_loss: -66.43361, policy_entropy: -4.37485, alpha: 0.02117, time: 51.40029
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 196.46736, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.74074, qf2_loss: 0.74592, policy_loss: -66.34565, policy_entropy: -4.43165, alpha: 0.02084, time: 51.44787
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 111.51050, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.79870, qf2_loss: 0.80242, policy_loss: -66.48873, policy_entropy: -5.24320, alpha: 0.02058, time: 51.61957
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 194.22616, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.77467, qf2_loss: 0.78235, policy_loss: -66.55984, policy_entropy: -5.67009, alpha: 0.02048, time: 51.42704
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 205.94449, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.90773, qf2_loss: 0.91799, policy_loss: -66.66326, policy_entropy: -5.65828, alpha: 0.02040, time: 51.14092
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 231.66782, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.84547, qf2_loss: 0.86291, policy_loss: -67.15928, policy_entropy: -5.77349, alpha: 0.02033, time: 51.10542
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 156.80769, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.79900, qf2_loss: 0.80038, policy_loss: -67.71927, policy_entropy: -5.76061, alpha: 0.02029, time: 51.16786
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 101.48042, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.84370, qf2_loss: 0.84964, policy_loss: -67.07870, policy_entropy: -5.56432, alpha: 0.02021, time: 51.71208
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 28.94222, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.90273, qf2_loss: 0.92137, policy_loss: -67.47916, policy_entropy: -5.18920, alpha: 0.02006, time: 51.36517
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 84.50748, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.75088, qf2_loss: 0.76054, policy_loss: -67.30924, policy_entropy: -4.79227, alpha: 0.01980, time: 51.45474
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 68.64632, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.72568, qf2_loss: 0.73533, policy_loss: -67.69452, policy_entropy: -5.18453, alpha: 0.01952, time: 51.26299
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 251.45799, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.76604, qf2_loss: 0.76220, policy_loss: -67.67483, policy_entropy: -5.15884, alpha: 0.01930, time: 51.11919
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 34.26778, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.79561, qf2_loss: 0.80418, policy_loss: -67.61559, policy_entropy: -5.12165, alpha: 0.01906, time: 51.19402
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 215.54847, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.95218, qf2_loss: 0.96877, policy_loss: -67.69343, policy_entropy: -5.43920, alpha: 0.01887, time: 50.99419
[CW] eval: return: 160.92501, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 44.76343, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.88759, qf2_loss: 0.89510, policy_loss: -67.78731, policy_entropy: -5.34359, alpha: 0.01872, time: 51.07507
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 267.16330, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.77196, qf2_loss: 0.77398, policy_loss: -68.01557, policy_entropy: -5.35747, alpha: 0.01851, time: 50.93861
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 32.37040, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.80421, qf2_loss: 0.80780, policy_loss: -67.73838, policy_entropy: -5.34438, alpha: 0.01831, time: 50.99629
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 32.89476, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.81241, qf2_loss: 0.82358, policy_loss: -68.25629, policy_entropy: -5.47584, alpha: 0.01815, time: 50.90012
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 145.41550, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.85170, qf2_loss: 0.85832, policy_loss: -68.41698, policy_entropy: -5.51667, alpha: 0.01800, time: 51.31789
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 279.49668, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.81301, qf2_loss: 0.81555, policy_loss: -68.33820, policy_entropy: -5.70241, alpha: 0.01787, time: 51.80834
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 266.70437, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.90621, qf2_loss: 0.91872, policy_loss: -68.19307, policy_entropy: -5.65370, alpha: 0.01775, time: 51.83230
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 282.14008, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.84214, qf2_loss: 0.84230, policy_loss: -69.40857, policy_entropy: -6.02478, alpha: 0.01769, time: 51.27046
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 41.97464, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.85487, qf2_loss: 0.86827, policy_loss: -68.43157, policy_entropy: -5.78311, alpha: 0.01769, time: 51.17412
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 34.64316, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.96864, qf2_loss: 0.96885, policy_loss: -68.83086, policy_entropy: -5.95889, alpha: 0.01764, time: 51.34649
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 37.43003, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.87403, qf2_loss: 0.87904, policy_loss: -69.17926, policy_entropy: -5.93264, alpha: 0.01762, time: 51.09744
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 238.19660, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 1.10008, qf2_loss: 1.10443, policy_loss: -68.70231, policy_entropy: -5.81872, alpha: 0.01758, time: 51.16246
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 110.20741, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 1.13902, qf2_loss: 1.15126, policy_loss: -68.65986, policy_entropy: -5.71769, alpha: 0.01746, time: 51.17053
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 260.31238, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 0.95109, qf2_loss: 0.95587, policy_loss: -68.79372, policy_entropy: -5.74764, alpha: 0.01736, time: 51.03957
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 231.68237, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 1.03658, qf2_loss: 1.04390, policy_loss: -69.66693, policy_entropy: -6.07265, alpha: 0.01732, time: 51.21035
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 31.54359, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 1.02509, qf2_loss: 1.02536, policy_loss: -69.01467, policy_entropy: -5.91773, alpha: 0.01731, time: 51.14293
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 188.12070, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.95638, qf2_loss: 0.96258, policy_loss: -69.07656, policy_entropy: -5.64461, alpha: 0.01723, time: 51.11511
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 166.22740, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.99408, qf2_loss: 0.99119, policy_loss: -70.04999, policy_entropy: -6.00816, alpha: 0.01713, time: 51.09359
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 43.53303, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.92876, qf2_loss: 0.93090, policy_loss: -69.42246, policy_entropy: -5.98645, alpha: 0.01714, time: 51.29099
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 227.80357, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 1.09531, qf2_loss: 1.10037, policy_loss: -70.01627, policy_entropy: -6.12371, alpha: 0.01719, time: 51.02058
[CW] eval: return: 207.07561, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 258.58210, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 1.21770, qf2_loss: 1.21235, policy_loss: -69.73663, policy_entropy: -5.89373, alpha: 0.01718, time: 51.05362
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 85.15197, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 1.02596, qf2_loss: 1.03705, policy_loss: -70.24910, policy_entropy: -6.01266, alpha: 0.01713, time: 51.13862
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 62.50843, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 1.01553, qf2_loss: 1.01412, policy_loss: -70.49620, policy_entropy: -6.06212, alpha: 0.01717, time: 50.90449
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 161.58843, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 1.03144, qf2_loss: 1.03163, policy_loss: -70.04150, policy_entropy: -6.26965, alpha: 0.01723, time: 51.11046
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 103.44165, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 1.65692, qf2_loss: 1.66326, policy_loss: -70.10466, policy_entropy: -6.07493, alpha: 0.01734, time: 50.88813
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 114.06258, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 1.09075, qf2_loss: 1.09409, policy_loss: -69.94083, policy_entropy: -6.19070, alpha: 0.01740, time: 51.07728
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 133.60614, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.98878, qf2_loss: 0.98320, policy_loss: -70.10197, policy_entropy: -6.36334, alpha: 0.01758, time: 50.73365
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 143.65769, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 1.08053, qf2_loss: 1.08167, policy_loss: -70.91667, policy_entropy: -6.61625, alpha: 0.01789, time: 52.99164
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 187.96689, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 1.25035, qf2_loss: 1.24944, policy_loss: -70.71619, policy_entropy: -6.25388, alpha: 0.01819, time: 50.87467
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 115.93717, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 1.49196, qf2_loss: 1.50071, policy_loss: -70.58199, policy_entropy: -6.30266, alpha: 0.01841, time: 51.04911
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 300.48156, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 1.59728, qf2_loss: 1.59645, policy_loss: -70.60545, policy_entropy: -6.13294, alpha: 0.01861, time: 50.98865
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 149.31353, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 1.22065, qf2_loss: 1.21693, policy_loss: -71.66489, policy_entropy: -6.03807, alpha: 0.01863, time: 50.69538
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 166.78866, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 1.27814, qf2_loss: 1.28278, policy_loss: -71.35146, policy_entropy: -6.18794, alpha: 0.01868, time: 50.83160
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 296.09122, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 1.15719, qf2_loss: 1.15759, policy_loss: -70.95769, policy_entropy: -6.14727, alpha: 0.01887, time: 50.97091
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 280.09254, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 1.14355, qf2_loss: 1.14036, policy_loss: -71.37262, policy_entropy: -6.00634, alpha: 0.01890, time: 50.79769
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 301.33237, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 1.07656, qf2_loss: 1.07453, policy_loss: -72.38549, policy_entropy: -6.06290, alpha: 0.01894, time: 51.22902
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 110.61602, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 3.48970, qf2_loss: 3.52005, policy_loss: -71.73736, policy_entropy: -6.13255, alpha: 0.01904, time: 50.99454
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 303.01148, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 1.99030, qf2_loss: 1.99492, policy_loss: -71.40525, policy_entropy: -5.77735, alpha: 0.01898, time: 51.00846
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 118.13067, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 1.42828, qf2_loss: 1.41597, policy_loss: -71.78197, policy_entropy: -6.47913, alpha: 0.01905, time: 50.80752
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 186.35858, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 1.17805, qf2_loss: 1.17012, policy_loss: -71.76506, policy_entropy: -5.98253, alpha: 0.01934, time: 50.76844
[CW] eval: return: 192.23388, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 359.61827, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 1.15584, qf2_loss: 1.15049, policy_loss: -72.36986, policy_entropy: -6.13020, alpha: 0.01931, time: 50.60295
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 257.63436, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 1.26924, qf2_loss: 1.26621, policy_loss: -73.42344, policy_entropy: -5.88565, alpha: 0.01934, time: 50.77004
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 312.19219, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 1.20487, qf2_loss: 1.20331, policy_loss: -72.97647, policy_entropy: -5.96747, alpha: 0.01929, time: 50.86288
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 84.60107, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 1.32446, qf2_loss: 1.33192, policy_loss: -73.46191, policy_entropy: -6.05859, alpha: 0.01928, time: 51.03292
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 328.04366, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 1.16399, qf2_loss: 1.16116, policy_loss: -73.31565, policy_entropy: -6.17888, alpha: 0.01940, time: 50.75812
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 124.30420, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 1.46682, qf2_loss: 1.45753, policy_loss: -72.52905, policy_entropy: -5.82443, alpha: 0.01941, time: 50.77250
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 329.78786, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 1.45494, qf2_loss: 1.45005, policy_loss: -73.27345, policy_entropy: -5.88648, alpha: 0.01926, time: 50.72424
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 246.57724, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 1.32961, qf2_loss: 1.33211, policy_loss: -73.58227, policy_entropy: -5.97729, alpha: 0.01921, time: 50.88597
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 221.38202, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 2.69179, qf2_loss: 2.67989, policy_loss: -73.69484, policy_entropy: -5.96720, alpha: 0.01921, time: 52.18299
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 303.56128, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 1.77328, qf2_loss: 1.77957, policy_loss: -73.93148, policy_entropy: -6.25392, alpha: 0.01932, time: 50.77424
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 351.72884, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 1.29657, qf2_loss: 1.28925, policy_loss: -74.17590, policy_entropy: -6.36559, alpha: 0.01947, time: 51.16336
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 183.76246, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 1.39015, qf2_loss: 1.37737, policy_loss: -73.93129, policy_entropy: -6.38491, alpha: 0.01979, time: 50.83610
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 355.14221, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 1.36717, qf2_loss: 1.35199, policy_loss: -74.81508, policy_entropy: -6.65626, alpha: 0.02027, time: 50.87582
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 332.05481, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 1.43078, qf2_loss: 1.43656, policy_loss: -74.66521, policy_entropy: -6.23003, alpha: 0.02064, time: 51.34630
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 321.80856, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 1.55496, qf2_loss: 1.54219, policy_loss: -74.81792, policy_entropy: -6.17348, alpha: 0.02081, time: 50.94516
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 58.03179, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 1.33494, qf2_loss: 1.33504, policy_loss: -75.24618, policy_entropy: -6.03493, alpha: 0.02092, time: 50.87520
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 203.60825, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 1.36375, qf2_loss: 1.36322, policy_loss: -74.15405, policy_entropy: -5.99820, alpha: 0.02094, time: 50.88367
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 253.44656, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 2.32749, qf2_loss: 2.33289, policy_loss: -75.30987, policy_entropy: -6.09164, alpha: 0.02104, time: 50.82786
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 261.12078, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 2.26851, qf2_loss: 2.26122, policy_loss: -74.73822, policy_entropy: -5.82759, alpha: 0.02092, time: 50.88572
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 382.67864, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 1.36789, qf2_loss: 1.36797, policy_loss: -75.11885, policy_entropy: -6.19261, alpha: 0.02095, time: 51.35162
[CW] eval: return: 242.66584, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 235.25949, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 1.27975, qf2_loss: 1.27308, policy_loss: -74.97983, policy_entropy: -6.16217, alpha: 0.02112, time: 51.26692
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 399.24118, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 1.37071, qf2_loss: 1.36657, policy_loss: -75.89648, policy_entropy: -6.30043, alpha: 0.02134, time: 51.40905
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 299.77959, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 1.31102, qf2_loss: 1.30624, policy_loss: -75.64830, policy_entropy: -6.24315, alpha: 0.02160, time: 51.22196
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 390.95860, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 1.64899, qf2_loss: 1.64836, policy_loss: -75.48741, policy_entropy: -6.33406, alpha: 0.02190, time: 51.33932
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 77.48774, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 2.29660, qf2_loss: 2.29554, policy_loss: -76.04817, policy_entropy: -6.03068, alpha: 0.02211, time: 51.05460
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 391.92382, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 1.74637, qf2_loss: 1.73012, policy_loss: -76.61060, policy_entropy: -5.98660, alpha: 0.02207, time: 51.14509
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 378.71525, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 1.40321, qf2_loss: 1.39881, policy_loss: -76.47314, policy_entropy: -6.11924, alpha: 0.02215, time: 51.36389
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 405.26629, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 1.51551, qf2_loss: 1.50807, policy_loss: -76.55370, policy_entropy: -6.01066, alpha: 0.02220, time: 51.32382
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 60.89243, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 1.56443, qf2_loss: 1.56334, policy_loss: -76.50716, policy_entropy: -6.10895, alpha: 0.02228, time: 51.13750
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 394.34944, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 1.62678, qf2_loss: 1.61468, policy_loss: -76.62923, policy_entropy: -6.00038, alpha: 0.02234, time: 51.22784
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 320.04988, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 1.89983, qf2_loss: 1.88925, policy_loss: -77.27828, policy_entropy: -6.21293, alpha: 0.02238, time: 51.14444
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 272.17459, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 1.84352, qf2_loss: 1.83296, policy_loss: -77.81162, policy_entropy: -6.14325, alpha: 0.02256, time: 51.18450
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 381.57699, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 1.88966, qf2_loss: 1.88023, policy_loss: -77.20068, policy_entropy: -5.94477, alpha: 0.02266, time: 50.94307
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 143.88681, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 2.02855, qf2_loss: 2.03598, policy_loss: -76.97176, policy_entropy: -5.95584, alpha: 0.02260, time: 51.00364
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 98.26044, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 1.89272, qf2_loss: 1.88401, policy_loss: -77.71259, policy_entropy: -6.21251, alpha: 0.02266, time: 50.96796
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 215.93955, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 1.90024, qf2_loss: 1.89474, policy_loss: -78.30335, policy_entropy: -6.20987, alpha: 0.02289, time: 51.10154
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 153.11334, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 1.78857, qf2_loss: 1.78746, policy_loss: -78.14278, policy_entropy: -6.24122, alpha: 0.02318, time: 51.45922
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 219.78597, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 1.97046, qf2_loss: 1.96047, policy_loss: -77.67383, policy_entropy: -5.93525, alpha: 0.02324, time: 51.16347
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 153.22563, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 2.13241, qf2_loss: 2.11667, policy_loss: -78.69243, policy_entropy: -6.31371, alpha: 0.02333, time: 51.20776
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 368.51675, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 2.06884, qf2_loss: 2.07130, policy_loss: -78.83993, policy_entropy: -6.12056, alpha: 0.02360, time: 51.26492
[CW] eval: return: 141.53658, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 85.27370, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 2.15389, qf2_loss: 2.14661, policy_loss: -78.28010, policy_entropy: -6.00839, alpha: 0.02369, time: 50.82375
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 89.07567, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 2.08076, qf2_loss: 2.07618, policy_loss: -79.05570, policy_entropy: -6.01047, alpha: 0.02373, time: 50.88783
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 137.08121, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 2.02124, qf2_loss: 2.00910, policy_loss: -78.39106, policy_entropy: -6.04087, alpha: 0.02372, time: 50.97035
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 111.63195, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 2.26531, qf2_loss: 2.26728, policy_loss: -79.04828, policy_entropy: -6.03119, alpha: 0.02374, time: 51.34468
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 118.65085, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 1.84066, qf2_loss: 1.81976, policy_loss: -79.27941, policy_entropy: -5.94836, alpha: 0.02377, time: 50.87328
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 125.14168, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 2.01278, qf2_loss: 2.00587, policy_loss: -78.25220, policy_entropy: -5.90261, alpha: 0.02363, time: 51.59600
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 162.92842, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 2.12944, qf2_loss: 2.12694, policy_loss: -78.31114, policy_entropy: -6.13038, alpha: 0.02361, time: 50.92131
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 128.35749, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 4.62943, qf2_loss: 4.64618, policy_loss: -78.35671, policy_entropy: -6.68149, alpha: 0.02411, time: 51.06661
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 259.16783, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 3.91386, qf2_loss: 3.89304, policy_loss: -78.42132, policy_entropy: -6.56682, alpha: 0.02475, time: 51.23105
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 435.69076, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 2.09587, qf2_loss: 2.07725, policy_loss: -78.32356, policy_entropy: -6.60395, alpha: 0.02554, time: 51.22776
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 430.42005, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 2.04055, qf2_loss: 2.00029, policy_loss: -79.54020, policy_entropy: -6.21397, alpha: 0.02600, time: 51.13001
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 425.80912, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 2.07051, qf2_loss: 2.03158, policy_loss: -78.92934, policy_entropy: -6.29765, alpha: 0.02635, time: 50.83670
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 433.55149, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 2.45199, qf2_loss: 2.43277, policy_loss: -79.90752, policy_entropy: -6.10184, alpha: 0.02663, time: 50.91183
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 382.25340, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 2.43724, qf2_loss: 2.41232, policy_loss: -80.17359, policy_entropy: -6.20723, alpha: 0.02680, time: 51.03960
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 404.37806, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 2.43197, qf2_loss: 2.42981, policy_loss: -80.53371, policy_entropy: -6.33872, alpha: 0.02725, time: 50.86098
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 68.66993, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 2.31438, qf2_loss: 2.30812, policy_loss: -79.50143, policy_entropy: -5.89969, alpha: 0.02746, time: 51.05756
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 252.78257, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 2.41145, qf2_loss: 2.38170, policy_loss: -80.06461, policy_entropy: -5.99195, alpha: 0.02731, time: 51.24075
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 425.53222, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 2.25156, qf2_loss: 2.25002, policy_loss: -81.22285, policy_entropy: -5.77334, alpha: 0.02727, time: 51.21221
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 392.29085, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 2.45644, qf2_loss: 2.43974, policy_loss: -81.24924, policy_entropy: -5.84639, alpha: 0.02680, time: 51.37410
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 417.88990, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 3.55278, qf2_loss: 3.53840, policy_loss: -80.91231, policy_entropy: -5.96499, alpha: 0.02668, time: 51.12966
[CW] eval: return: 209.28513, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 50.47870, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 2.83084, qf2_loss: 2.76756, policy_loss: -81.27765, policy_entropy: -5.54595, alpha: 0.02655, time: 51.24248
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 425.44956, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 3.10594, qf2_loss: 3.08300, policy_loss: -81.68778, policy_entropy: -5.31768, alpha: 0.02570, time: 50.91031
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 419.18405, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 2.52292, qf2_loss: 2.47761, policy_loss: -80.86928, policy_entropy: -5.47428, alpha: 0.02492, time: 50.95706
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 210.66024, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 2.30888, qf2_loss: 2.28645, policy_loss: -81.45546, policy_entropy: -5.97523, alpha: 0.02459, time: 51.01178
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 411.28836, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 2.24058, qf2_loss: 2.21877, policy_loss: -81.54217, policy_entropy: -6.02211, alpha: 0.02458, time: 50.99073
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 462.42823, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 2.17271, qf2_loss: 2.13385, policy_loss: -81.80821, policy_entropy: -6.14844, alpha: 0.02468, time: 51.31393
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 299.33708, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 2.61478, qf2_loss: 2.59162, policy_loss: -82.86014, policy_entropy: -6.37268, alpha: 0.02499, time: 51.27852
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 144.75938, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 4.25193, qf2_loss: 4.20502, policy_loss: -82.55201, policy_entropy: -6.15717, alpha: 0.02534, time: 51.25029
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 377.65203, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 2.54356, qf2_loss: 2.53457, policy_loss: -82.84357, policy_entropy: -5.96841, alpha: 0.02537, time: 51.12799
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 103.82500, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 2.49937, qf2_loss: 2.46638, policy_loss: -83.74773, policy_entropy: -6.11228, alpha: 0.02549, time: 51.09828
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 346.00955, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 3.39258, qf2_loss: 3.34924, policy_loss: -82.87427, policy_entropy: -6.14974, alpha: 0.02559, time: 51.02482
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 392.92086, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 2.70838, qf2_loss: 2.67085, policy_loss: -82.95256, policy_entropy: -6.01411, alpha: 0.02577, time: 51.02284
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 405.23340, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 2.47475, qf2_loss: 2.44971, policy_loss: -83.97868, policy_entropy: -6.24093, alpha: 0.02588, time: 51.06739
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 400.68305, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 2.48323, qf2_loss: 2.44755, policy_loss: -83.41619, policy_entropy: -6.06034, alpha: 0.02622, time: 51.24784
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 444.84560, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 2.64560, qf2_loss: 2.59803, policy_loss: -84.42067, policy_entropy: -6.07316, alpha: 0.02616, time: 51.09105
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 467.03704, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 2.64907, qf2_loss: 2.62398, policy_loss: -83.76197, policy_entropy: -6.07207, alpha: 0.02629, time: 51.08276
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 473.37256, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 2.96120, qf2_loss: 2.89957, policy_loss: -83.49542, policy_entropy: -5.99828, alpha: 0.02629, time: 51.85567
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 404.53965, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 4.13172, qf2_loss: 4.11855, policy_loss: -84.88063, policy_entropy: -6.13664, alpha: 0.02639, time: 51.29654
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 394.36221, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 6.53566, qf2_loss: 6.51388, policy_loss: -85.46968, policy_entropy: -6.29267, alpha: 0.02668, time: 51.17783
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 442.55839, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 3.12650, qf2_loss: 3.10107, policy_loss: -85.42377, policy_entropy: -6.00584, alpha: 0.02697, time: 51.07481
[CW] eval: return: 254.83861, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 50.69038, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 2.65652, qf2_loss: 2.64293, policy_loss: -85.97960, policy_entropy: -6.00495, alpha: 0.02699, time: 53.28767
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 313.14074, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 2.73730, qf2_loss: 2.72903, policy_loss: -84.79220, policy_entropy: -5.69299, alpha: 0.02674, time: 51.19769
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 120.55820, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 2.60495, qf2_loss: 2.55420, policy_loss: -85.32829, policy_entropy: -6.26328, alpha: 0.02663, time: 51.17613
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 191.90009, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 2.55274, qf2_loss: 2.53940, policy_loss: -85.80882, policy_entropy: -6.78247, alpha: 0.02737, time: 51.01306
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 432.55336, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 2.81181, qf2_loss: 2.78615, policy_loss: -85.38279, policy_entropy: -6.50434, alpha: 0.02826, time: 50.96782
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 410.38649, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 2.81339, qf2_loss: 2.78461, policy_loss: -87.19909, policy_entropy: -6.42385, alpha: 0.02892, time: 51.07373
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 429.95697, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 2.97559, qf2_loss: 2.90594, policy_loss: -86.19142, policy_entropy: -6.18056, alpha: 0.02937, time: 51.36415
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 417.26759, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 3.41781, qf2_loss: 3.39072, policy_loss: -85.80623, policy_entropy: -6.38620, alpha: 0.02977, time: 52.64442
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 469.03263, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 3.16768, qf2_loss: 3.17211, policy_loss: -87.20670, policy_entropy: -6.07353, alpha: 0.03029, time: 51.24288
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 313.15111, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 4.15599, qf2_loss: 4.13203, policy_loss: -87.14180, policy_entropy: -6.11312, alpha: 0.03033, time: 51.01383
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 417.14744, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 3.84281, qf2_loss: 3.79354, policy_loss: -88.04357, policy_entropy: -6.32368, alpha: 0.03063, time: 51.22384
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 376.82238, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 3.43290, qf2_loss: 3.40836, policy_loss: -86.79062, policy_entropy: -6.20904, alpha: 0.03117, time: 51.32970
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 437.25713, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 2.76430, qf2_loss: 2.73453, policy_loss: -87.45650, policy_entropy: -6.35994, alpha: 0.03168, time: 51.67943
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 296.97671, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 3.46258, qf2_loss: 3.41786, policy_loss: -87.98716, policy_entropy: -6.50579, alpha: 0.03250, time: 51.07174
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 419.45053, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 3.70944, qf2_loss: 3.67540, policy_loss: -88.80475, policy_entropy: -6.31487, alpha: 0.03332, time: 50.94115
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 438.01309, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 3.20604, qf2_loss: 3.20088, policy_loss: -88.69516, policy_entropy: -6.29657, alpha: 0.03392, time: 50.98694
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 415.49532, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 3.14780, qf2_loss: 3.09966, policy_loss: -89.74672, policy_entropy: -6.29832, alpha: 0.03451, time: 51.48716
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 388.93276, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 3.25398, qf2_loss: 3.23083, policy_loss: -90.03752, policy_entropy: -6.15320, alpha: 0.03506, time: 51.25594
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 460.65380, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 3.01284, qf2_loss: 3.00017, policy_loss: -88.55282, policy_entropy: -6.18531, alpha: 0.03551, time: 51.26281
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 481.21939, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 3.13082, qf2_loss: 3.12125, policy_loss: -89.44164, policy_entropy: -6.18667, alpha: 0.03598, time: 51.18816
[CW] eval: return: 411.47343, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 318.12090, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 2.89057, qf2_loss: 2.85699, policy_loss: -89.35593, policy_entropy: -6.03965, alpha: 0.03631, time: 50.92945
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 459.77343, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 3.21536, qf2_loss: 3.18657, policy_loss: -90.17897, policy_entropy: -6.17060, alpha: 0.03642, time: 50.78465
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 456.78793, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 2.88000, qf2_loss: 2.83481, policy_loss: -90.25693, policy_entropy: -6.01146, alpha: 0.03666, time: 51.07121
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 439.55681, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 3.24705, qf2_loss: 3.20676, policy_loss: -90.71469, policy_entropy: -6.21287, alpha: 0.03696, time: 50.90505
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 476.64973, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 3.34616, qf2_loss: 3.30133, policy_loss: -90.76475, policy_entropy: -6.04266, alpha: 0.03755, time: 50.88994
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 476.20487, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 4.00844, qf2_loss: 3.93329, policy_loss: -91.75135, policy_entropy: -5.89446, alpha: 0.03732, time: 50.73473
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 255.42222, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 9.73983, qf2_loss: 9.73812, policy_loss: -91.58824, policy_entropy: -5.85840, alpha: 0.03697, time: 50.75925
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 393.34717, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 5.17955, qf2_loss: 5.11764, policy_loss: -92.24396, policy_entropy: -5.95881, alpha: 0.03670, time: 50.68676
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 423.43524, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 3.31075, qf2_loss: 3.26793, policy_loss: -91.64791, policy_entropy: -5.78739, alpha: 0.03628, time: 50.84739
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 420.84891, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 3.60485, qf2_loss: 3.56903, policy_loss: -91.61788, policy_entropy: -5.69890, alpha: 0.03580, time: 50.83185
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 463.64710, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 3.21910, qf2_loss: 3.15046, policy_loss: -91.15664, policy_entropy: -5.67054, alpha: 0.03501, time: 52.21667
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 108.01346, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 2.87475, qf2_loss: 2.82042, policy_loss: -92.89741, policy_entropy: -5.73729, alpha: 0.03438, time: 51.16913
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 366.58455, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 2.75754, qf2_loss: 2.72482, policy_loss: -92.32462, policy_entropy: -5.46169, alpha: 0.03357, time: 51.23172
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 483.36551, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 2.64735, qf2_loss: 2.61566, policy_loss: -92.61829, policy_entropy: -5.69693, alpha: 0.03278, time: 50.90886
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 468.55312, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 2.91043, qf2_loss: 2.82044, policy_loss: -93.08278, policy_entropy: -5.41543, alpha: 0.03208, time: 51.22972
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 438.99926, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 2.99162, qf2_loss: 2.95997, policy_loss: -94.27633, policy_entropy: -5.54302, alpha: 0.03124, time: 51.19726
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 69.95960, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 2.98003, qf2_loss: 2.92609, policy_loss: -93.98695, policy_entropy: -5.65633, alpha: 0.03066, time: 51.15747
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 487.89526, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 3.32296, qf2_loss: 3.27640, policy_loss: -93.53144, policy_entropy: -5.53404, alpha: 0.03019, time: 51.15431
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 80.66870, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 3.05641, qf2_loss: 3.00197, policy_loss: -94.39056, policy_entropy: -5.60839, alpha: 0.02954, time: 50.92218
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 366.40859, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 2.89915, qf2_loss: 2.86161, policy_loss: -93.93400, policy_entropy: -5.69363, alpha: 0.02910, time: 50.78856
[CW] eval: return: 314.55264, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 324.14109, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 3.42797, qf2_loss: 3.37910, policy_loss: -94.38932, policy_entropy: -5.90745, alpha: 0.02889, time: 51.31327
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 320.79641, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 3.38725, qf2_loss: 3.30750, policy_loss: -94.95174, policy_entropy: -5.75662, alpha: 0.02873, time: 51.26353
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 401.85226, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 3.02324, qf2_loss: 3.00051, policy_loss: -93.82912, policy_entropy: -5.81142, alpha: 0.02848, time: 51.38600
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 351.52639, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 3.98786, qf2_loss: 3.98619, policy_loss: -95.37102, policy_entropy: -6.06182, alpha: 0.02833, time: 51.35797
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 322.25049, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 4.78972, qf2_loss: 4.71045, policy_loss: -95.17691, policy_entropy: -5.97830, alpha: 0.02836, time: 51.16339
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 345.64351, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 4.05542, qf2_loss: 3.98255, policy_loss: -95.98540, policy_entropy: -6.34537, alpha: 0.02851, time: 51.07269
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 270.46988, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 3.02612, qf2_loss: 2.95944, policy_loss: -95.89063, policy_entropy: -6.05464, alpha: 0.02888, time: 51.23425
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 367.50493, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 3.54434, qf2_loss: 3.47342, policy_loss: -95.63804, policy_entropy: -5.97896, alpha: 0.02889, time: 51.15869
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 252.76504, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 3.33169, qf2_loss: 3.25282, policy_loss: -96.18757, policy_entropy: -6.46056, alpha: 0.02904, time: 51.01248
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 98.71536, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 3.15533, qf2_loss: 3.13409, policy_loss: -96.03611, policy_entropy: -6.25157, alpha: 0.02966, time: 51.61827
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 330.97497, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 2.88050, qf2_loss: 2.81811, policy_loss: -96.50866, policy_entropy: -5.63754, alpha: 0.02962, time: 51.33256
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 298.35786, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 2.99187, qf2_loss: 2.93592, policy_loss: -96.56983, policy_entropy: -5.90073, alpha: 0.02920, time: 51.16298
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 349.77824, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 3.29078, qf2_loss: 3.24385, policy_loss: -97.48638, policy_entropy: -6.24378, alpha: 0.02933, time: 51.14080
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 302.05096, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 4.12636, qf2_loss: 4.10811, policy_loss: -96.91967, policy_entropy: -6.25121, alpha: 0.02964, time: 51.26154
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 373.10610, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 3.49560, qf2_loss: 3.47177, policy_loss: -97.94168, policy_entropy: -6.14892, alpha: 0.03002, time: 51.03979
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 314.99706, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 3.51577, qf2_loss: 3.45164, policy_loss: -96.66506, policy_entropy: -5.85383, alpha: 0.03001, time: 51.10747
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 353.01254, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 3.26251, qf2_loss: 3.22425, policy_loss: -97.92567, policy_entropy: -6.05488, alpha: 0.02995, time: 51.00088
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 324.29691, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 3.36524, qf2_loss: 3.33241, policy_loss: -97.47568, policy_entropy: -6.15303, alpha: 0.03001, time: 51.26366
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 307.55011, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 3.71418, qf2_loss: 3.66432, policy_loss: -97.65060, policy_entropy: -6.03289, alpha: 0.03024, time: 51.08159
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 90.58697, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 3.44422, qf2_loss: 3.40235, policy_loss: -97.61507, policy_entropy: -6.01715, alpha: 0.03023, time: 50.90724
[CW] eval: return: 339.64455, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 367.74918, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 3.24970, qf2_loss: 3.20160, policy_loss: -97.83139, policy_entropy: -6.14714, alpha: 0.03044, time: 51.11842
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 380.57267, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 3.35085, qf2_loss: 3.30429, policy_loss: -98.39658, policy_entropy: -6.05577, alpha: 0.03056, time: 50.96271
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 27.92974, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 4.52818, qf2_loss: 4.54404, policy_loss: -99.55574, policy_entropy: -6.15461, alpha: 0.03076, time: 51.17837
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 151.70060, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 4.50490, qf2_loss: 4.48649, policy_loss: -98.76678, policy_entropy: -5.74904, alpha: 0.03071, time: 50.98917
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 326.88314, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 4.33604, qf2_loss: 4.30219, policy_loss: -98.11647, policy_entropy: -5.76160, alpha: 0.03027, time: 51.08431
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 418.06638, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 3.69100, qf2_loss: 3.67287, policy_loss: -99.07650, policy_entropy: -5.96498, alpha: 0.03005, time: 51.99486
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 313.96352, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 3.59345, qf2_loss: 3.55047, policy_loss: -98.97989, policy_entropy: -5.94468, alpha: 0.02995, time: 51.05913
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 15.37685, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 3.48250, qf2_loss: 3.46039, policy_loss: -98.80564, policy_entropy: -5.95342, alpha: 0.02988, time: 50.99308
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 475.80513, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 3.57614, qf2_loss: 3.51155, policy_loss: -99.21424, policy_entropy: -6.21165, alpha: 0.02996, time: 50.87557
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 116.47101, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 3.24630, qf2_loss: 3.22189, policy_loss: -99.12436, policy_entropy: -5.94605, alpha: 0.03012, time: 51.37903
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 378.51311, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 5.06809, qf2_loss: 5.01226, policy_loss: -100.35101, policy_entropy: -6.12736, alpha: 0.03014, time: 51.28855
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 400.02266, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 3.67638, qf2_loss: 3.61953, policy_loss: -99.35946, policy_entropy: -6.10516, alpha: 0.03039, time: 51.06870
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 413.79349, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 3.51512, qf2_loss: 3.45041, policy_loss: -98.68663, policy_entropy: -6.07136, alpha: 0.03043, time: 50.98401
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 409.54891, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 5.01424, qf2_loss: 4.97345, policy_loss: -100.00568, policy_entropy: -6.39913, alpha: 0.03084, time: 50.85726
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 475.75557, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 3.81773, qf2_loss: 3.74049, policy_loss: -100.45952, policy_entropy: -6.52628, alpha: 0.03163, time: 50.95326
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 345.71859, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 3.78972, qf2_loss: 3.72022, policy_loss: -100.34460, policy_entropy: -6.50395, alpha: 0.03254, time: 51.31455
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 337.63004, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 4.02665, qf2_loss: 3.99542, policy_loss: -100.69748, policy_entropy: -6.34977, alpha: 0.03340, time: 51.34873
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 392.21661, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 3.45411, qf2_loss: 3.39398, policy_loss: -101.99141, policy_entropy: -6.22661, alpha: 0.03395, time: 51.58846
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 387.84713, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 3.11815, qf2_loss: 3.06900, policy_loss: -100.61913, policy_entropy: -6.03531, alpha: 0.03416, time: 51.58940
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 441.55753, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 3.48952, qf2_loss: 3.44400, policy_loss: -100.67384, policy_entropy: -6.07937, alpha: 0.03439, time: 51.40969
[CW] eval: return: 409.23627, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 150.99250, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 4.28226, qf2_loss: 4.18131, policy_loss: -100.09961, policy_entropy: -5.84397, alpha: 0.03429, time: 51.23028
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 460.51203, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 5.30137, qf2_loss: 5.18989, policy_loss: -100.45310, policy_entropy: -6.08917, alpha: 0.03414, time: 51.15105
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 414.67998, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 3.72943, qf2_loss: 3.68124, policy_loss: -101.99250, policy_entropy: -6.15432, alpha: 0.03440, time: 51.00981
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 11.58743, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 4.50197, qf2_loss: 4.40543, policy_loss: -101.02098, policy_entropy: -5.67077, alpha: 0.03431, time: 51.07845
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 369.66584, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 4.37241, qf2_loss: 4.33597, policy_loss: -101.74316, policy_entropy: -5.71787, alpha: 0.03366, time: 51.05272
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 408.35541, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 4.34550, qf2_loss: 4.29841, policy_loss: -102.03214, policy_entropy: -5.73113, alpha: 0.03317, time: 53.44632
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 431.08596, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 3.38879, qf2_loss: 3.37854, policy_loss: -102.21849, policy_entropy: -5.77685, alpha: 0.03264, time: 51.01731
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 339.68949, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 3.54786, qf2_loss: 3.49458, policy_loss: -103.59970, policy_entropy: -5.84856, alpha: 0.03233, time: 51.23877
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 444.33366, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 4.82412, qf2_loss: 4.74493, policy_loss: -101.84120, policy_entropy: -5.63807, alpha: 0.03193, time: 51.01133
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 429.61298, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 4.48182, qf2_loss: 4.43937, policy_loss: -103.13903, policy_entropy: -6.03134, alpha: 0.03157, time: 51.14699
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 473.70301, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 3.60350, qf2_loss: 3.53673, policy_loss: -103.45296, policy_entropy: -6.07991, alpha: 0.03179, time: 50.95773
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 210.85162, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 4.64000, qf2_loss: 4.59360, policy_loss: -104.44250, policy_entropy: -6.03644, alpha: 0.03187, time: 51.31532
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 94.69304, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 3.58804, qf2_loss: 3.54326, policy_loss: -104.08253, policy_entropy: -5.86478, alpha: 0.03173, time: 51.18113
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 451.03064, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 3.28911, qf2_loss: 3.23781, policy_loss: -104.88360, policy_entropy: -6.11254, alpha: 0.03168, time: 50.94237
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 422.05095, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 4.01523, qf2_loss: 3.97278, policy_loss: -103.99490, policy_entropy: -5.93177, alpha: 0.03183, time: 51.15160
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 397.86164, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 3.71820, qf2_loss: 3.65745, policy_loss: -105.16879, policy_entropy: -5.93511, alpha: 0.03172, time: 51.58872
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 419.31432, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 3.91388, qf2_loss: 3.86322, policy_loss: -105.82560, policy_entropy: -5.96022, alpha: 0.03161, time: 51.35924
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 436.19435, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 3.85088, qf2_loss: 3.76663, policy_loss: -105.67676, policy_entropy: -5.93932, alpha: 0.03151, time: 51.12470
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 353.08077, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 4.59050, qf2_loss: 4.48445, policy_loss: -106.16593, policy_entropy: -5.94375, alpha: 0.03145, time: 51.19209
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 522.71593, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 4.73073, qf2_loss: 4.69637, policy_loss: -107.02211, policy_entropy: -6.08347, alpha: 0.03149, time: 51.30751
[CW] eval: return: 466.99119, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 461.63214, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 3.52503, qf2_loss: 3.48791, policy_loss: -105.13545, policy_entropy: -5.71569, alpha: 0.03132, time: 51.15561
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 444.60403, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 4.48659, qf2_loss: 4.41891, policy_loss: -107.31510, policy_entropy: -6.14542, alpha: 0.03125, time: 51.01155
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 480.88043, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 7.65059, qf2_loss: 7.63145, policy_loss: -107.01029, policy_entropy: -6.08312, alpha: 0.03133, time: 51.45154
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 167.96693, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 5.48580, qf2_loss: 5.41357, policy_loss: -106.54954, policy_entropy: -6.04051, alpha: 0.03157, time: 51.28772
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 471.26361, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 3.93962, qf2_loss: 3.90845, policy_loss: -106.94325, policy_entropy: -5.80828, alpha: 0.03140, time: 50.93861
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 479.21128, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 4.25078, qf2_loss: 4.14459, policy_loss: -105.84767, policy_entropy: -5.85728, alpha: 0.03118, time: 51.13362
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 389.80929, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 5.36394, qf2_loss: 5.32861, policy_loss: -106.53483, policy_entropy: -5.72304, alpha: 0.03091, time: 51.94768
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 501.29847, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 4.64945, qf2_loss: 4.59282, policy_loss: -108.28240, policy_entropy: -6.07077, alpha: 0.03074, time: 51.47174
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 502.13988, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 3.98569, qf2_loss: 3.91467, policy_loss: -107.62514, policy_entropy: -5.85066, alpha: 0.03073, time: 51.40282
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 419.84927, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 5.71134, qf2_loss: 5.69248, policy_loss: -108.06938, policy_entropy: -5.89470, alpha: 0.03056, time: 51.31411
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 553.07055, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 8.04851, qf2_loss: 7.98591, policy_loss: -107.26959, policy_entropy: -5.85666, alpha: 0.03035, time: 51.47664
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 501.18714, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 3.88301, qf2_loss: 3.83691, policy_loss: -108.23026, policy_entropy: -5.92276, alpha: 0.03020, time: 51.30333
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 482.82650, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 3.71068, qf2_loss: 3.67986, policy_loss: -108.39212, policy_entropy: -5.81719, alpha: 0.03009, time: 51.25971
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 440.07798, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 4.26844, qf2_loss: 4.17777, policy_loss: -108.62479, policy_entropy: -6.04397, alpha: 0.02998, time: 51.38521
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 463.79193, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 4.89450, qf2_loss: 4.78173, policy_loss: -108.23301, policy_entropy: -5.97844, alpha: 0.02995, time: 51.02875
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 489.63848, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 5.09855, qf2_loss: 5.09096, policy_loss: -109.30802, policy_entropy: -5.95935, alpha: 0.02989, time: 51.04751
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 486.51505, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 4.99731, qf2_loss: 4.90218, policy_loss: -108.78327, policy_entropy: -6.08002, alpha: 0.02992, time: 50.99819
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 523.06045, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 15.64973, qf2_loss: 15.80429, policy_loss: -109.10327, policy_entropy: -6.33802, alpha: 0.03005, time: 51.09683
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 440.85862, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 7.58478, qf2_loss: 7.50693, policy_loss: -111.25981, policy_entropy: -6.34298, alpha: 0.03069, time: 51.16550
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 420.07177, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 4.22818, qf2_loss: 4.18825, policy_loss: -110.79871, policy_entropy: -5.97190, alpha: 0.03086, time: 51.38075
[CW] eval: return: 428.30275, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 427.58876, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 3.82255, qf2_loss: 3.76495, policy_loss: -110.28695, policy_entropy: -6.31994, alpha: 0.03097, time: 50.94497
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 437.34504, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 4.50645, qf2_loss: 4.40883, policy_loss: -111.26775, policy_entropy: -6.53115, alpha: 0.03157, time: 51.00682
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 502.63291, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 3.77797, qf2_loss: 3.69312, policy_loss: -109.87996, policy_entropy: -6.22407, alpha: 0.03221, time: 51.14830
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 453.64301, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 3.78603, qf2_loss: 3.72289, policy_loss: -111.46634, policy_entropy: -6.16020, alpha: 0.03249, time: 51.08715
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 443.71481, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 3.91937, qf2_loss: 3.82122, policy_loss: -109.75988, policy_entropy: -6.06828, alpha: 0.03263, time: 50.73901
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 436.62255, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 4.17443, qf2_loss: 4.18075, policy_loss: -111.94600, policy_entropy: -6.09006, alpha: 0.03281, time: 51.03818
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 530.71196, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 3.94236, qf2_loss: 3.82890, policy_loss: -112.74984, policy_entropy: -6.10309, alpha: 0.03294, time: 50.99630
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 530.79375, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 3.66043, qf2_loss: 3.59634, policy_loss: -111.52823, policy_entropy: -5.92530, alpha: 0.03305, time: 51.08402
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 498.24262, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 3.90398, qf2_loss: 3.83625, policy_loss: -113.04838, policy_entropy: -6.09380, alpha: 0.03295, time: 51.03530
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 516.92287, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 4.03382, qf2_loss: 3.97499, policy_loss: -113.32600, policy_entropy: -6.05921, alpha: 0.03310, time: 50.84611
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 528.61434, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 4.41260, qf2_loss: 4.38523, policy_loss: -112.80886, policy_entropy: -5.95097, alpha: 0.03312, time: 51.09890
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 518.36644, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 4.99205, qf2_loss: 4.82948, policy_loss: -113.63453, policy_entropy: -6.04683, alpha: 0.03308, time: 51.78086
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 469.61954, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 4.05314, qf2_loss: 3.97429, policy_loss: -113.47356, policy_entropy: -5.97609, alpha: 0.03313, time: 51.68583
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 524.21902, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 4.66500, qf2_loss: 4.61934, policy_loss: -113.50615, policy_entropy: -5.84166, alpha: 0.03304, time: 51.65678
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 511.16934, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 4.74784, qf2_loss: 4.72105, policy_loss: -113.66699, policy_entropy: -5.90823, alpha: 0.03283, time: 51.58433
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 534.35483, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 4.35226, qf2_loss: 4.31792, policy_loss: -114.44511, policy_entropy: -5.89300, alpha: 0.03264, time: 51.73402
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 462.52544, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 5.27780, qf2_loss: 5.10872, policy_loss: -113.95652, policy_entropy: -5.88091, alpha: 0.03246, time: 51.53836
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 390.56656, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 5.34710, qf2_loss: 5.35613, policy_loss: -114.78561, policy_entropy: -6.02550, alpha: 0.03239, time: 51.53036
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 217.51416, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 4.96255, qf2_loss: 4.88059, policy_loss: -115.23769, policy_entropy: -6.05771, alpha: 0.03244, time: 51.36350
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 519.53477, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 5.08848, qf2_loss: 5.02841, policy_loss: -115.92670, policy_entropy: -5.95416, alpha: 0.03250, time: 51.42133
[CW] eval: return: 424.61842, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 504.59237, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 5.99391, qf2_loss: 5.91009, policy_loss: -115.24965, policy_entropy: -5.95648, alpha: 0.03238, time: 51.37598
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 402.25650, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 9.11774, qf2_loss: 9.05047, policy_loss: -117.10192, policy_entropy: -6.06591, alpha: 0.03243, time: 51.21466
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 411.49716, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 5.04391, qf2_loss: 4.98315, policy_loss: -116.43823, policy_entropy: -6.06576, alpha: 0.03251, time: 51.52074
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 458.65510, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 4.98966, qf2_loss: 4.91285, policy_loss: -116.81126, policy_entropy: -6.04988, alpha: 0.03258, time: 51.49900
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 220.75106, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 4.67069, qf2_loss: 4.59031, policy_loss: -117.00219, policy_entropy: -6.07123, alpha: 0.03263, time: 51.32736
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 128.96668, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 4.15361, qf2_loss: 4.10966, policy_loss: -116.26262, policy_entropy: -5.92392, alpha: 0.03274, time: 51.88858
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 339.77501, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 5.79754, qf2_loss: 5.79217, policy_loss: -117.98169, policy_entropy: -5.98624, alpha: 0.03264, time: 51.69725
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 425.91142, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 6.53052, qf2_loss: 6.47314, policy_loss: -117.38412, policy_entropy: -5.92178, alpha: 0.03254, time: 50.88704
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 521.62673, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 7.83551, qf2_loss: 7.72851, policy_loss: -117.38397, policy_entropy: -5.86430, alpha: 0.03245, time: 50.97102
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 336.78319, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 5.98894, qf2_loss: 5.99011, policy_loss: -116.33523, policy_entropy: -5.90974, alpha: 0.03221, time: 51.70760
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 394.48088, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 5.13702, qf2_loss: 5.00237, policy_loss: -116.45085, policy_entropy: -6.19240, alpha: 0.03224, time: 51.76106
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 467.12845, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 4.54363, qf2_loss: 4.49606, policy_loss: -117.60422, policy_entropy: -6.09796, alpha: 0.03249, time: 51.74516
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 453.06400, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 4.47270, qf2_loss: 4.38238, policy_loss: -118.11265, policy_entropy: -6.04432, alpha: 0.03255, time: 51.76071
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 486.87123, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 4.87170, qf2_loss: 4.78402, policy_loss: -119.83114, policy_entropy: -6.00910, alpha: 0.03269, time: 52.97548
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 505.33465, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 4.73722, qf2_loss: 4.60029, policy_loss: -117.77612, policy_entropy: -6.05423, alpha: 0.03269, time: 51.75910
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 477.48888, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 4.16834, qf2_loss: 4.07857, policy_loss: -118.69863, policy_entropy: -6.13935, alpha: 0.03288, time: 51.87767
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 418.42902, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 4.94840, qf2_loss: 4.84774, policy_loss: -117.57629, policy_entropy: -5.80392, alpha: 0.03289, time: 51.74036
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 461.91455, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 4.19233, qf2_loss: 4.13360, policy_loss: -118.13410, policy_entropy: -5.78642, alpha: 0.03256, time: 51.95114
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 455.44412, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 4.10093, qf2_loss: 4.02379, policy_loss: -119.36056, policy_entropy: -5.79521, alpha: 0.03210, time: 52.06938
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 341.27723, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 21.33963, qf2_loss: 20.76846, policy_loss: -117.90669, policy_entropy: -5.54833, alpha: 0.03177, time: 51.73237
[CW] eval: return: 344.82479, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 306.57743, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 17.90866, qf2_loss: 17.84657, policy_loss: -118.12237, policy_entropy: -5.87737, alpha: 0.03120, time: 52.05970
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 402.97973, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 7.41215, qf2_loss: 7.43014, policy_loss: -119.13376, policy_entropy: -6.39405, alpha: 0.03141, time: 51.91299
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 393.55075, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 4.84169, qf2_loss: 4.73548, policy_loss: -120.86875, policy_entropy: -6.35602, alpha: 0.03198, time: 51.86233
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 462.74387, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 4.60625, qf2_loss: 4.47421, policy_loss: -121.39460, policy_entropy: -6.23881, alpha: 0.03239, time: 51.73479
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 383.35992, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 4.55099, qf2_loss: 4.44539, policy_loss: -119.71569, policy_entropy: -5.99300, alpha: 0.03265, time: 51.79833
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 336.11422, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 4.73441, qf2_loss: 4.63961, policy_loss: -120.56596, policy_entropy: -5.96634, alpha: 0.03260, time: 52.29555
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 385.72452, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 4.83414, qf2_loss: 4.72771, policy_loss: -121.07055, policy_entropy: -6.02768, alpha: 0.03261, time: 51.76074
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 564.47192, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 5.68350, qf2_loss: 5.61239, policy_loss: -120.54996, policy_entropy: -6.11783, alpha: 0.03263, time: 51.79989
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 42.54457, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 4.27247, qf2_loss: 4.20968, policy_loss: -121.40520, policy_entropy: -6.17916, alpha: 0.03288, time: 51.93196
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 530.80070, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 4.29922, qf2_loss: 4.19283, policy_loss: -122.15711, policy_entropy: -6.20371, alpha: 0.03321, time: 51.99101
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 417.96551, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 4.16883, qf2_loss: 4.09437, policy_loss: -121.77679, policy_entropy: -6.17558, alpha: 0.03362, time: 52.12373
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 10.73639, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 4.54809, qf2_loss: 4.41647, policy_loss: -122.21820, policy_entropy: -6.01933, alpha: 0.03373, time: 51.89393
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 363.59114, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 4.23628, qf2_loss: 4.11398, policy_loss: -121.83172, policy_entropy: -6.06564, alpha: 0.03377, time: 51.76935
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 462.56441, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 5.03798, qf2_loss: 4.92260, policy_loss: -120.39833, policy_entropy: -5.82609, alpha: 0.03379, time: 52.30687
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 526.81201, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 4.67318, qf2_loss: 4.58185, policy_loss: -121.65574, policy_entropy: -6.08078, alpha: 0.03358, time: 51.79871
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 531.08203, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 4.39938, qf2_loss: 4.31285, policy_loss: -122.26288, policy_entropy: -5.99705, alpha: 0.03371, time: 51.67872
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 488.23998, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 4.80124, qf2_loss: 4.69629, policy_loss: -121.65736, policy_entropy: -6.01947, alpha: 0.03375, time: 51.96461
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 518.71727, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 4.60107, qf2_loss: 4.54069, policy_loss: -123.29521, policy_entropy: -6.19848, alpha: 0.03391, time: 51.92414
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 547.02479, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 10.68467, qf2_loss: 10.48849, policy_loss: -121.13438, policy_entropy: -6.26319, alpha: 0.03418, time: 51.66576
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 475.35292, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 19.30297, qf2_loss: 19.02040, policy_loss: -121.92999, policy_entropy: -6.50780, alpha: 0.03496, time: 52.01763
[CW] eval: return: 406.42842, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 532.97745, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 9.68357, qf2_loss: 9.49903, policy_loss: -121.89970, policy_entropy: -6.48601, alpha: 0.03572, time: 52.28787
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 542.43061, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 6.19015, qf2_loss: 6.08970, policy_loss: -123.86331, policy_entropy: -6.38150, alpha: 0.03663, time: 51.85056
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 516.10138, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 4.89576, qf2_loss: 4.79481, policy_loss: -122.72579, policy_entropy: -6.00746, alpha: 0.03709, time: 51.81187
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 575.94425, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 5.15430, qf2_loss: 5.02325, policy_loss: -123.77189, policy_entropy: -6.08419, alpha: 0.03711, time: 52.23445
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 559.46655, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 4.89731, qf2_loss: 4.75402, policy_loss: -122.29919, policy_entropy: -6.00050, alpha: 0.03717, time: 52.29213
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 555.58005, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 4.51827, qf2_loss: 4.39792, policy_loss: -123.51741, policy_entropy: -6.01661, alpha: 0.03720, time: 51.92657
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 368.67778, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 5.08752, qf2_loss: 4.93558, policy_loss: -124.54148, policy_entropy: -6.13820, alpha: 0.03739, time: 51.94198
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 512.97334, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 5.17286, qf2_loss: 5.01021, policy_loss: -123.12994, policy_entropy: -6.00553, alpha: 0.03746, time: 51.85592
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 550.55361, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 5.75759, qf2_loss: 5.53344, policy_loss: -124.06361, policy_entropy: -6.07110, alpha: 0.03750, time: 51.74574
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 531.88554, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 5.01621, qf2_loss: 4.90235, policy_loss: -124.75756, policy_entropy: -5.96936, alpha: 0.03755, time: 51.84222
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 563.18729, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 4.88596, qf2_loss: 4.76044, policy_loss: -124.78459, policy_entropy: -5.96995, alpha: 0.03749, time: 51.87880
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 590.54239, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 5.29188, qf2_loss: 5.18896, policy_loss: -124.54281, policy_entropy: -6.21073, alpha: 0.03763, time: 51.74514
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 527.35072, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 4.90833, qf2_loss: 4.75851, policy_loss: -124.83127, policy_entropy: -6.06864, alpha: 0.03795, time: 51.85377
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 560.43554, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 4.97778, qf2_loss: 4.86805, policy_loss: -127.41599, policy_entropy: -6.14118, alpha: 0.03818, time: 52.00082
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 567.90847, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 5.29246, qf2_loss: 5.17478, policy_loss: -125.43652, policy_entropy: -5.82821, alpha: 0.03823, time: 51.80765
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 542.83717, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 6.04903, qf2_loss: 5.93487, policy_loss: -125.79740, policy_entropy: -6.01224, alpha: 0.03807, time: 51.86727
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 497.58846, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 5.25035, qf2_loss: 5.10814, policy_loss: -125.93051, policy_entropy: -5.90809, alpha: 0.03783, time: 51.82199
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 529.44337, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 5.37872, qf2_loss: 5.25216, policy_loss: -125.64935, policy_entropy: -5.96508, alpha: 0.03777, time: 51.76066
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 570.51157, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 5.56354, qf2_loss: 5.55561, policy_loss: -126.89863, policy_entropy: -6.04507, alpha: 0.03785, time: 51.82777
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 45.08891, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 5.97376, qf2_loss: 5.77971, policy_loss: -126.56826, policy_entropy: -5.96593, alpha: 0.03780, time: 52.00079
[CW] eval: return: 530.69791, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 586.86154, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 6.01878, qf2_loss: 5.99203, policy_loss: -128.23544, policy_entropy: -6.07484, alpha: 0.03777, time: 51.83163
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 531.43398, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 7.65899, qf2_loss: 7.52887, policy_loss: -128.48724, policy_entropy: -5.99667, alpha: 0.03797, time: 51.69296
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 553.94065, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 8.30708, qf2_loss: 8.08633, policy_loss: -128.68203, policy_entropy: -5.95044, alpha: 0.03786, time: 51.84151
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 536.18535, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 22.29993, qf2_loss: 22.29793, policy_loss: -127.98549, policy_entropy: -6.40684, alpha: 0.03800, time: 51.82182
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 416.82740, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 10.05868, qf2_loss: 10.02337, policy_loss: -129.32150, policy_entropy: -6.50257, alpha: 0.03909, time: 51.75919
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 563.84680, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 6.74205, qf2_loss: 6.56069, policy_loss: -128.31909, policy_entropy: -6.35119, alpha: 0.03990, time: 51.89849
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 563.61541, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 6.30111, qf2_loss: 6.19616, policy_loss: -129.46132, policy_entropy: -6.16360, alpha: 0.04046, time: 51.83270
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 569.16677, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 6.37661, qf2_loss: 6.28396, policy_loss: -128.93196, policy_entropy: -6.37397, alpha: 0.04102, time: 51.74292
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 536.79276, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 5.94607, qf2_loss: 5.80439, policy_loss: -128.43530, policy_entropy: -6.28067, alpha: 0.04172, time: 51.90496
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 544.22041, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 6.49235, qf2_loss: 6.38072, policy_loss: -130.33344, policy_entropy: -6.26537, alpha: 0.04233, time: 51.88309
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 555.15641, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 8.34953, qf2_loss: 8.15382, policy_loss: -129.16122, policy_entropy: -6.23009, alpha: 0.04296, time: 51.92313
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 499.91232, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 6.14159, qf2_loss: 6.12396, policy_loss: -131.54599, policy_entropy: -6.31080, alpha: 0.04362, time: 51.82076
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 564.27853, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 6.09685, qf2_loss: 5.92610, policy_loss: -128.72939, policy_entropy: -6.21380, alpha: 0.04432, time: 51.80935
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 104.70497, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 6.16158, qf2_loss: 6.11872, policy_loss: -131.07788, policy_entropy: -6.32508, alpha: 0.04496, time: 51.78243
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 540.11152, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 6.63092, qf2_loss: 6.51273, policy_loss: -131.79188, policy_entropy: -6.27327, alpha: 0.04593, time: 53.19105
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 551.75325, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 6.63607, qf2_loss: 6.34491, policy_loss: -130.93344, policy_entropy: -6.09016, alpha: 0.04629, time: 51.92011
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 579.20757, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 8.91840, qf2_loss: 8.88973, policy_loss: -131.38284, policy_entropy: -5.99596, alpha: 0.04652, time: 51.81479
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 357.33414, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 7.27084, qf2_loss: 6.99460, policy_loss: -132.03881, policy_entropy: -6.18571, alpha: 0.04684, time: 52.71618
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 513.35254, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 7.43359, qf2_loss: 7.37875, policy_loss: -131.43460, policy_entropy: -5.89806, alpha: 0.04684, time: 51.60005
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 540.17054, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 6.96688, qf2_loss: 6.77830, policy_loss: -130.92846, policy_entropy: -5.79677, alpha: 0.04644, time: 51.72225
[CW] eval: return: 440.11857, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 546.45627, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 7.03593, qf2_loss: 6.97659, policy_loss: -131.43985, policy_entropy: -6.02234, alpha: 0.04617, time: 51.73963
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 50.87027, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 8.43656, qf2_loss: 8.20747, policy_loss: -131.44120, policy_entropy: -6.03410, alpha: 0.04620, time: 51.93467
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 485.71636, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 8.22198, qf2_loss: 8.19983, policy_loss: -134.01276, policy_entropy: -6.20463, alpha: 0.04653, time: 52.00543
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 75.19849, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 6.55224, qf2_loss: 6.46661, policy_loss: -133.17609, policy_entropy: -6.10233, alpha: 0.04710, time: 51.77694
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 341.64058, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 6.79788, qf2_loss: 6.63925, policy_loss: -132.79810, policy_entropy: -5.76098, alpha: 0.04704, time: 51.81043
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 592.96056, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 6.69632, qf2_loss: 6.52834, policy_loss: -132.99172, policy_entropy: -5.77967, alpha: 0.04626, time: 51.87079
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 75.16618, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 6.80225, qf2_loss: 6.67619, policy_loss: -131.07252, policy_entropy: -5.54078, alpha: 0.04538, time: 51.84318
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 496.19280, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 6.33794, qf2_loss: 6.27954, policy_loss: -135.83556, policy_entropy: -5.82250, alpha: 0.04451, time: 51.87132
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 511.80194, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 6.24339, qf2_loss: 6.07261, policy_loss: -134.39360, policy_entropy: -5.80684, alpha: 0.04411, time: 51.85264
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 532.34672, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 6.26093, qf2_loss: 6.08297, policy_loss: -133.40659, policy_entropy: -5.72713, alpha: 0.04353, time: 51.84739
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 48.98516, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 7.32439, qf2_loss: 7.21902, policy_loss: -134.10401, policy_entropy: -5.77599, alpha: 0.04301, time: 51.95081
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 598.45670, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 15.28358, qf2_loss: 15.24734, policy_loss: -133.95939, policy_entropy: -5.91432, alpha: 0.04257, time: 51.90276
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 392.40525, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 11.57615, qf2_loss: 11.33150, policy_loss: -134.42003, policy_entropy: -5.80966, alpha: 0.04237, time: 51.80614
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 330.86323, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 7.54943, qf2_loss: 7.30333, policy_loss: -134.18968, policy_entropy: -5.80585, alpha: 0.04194, time: 51.96242
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 552.66317, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 6.65414, qf2_loss: 6.41747, policy_loss: -134.19544, policy_entropy: -5.75460, alpha: 0.04154, time: 51.85852
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 505.01548, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 6.37348, qf2_loss: 6.21388, policy_loss: -136.67472, policy_entropy: -5.92962, alpha: 0.04121, time: 51.79534
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 400.71624, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 6.37048, qf2_loss: 6.21842, policy_loss: -135.96102, policy_entropy: -5.84520, alpha: 0.04099, time: 51.86973
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 317.16006, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 8.05718, qf2_loss: 7.83831, policy_loss: -135.75369, policy_entropy: -5.84724, alpha: 0.04061, time: 52.02503
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 453.87688, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 7.10062, qf2_loss: 6.84045, policy_loss: -137.53476, policy_entropy: -6.02654, alpha: 0.04049, time: 51.99803
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 366.60191, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 7.56756, qf2_loss: 7.38560, policy_loss: -137.16238, policy_entropy: -5.89298, alpha: 0.04040, time: 51.82489
[CW] eval: return: 518.41634, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 260.94796, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 7.18426, qf2_loss: 7.01674, policy_loss: -137.01424, policy_entropy: -5.96415, alpha: 0.04032, time: 51.89655
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 219.77941, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 7.15968, qf2_loss: 6.98783, policy_loss: -135.92746, policy_entropy: -5.85690, alpha: 0.04015, time: 51.86613
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 549.23268, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 8.50972, qf2_loss: 8.40670, policy_loss: -137.00465, policy_entropy: -5.96953, alpha: 0.03995, time: 51.84254
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 482.31755, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 9.73688, qf2_loss: 9.56554, policy_loss: -137.96529, policy_entropy: -6.08259, alpha: 0.04005, time: 51.79116
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 590.92567, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 16.58344, qf2_loss: 16.44640, policy_loss: -139.07796, policy_entropy: -6.30132, alpha: 0.04038, time: 51.91840
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 480.28252, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 12.66362, qf2_loss: 12.24580, policy_loss: -138.85077, policy_entropy: -6.16215, alpha: 0.04081, time: 52.05268
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 564.24960, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 8.18534, qf2_loss: 8.07939, policy_loss: -137.09107, policy_entropy: -6.47709, alpha: 0.04124, time: 51.87958
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 512.14507, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 7.27762, qf2_loss: 7.07566, policy_loss: -137.18449, policy_entropy: -6.42509, alpha: 0.04237, time: 51.79014
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 565.16955, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 7.05417, qf2_loss: 6.92613, policy_loss: -138.88364, policy_entropy: -6.37986, alpha: 0.04322, time: 51.87264
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 528.38297, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 7.44808, qf2_loss: 7.27099, policy_loss: -138.32861, policy_entropy: -5.94403, alpha: 0.04365, time: 51.79311
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 507.48265, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 6.64328, qf2_loss: 6.38640, policy_loss: -138.91119, policy_entropy: -5.80967, alpha: 0.04339, time: 51.54759
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 487.09585, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 6.88968, qf2_loss: 6.67080, policy_loss: -141.77719, policy_entropy: -5.88122, alpha: 0.04300, time: 51.61884
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 548.01755, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 7.13648, qf2_loss: 6.92130, policy_loss: -139.88301, policy_entropy: -5.75334, alpha: 0.04264, time: 51.82276
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 513.74880, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 6.19466, qf2_loss: 5.95563, policy_loss: -142.79012, policy_entropy: -5.87456, alpha: 0.04216, time: 51.56905
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 334.57083, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 7.26601, qf2_loss: 7.22891, policy_loss: -141.30446, policy_entropy: -5.82245, alpha: 0.04185, time: 51.69335
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 566.26664, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 9.00487, qf2_loss: 8.64845, policy_loss: -140.20480, policy_entropy: -5.80305, alpha: 0.04150, time: 51.59504
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 418.79686, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 8.71676, qf2_loss: 8.49718, policy_loss: -141.59997, policy_entropy: -5.83392, alpha: 0.04116, time: 51.61758
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 450.34983, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 9.14027, qf2_loss: 8.92864, policy_loss: -141.97631, policy_entropy: -5.96275, alpha: 0.04091, time: 51.82720
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 511.85236, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 7.36991, qf2_loss: 7.16628, policy_loss: -141.89691, policy_entropy: -5.77475, alpha: 0.04071, time: 51.65455
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 540.07923, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 7.07469, qf2_loss: 6.93207, policy_loss: -142.16970, policy_entropy: -5.92746, alpha: 0.04030, time: 51.47496
[CW] eval: return: 405.84460, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 217.31869, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 6.33088, qf2_loss: 6.13469, policy_loss: -140.81196, policy_entropy: -5.84288, alpha: 0.04005, time: 51.83861
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 557.60879, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 6.09784, qf2_loss: 5.99388, policy_loss: -143.36389, policy_entropy: -5.93134, alpha: 0.03980, time: 51.81968
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 407.33988, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 7.67678, qf2_loss: 7.46880, policy_loss: -141.89543, policy_entropy: -5.77704, alpha: 0.03953, time: 52.02728
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 475.51845, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 7.44408, qf2_loss: 7.26631, policy_loss: -142.47230, policy_entropy: -5.91431, alpha: 0.03928, time: 51.79098
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 568.22390, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 7.10987, qf2_loss: 6.86752, policy_loss: -142.26914, policy_entropy: -5.72028, alpha: 0.03894, time: 51.78803
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 507.31407, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 7.96989, qf2_loss: 7.86374, policy_loss: -141.83779, policy_entropy: -5.72399, alpha: 0.03845, time: 51.84212
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 519.36756, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 6.77303, qf2_loss: 6.55613, policy_loss: -143.59449, policy_entropy: -5.77168, alpha: 0.03792, time: 51.77027
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 631.07526, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 8.12761, qf2_loss: 7.94667, policy_loss: -143.08383, policy_entropy: -5.86820, alpha: 0.03754, time: 51.75428
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 565.93620, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 7.82592, qf2_loss: 7.54526, policy_loss: -145.06868, policy_entropy: -6.00101, alpha: 0.03742, time: 51.86715
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 610.44370, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 9.79703, qf2_loss: 9.60937, policy_loss: -142.78328, policy_entropy: -5.89672, alpha: 0.03735, time: 51.77562
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 392.33323, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 6.84826, qf2_loss: 6.73812, policy_loss: -144.47735, policy_entropy: -6.18413, alpha: 0.03742, time: 51.79169
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 521.31271, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 6.09463, qf2_loss: 5.92031, policy_loss: -143.69464, policy_entropy: -6.15563, alpha: 0.03774, time: 51.80109
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 36.62887, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 6.37776, qf2_loss: 6.20630, policy_loss: -144.32160, policy_entropy: -6.06895, alpha: 0.03801, time: 51.73477
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 474.75665, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 7.33631, qf2_loss: 7.18939, policy_loss: -144.86184, policy_entropy: -6.02063, alpha: 0.03805, time: 51.76755
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 585.90334, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 7.44332, qf2_loss: 7.25178, policy_loss: -145.77861, policy_entropy: -6.20827, alpha: 0.03838, time: 51.87142
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 518.51366, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 8.00573, qf2_loss: 7.74222, policy_loss: -146.44019, policy_entropy: -6.03205, alpha: 0.03861, time: 51.77884
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 25.67054, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 9.03055, qf2_loss: 8.78211, policy_loss: -145.82022, policy_entropy: -5.93458, alpha: 0.03858, time: 51.76360
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 157.46267, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 7.25357, qf2_loss: 6.95978, policy_loss: -144.20102, policy_entropy: -5.98398, alpha: 0.03844, time: 51.81967
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 137.03491, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 13.79452, qf2_loss: 13.62994, policy_loss: -145.70331, policy_entropy: -6.28299, alpha: 0.03838, time: 51.75375
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 33.11244, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 10.25529, qf2_loss: 10.04636, policy_loss: -142.83178, policy_entropy: -7.24830, alpha: 0.04004, time: 51.75306
[CW] eval: return: 435.35249, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 321.41842, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 9.33026, qf2_loss: 9.13471, policy_loss: -143.36589, policy_entropy: -6.37855, alpha: 0.04138, time: 51.90534
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 562.07631, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 9.28158, qf2_loss: 8.97334, policy_loss: -144.88442, policy_entropy: -6.06258, alpha: 0.04186, time: 51.90626
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 588.12238, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 11.46181, qf2_loss: 11.21223, policy_loss: -145.37557, policy_entropy: -6.11741, alpha: 0.04194, time: 51.81333
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 577.21770, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 15.48479, qf2_loss: 15.26396, policy_loss: -145.68545, policy_entropy: -5.83642, alpha: 0.04207, time: 51.81047
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 559.07009, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 10.76123, qf2_loss: 10.40919, policy_loss: -144.43362, policy_entropy: -5.76456, alpha: 0.04163, time: 51.84683
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 567.84621, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 7.56370, qf2_loss: 7.41976, policy_loss: -145.71541, policy_entropy: -5.87501, alpha: 0.04121, time: 51.79756
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 580.32667, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 10.84013, qf2_loss: 10.59933, policy_loss: -145.69903, policy_entropy: -5.83744, alpha: 0.04096, time: 51.73517
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 514.44654, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 10.43984, qf2_loss: 10.20255, policy_loss: -146.54693, policy_entropy: -5.87490, alpha: 0.04074, time: 51.80887
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 571.39260, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 7.26397, qf2_loss: 7.13436, policy_loss: -146.53329, policy_entropy: -5.82695, alpha: 0.04040, time: 51.70996
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 397.48072, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 7.25428, qf2_loss: 6.99373, policy_loss: -146.30834, policy_entropy: -5.77180, alpha: 0.04010, time: 51.59810
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 556.05568, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 7.25194, qf2_loss: 7.12118, policy_loss: -146.29008, policy_entropy: -5.83731, alpha: 0.03964, time: 51.66961
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 549.88180, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 8.91269, qf2_loss: 8.71494, policy_loss: -147.00560, policy_entropy: -5.85064, alpha: 0.03936, time: 51.58659
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 551.30041, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 8.35135, qf2_loss: 8.01323, policy_loss: -146.52983, policy_entropy: -5.84395, alpha: 0.03911, time: 52.56589
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 537.95483, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 6.77304, qf2_loss: 6.70847, policy_loss: -147.19790, policy_entropy: -5.97402, alpha: 0.03897, time: 52.49231
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 556.77755, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 8.26212, qf2_loss: 8.13157, policy_loss: -147.90717, policy_entropy: -6.05953, alpha: 0.03890, time: 51.84608
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 180.15194, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 8.51340, qf2_loss: 8.23887, policy_loss: -147.57426, policy_entropy: -6.00738, alpha: 0.03905, time: 51.69407
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 452.13299, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 9.92774, qf2_loss: 9.63447, policy_loss: -147.96302, policy_entropy: -6.16567, alpha: 0.03904, time: 52.65310
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 365.82614, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 11.45180, qf2_loss: 11.22616, policy_loss: -146.54789, policy_entropy: -6.31078, alpha: 0.03953, time: 51.85081
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 618.53707, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 11.87130, qf2_loss: 11.90871, policy_loss: -148.28202, policy_entropy: -6.10535, alpha: 0.03999, time: 51.73656
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 491.57185, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 9.83958, qf2_loss: 9.58203, policy_loss: -147.50071, policy_entropy: -5.76595, alpha: 0.03984, time: 51.49364
[CW] eval: return: 495.23738, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 567.55117, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 8.99019, qf2_loss: 8.67721, policy_loss: -148.25504, policy_entropy: -5.88087, alpha: 0.03959, time: 51.78023
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 525.85510, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 7.49927, qf2_loss: 7.29149, policy_loss: -148.47377, policy_entropy: -6.01765, alpha: 0.03953, time: 51.75754
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 594.63174, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 11.46921, qf2_loss: 11.13231, policy_loss: -150.39283, policy_entropy: -6.14331, alpha: 0.03956, time: 52.88872
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 580.81085, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 11.69985, qf2_loss: 11.31245, policy_loss: -147.83691, policy_entropy: -6.04507, alpha: 0.03977, time: 51.71876
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 617.53222, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 11.83085, qf2_loss: 11.60147, policy_loss: -149.97903, policy_entropy: -6.17533, alpha: 0.03993, time: 52.78641
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 598.15251, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 8.96063, qf2_loss: 8.84188, policy_loss: -149.87364, policy_entropy: -6.15174, alpha: 0.04031, time: 52.46140
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 543.86323, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 8.08855, qf2_loss: 7.85816, policy_loss: -150.08184, policy_entropy: -5.92841, alpha: 0.04048, time: 51.72628
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 603.14394, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 8.24716, qf2_loss: 8.14523, policy_loss: -149.57063, policy_entropy: -5.96339, alpha: 0.04026, time: 51.75315
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 618.86777, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 8.95299, qf2_loss: 8.65937, policy_loss: -150.42417, policy_entropy: -6.08431, alpha: 0.04034, time: 51.87771
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 585.76314, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 8.87636, qf2_loss: 8.67402, policy_loss: -150.00960, policy_entropy: -5.95616, alpha: 0.04038, time: 51.88588
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 52.94137, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 9.39724, qf2_loss: 9.22075, policy_loss: -150.09251, policy_entropy: -6.01622, alpha: 0.04038, time: 51.78529
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 563.23105, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 11.09733, qf2_loss: 10.83634, policy_loss: -150.23279, policy_entropy: -6.21122, alpha: 0.04043, time: 51.83853
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 654.41217, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 15.25533, qf2_loss: 15.08870, policy_loss: -149.46018, policy_entropy: -6.12929, alpha: 0.04094, time: 51.88013
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 578.44227, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 14.13140, qf2_loss: 13.84618, policy_loss: -150.92658, policy_entropy: -6.11859, alpha: 0.04116, time: 51.79352
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 44.49514, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 10.83053, qf2_loss: 10.65928, policy_loss: -151.13218, policy_entropy: -6.23395, alpha: 0.04149, time: 51.77065
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 47.07398, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 11.26457, qf2_loss: 11.05993, policy_loss: -148.33660, policy_entropy: -5.93450, alpha: 0.04170, time: 51.85340
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 592.15713, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 12.24232, qf2_loss: 11.95595, policy_loss: -149.98110, policy_entropy: -6.01625, alpha: 0.04160, time: 51.79610
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 572.25146, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 10.04295, qf2_loss: 9.70727, policy_loss: -151.80568, policy_entropy: -6.23553, alpha: 0.04187, time: 51.73989
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 57.31601, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 11.01488, qf2_loss: 10.60818, policy_loss: -150.39991, policy_entropy: -6.12031, alpha: 0.04220, time: 51.78587
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 53.65494, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 11.39923, qf2_loss: 11.23049, policy_loss: -150.79411, policy_entropy: -5.90373, alpha: 0.04235, time: 51.84195
[CW] eval: return: 535.59933, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 605.65532, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 12.51854, qf2_loss: 12.28362, policy_loss: -150.14297, policy_entropy: -5.78799, alpha: 0.04201, time: 51.73775
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 493.76014, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 11.05999, qf2_loss: 10.97104, policy_loss: -150.14754, policy_entropy: -5.80269, alpha: 0.04167, time: 51.72995
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 589.89853, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 10.68255, qf2_loss: 10.49232, policy_loss: -151.38974, policy_entropy: -5.96103, alpha: 0.04130, time: 51.73307
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 127.61623, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 10.53735, qf2_loss: 10.15679, policy_loss: -150.87735, policy_entropy: -5.98953, alpha: 0.04136, time: 51.88336
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 529.87846, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 11.95991, qf2_loss: 11.73910, policy_loss: -151.81322, policy_entropy: -5.91710, alpha: 0.04124, time: 51.85698
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 556.63986, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 10.01862, qf2_loss: 9.79736, policy_loss: -151.73937, policy_entropy: -5.91384, alpha: 0.04106, time: 51.83920
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 130.84852, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 10.17438, qf2_loss: 9.82389, policy_loss: -151.95602, policy_entropy: -5.96528, alpha: 0.04097, time: 52.05812
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 126.31347, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 11.28982, qf2_loss: 11.11387, policy_loss: -152.80338, policy_entropy: -5.86859, alpha: 0.04073, time: 51.74451
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 91.16330, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 12.48065, qf2_loss: 12.10615, policy_loss: -151.41400, policy_entropy: -5.86203, alpha: 0.04052, time: 51.58658
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 496.17117, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 11.77418, qf2_loss: 11.59076, policy_loss: -151.65933, policy_entropy: -5.81531, alpha: 0.04019, time: 51.61774
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 451.44647, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 10.84673, qf2_loss: 10.53704, policy_loss: -152.07257, policy_entropy: -6.05887, alpha: 0.04010, time: 51.78512
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 277.66198, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 11.50111, qf2_loss: 11.32194, policy_loss: -150.80486, policy_entropy: -6.02741, alpha: 0.04017, time: 51.81166
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 473.61725, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 14.76759, qf2_loss: 14.37891, policy_loss: -153.25923, policy_entropy: -6.18736, alpha: 0.04030, time: 51.74340
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 196.78268, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 14.16661, qf2_loss: 14.19870, policy_loss: -152.69140, policy_entropy: -5.92820, alpha: 0.04051, time: 51.73295
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 496.63704, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 16.84954, qf2_loss: 16.40823, policy_loss: -151.99579, policy_entropy: -6.07532, alpha: 0.04050, time: 51.90033
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 533.77742, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 12.22616, qf2_loss: 11.94645, policy_loss: -154.24868, policy_entropy: -6.02563, alpha: 0.04059, time: 51.69994
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 459.95039, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 13.40749, qf2_loss: 13.03675, policy_loss: -152.14940, policy_entropy: -6.05063, alpha: 0.04064, time: 51.75884
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 540.06928, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 23.02811, qf2_loss: 23.11115, policy_loss: -153.75765, policy_entropy: -6.16196, alpha: 0.04080, time: 51.78985
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 357.88129, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 32.36440, qf2_loss: 31.83267, policy_loss: -154.05642, policy_entropy: -6.09903, alpha: 0.04117, time: 51.65847
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 405.13766, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 43.70086, qf2_loss: 43.82193, policy_loss: -153.00855, policy_entropy: -6.39308, alpha: 0.04148, time: 51.81097
[CW] eval: return: 446.25098, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 555.68961, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 13.81096, qf2_loss: 13.51927, policy_loss: -153.71182, policy_entropy: -6.19203, alpha: 0.04232, time: 51.55641
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 605.48228, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 10.22432, qf2_loss: 10.01015, policy_loss: -152.93617, policy_entropy: -6.12057, alpha: 0.04261, time: 51.77938
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 238.72288, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 10.95211, qf2_loss: 10.78210, policy_loss: -155.75604, policy_entropy: -6.06292, alpha: 0.04280, time: 51.65502
[CW] ---------------------------
