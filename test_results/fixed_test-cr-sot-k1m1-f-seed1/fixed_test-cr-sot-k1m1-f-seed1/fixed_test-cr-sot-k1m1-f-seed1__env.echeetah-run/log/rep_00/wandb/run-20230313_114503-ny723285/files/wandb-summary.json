{"collect/return": 238.7228813569527, "collect/steps": 1000.0, "collect/total_steps": 669000.0, "train/qf1_loss": 10.952111778259278, "train/qf2_loss": 10.782101774215699, "train/policy_loss": -155.7560430908203, "train/policy_entropy": -6.0629169511795045, "train/alpha": 0.04280489455908537, "train/time": 51.65501952171326, "eval/return": 446.25097579344583, "eval/steps": 1000.0, "_timestamp": 1678740149.9054568, "_runtime": 35845.89775276184, "_step": 663}