{"collect/return": 633.2693998813629, "collect/steps": 1000.0, "collect/total_steps": 1006000.0, "train/qf1_loss": 5.845627655982971, "train/qf2_loss": 5.859623277187348, "train/policy_loss": -199.59529541015624, "train/policy_entropy": -6.007021250724793, "train/alpha": 0.05774918928742409, "train/time": 32.65782570838928, "eval/return": 680.415656286967, "eval/steps": 1000.0, "_timestamp": 1678729999.1665602, "_runtime": 35158.76944708824, "_step": 1000, "_wandb": {"runtime": 35159}}