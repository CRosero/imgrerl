[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 66.66103, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.79243, qf2_loss: 1.83261, policy_loss: -2.23940, policy_entropy: 0.68262, alpha: 0.98504, time: 71.66685
[CW] eval: return: 121.73166, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 151.84296, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.13932, qf2_loss: 0.13954, policy_loss: -2.73007, policy_entropy: 0.68195, alpha: 0.95627, time: 66.99479
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 172.69751, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.14636, qf2_loss: 0.14847, policy_loss: -3.26841, policy_entropy: 0.67970, alpha: 0.92874, time: 66.91356
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 223.32792, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.21950, qf2_loss: 0.22314, policy_loss: -3.99293, policy_entropy: 0.67435, alpha: 0.90239, time: 67.01414
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 202.64817, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.30390, qf2_loss: 0.31429, policy_loss: -4.86956, policy_entropy: 0.66743, alpha: 0.87719, time: 66.94618
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 111.27133, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.38461, qf2_loss: 0.40269, policy_loss: -5.56535, policy_entropy: 0.66246, alpha: 0.85303, time: 66.99094
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 144.98246, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.38396, qf2_loss: 0.40060, policy_loss: -6.32512, policy_entropy: 0.65758, alpha: 0.82984, time: 66.91085
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 122.32202, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.54490, qf2_loss: 0.55958, policy_loss: -6.91360, policy_entropy: 0.65411, alpha: 0.80754, time: 66.93184
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 85.34701, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.50772, qf2_loss: 0.51463, policy_loss: -7.46230, policy_entropy: 0.65510, alpha: 0.78606, time: 66.99695
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 118.89844, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.59443, qf2_loss: 0.60165, policy_loss: -8.05409, policy_entropy: 0.65346, alpha: 0.76534, time: 67.01004
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 144.98424, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.73680, qf2_loss: 0.74578, policy_loss: -8.71890, policy_entropy: 0.65140, alpha: 0.74537, time: 67.01446
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 215.85730, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.85376, qf2_loss: 0.85494, policy_loss: -9.57000, policy_entropy: 0.64552, alpha: 0.72611, time: 67.02534
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 108.27658, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 1.02712, qf2_loss: 1.03337, policy_loss: -10.11003, policy_entropy: 0.64113, alpha: 0.70755, time: 67.05230
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 148.27601, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 1.04022, qf2_loss: 1.05051, policy_loss: -10.89985, policy_entropy: 0.62680, alpha: 0.68967, time: 67.05038
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 181.76565, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 1.33845, qf2_loss: 1.36358, policy_loss: -11.72599, policy_entropy: 0.61405, alpha: 0.67250, time: 67.07264
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 115.25871, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 1.28836, qf2_loss: 1.30737, policy_loss: -12.36106, policy_entropy: 0.59633, alpha: 0.65599, time: 67.07234
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 194.38894, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 1.61252, qf2_loss: 1.63245, policy_loss: -13.30107, policy_entropy: 0.57006, alpha: 0.64015, time: 67.06901
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 226.35710, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 1.78288, qf2_loss: 1.79411, policy_loss: -14.11018, policy_entropy: 0.54061, alpha: 0.62499, time: 67.08220
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 226.27794, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 2.05632, qf2_loss: 2.07549, policy_loss: -14.91693, policy_entropy: 0.51518, alpha: 0.61047, time: 67.04346
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 217.16159, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 2.11812, qf2_loss: 2.13318, policy_loss: -15.85503, policy_entropy: 0.48214, alpha: 0.59654, time: 67.05464
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 199.10667, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 2.38025, qf2_loss: 2.40563, policy_loss: -16.68472, policy_entropy: 0.45207, alpha: 0.58320, time: 67.01949
[CW] eval: return: 221.29199, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 138.87236, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 2.38860, qf2_loss: 2.42672, policy_loss: -17.29532, policy_entropy: 0.42318, alpha: 0.57036, time: 67.03979
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 175.05986, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 2.44462, qf2_loss: 2.46376, policy_loss: -18.21768, policy_entropy: 0.38604, alpha: 0.55804, time: 66.96342
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 202.69897, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 2.60159, qf2_loss: 2.63327, policy_loss: -19.00967, policy_entropy: 0.35268, alpha: 0.54622, time: 66.94649
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 237.48604, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 2.81209, qf2_loss: 2.84312, policy_loss: -19.97530, policy_entropy: 0.31369, alpha: 0.53487, time: 66.97409
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 242.97587, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 3.16966, qf2_loss: 3.19320, policy_loss: -20.69044, policy_entropy: 0.28456, alpha: 0.52397, time: 67.03759
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 283.65083, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 3.29353, qf2_loss: 3.34014, policy_loss: -21.95972, policy_entropy: 0.23512, alpha: 0.51350, time: 66.99931
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 323.01902, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 3.13009, qf2_loss: 3.16409, policy_loss: -22.85309, policy_entropy: 0.21742, alpha: 0.50343, time: 66.97863
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 200.62729, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 4.02741, qf2_loss: 4.04137, policy_loss: -23.73353, policy_entropy: 0.18743, alpha: 0.49364, time: 66.95268
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 218.88181, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 4.53652, qf2_loss: 4.57861, policy_loss: -24.88517, policy_entropy: 0.15387, alpha: 0.48421, time: 66.99451
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 262.49492, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 4.10102, qf2_loss: 4.15232, policy_loss: -25.79569, policy_entropy: 0.12174, alpha: 0.47505, time: 66.97139
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 224.33542, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 3.76124, qf2_loss: 3.76544, policy_loss: -26.78495, policy_entropy: 0.08630, alpha: 0.46624, time: 67.04473
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 210.12589, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 3.94410, qf2_loss: 3.98112, policy_loss: -27.95180, policy_entropy: 0.05945, alpha: 0.45772, time: 67.00033
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 267.77913, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 3.88036, qf2_loss: 3.89690, policy_loss: -28.83453, policy_entropy: 0.04149, alpha: 0.44943, time: 67.02321
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 307.83292, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 4.44059, qf2_loss: 4.48381, policy_loss: -30.17906, policy_entropy: 0.01244, alpha: 0.44132, time: 66.99111
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 238.26804, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 4.27427, qf2_loss: 4.32047, policy_loss: -30.98233, policy_entropy: -0.02182, alpha: 0.43351, time: 67.01685
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 244.19695, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 3.85378, qf2_loss: 3.92303, policy_loss: -31.71106, policy_entropy: -0.04905, alpha: 0.42595, time: 66.91178
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 229.58821, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 4.04941, qf2_loss: 4.12307, policy_loss: -32.86531, policy_entropy: -0.07906, alpha: 0.41862, time: 66.92290
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 233.88372, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 4.42853, qf2_loss: 4.48845, policy_loss: -33.81244, policy_entropy: -0.09518, alpha: 0.41147, time: 66.94907
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 249.29924, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 4.56569, qf2_loss: 4.62688, policy_loss: -34.90767, policy_entropy: -0.12584, alpha: 0.40451, time: 66.98169
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 269.73662, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 4.70259, qf2_loss: 4.76878, policy_loss: -35.87944, policy_entropy: -0.15490, alpha: 0.39777, time: 66.97680
[CW] eval: return: 300.82464, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 279.60717, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 4.89208, qf2_loss: 4.94143, policy_loss: -37.06060, policy_entropy: -0.16882, alpha: 0.39119, time: 67.08090
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 259.94574, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 5.01892, qf2_loss: 5.03877, policy_loss: -38.01607, policy_entropy: -0.19269, alpha: 0.38473, time: 67.09252
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 301.53182, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 5.31137, qf2_loss: 5.37424, policy_loss: -38.85259, policy_entropy: -0.20527, alpha: 0.37844, time: 67.08006
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 272.41006, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 5.23307, qf2_loss: 5.29999, policy_loss: -40.10161, policy_entropy: -0.21169, alpha: 0.37220, time: 67.03591
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 295.18940, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 5.34052, qf2_loss: 5.37570, policy_loss: -41.03894, policy_entropy: -0.22196, alpha: 0.36601, time: 67.03988
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 241.72706, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 5.56988, qf2_loss: 5.65366, policy_loss: -41.97751, policy_entropy: -0.22729, alpha: 0.35989, time: 67.06369
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 350.71328, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 5.59974, qf2_loss: 5.65796, policy_loss: -43.15404, policy_entropy: -0.23353, alpha: 0.35380, time: 67.01587
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 225.94156, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 5.64831, qf2_loss: 5.71044, policy_loss: -44.18020, policy_entropy: -0.23746, alpha: 0.34775, time: 67.12758
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 238.32384, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 5.66679, qf2_loss: 5.68564, policy_loss: -44.87356, policy_entropy: -0.25885, alpha: 0.34180, time: 67.10774
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 214.52259, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 6.00213, qf2_loss: 6.03616, policy_loss: -45.80552, policy_entropy: -0.26283, alpha: 0.33597, time: 67.11253
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 352.28010, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 5.88333, qf2_loss: 5.93420, policy_loss: -47.14927, policy_entropy: -0.27735, alpha: 0.33018, time: 67.01403
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 374.05231, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 6.36055, qf2_loss: 6.38850, policy_loss: -48.10384, policy_entropy: -0.28911, alpha: 0.32454, time: 67.00996
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 326.94562, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 6.62040, qf2_loss: 6.68323, policy_loss: -49.25471, policy_entropy: -0.29893, alpha: 0.31897, time: 67.02775
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 285.39931, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 6.94193, qf2_loss: 7.05043, policy_loss: -50.24884, policy_entropy: -0.31342, alpha: 0.31347, time: 66.99904
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 377.37931, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 5.96181, qf2_loss: 5.98885, policy_loss: -51.44490, policy_entropy: -0.32546, alpha: 0.30815, time: 67.15764
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 386.85426, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 6.37294, qf2_loss: 6.39252, policy_loss: -52.53960, policy_entropy: -0.35460, alpha: 0.30294, time: 68.06103
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 287.36005, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 6.32889, qf2_loss: 6.36032, policy_loss: -53.05927, policy_entropy: -0.37943, alpha: 0.29796, time: 67.52298
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 343.31872, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 6.53845, qf2_loss: 6.61082, policy_loss: -54.62915, policy_entropy: -0.39185, alpha: 0.29310, time: 67.49488
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 303.21226, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 6.25154, qf2_loss: 6.30411, policy_loss: -55.69241, policy_entropy: -0.42038, alpha: 0.28842, time: 67.52182
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 355.54397, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 6.30287, qf2_loss: 6.33900, policy_loss: -56.78842, policy_entropy: -0.41627, alpha: 0.28377, time: 67.85925
[CW] eval: return: 289.72188, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 349.82361, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 6.57273, qf2_loss: 6.62395, policy_loss: -57.93764, policy_entropy: -0.43083, alpha: 0.27919, time: 67.88701
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 308.69529, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 7.59611, qf2_loss: 7.61928, policy_loss: -59.04993, policy_entropy: -0.44944, alpha: 0.27474, time: 67.83871
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 418.98273, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 7.19822, qf2_loss: 7.25809, policy_loss: -60.27101, policy_entropy: -0.46823, alpha: 0.27037, time: 67.82452
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 366.60742, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 6.82857, qf2_loss: 6.85649, policy_loss: -60.91416, policy_entropy: -0.48752, alpha: 0.26619, time: 67.85026
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 417.08964, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 6.77552, qf2_loss: 6.79145, policy_loss: -62.50938, policy_entropy: -0.51281, alpha: 0.26212, time: 67.74288
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 413.87828, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 8.29770, qf2_loss: 8.36969, policy_loss: -63.73770, policy_entropy: -0.52526, alpha: 0.25819, time: 67.76371
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 325.12424, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 7.08225, qf2_loss: 7.12055, policy_loss: -64.48388, policy_entropy: -0.54300, alpha: 0.25434, time: 67.75945
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 295.38015, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 6.60693, qf2_loss: 6.63354, policy_loss: -65.65568, policy_entropy: -0.57057, alpha: 0.25065, time: 67.72388
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 409.01789, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 6.72451, qf2_loss: 6.73710, policy_loss: -66.85329, policy_entropy: -0.58758, alpha: 0.24717, time: 67.69358
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 348.45104, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 7.96982, qf2_loss: 8.01927, policy_loss: -67.99131, policy_entropy: -0.60799, alpha: 0.24375, time: 67.61745
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 422.07841, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 7.17818, qf2_loss: 7.15638, policy_loss: -69.19528, policy_entropy: -0.63341, alpha: 0.24048, time: 67.66211
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 280.95122, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 7.31763, qf2_loss: 7.22936, policy_loss: -70.38648, policy_entropy: -0.64594, alpha: 0.23736, time: 67.60783
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 282.29161, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 6.71732, qf2_loss: 6.79235, policy_loss: -71.67092, policy_entropy: -0.67053, alpha: 0.23433, time: 67.59373
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 300.42478, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 7.17948, qf2_loss: 7.17255, policy_loss: -72.74393, policy_entropy: -0.69127, alpha: 0.23148, time: 67.55999
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 284.99891, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 7.02646, qf2_loss: 6.96377, policy_loss: -74.19626, policy_entropy: -0.70678, alpha: 0.22873, time: 67.57902
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 249.70509, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 7.57013, qf2_loss: 7.53645, policy_loss: -75.03295, policy_entropy: -0.72469, alpha: 0.22605, time: 67.57566
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 311.90814, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 9.93964, qf2_loss: 9.98690, policy_loss: -75.91733, policy_entropy: -0.73480, alpha: 0.22354, time: 67.53016
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 365.16173, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 7.56983, qf2_loss: 7.52757, policy_loss: -77.20260, policy_entropy: -0.75338, alpha: 0.22103, time: 67.50682
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 323.51429, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 6.84947, qf2_loss: 6.84828, policy_loss: -78.30491, policy_entropy: -0.76737, alpha: 0.21865, time: 67.38971
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 338.21972, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 7.14911, qf2_loss: 7.12892, policy_loss: -79.52464, policy_entropy: -0.78835, alpha: 0.21642, time: 67.39470
[CW] eval: return: 345.63649, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 341.16924, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 7.60212, qf2_loss: 7.60527, policy_loss: -80.53316, policy_entropy: -0.80464, alpha: 0.21431, time: 67.44435
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 338.56834, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 7.38239, qf2_loss: 7.37378, policy_loss: -81.81365, policy_entropy: -0.81623, alpha: 0.21228, time: 67.45505
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 381.47973, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 7.51247, qf2_loss: 7.44231, policy_loss: -82.76022, policy_entropy: -0.82818, alpha: 0.21037, time: 67.42892
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 386.53641, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 8.01028, qf2_loss: 8.04218, policy_loss: -84.02618, policy_entropy: -0.83834, alpha: 0.20850, time: 67.42158
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 433.93288, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 7.55166, qf2_loss: 7.52852, policy_loss: -84.78752, policy_entropy: -0.85200, alpha: 0.20672, time: 67.44010
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 400.76567, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 7.47912, qf2_loss: 7.49473, policy_loss: -86.44739, policy_entropy: -0.86747, alpha: 0.20502, time: 67.39469
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 375.88056, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 7.53263, qf2_loss: 7.50402, policy_loss: -87.02421, policy_entropy: -0.88467, alpha: 0.20346, time: 67.40067
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 373.57341, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 8.99335, qf2_loss: 9.02176, policy_loss: -88.69770, policy_entropy: -0.89014, alpha: 0.20209, time: 67.36100
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 453.71617, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 7.62541, qf2_loss: 7.65435, policy_loss: -89.90983, policy_entropy: -0.90569, alpha: 0.20072, time: 67.35951
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 394.44116, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 7.84011, qf2_loss: 7.88434, policy_loss: -91.01211, policy_entropy: -0.92481, alpha: 0.19957, time: 67.33139
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 381.57746, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 8.34211, qf2_loss: 8.33877, policy_loss: -91.98353, policy_entropy: -0.93265, alpha: 0.19859, time: 67.32622
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 383.71530, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 9.91344, qf2_loss: 9.92926, policy_loss: -93.19239, policy_entropy: -0.93765, alpha: 0.19762, time: 67.30917
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 372.23151, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 8.12445, qf2_loss: 8.14275, policy_loss: -93.83778, policy_entropy: -0.96217, alpha: 0.19686, time: 67.32512
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 420.77787, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 10.08017, qf2_loss: 10.06195, policy_loss: -95.23822, policy_entropy: -0.96367, alpha: 0.19622, time: 67.29594
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 310.86748, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 8.36248, qf2_loss: 8.31028, policy_loss: -96.71970, policy_entropy: -0.97245, alpha: 0.19569, time: 67.25789
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 471.95316, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 9.52237, qf2_loss: 9.55816, policy_loss: -97.68922, policy_entropy: -0.99060, alpha: 0.19541, time: 67.31323
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 347.80835, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 8.87869, qf2_loss: 8.86954, policy_loss: -98.54978, policy_entropy: -0.98864, alpha: 0.19519, time: 67.28409
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 363.94458, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 8.92599, qf2_loss: 8.91352, policy_loss: -99.62957, policy_entropy: -1.00821, alpha: 0.19516, time: 67.28684
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 399.11250, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 9.69992, qf2_loss: 9.76221, policy_loss: -100.98281, policy_entropy: -1.01443, alpha: 0.19536, time: 67.23117
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 293.92367, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 9.32577, qf2_loss: 9.29204, policy_loss: -101.93154, policy_entropy: -1.01501, alpha: 0.19565, time: 67.17152
[CW] eval: return: 383.04994, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 426.24864, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 8.66136, qf2_loss: 8.65245, policy_loss: -103.23508, policy_entropy: -1.03582, alpha: 0.19622, time: 67.18675
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 320.02557, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 9.18316, qf2_loss: 9.09576, policy_loss: -103.96121, policy_entropy: -1.04070, alpha: 0.19703, time: 67.23224
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 429.70120, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 9.73297, qf2_loss: 9.76152, policy_loss: -105.43127, policy_entropy: -1.03380, alpha: 0.19800, time: 67.24690
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 358.07121, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 11.63689, qf2_loss: 11.65050, policy_loss: -106.07928, policy_entropy: -1.03256, alpha: 0.19891, time: 67.18803
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 411.95126, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 12.18650, qf2_loss: 12.18861, policy_loss: -107.65839, policy_entropy: -1.03508, alpha: 0.19988, time: 67.24134
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 427.00585, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 10.73998, qf2_loss: 10.72547, policy_loss: -108.79159, policy_entropy: -1.05036, alpha: 0.20097, time: 67.16884
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 371.41723, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 9.64332, qf2_loss: 9.60938, policy_loss: -109.67208, policy_entropy: -1.05103, alpha: 0.20255, time: 67.18759
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 393.68547, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 9.61525, qf2_loss: 9.68228, policy_loss: -111.15587, policy_entropy: -1.04752, alpha: 0.20420, time: 67.09036
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 320.91778, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 10.70159, qf2_loss: 10.72092, policy_loss: -112.31761, policy_entropy: -1.04717, alpha: 0.20578, time: 67.19104
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 433.28513, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 12.04996, qf2_loss: 12.20275, policy_loss: -113.39402, policy_entropy: -1.03890, alpha: 0.20734, time: 67.23876
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 392.11887, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 10.78480, qf2_loss: 10.73513, policy_loss: -113.86163, policy_entropy: -1.03522, alpha: 0.20865, time: 68.00433
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 368.50040, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 10.47882, qf2_loss: 10.61073, policy_loss: -115.60598, policy_entropy: -1.04295, alpha: 0.21031, time: 72.14679
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 300.21019, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 10.77834, qf2_loss: 10.79329, policy_loss: -116.71556, policy_entropy: -1.04219, alpha: 0.21221, time: 68.52487
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 371.65766, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 10.08957, qf2_loss: 10.12653, policy_loss: -117.75800, policy_entropy: -1.06185, alpha: 0.21442, time: 67.74556
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 378.66927, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 10.39887, qf2_loss: 10.44781, policy_loss: -118.94105, policy_entropy: -1.04697, alpha: 0.21725, time: 67.72201
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 351.04435, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 10.86258, qf2_loss: 10.91168, policy_loss: -119.38217, policy_entropy: -1.04302, alpha: 0.21962, time: 67.73644
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 337.28368, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 13.55837, qf2_loss: 13.61726, policy_loss: -120.48120, policy_entropy: -1.05150, alpha: 0.22203, time: 68.57814
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 175.35515, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 11.44999, qf2_loss: 11.48881, policy_loss: -121.39114, policy_entropy: -1.02967, alpha: 0.22462, time: 67.79793
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 326.32883, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 10.94329, qf2_loss: 11.01690, policy_loss: -122.40349, policy_entropy: -1.02529, alpha: 0.22622, time: 67.83345
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 306.94344, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 11.60732, qf2_loss: 11.58808, policy_loss: -122.72025, policy_entropy: -1.01926, alpha: 0.22765, time: 67.80060
[CW] eval: return: 356.64080, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 325.50714, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 13.74723, qf2_loss: 13.82365, policy_loss: -124.15298, policy_entropy: -1.02188, alpha: 0.22859, time: 67.78408
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 337.24619, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 13.49685, qf2_loss: 13.59947, policy_loss: -124.79879, policy_entropy: -1.03352, alpha: 0.23050, time: 67.77703
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 317.48267, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 12.71618, qf2_loss: 12.83581, policy_loss: -125.76724, policy_entropy: -1.01760, alpha: 0.23262, time: 67.77148
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 383.51047, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 12.25459, qf2_loss: 12.22509, policy_loss: -126.98488, policy_entropy: -1.00945, alpha: 0.23352, time: 67.77398
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 330.28203, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 11.53090, qf2_loss: 11.50198, policy_loss: -127.60811, policy_entropy: -1.01082, alpha: 0.23476, time: 67.81901
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 337.13486, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 16.67611, qf2_loss: 16.87954, policy_loss: -129.08184, policy_entropy: -1.00069, alpha: 0.23535, time: 67.77889
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 346.36518, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 13.11424, qf2_loss: 13.16445, policy_loss: -130.51384, policy_entropy: -1.02258, alpha: 0.23561, time: 67.71826
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 359.19446, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 11.71054, qf2_loss: 11.73508, policy_loss: -131.23620, policy_entropy: -1.00612, alpha: 0.23725, time: 67.69350
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 314.53265, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 12.87488, qf2_loss: 12.98055, policy_loss: -131.77846, policy_entropy: -1.00472, alpha: 0.23770, time: 67.79041
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 380.21098, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 12.94703, qf2_loss: 12.89879, policy_loss: -133.60624, policy_entropy: -1.01165, alpha: 0.23863, time: 67.75901
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 324.07011, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 12.30441, qf2_loss: 12.28891, policy_loss: -134.27349, policy_entropy: -1.00274, alpha: 0.23941, time: 67.75613
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 363.35260, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 12.90194, qf2_loss: 12.94665, policy_loss: -134.92802, policy_entropy: -1.01672, alpha: 0.24029, time: 67.75330
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 383.81012, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 12.31336, qf2_loss: 12.41833, policy_loss: -136.55028, policy_entropy: -1.01416, alpha: 0.24186, time: 67.71814
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 259.76788, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 12.85313, qf2_loss: 12.89521, policy_loss: -136.50805, policy_entropy: -1.01233, alpha: 0.24385, time: 67.67184
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 233.94977, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 13.78471, qf2_loss: 13.87908, policy_loss: -137.99653, policy_entropy: -1.00444, alpha: 0.24419, time: 67.92101
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 404.07350, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 13.21194, qf2_loss: 13.43472, policy_loss: -139.59471, policy_entropy: -1.01434, alpha: 0.24518, time: 68.16531
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 425.20157, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 13.32189, qf2_loss: 13.39630, policy_loss: -139.74620, policy_entropy: -1.00890, alpha: 0.24670, time: 68.02516
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 419.28663, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 17.29060, qf2_loss: 17.32995, policy_loss: -140.29278, policy_entropy: -1.00801, alpha: 0.24789, time: 67.95442
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 381.46074, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 13.93519, qf2_loss: 13.95073, policy_loss: -141.55526, policy_entropy: -1.02399, alpha: 0.24988, time: 67.97839
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 366.93330, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 12.68596, qf2_loss: 12.69333, policy_loss: -143.44540, policy_entropy: -1.00838, alpha: 0.25211, time: 67.96618
[CW] eval: return: 382.64132, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 386.07803, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 20.47247, qf2_loss: 20.96394, policy_loss: -143.65383, policy_entropy: -1.00197, alpha: 0.25281, time: 68.07707
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 329.50089, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 13.33174, qf2_loss: 13.28891, policy_loss: -145.31555, policy_entropy: -0.99592, alpha: 0.25248, time: 68.07730
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 392.64856, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 12.08947, qf2_loss: 12.08697, policy_loss: -145.91244, policy_entropy: -1.01227, alpha: 0.25322, time: 68.07738
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 429.72989, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 12.40352, qf2_loss: 12.49016, policy_loss: -147.08124, policy_entropy: -1.01726, alpha: 0.25551, time: 68.13095
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 388.80104, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 12.38903, qf2_loss: 12.24154, policy_loss: -148.32172, policy_entropy: -1.00425, alpha: 0.25668, time: 67.83211
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 455.97369, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 13.16605, qf2_loss: 13.24488, policy_loss: -149.28357, policy_entropy: -1.01008, alpha: 0.25796, time: 67.67126
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 431.89358, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 13.68621, qf2_loss: 13.79144, policy_loss: -149.81602, policy_entropy: -1.01129, alpha: 0.25930, time: 67.70879
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 416.88283, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 13.32565, qf2_loss: 13.32273, policy_loss: -150.36797, policy_entropy: -1.01805, alpha: 0.26144, time: 67.68077
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 419.58151, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 12.76627, qf2_loss: 12.77257, policy_loss: -151.52481, policy_entropy: -1.00929, alpha: 0.26352, time: 67.65240
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 414.37410, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 13.25219, qf2_loss: 13.11742, policy_loss: -152.82221, policy_entropy: -1.00793, alpha: 0.26571, time: 67.63917
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 364.09597, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 14.94452, qf2_loss: 14.99840, policy_loss: -153.19384, policy_entropy: -1.00422, alpha: 0.26635, time: 67.70480
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 384.44447, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 12.88815, qf2_loss: 12.88929, policy_loss: -154.60125, policy_entropy: -1.00046, alpha: 0.26671, time: 67.64067
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 452.73387, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 13.44416, qf2_loss: 13.54723, policy_loss: -155.29601, policy_entropy: -1.00395, alpha: 0.26686, time: 67.61152
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 402.68168, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 13.03936, qf2_loss: 13.06573, policy_loss: -156.22332, policy_entropy: -1.00155, alpha: 0.26785, time: 67.63911
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 432.70965, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 14.30306, qf2_loss: 14.38589, policy_loss: -157.07459, policy_entropy: -1.01290, alpha: 0.26851, time: 67.61198
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 387.28883, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 13.13730, qf2_loss: 13.11720, policy_loss: -158.38405, policy_entropy: -1.00997, alpha: 0.27088, time: 67.57868
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 417.14845, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 11.41945, qf2_loss: 11.44754, policy_loss: -158.86775, policy_entropy: -1.01537, alpha: 0.27280, time: 67.52548
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 446.04958, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 12.97429, qf2_loss: 13.07540, policy_loss: -160.34827, policy_entropy: -1.00100, alpha: 0.27422, time: 67.58689
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 439.25069, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 12.48399, qf2_loss: 12.45052, policy_loss: -161.35017, policy_entropy: -1.01032, alpha: 0.27493, time: 67.51348
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 410.65931, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 12.22379, qf2_loss: 12.25697, policy_loss: -161.94151, policy_entropy: -1.00258, alpha: 0.27654, time: 67.49033
[CW] eval: return: 429.28348, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 457.66261, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 14.11691, qf2_loss: 14.13021, policy_loss: -163.74653, policy_entropy: -0.99776, alpha: 0.27719, time: 67.63818
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 426.21794, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 13.71853, qf2_loss: 13.80728, policy_loss: -164.43261, policy_entropy: -0.99933, alpha: 0.27611, time: 68.00668
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 390.44139, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 12.38943, qf2_loss: 12.45187, policy_loss: -164.61083, policy_entropy: -1.01627, alpha: 0.27694, time: 68.19195
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 456.30858, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 14.43881, qf2_loss: 14.42224, policy_loss: -165.15537, policy_entropy: -1.00438, alpha: 0.27954, time: 68.26227
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 450.19629, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 20.22568, qf2_loss: 20.47801, policy_loss: -166.67688, policy_entropy: -0.99260, alpha: 0.27936, time: 68.05119
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 416.60408, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 12.73184, qf2_loss: 12.77135, policy_loss: -167.39500, policy_entropy: -1.00792, alpha: 0.27953, time: 68.16986
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 496.03506, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 12.05061, qf2_loss: 12.12393, policy_loss: -168.24551, policy_entropy: -1.01348, alpha: 0.28117, time: 68.10840
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 432.65338, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 11.23196, qf2_loss: 11.31433, policy_loss: -169.60150, policy_entropy: -1.00588, alpha: 0.28305, time: 76.76669
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 456.78297, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 11.69974, qf2_loss: 11.83477, policy_loss: -170.05906, policy_entropy: -1.00296, alpha: 0.28438, time: 69.74585
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 395.45803, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 12.93671, qf2_loss: 13.02209, policy_loss: -171.34596, policy_entropy: -1.01076, alpha: 0.28550, time: 67.87435
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 370.11565, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 12.96688, qf2_loss: 12.82539, policy_loss: -172.22365, policy_entropy: -1.00873, alpha: 0.28702, time: 67.87065
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 466.42581, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 12.22205, qf2_loss: 12.24068, policy_loss: -173.11034, policy_entropy: -1.00276, alpha: 0.28845, time: 67.82341
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 420.38181, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 11.77792, qf2_loss: 11.76521, policy_loss: -173.79567, policy_entropy: -1.00574, alpha: 0.28942, time: 67.80557
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 426.22731, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 12.01030, qf2_loss: 12.09458, policy_loss: -174.45233, policy_entropy: -0.99949, alpha: 0.29096, time: 67.78305
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 443.21796, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 12.43929, qf2_loss: 12.50114, policy_loss: -175.97477, policy_entropy: -1.00277, alpha: 0.29083, time: 67.78952
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 383.65776, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 13.49402, qf2_loss: 13.63411, policy_loss: -176.12041, policy_entropy: -1.00000, alpha: 0.29069, time: 67.74788
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 439.78350, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 13.10625, qf2_loss: 13.13269, policy_loss: -176.97908, policy_entropy: -1.00157, alpha: 0.29068, time: 67.71042
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 427.89021, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 11.35829, qf2_loss: 11.37526, policy_loss: -177.97077, policy_entropy: -1.00355, alpha: 0.29096, time: 67.71850
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 443.08298, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 14.02301, qf2_loss: 14.05961, policy_loss: -178.74745, policy_entropy: -0.99282, alpha: 0.29119, time: 67.60791
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 410.91872, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 20.98126, qf2_loss: 21.05762, policy_loss: -180.29591, policy_entropy: -0.97914, alpha: 0.28871, time: 67.65451
[CW] eval: return: 445.36155, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 456.10787, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 12.34625, qf2_loss: 12.36784, policy_loss: -179.67936, policy_entropy: -1.00635, alpha: 0.28634, time: 67.67075
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 464.12478, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 11.65024, qf2_loss: 11.68559, policy_loss: -181.24334, policy_entropy: -1.00329, alpha: 0.28815, time: 67.66221
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 444.90500, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 10.81114, qf2_loss: 10.79364, policy_loss: -182.37622, policy_entropy: -1.00516, alpha: 0.28916, time: 67.65265
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 461.56105, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 12.53447, qf2_loss: 12.64104, policy_loss: -183.59381, policy_entropy: -1.00078, alpha: 0.28906, time: 67.61924
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 418.73528, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 11.55391, qf2_loss: 11.68253, policy_loss: -184.22783, policy_entropy: -1.01475, alpha: 0.29032, time: 67.57575
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 441.92625, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 11.63434, qf2_loss: 11.76466, policy_loss: -185.45002, policy_entropy: -1.01326, alpha: 0.29331, time: 67.62091
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 401.37452, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 11.36978, qf2_loss: 11.36819, policy_loss: -186.31293, policy_entropy: -1.00001, alpha: 0.29499, time: 67.63130
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 414.34476, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 11.63624, qf2_loss: 11.78854, policy_loss: -186.16717, policy_entropy: -1.01317, alpha: 0.29628, time: 67.69357
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 401.26638, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 11.90473, qf2_loss: 11.98938, policy_loss: -188.08516, policy_entropy: -0.99234, alpha: 0.29702, time: 68.15956
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 450.86095, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 15.60406, qf2_loss: 15.72309, policy_loss: -188.98192, policy_entropy: -0.99364, alpha: 0.29620, time: 68.21964
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 405.04077, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 13.40575, qf2_loss: 13.38298, policy_loss: -189.11207, policy_entropy: -1.00034, alpha: 0.29479, time: 68.16200
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 416.91841, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 11.94715, qf2_loss: 11.84127, policy_loss: -189.95178, policy_entropy: -1.00097, alpha: 0.29526, time: 68.17840
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 482.13723, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 10.87734, qf2_loss: 10.91310, policy_loss: -190.80652, policy_entropy: -1.00595, alpha: 0.29556, time: 68.19895
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 476.57334, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 11.21544, qf2_loss: 11.30800, policy_loss: -191.60531, policy_entropy: -1.00021, alpha: 0.29615, time: 68.00830
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 410.29280, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 11.35286, qf2_loss: 11.39032, policy_loss: -192.49191, policy_entropy: -1.00022, alpha: 0.29684, time: 67.54068
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 461.92905, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 11.66324, qf2_loss: 11.72427, policy_loss: -193.70646, policy_entropy: -1.00250, alpha: 0.29624, time: 67.55914
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 506.86891, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 10.32121, qf2_loss: 10.27505, policy_loss: -194.70611, policy_entropy: -1.00551, alpha: 0.29798, time: 67.55821
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 445.58895, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 10.27982, qf2_loss: 10.33536, policy_loss: -195.37059, policy_entropy: -1.00231, alpha: 0.29867, time: 67.49665
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 463.67207, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 11.44249, qf2_loss: 11.52600, policy_loss: -195.69615, policy_entropy: -1.00103, alpha: 0.29952, time: 67.55287
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 369.94680, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 12.23285, qf2_loss: 12.29521, policy_loss: -196.59472, policy_entropy: -0.98869, alpha: 0.29792, time: 67.53960
[CW] eval: return: 433.44406, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 424.68530, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 11.79811, qf2_loss: 11.87208, policy_loss: -197.58467, policy_entropy: -0.99453, alpha: 0.29649, time: 67.41098
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 382.05016, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 12.39359, qf2_loss: 12.38991, policy_loss: -198.44688, policy_entropy: -0.99787, alpha: 0.29544, time: 67.50581
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 450.99494, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 13.30198, qf2_loss: 13.48521, policy_loss: -198.55524, policy_entropy: -0.99182, alpha: 0.29396, time: 67.48625
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 423.79403, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 10.71962, qf2_loss: 10.82324, policy_loss: -200.28779, policy_entropy: -1.00977, alpha: 0.29355, time: 67.39671
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 464.12604, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 11.45795, qf2_loss: 11.28387, policy_loss: -200.93027, policy_entropy: -1.00514, alpha: 0.29580, time: 67.50414
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 409.79995, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 14.42683, qf2_loss: 14.77849, policy_loss: -201.49700, policy_entropy: -1.00236, alpha: 0.29671, time: 67.47809
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 370.50135, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 13.25186, qf2_loss: 13.29041, policy_loss: -201.81107, policy_entropy: -1.00162, alpha: 0.29638, time: 67.43098
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 541.76875, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 10.83921, qf2_loss: 10.93328, policy_loss: -203.59292, policy_entropy: -1.00234, alpha: 0.29751, time: 67.44772
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 402.15571, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 10.43066, qf2_loss: 10.46901, policy_loss: -203.57439, policy_entropy: -1.00603, alpha: 0.29821, time: 67.48872
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 462.85207, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 11.20488, qf2_loss: 11.24337, policy_loss: -204.55657, policy_entropy: -1.00594, alpha: 0.29969, time: 67.42805
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 456.90243, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 11.43602, qf2_loss: 11.46119, policy_loss: -205.16465, policy_entropy: -1.00352, alpha: 0.30017, time: 67.41624
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 438.03270, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 11.17060, qf2_loss: 11.20614, policy_loss: -206.56642, policy_entropy: -1.00128, alpha: 0.30098, time: 67.48369
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 462.51178, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 12.88610, qf2_loss: 13.02328, policy_loss: -206.32690, policy_entropy: -0.99206, alpha: 0.29990, time: 67.40730
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 442.45319, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 12.11324, qf2_loss: 12.04835, policy_loss: -206.96734, policy_entropy: -1.01110, alpha: 0.30070, time: 67.39613
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 405.53683, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 12.18883, qf2_loss: 12.33970, policy_loss: -208.04764, policy_entropy: -1.00529, alpha: 0.30334, time: 67.48270
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 357.51445, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 11.09209, qf2_loss: 11.13969, policy_loss: -209.70383, policy_entropy: -1.00211, alpha: 0.30379, time: 67.44303
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 439.20643, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 13.20417, qf2_loss: 13.24664, policy_loss: -210.29060, policy_entropy: -0.99638, alpha: 0.30361, time: 67.32247
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 303.11248, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 14.78105, qf2_loss: 14.87499, policy_loss: -210.05702, policy_entropy: -0.99913, alpha: 0.30297, time: 67.38805
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 416.84376, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 14.42731, qf2_loss: 14.65083, policy_loss: -211.46927, policy_entropy: -1.00487, alpha: 0.30348, time: 67.39711
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 474.58266, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 11.96255, qf2_loss: 11.92988, policy_loss: -212.69809, policy_entropy: -0.99850, alpha: 0.30379, time: 67.93202
[CW] eval: return: 428.86885, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 493.63638, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 11.75633, qf2_loss: 11.85205, policy_loss: -212.62538, policy_entropy: -1.01161, alpha: 0.30458, time: 67.32885
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 462.80222, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 12.36383, qf2_loss: 12.43156, policy_loss: -213.50169, policy_entropy: -0.99803, alpha: 0.30604, time: 67.31876
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 464.99952, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 12.05180, qf2_loss: 12.12055, policy_loss: -214.46029, policy_entropy: -1.00322, alpha: 0.30615, time: 67.34419
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 464.94861, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 11.49235, qf2_loss: 11.52243, policy_loss: -215.82820, policy_entropy: -0.99324, alpha: 0.30560, time: 70.20801
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 409.76909, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 13.22451, qf2_loss: 13.33367, policy_loss: -215.27352, policy_entropy: -0.99917, alpha: 0.30474, time: 67.32206
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 453.24283, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 12.08142, qf2_loss: 12.21737, policy_loss: -216.40483, policy_entropy: -1.00066, alpha: 0.30487, time: 67.33705
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 465.56332, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 12.65006, qf2_loss: 12.56788, policy_loss: -218.28342, policy_entropy: -1.00692, alpha: 0.30531, time: 67.32925
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 492.71465, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 12.25557, qf2_loss: 12.32368, policy_loss: -218.62842, policy_entropy: -0.99924, alpha: 0.30639, time: 67.35048
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 467.39101, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 11.24551, qf2_loss: 11.26110, policy_loss: -218.86339, policy_entropy: -1.00547, alpha: 0.30706, time: 67.28613
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 451.55641, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 11.94438, qf2_loss: 11.99664, policy_loss: -219.63299, policy_entropy: -1.01101, alpha: 0.30828, time: 67.28657
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 527.99443, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 11.23690, qf2_loss: 11.27398, policy_loss: -221.22032, policy_entropy: -0.99642, alpha: 0.30935, time: 67.29673
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 480.39183, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 11.86134, qf2_loss: 11.92802, policy_loss: -221.74421, policy_entropy: -0.99919, alpha: 0.30893, time: 67.33105
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 428.90406, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 11.93934, qf2_loss: 12.05948, policy_loss: -222.09918, policy_entropy: -1.00359, alpha: 0.30928, time: 67.32666
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 459.94532, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 13.64122, qf2_loss: 13.85302, policy_loss: -222.97926, policy_entropy: -1.00100, alpha: 0.30991, time: 67.36004
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 468.18065, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 11.86323, qf2_loss: 11.90674, policy_loss: -224.05854, policy_entropy: -1.00406, alpha: 0.31021, time: 67.36645
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 474.86172, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 12.34906, qf2_loss: 12.50588, policy_loss: -224.74933, policy_entropy: -0.99862, alpha: 0.31050, time: 67.31326
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 499.13135, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 11.34442, qf2_loss: 11.36184, policy_loss: -225.10564, policy_entropy: -1.00470, alpha: 0.31092, time: 67.29676
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 469.56409, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 12.13925, qf2_loss: 12.13780, policy_loss: -226.15557, policy_entropy: -1.00905, alpha: 0.31207, time: 67.30520
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 468.39073, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 11.77936, qf2_loss: 11.87292, policy_loss: -226.72226, policy_entropy: -1.01194, alpha: 0.31493, time: 67.31694
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 463.02360, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 12.77398, qf2_loss: 12.83863, policy_loss: -227.75246, policy_entropy: -1.00193, alpha: 0.31645, time: 67.24849
[CW] eval: return: 462.32931, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 508.06073, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 12.40690, qf2_loss: 12.31327, policy_loss: -228.21719, policy_entropy: -1.01254, alpha: 0.31734, time: 67.31497
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 420.13519, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 12.07337, qf2_loss: 12.05022, policy_loss: -229.28752, policy_entropy: -0.98841, alpha: 0.31834, time: 67.33884
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 445.50341, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 13.47524, qf2_loss: 13.54089, policy_loss: -230.01486, policy_entropy: -1.00421, alpha: 0.31712, time: 67.26460
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 502.91341, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 11.75432, qf2_loss: 11.82044, policy_loss: -230.49957, policy_entropy: -0.99987, alpha: 0.31768, time: 67.30281
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 443.09793, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 15.74737, qf2_loss: 15.70769, policy_loss: -231.27693, policy_entropy: -1.00542, alpha: 0.31853, time: 67.27166
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 532.82331, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 13.01856, qf2_loss: 13.16871, policy_loss: -231.64907, policy_entropy: -1.01044, alpha: 0.31996, time: 67.33360
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 489.77584, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 11.53398, qf2_loss: 11.49746, policy_loss: -233.03440, policy_entropy: -1.01282, alpha: 0.32265, time: 67.34487
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 450.75248, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 12.21594, qf2_loss: 12.21862, policy_loss: -233.03276, policy_entropy: -1.00395, alpha: 0.32472, time: 67.20043
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 437.72871, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 12.53250, qf2_loss: 12.69079, policy_loss: -234.43370, policy_entropy: -1.00726, alpha: 0.32621, time: 67.26290
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 471.64980, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 12.63927, qf2_loss: 12.67033, policy_loss: -235.49084, policy_entropy: -0.99125, alpha: 0.32655, time: 67.27156
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 501.14318, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 13.79575, qf2_loss: 13.93710, policy_loss: -235.54147, policy_entropy: -1.00979, alpha: 0.32648, time: 67.24207
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 478.90752, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 12.80661, qf2_loss: 12.83372, policy_loss: -236.72526, policy_entropy: -1.00398, alpha: 0.32831, time: 67.31841
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 510.22792, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 14.30503, qf2_loss: 14.33172, policy_loss: -236.77069, policy_entropy: -0.99765, alpha: 0.32837, time: 67.26248
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 459.69063, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 12.56476, qf2_loss: 12.51010, policy_loss: -237.75104, policy_entropy: -0.99664, alpha: 0.32817, time: 67.26485
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 497.36587, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 12.78142, qf2_loss: 12.77570, policy_loss: -238.63703, policy_entropy: -1.00703, alpha: 0.32861, time: 67.22161
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 488.48151, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 11.72723, qf2_loss: 11.77627, policy_loss: -239.29696, policy_entropy: -1.00401, alpha: 0.33017, time: 67.24929
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 495.25302, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 13.62523, qf2_loss: 13.85799, policy_loss: -239.49520, policy_entropy: -1.00467, alpha: 0.33091, time: 67.35979
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 507.64911, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 14.45917, qf2_loss: 14.39936, policy_loss: -240.33372, policy_entropy: -0.99476, alpha: 0.33138, time: 67.23585
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 459.12618, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 11.93386, qf2_loss: 11.98024, policy_loss: -241.23916, policy_entropy: -1.00344, alpha: 0.32986, time: 67.30862
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 457.63037, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 13.07284, qf2_loss: 13.00771, policy_loss: -242.30869, policy_entropy: -0.99947, alpha: 0.33054, time: 67.29438
[CW] eval: return: 473.63670, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 462.08051, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 12.71925, qf2_loss: 12.92867, policy_loss: -242.78065, policy_entropy: -1.00485, alpha: 0.33110, time: 67.25803
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 472.23025, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 12.21767, qf2_loss: 12.15434, policy_loss: -243.56276, policy_entropy: -1.01043, alpha: 0.33393, time: 67.26826
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 470.27929, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 12.33007, qf2_loss: 12.19572, policy_loss: -244.44462, policy_entropy: -0.99698, alpha: 0.33406, time: 67.25932
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 453.14157, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 12.78239, qf2_loss: 12.97213, policy_loss: -244.93997, policy_entropy: -1.00197, alpha: 0.33352, time: 67.26504
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 452.18204, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 11.82791, qf2_loss: 11.78305, policy_loss: -244.63113, policy_entropy: -1.01083, alpha: 0.33578, time: 67.21247
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 467.27313, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 11.70643, qf2_loss: 11.73345, policy_loss: -246.01634, policy_entropy: -1.00714, alpha: 0.33742, time: 67.20650
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 465.03146, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 11.80907, qf2_loss: 11.84188, policy_loss: -246.74308, policy_entropy: -1.00412, alpha: 0.33904, time: 67.29237
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 449.91829, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 12.36313, qf2_loss: 12.33692, policy_loss: -246.77762, policy_entropy: -1.00525, alpha: 0.34036, time: 67.22692
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 441.72335, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 14.58654, qf2_loss: 14.50199, policy_loss: -247.96954, policy_entropy: -0.99420, alpha: 0.34134, time: 67.17722
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 497.07069, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 13.16084, qf2_loss: 13.15479, policy_loss: -248.71328, policy_entropy: -0.99919, alpha: 0.33990, time: 67.25001
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 508.93723, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 12.61474, qf2_loss: 12.52697, policy_loss: -249.53148, policy_entropy: -0.99348, alpha: 0.33873, time: 68.21952
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 534.33317, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 12.62367, qf2_loss: 12.55756, policy_loss: -250.19588, policy_entropy: -0.99587, alpha: 0.33756, time: 67.12708
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 462.90746, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 15.77500, qf2_loss: 15.95865, policy_loss: -251.00170, policy_entropy: -0.99978, alpha: 0.33662, time: 67.15615
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 517.74956, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 13.05436, qf2_loss: 13.02467, policy_loss: -251.89710, policy_entropy: -1.00133, alpha: 0.33638, time: 67.11018
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 470.59793, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 13.15134, qf2_loss: 13.33246, policy_loss: -251.94044, policy_entropy: -0.99887, alpha: 0.33729, time: 67.11627
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 395.44104, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 11.99718, qf2_loss: 12.04641, policy_loss: -253.12660, policy_entropy: -1.00951, alpha: 0.33738, time: 67.17253
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 519.35464, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 14.36123, qf2_loss: 14.54591, policy_loss: -253.30298, policy_entropy: -0.99905, alpha: 0.33969, time: 69.81935
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 489.67910, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 12.19129, qf2_loss: 12.04680, policy_loss: -254.48765, policy_entropy: -1.00841, alpha: 0.34055, time: 67.14695
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 440.02667, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 12.06080, qf2_loss: 12.07870, policy_loss: -255.03669, policy_entropy: -1.00438, alpha: 0.34168, time: 67.06463
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 501.13960, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 12.07183, qf2_loss: 12.00422, policy_loss: -255.84436, policy_entropy: -1.00651, alpha: 0.34282, time: 67.19839
[CW] eval: return: 507.38939, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 359.86223, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 16.35409, qf2_loss: 16.50639, policy_loss: -256.71957, policy_entropy: -0.98899, alpha: 0.34260, time: 70.24972
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 529.59340, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 12.23956, qf2_loss: 12.17296, policy_loss: -257.53944, policy_entropy: -1.00445, alpha: 0.34177, time: 67.12124
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 525.04014, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 11.51918, qf2_loss: 11.46128, policy_loss: -257.54444, policy_entropy: -1.01198, alpha: 0.34372, time: 67.09353
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 481.79357, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 11.26054, qf2_loss: 11.24450, policy_loss: -257.58277, policy_entropy: -1.01375, alpha: 0.34757, time: 67.18983
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 514.28863, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 11.89240, qf2_loss: 11.86355, policy_loss: -259.34800, policy_entropy: -1.00648, alpha: 0.35015, time: 67.12110
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 518.86162, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 12.10904, qf2_loss: 12.23328, policy_loss: -259.76913, policy_entropy: -0.99528, alpha: 0.35071, time: 67.13182
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 516.78607, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 13.51445, qf2_loss: 13.47399, policy_loss: -260.72048, policy_entropy: -0.99977, alpha: 0.35060, time: 67.16961
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 462.62678, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 13.07851, qf2_loss: 13.05312, policy_loss: -261.82752, policy_entropy: -0.99863, alpha: 0.34964, time: 67.18472
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 518.40176, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 13.72273, qf2_loss: 13.78060, policy_loss: -262.03627, policy_entropy: -1.00214, alpha: 0.34996, time: 67.20283
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 449.14229, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 13.77475, qf2_loss: 13.86334, policy_loss: -262.29787, policy_entropy: -1.00146, alpha: 0.34941, time: 67.08931
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 411.97910, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 13.50766, qf2_loss: 13.36528, policy_loss: -262.62358, policy_entropy: -1.00138, alpha: 0.35044, time: 67.08246
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 439.88012, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 13.04608, qf2_loss: 13.14117, policy_loss: -264.03275, policy_entropy: -0.99810, alpha: 0.35051, time: 67.14247
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 593.65155, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 13.13687, qf2_loss: 13.20232, policy_loss: -264.69210, policy_entropy: -1.00445, alpha: 0.35169, time: 67.13961
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 520.09527, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 12.53014, qf2_loss: 12.55431, policy_loss: -264.62639, policy_entropy: -0.99961, alpha: 0.35161, time: 67.18131
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 520.87867, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 10.94710, qf2_loss: 10.99993, policy_loss: -265.69474, policy_entropy: -0.99849, alpha: 0.35134, time: 67.09305
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 527.69304, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 13.04474, qf2_loss: 13.16152, policy_loss: -266.36788, policy_entropy: -1.00053, alpha: 0.35165, time: 67.13020
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 453.20832, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 13.70884, qf2_loss: 13.61657, policy_loss: -266.51325, policy_entropy: -1.00039, alpha: 0.35279, time: 67.14547
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 520.57816, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 13.36170, qf2_loss: 13.31414, policy_loss: -268.39134, policy_entropy: -1.00146, alpha: 0.35165, time: 67.07560
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 497.43541, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 13.15275, qf2_loss: 13.31055, policy_loss: -267.80867, policy_entropy: -1.00467, alpha: 0.35223, time: 67.16767
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 585.51334, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 13.91608, qf2_loss: 13.97133, policy_loss: -269.06515, policy_entropy: -0.99877, alpha: 0.35324, time: 67.18089
[CW] eval: return: 511.68055, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 517.24264, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 12.88711, qf2_loss: 12.82997, policy_loss: -268.84450, policy_entropy: -0.99781, alpha: 0.35315, time: 67.15305
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 471.18111, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 12.64134, qf2_loss: 12.65055, policy_loss: -269.91675, policy_entropy: -1.00027, alpha: 0.35245, time: 67.08338
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 448.87024, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 12.33660, qf2_loss: 12.37531, policy_loss: -269.97946, policy_entropy: -1.00318, alpha: 0.35311, time: 67.13903
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 487.62649, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 15.15346, qf2_loss: 15.04007, policy_loss: -271.57839, policy_entropy: -0.98340, alpha: 0.35115, time: 67.11911
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 465.40938, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 13.96771, qf2_loss: 14.07640, policy_loss: -272.23357, policy_entropy: -0.99900, alpha: 0.34818, time: 67.12918
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 516.57040, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 13.29487, qf2_loss: 13.29160, policy_loss: -273.22236, policy_entropy: -1.00705, alpha: 0.34930, time: 67.10803
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 521.59988, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 13.70403, qf2_loss: 13.67008, policy_loss: -273.19700, policy_entropy: -1.00525, alpha: 0.35065, time: 67.06438
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 502.61535, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 14.08670, qf2_loss: 14.20860, policy_loss: -274.21662, policy_entropy: -1.00528, alpha: 0.35184, time: 67.18957
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 504.05775, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 12.24060, qf2_loss: 12.25305, policy_loss: -273.90984, policy_entropy: -1.00215, alpha: 0.35359, time: 67.15078
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 455.10579, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 12.64189, qf2_loss: 12.74030, policy_loss: -275.45871, policy_entropy: -1.00999, alpha: 0.35448, time: 67.16842
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 496.37211, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 12.88986, qf2_loss: 12.89903, policy_loss: -275.98724, policy_entropy: -0.99607, alpha: 0.35598, time: 67.10944
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 556.38689, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 12.53793, qf2_loss: 12.51011, policy_loss: -276.49731, policy_entropy: -0.99934, alpha: 0.35535, time: 67.14553
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 497.86508, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 12.33455, qf2_loss: 12.32957, policy_loss: -276.62585, policy_entropy: -1.00812, alpha: 0.35666, time: 67.13781
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 491.02877, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 14.20203, qf2_loss: 14.18107, policy_loss: -277.94401, policy_entropy: -0.99559, alpha: 0.35760, time: 67.08371
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 514.25891, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 14.82244, qf2_loss: 14.98437, policy_loss: -277.89645, policy_entropy: -0.99530, alpha: 0.35601, time: 67.07472
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 537.50198, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 14.99311, qf2_loss: 14.89670, policy_loss: -278.04718, policy_entropy: -1.00581, alpha: 0.35547, time: 67.10334
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 598.92837, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 13.10120, qf2_loss: 13.00780, policy_loss: -279.03349, policy_entropy: -1.00463, alpha: 0.35755, time: 67.16830
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 436.54587, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 13.29187, qf2_loss: 13.34688, policy_loss: -280.70088, policy_entropy: -1.00316, alpha: 0.35821, time: 67.12344
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 501.30609, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 13.58403, qf2_loss: 13.73224, policy_loss: -280.21366, policy_entropy: -0.99807, alpha: 0.35826, time: 67.12269
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 514.72059, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 15.14718, qf2_loss: 15.20426, policy_loss: -281.56514, policy_entropy: -0.99395, alpha: 0.35703, time: 67.21578
[CW] eval: return: 518.81272, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 511.07280, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 16.65649, qf2_loss: 16.91141, policy_loss: -281.97131, policy_entropy: -0.99695, alpha: 0.35629, time: 67.20588
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 521.62902, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 13.67810, qf2_loss: 13.65732, policy_loss: -282.79911, policy_entropy: -0.99950, alpha: 0.35593, time: 67.11231
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 538.21684, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 12.24230, qf2_loss: 12.30024, policy_loss: -283.12146, policy_entropy: -1.00596, alpha: 0.35631, time: 67.25623
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 547.58354, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 12.23869, qf2_loss: 12.26747, policy_loss: -283.45739, policy_entropy: -1.00436, alpha: 0.35773, time: 67.19003
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 509.32310, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 14.23035, qf2_loss: 14.30277, policy_loss: -284.07027, policy_entropy: -1.00053, alpha: 0.35905, time: 67.17292
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 585.79995, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 14.45458, qf2_loss: 14.50106, policy_loss: -285.00065, policy_entropy: -0.99910, alpha: 0.35806, time: 67.10354
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 498.67448, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 13.61330, qf2_loss: 13.58155, policy_loss: -284.83986, policy_entropy: -1.00477, alpha: 0.35883, time: 67.20641
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 545.20554, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 13.10238, qf2_loss: 13.17542, policy_loss: -287.08357, policy_entropy: -0.98894, alpha: 0.35882, time: 67.30403
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 528.53720, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 13.48696, qf2_loss: 13.51740, policy_loss: -286.99029, policy_entropy: -0.99974, alpha: 0.35751, time: 67.08897
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 528.10526, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 13.47968, qf2_loss: 13.43476, policy_loss: -287.19683, policy_entropy: -0.99743, alpha: 0.35626, time: 67.13486
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 566.47974, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 15.14255, qf2_loss: 15.38248, policy_loss: -288.64082, policy_entropy: -1.00764, alpha: 0.35802, time: 67.17587
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 583.78633, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 17.19166, qf2_loss: 17.09283, policy_loss: -289.09229, policy_entropy: -0.99866, alpha: 0.35743, time: 67.10037
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 538.40531, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 15.19713, qf2_loss: 15.19328, policy_loss: -289.83184, policy_entropy: -1.00589, alpha: 0.35770, time: 67.11639
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 441.92758, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 15.04896, qf2_loss: 15.09191, policy_loss: -288.81893, policy_entropy: -1.00269, alpha: 0.35996, time: 67.18324
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 521.23282, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 15.10349, qf2_loss: 15.28403, policy_loss: -290.67026, policy_entropy: -0.99819, alpha: 0.35980, time: 69.94798
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 571.15256, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 14.39821, qf2_loss: 14.19779, policy_loss: -290.53353, policy_entropy: -1.00517, alpha: 0.35902, time: 67.06482
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 460.50004, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 13.65717, qf2_loss: 13.84890, policy_loss: -291.56913, policy_entropy: -0.99985, alpha: 0.36144, time: 67.13694
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 550.36942, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 15.31022, qf2_loss: 15.13907, policy_loss: -291.51961, policy_entropy: -1.00070, alpha: 0.36126, time: 70.17154
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 557.44902, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 15.00081, qf2_loss: 14.91350, policy_loss: -292.86625, policy_entropy: -1.00038, alpha: 0.36090, time: 67.04544
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 516.12138, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 13.54907, qf2_loss: 13.46670, policy_loss: -293.56965, policy_entropy: -1.00192, alpha: 0.36117, time: 67.15284
[CW] eval: return: 543.68089, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 472.82938, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 20.80193, qf2_loss: 21.15849, policy_loss: -293.37299, policy_entropy: -0.99879, alpha: 0.36120, time: 67.17064
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 547.46952, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 13.48998, qf2_loss: 13.48269, policy_loss: -294.77826, policy_entropy: -1.00242, alpha: 0.36103, time: 67.13678
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 542.58708, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 12.92593, qf2_loss: 12.89586, policy_loss: -295.39468, policy_entropy: -1.00141, alpha: 0.36149, time: 67.12747
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 594.24224, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 14.63725, qf2_loss: 14.72038, policy_loss: -295.16778, policy_entropy: -1.00018, alpha: 0.36198, time: 67.13091
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 508.64438, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 16.15181, qf2_loss: 16.11619, policy_loss: -295.64425, policy_entropy: -0.99366, alpha: 0.36153, time: 67.09460
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 558.58755, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 16.32606, qf2_loss: 16.22765, policy_loss: -295.82769, policy_entropy: -0.99610, alpha: 0.36012, time: 67.00822
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 593.69910, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 15.26881, qf2_loss: 15.23139, policy_loss: -297.07395, policy_entropy: -0.99994, alpha: 0.35957, time: 67.13414
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 585.00491, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 14.91898, qf2_loss: 15.02620, policy_loss: -298.06204, policy_entropy: -0.99791, alpha: 0.35943, time: 67.16773
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 567.65452, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 14.69268, qf2_loss: 14.72390, policy_loss: -298.57855, policy_entropy: -0.99950, alpha: 0.35907, time: 67.10127
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 466.88107, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 15.15782, qf2_loss: 15.21946, policy_loss: -298.79817, policy_entropy: -1.00440, alpha: 0.35927, time: 67.14338
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 661.17188, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 14.50511, qf2_loss: 14.53846, policy_loss: -299.80280, policy_entropy: -1.00452, alpha: 0.36028, time: 67.14770
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 566.59318, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 15.13955, qf2_loss: 15.05933, policy_loss: -300.33511, policy_entropy: -0.99774, alpha: 0.36102, time: 67.11773
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 511.40504, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 16.20323, qf2_loss: 16.12333, policy_loss: -300.82351, policy_entropy: -1.00379, alpha: 0.36086, time: 67.13008
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 600.75417, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 16.43672, qf2_loss: 16.35755, policy_loss: -300.81436, policy_entropy: -1.00078, alpha: 0.36243, time: 67.16972
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 618.92112, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 16.27792, qf2_loss: 16.27041, policy_loss: -302.30251, policy_entropy: -0.99822, alpha: 0.36200, time: 67.16132
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 576.93296, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 22.04504, qf2_loss: 22.00317, policy_loss: -302.06349, policy_entropy: -0.98596, alpha: 0.35995, time: 67.07523
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 530.53951, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 17.36941, qf2_loss: 17.38693, policy_loss: -302.68980, policy_entropy: -0.99640, alpha: 0.35686, time: 67.10406
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 570.45984, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 15.70110, qf2_loss: 15.55383, policy_loss: -303.71884, policy_entropy: -1.01035, alpha: 0.35716, time: 67.07402
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 523.69983, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 15.29021, qf2_loss: 15.11825, policy_loss: -303.85619, policy_entropy: -1.00929, alpha: 0.36065, time: 67.03557
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 515.45653, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 17.63905, qf2_loss: 18.08578, policy_loss: -304.63128, policy_entropy: -1.00260, alpha: 0.36216, time: 67.09151
[CW] eval: return: 535.32621, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 598.51260, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 15.90127, qf2_loss: 15.77900, policy_loss: -304.67747, policy_entropy: -1.00178, alpha: 0.36247, time: 67.17321
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 531.90194, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 20.75076, qf2_loss: 20.77301, policy_loss: -305.75334, policy_entropy: -1.00033, alpha: 0.36345, time: 67.10065
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 596.95423, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 17.84014, qf2_loss: 17.80827, policy_loss: -306.38741, policy_entropy: -1.00259, alpha: 0.36269, time: 67.11547
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 518.36007, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 15.58586, qf2_loss: 15.68172, policy_loss: -306.99534, policy_entropy: -1.00036, alpha: 0.36349, time: 67.17882
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 536.08435, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 16.51599, qf2_loss: 16.53941, policy_loss: -307.18173, policy_entropy: -1.00201, alpha: 0.36423, time: 67.14198
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 546.10535, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 20.40174, qf2_loss: 20.32963, policy_loss: -308.80822, policy_entropy: -0.99991, alpha: 0.36412, time: 67.11465
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 527.87311, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 18.58186, qf2_loss: 18.58791, policy_loss: -308.31996, policy_entropy: -1.00252, alpha: 0.36445, time: 67.16901
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 581.50387, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 17.31508, qf2_loss: 17.49441, policy_loss: -309.96699, policy_entropy: -1.00457, alpha: 0.36567, time: 67.12244
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 536.55465, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 17.50955, qf2_loss: 17.73120, policy_loss: -310.35437, policy_entropy: -1.00896, alpha: 0.36751, time: 67.02383
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 489.38761, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 19.45989, qf2_loss: 19.35650, policy_loss: -310.94379, policy_entropy: -1.00379, alpha: 0.36921, time: 67.15482
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 522.44931, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 18.10414, qf2_loss: 18.05212, policy_loss: -310.96395, policy_entropy: -1.00957, alpha: 0.37076, time: 67.19532
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 472.26188, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 18.36333, qf2_loss: 18.18175, policy_loss: -311.27548, policy_entropy: -0.99862, alpha: 0.37242, time: 67.10514
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 596.39847, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 20.70778, qf2_loss: 20.97947, policy_loss: -311.94446, policy_entropy: -0.99446, alpha: 0.37151, time: 67.14926
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 603.57834, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 17.26117, qf2_loss: 17.19905, policy_loss: -311.81196, policy_entropy: -1.00631, alpha: 0.37202, time: 67.12858
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 594.42409, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 17.56562, qf2_loss: 17.74822, policy_loss: -313.01547, policy_entropy: -1.00315, alpha: 0.37374, time: 67.08126
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 518.97336, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 16.94103, qf2_loss: 16.85907, policy_loss: -313.63799, policy_entropy: -1.00149, alpha: 0.37392, time: 68.54201
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 629.27850, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 17.13932, qf2_loss: 17.17525, policy_loss: -314.45474, policy_entropy: -0.99649, alpha: 0.37394, time: 67.12655
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 779.49482, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 16.92345, qf2_loss: 16.86233, policy_loss: -314.66625, policy_entropy: -1.00341, alpha: 0.37373, time: 67.04673
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 464.09680, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 20.19104, qf2_loss: 20.27926, policy_loss: -315.42043, policy_entropy: -1.00132, alpha: 0.37431, time: 67.13752
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 533.88175, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 27.38893, qf2_loss: 27.75478, policy_loss: -316.76363, policy_entropy: -0.99352, alpha: 0.37321, time: 67.14688
[CW] eval: return: 583.11636, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 654.03655, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 19.14657, qf2_loss: 19.09510, policy_loss: -316.60131, policy_entropy: -1.00725, alpha: 0.37343, time: 67.10171
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 530.86691, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 19.37108, qf2_loss: 19.37960, policy_loss: -316.08912, policy_entropy: -1.01265, alpha: 0.37606, time: 67.15806
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 519.64401, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 19.19695, qf2_loss: 19.11861, policy_loss: -317.64365, policy_entropy: -1.00606, alpha: 0.37957, time: 67.17048
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 532.24031, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 20.92150, qf2_loss: 21.06976, policy_loss: -318.31456, policy_entropy: -1.00632, alpha: 0.38135, time: 67.06938
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 528.16939, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 18.84652, qf2_loss: 18.80229, policy_loss: -319.27288, policy_entropy: -1.00049, alpha: 0.38271, time: 67.14558
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 619.53909, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 18.46326, qf2_loss: 18.52993, policy_loss: -318.79807, policy_entropy: -1.00432, alpha: 0.38364, time: 67.11873
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 497.63855, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 24.39781, qf2_loss: 24.28042, policy_loss: -320.28738, policy_entropy: -0.99867, alpha: 0.38477, time: 67.10889
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 465.27235, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 18.81473, qf2_loss: 18.73956, policy_loss: -319.83134, policy_entropy: -0.99912, alpha: 0.38387, time: 67.18584
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 606.17701, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 21.19963, qf2_loss: 21.01395, policy_loss: -319.99702, policy_entropy: -1.00337, alpha: 0.38421, time: 67.16860
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 524.84147, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 23.87973, qf2_loss: 24.01475, policy_loss: -321.41196, policy_entropy: -1.00236, alpha: 0.38450, time: 67.34087
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 520.11137, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 19.70563, qf2_loss: 19.74325, policy_loss: -321.74728, policy_entropy: -1.00239, alpha: 0.38528, time: 67.12470
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 594.50126, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 17.98918, qf2_loss: 17.83346, policy_loss: -322.04182, policy_entropy: -1.00362, alpha: 0.38732, time: 69.42508
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 644.39819, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 19.27764, qf2_loss: 19.16355, policy_loss: -322.76426, policy_entropy: -1.00546, alpha: 0.38788, time: 67.15393
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 441.82872, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 19.19986, qf2_loss: 19.39017, policy_loss: -323.56791, policy_entropy: -1.00645, alpha: 0.39061, time: 67.13737
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 589.02033, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 21.79868, qf2_loss: 21.79491, policy_loss: -324.46230, policy_entropy: -1.00238, alpha: 0.39152, time: 70.24263
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 521.27560, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 20.52792, qf2_loss: 20.41572, policy_loss: -325.30828, policy_entropy: -1.00381, alpha: 0.39262, time: 67.18148
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 598.05609, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 20.32295, qf2_loss: 20.21825, policy_loss: -325.34818, policy_entropy: -1.00716, alpha: 0.39423, time: 67.15673
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 663.34921, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 30.74349, qf2_loss: 30.67527, policy_loss: -325.47512, policy_entropy: -1.00030, alpha: 0.39551, time: 67.11506
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 494.86384, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 24.19979, qf2_loss: 24.09261, policy_loss: -326.02750, policy_entropy: -1.00333, alpha: 0.39572, time: 67.10017
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 612.09437, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 21.31121, qf2_loss: 21.25183, policy_loss: -326.36092, policy_entropy: -1.01324, alpha: 0.39865, time: 67.12147
[CW] eval: return: 538.45200, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 601.02550, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 20.28926, qf2_loss: 20.20775, policy_loss: -327.85449, policy_entropy: -1.00262, alpha: 0.40148, time: 67.17294
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 585.10085, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 20.64063, qf2_loss: 20.86746, policy_loss: -328.24379, policy_entropy: -1.01189, alpha: 0.40388, time: 67.18114
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 518.46874, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 27.18194, qf2_loss: 27.56743, policy_loss: -328.55739, policy_entropy: -0.99874, alpha: 0.40694, time: 67.13234
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 474.17418, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 23.27041, qf2_loss: 23.07735, policy_loss: -329.09766, policy_entropy: -0.99821, alpha: 0.40513, time: 67.11072
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 452.37922, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 21.47148, qf2_loss: 21.52699, policy_loss: -329.53268, policy_entropy: -1.00439, alpha: 0.40640, time: 67.10214
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 657.32249, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 21.70312, qf2_loss: 21.69121, policy_loss: -329.92561, policy_entropy: -1.00969, alpha: 0.40761, time: 67.05202
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 514.73663, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 26.01311, qf2_loss: 25.64110, policy_loss: -331.06703, policy_entropy: -1.00572, alpha: 0.41069, time: 67.10066
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 606.97621, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 23.57640, qf2_loss: 23.49606, policy_loss: -330.71366, policy_entropy: -0.99962, alpha: 0.41147, time: 67.13928
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 582.32340, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 40.96230, qf2_loss: 41.44562, policy_loss: -331.76968, policy_entropy: -0.98690, alpha: 0.41087, time: 67.11213
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 521.55982, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 25.64219, qf2_loss: 25.86469, policy_loss: -331.56638, policy_entropy: -1.00468, alpha: 0.40865, time: 67.14126
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 535.39975, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 20.44347, qf2_loss: 20.31695, policy_loss: -333.17182, policy_entropy: -1.01002, alpha: 0.41058, time: 67.12026
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 732.68039, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 25.69265, qf2_loss: 25.59317, policy_loss: -333.14107, policy_entropy: -1.01501, alpha: 0.41508, time: 67.14445
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 597.39585, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 21.92915, qf2_loss: 21.70007, policy_loss: -334.16217, policy_entropy: -1.00605, alpha: 0.41828, time: 67.17057
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 579.18041, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 28.71321, qf2_loss: 28.75929, policy_loss: -334.77553, policy_entropy: -1.00118, alpha: 0.42044, time: 67.11542
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 453.11534, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 29.51057, qf2_loss: 29.80520, policy_loss: -334.97404, policy_entropy: -0.99355, alpha: 0.41831, time: 67.18133
[CW] ---------------------------
