{"collect/return": 453.1153446962435, "collect/steps": 1000.0, "collect/total_steps": 421000.0, "train/qf1_loss": 29.510567531585693, "train/qf2_loss": 29.805203762054443, "train/policy_loss": -334.9740408325195, "train/policy_entropy": -0.9935504829883576, "train/alpha": 0.41830779939889906, "train/time": 67.18132853507996, "eval/return": 538.4520047985501, "eval/steps": 1000.0, "_timestamp": 1678336163.6938756, "_runtime": 28676.349328517914, "_step": 415}