Hostname: uc2n905.localdomain
Found slurm config: SLURM
Seeting default slurm config
/home/kit/stud/uprnr/imgrerl/test_results/fixed_test-wr-sot-k1m1-f-seed0/fixed_test-wr-sot-k1m1-f-seed0/fixed_test-wr-sot-k1m1-f-seed0__env.ewalker-run/log/rep_00
[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
False
params: 
 {'env': {'env': 'walker-run'}} 

additionalVars: 
 {'seed': 0, 'agent': {'image_augmentation_K': 1, 'image_augmentation_M': 1, 'image_augmentation_type': <AugmentationType.SAME_OVER_TIME: 2>, 'image_augmentation_actor_critic_same_aug': False}}
conf_dict: 
 --------Config-------- 
seed: 0
cuda_id: 0
Subconfig: env
	env: walker-run
	obs_type: image
Subconfig: agent
	algo_name: sac
	encoder: lstm
	action_embedding_size: 32
	observ_embedding_size: 0
	reward_embedding_size: 32
	rnn_hidden_size: 512
	dqn_layers: [512, 512, 512]
	policy_layers: [512, 512, 512]
	lr: 0.0003
	gamma: 0.99
	tau: 0.005
	entropy_alpha: 1.0
	image_augmentation_type: AugmentationType.SAME_OVER_TIME
	image_augmentation_K: 1
	image_augmentation_M: 1
	image_augmentation_actor_critic_same_aug: False
Subconfig: rl
	sampled_seq_len: 64
	buffer_size: 1000000.0
	batch_size: 32
	rollouts_per_iter: 1
	num_updates_per_iter: 100
	num_init_rollouts: 5
Subconfig: eval
	interval: 20
	num_rollouts: 20
---------------------- 
 
Init feature extractor:  6 ;  32 ;  <function relu at 0x14a5129367a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x14a5129367a0>
Init feature extractor:  6 ;  164 ;  <function relu at 0x14a5129367a0>
Critic: 
Critic_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (current_shortcut_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=164, bias=True)
  )
  (qf1): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
  (qf2): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
)
Init feature extractor:  6 ;  32 ;  <function relu at 0x14a5129367a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x14a5129367a0>
Actor: 
Actor_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (policy): TanhGaussianPolicy(
    (fc0): Linear(in_features=612, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=6, bias=True)
    (last_fc_log_std): Linear(in_features=512, out_features=6, bias=True)
  )
)
buffer RAM usage: 11.48 GB
Collecting train stats
[CW] ---- Iteration:     0 ----
[CW] collect: return: 25.80113, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.67448, qf2_loss: 1.67528, policy_loss: -7.85521, policy_entropy: 4.09803, alpha: 0.98504, time: 39.34378
[CW] eval: return: 24.56383, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     1 ----
[CW] collect: return: 22.55400, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.08463, qf2_loss: 0.08495, policy_loss: -8.55539, policy_entropy: 4.10042, alpha: 0.95626, time: 32.42037
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     2 ----
[CW] collect: return: 23.88800, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.07539, qf2_loss: 0.07576, policy_loss: -9.27605, policy_entropy: 4.10055, alpha: 0.92871, time: 32.37056
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     3 ----
[CW] collect: return: 26.40622, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.06767, qf2_loss: 0.06794, policy_loss: -10.19984, policy_entropy: 4.09990, alpha: 0.90231, time: 32.31035
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     4 ----
[CW] collect: return: 26.38419, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06363, qf2_loss: 0.06367, policy_loss: -11.20642, policy_entropy: 4.10134, alpha: 0.87699, time: 32.39301
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     5 ----
[CW] collect: return: 22.68626, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.05965, qf2_loss: 0.05963, policy_loss: -12.26302, policy_entropy: 4.09977, alpha: 0.85267, time: 32.48368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     6 ----
[CW] collect: return: 25.60612, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.07004, qf2_loss: 0.06962, policy_loss: -13.37272, policy_entropy: 4.10124, alpha: 0.82931, time: 32.48900
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     7 ----
[CW] collect: return: 23.51585, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.07126, qf2_loss: 0.07118, policy_loss: -14.53230, policy_entropy: 4.10024, alpha: 0.80683, time: 32.48630
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     8 ----
[CW] collect: return: 28.03388, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.07713, qf2_loss: 0.07737, policy_loss: -15.72427, policy_entropy: 4.10222, alpha: 0.78520, time: 32.47311
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     9 ----
[CW] collect: return: 25.28811, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.08081, qf2_loss: 0.08126, policy_loss: -16.91856, policy_entropy: 4.10188, alpha: 0.76435, time: 32.39846
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    10 ----
[CW] collect: return: 25.26965, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.08379, qf2_loss: 0.08438, policy_loss: -18.09714, policy_entropy: 4.10136, alpha: 0.74426, time: 32.55795
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    11 ----
[CW] collect: return: 24.36337, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.08091, qf2_loss: 0.08158, policy_loss: -19.26027, policy_entropy: 4.10084, alpha: 0.72488, time: 32.52932
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    12 ----
[CW] collect: return: 25.13634, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.07200, qf2_loss: 0.07261, policy_loss: -20.40099, policy_entropy: 4.10053, alpha: 0.70617, time: 32.64018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    13 ----
[CW] collect: return: 25.99010, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.08457, qf2_loss: 0.08547, policy_loss: -21.51570, policy_entropy: 4.10115, alpha: 0.68809, time: 32.40734
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    14 ----
[CW] collect: return: 19.85541, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.06779, qf2_loss: 0.06848, policy_loss: -22.60248, policy_entropy: 4.10159, alpha: 0.67062, time: 32.65717
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    15 ----
[CW] collect: return: 30.99889, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.07651, qf2_loss: 0.07742, policy_loss: -23.66069, policy_entropy: 4.10190, alpha: 0.65372, time: 32.41261
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    16 ----
[CW] collect: return: 26.03716, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.08212, qf2_loss: 0.08323, policy_loss: -24.69537, policy_entropy: 4.10174, alpha: 0.63736, time: 32.42705
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    17 ----
[CW] collect: return: 25.05299, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.07210, qf2_loss: 0.07305, policy_loss: -25.69976, policy_entropy: 4.10206, alpha: 0.62152, time: 32.38279
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    18 ----
[CW] collect: return: 23.17140, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.08058, qf2_loss: 0.08179, policy_loss: -26.68043, policy_entropy: 4.10174, alpha: 0.60618, time: 32.32672
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    19 ----
[CW] collect: return: 21.56820, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.07378, qf2_loss: 0.07478, policy_loss: -27.62852, policy_entropy: 4.10227, alpha: 0.59131, time: 32.80270
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    20 ----
[CW] collect: return: 25.12112, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.10014, qf2_loss: 0.10192, policy_loss: -28.55687, policy_entropy: 4.10144, alpha: 0.57689, time: 32.41820
[CW] eval: return: 25.24865, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    21 ----
[CW] collect: return: 31.05537, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.05949, qf2_loss: 0.06027, policy_loss: -29.45335, policy_entropy: 4.10067, alpha: 0.56290, time: 32.29180
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    22 ----
[CW] collect: return: 26.45964, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.07966, qf2_loss: 0.08096, policy_loss: -30.32340, policy_entropy: 4.10187, alpha: 0.54933, time: 32.53377
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    23 ----
[CW] collect: return: 26.96223, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.07648, qf2_loss: 0.07788, policy_loss: -31.17054, policy_entropy: 4.10163, alpha: 0.53614, time: 32.70632
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    24 ----
[CW] collect: return: 26.90412, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.10830, qf2_loss: 0.11025, policy_loss: -31.99055, policy_entropy: 4.10153, alpha: 0.52334, time: 32.64735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    25 ----
[CW] collect: return: 24.35703, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.03650, qf2_loss: 0.03681, policy_loss: -32.78844, policy_entropy: 4.10140, alpha: 0.51090, time: 32.73352
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    26 ----
[CW] collect: return: 25.07290, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.09826, qf2_loss: 0.09997, policy_loss: -33.55622, policy_entropy: 4.10172, alpha: 0.49880, time: 32.65962
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    27 ----
[CW] collect: return: 24.34263, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.07456, qf2_loss: 0.07588, policy_loss: -34.30517, policy_entropy: 4.10135, alpha: 0.48704, time: 32.53083
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    28 ----
[CW] collect: return: 23.97589, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.06756, qf2_loss: 0.06863, policy_loss: -35.02818, policy_entropy: 4.10161, alpha: 0.47560, time: 32.65014
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    29 ----
[CW] collect: return: 24.20272, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.09645, qf2_loss: 0.09815, policy_loss: -35.73139, policy_entropy: 4.10100, alpha: 0.46448, time: 32.75862
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    30 ----
[CW] collect: return: 28.06420, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.05168, qf2_loss: 0.05240, policy_loss: -36.41669, policy_entropy: 4.10293, alpha: 0.45364, time: 32.56774
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    31 ----
[CW] collect: return: 23.31116, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.07870, qf2_loss: 0.08013, policy_loss: -37.07133, policy_entropy: 4.10279, alpha: 0.44310, time: 32.81774
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    32 ----
[CW] collect: return: 21.74355, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.05923, qf2_loss: 0.06006, policy_loss: -37.70577, policy_entropy: 4.10095, alpha: 0.43283, time: 32.58806
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    33 ----
[CW] collect: return: 24.98800, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.08087, qf2_loss: 0.08209, policy_loss: -38.32098, policy_entropy: 4.10122, alpha: 0.42283, time: 32.66962
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    34 ----
[CW] collect: return: 31.17913, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.05926, qf2_loss: 0.06013, policy_loss: -38.92139, policy_entropy: 4.10273, alpha: 0.41309, time: 32.48840
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    35 ----
[CW] collect: return: 25.89976, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.07996, qf2_loss: 0.08138, policy_loss: -39.49902, policy_entropy: 4.10078, alpha: 0.40359, time: 32.64773
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    36 ----
[CW] collect: return: 26.55498, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.07503, qf2_loss: 0.07620, policy_loss: -40.04528, policy_entropy: 4.10130, alpha: 0.39434, time: 32.42811
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    37 ----
[CW] collect: return: 22.68764, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.07735, qf2_loss: 0.07859, policy_loss: -40.59246, policy_entropy: 4.10174, alpha: 0.38532, time: 32.52372
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    38 ----
[CW] collect: return: 24.72660, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.05237, qf2_loss: 0.05311, policy_loss: -41.10755, policy_entropy: 4.10201, alpha: 0.37652, time: 32.57171
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    39 ----
[CW] collect: return: 20.34443, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.08334, qf2_loss: 0.08471, policy_loss: -41.60830, policy_entropy: 4.10098, alpha: 0.36794, time: 32.59387
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    40 ----
[CW] collect: return: 26.17942, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.08249, qf2_loss: 0.08371, policy_loss: -42.08618, policy_entropy: 4.10100, alpha: 0.35958, time: 32.55452
[CW] eval: return: 24.93348, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    41 ----
[CW] collect: return: 22.08771, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.05990, qf2_loss: 0.06078, policy_loss: -42.55275, policy_entropy: 4.10194, alpha: 0.35142, time: 32.45205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    42 ----
[CW] collect: return: 22.24678, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.10363, qf2_loss: 0.10540, policy_loss: -43.00258, policy_entropy: 4.10245, alpha: 0.34345, time: 32.52997
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    43 ----
[CW] collect: return: 24.39830, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.02573, qf2_loss: 0.02606, policy_loss: -43.43256, policy_entropy: 4.10153, alpha: 0.33568, time: 32.53387
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    44 ----
[CW] collect: return: 23.56806, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.06780, qf2_loss: 0.06879, policy_loss: -43.84769, policy_entropy: 4.10146, alpha: 0.32810, time: 32.61258
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    45 ----
[CW] collect: return: 22.09628, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.07007, qf2_loss: 0.07113, policy_loss: -44.24608, policy_entropy: 4.10033, alpha: 0.32070, time: 32.60429
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    46 ----
[CW] collect: return: 28.18570, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.09127, qf2_loss: 0.09249, policy_loss: -44.63088, policy_entropy: 4.10165, alpha: 0.31348, time: 32.57386
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    47 ----
[CW] collect: return: 23.69187, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.04080, qf2_loss: 0.04132, policy_loss: -45.00102, policy_entropy: 4.10177, alpha: 0.30642, time: 32.41782
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    48 ----
[CW] collect: return: 23.67653, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.07132, qf2_loss: 0.07236, policy_loss: -45.35829, policy_entropy: 4.10093, alpha: 0.29954, time: 32.45727
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    49 ----
[CW] collect: return: 25.07398, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.08849, qf2_loss: 0.08980, policy_loss: -45.70411, policy_entropy: 4.10148, alpha: 0.29281, time: 32.61596
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    50 ----
[CW] collect: return: 24.92068, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.02482, qf2_loss: 0.02501, policy_loss: -46.02645, policy_entropy: 4.10096, alpha: 0.28625, time: 32.50378
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    51 ----
[CW] collect: return: 25.14144, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.06122, qf2_loss: 0.06200, policy_loss: -46.34244, policy_entropy: 4.10130, alpha: 0.27983, time: 32.48850
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    52 ----
[CW] collect: return: 22.42784, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.06549, qf2_loss: 0.06635, policy_loss: -46.64450, policy_entropy: 4.10169, alpha: 0.27357, time: 32.39642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    53 ----
[CW] collect: return: 26.02286, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.05713, qf2_loss: 0.05794, policy_loss: -46.93135, policy_entropy: 4.10163, alpha: 0.26745, time: 32.59667
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    54 ----
[CW] collect: return: 24.82251, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.06691, qf2_loss: 0.06792, policy_loss: -47.21222, policy_entropy: 4.10049, alpha: 0.26147, time: 32.46884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    55 ----
[CW] collect: return: 25.38384, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.06035, qf2_loss: 0.06121, policy_loss: -47.47445, policy_entropy: 4.10187, alpha: 0.25563, time: 32.42323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    56 ----
[CW] collect: return: 27.37042, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.06544, qf2_loss: 0.06622, policy_loss: -47.72337, policy_entropy: 4.10161, alpha: 0.24993, time: 32.68191
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    57 ----
[CW] collect: return: 20.23084, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.05920, qf2_loss: 0.05984, policy_loss: -47.96544, policy_entropy: 4.10128, alpha: 0.24435, time: 32.43923
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    58 ----
[CW] collect: return: 24.54062, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.05118, qf2_loss: 0.05187, policy_loss: -48.19273, policy_entropy: 4.10165, alpha: 0.23890, time: 32.48142
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    59 ----
[CW] collect: return: 26.87023, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.06421, qf2_loss: 0.06497, policy_loss: -48.41140, policy_entropy: 4.10115, alpha: 0.23358, time: 32.58671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    60 ----
[CW] collect: return: 24.01567, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.05616, qf2_loss: 0.05680, policy_loss: -48.61902, policy_entropy: 4.10180, alpha: 0.22838, time: 32.53985
[CW] eval: return: 26.03103, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    61 ----
[CW] collect: return: 28.84614, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.05889, qf2_loss: 0.05980, policy_loss: -48.81514, policy_entropy: 4.10205, alpha: 0.22330, time: 32.52096
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    62 ----
[CW] collect: return: 25.47175, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.06251, qf2_loss: 0.06339, policy_loss: -49.00191, policy_entropy: 4.10200, alpha: 0.21833, time: 32.43432
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    63 ----
[CW] collect: return: 29.64792, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.05392, qf2_loss: 0.05474, policy_loss: -49.17511, policy_entropy: 4.10148, alpha: 0.21347, time: 32.48072
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    64 ----
[CW] collect: return: 25.59118, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.04492, qf2_loss: 0.04542, policy_loss: -49.34313, policy_entropy: 4.10085, alpha: 0.20873, time: 32.61823
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    65 ----
[CW] collect: return: 24.77004, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.07024, qf2_loss: 0.07107, policy_loss: -49.49918, policy_entropy: 4.10108, alpha: 0.20409, time: 32.44487
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    66 ----
[CW] collect: return: 22.32423, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.03627, qf2_loss: 0.03670, policy_loss: -49.64838, policy_entropy: 4.10261, alpha: 0.19956, time: 32.41806
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    67 ----
[CW] collect: return: 27.11711, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.07595, qf2_loss: 0.07701, policy_loss: -49.78646, policy_entropy: 4.10074, alpha: 0.19513, time: 32.57134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    68 ----
[CW] collect: return: 23.52169, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.04945, qf2_loss: 0.05001, policy_loss: -49.91796, policy_entropy: 4.10118, alpha: 0.19080, time: 32.60668
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    69 ----
[CW] collect: return: 23.13002, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.05171, qf2_loss: 0.05225, policy_loss: -50.03664, policy_entropy: 4.10118, alpha: 0.18656, time: 32.55019
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    70 ----
[CW] collect: return: 23.57621, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.04723, qf2_loss: 0.04776, policy_loss: -50.14616, policy_entropy: 4.10157, alpha: 0.18242, time: 32.67676
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    71 ----
[CW] collect: return: 23.20135, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.05403, qf2_loss: 0.05477, policy_loss: -50.25451, policy_entropy: 4.10260, alpha: 0.17838, time: 32.55310
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    72 ----
[CW] collect: return: 24.44142, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.05626, qf2_loss: 0.05693, policy_loss: -50.34737, policy_entropy: 4.10149, alpha: 0.17442, time: 32.83383
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    73 ----
[CW] collect: return: 30.69691, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.04475, qf2_loss: 0.04521, policy_loss: -50.43540, policy_entropy: 4.10082, alpha: 0.17055, time: 32.65575
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    74 ----
[CW] collect: return: 25.63702, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.06085, qf2_loss: 0.06162, policy_loss: -50.51898, policy_entropy: 4.10095, alpha: 0.16677, time: 32.49388
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    75 ----
[CW] collect: return: 24.14577, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.03737, qf2_loss: 0.03780, policy_loss: -50.59236, policy_entropy: 4.10261, alpha: 0.16308, time: 32.53620
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    76 ----
[CW] collect: return: 22.59462, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.05286, qf2_loss: 0.05340, policy_loss: -50.65852, policy_entropy: 4.10166, alpha: 0.15946, time: 32.84745
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    77 ----
[CW] collect: return: 26.80680, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.05402, qf2_loss: 0.05478, policy_loss: -50.71499, policy_entropy: 4.10144, alpha: 0.15593, time: 32.41064
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    78 ----
[CW] collect: return: 26.02430, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.04139, qf2_loss: 0.04177, policy_loss: -50.77131, policy_entropy: 4.10148, alpha: 0.15248, time: 32.55701
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    79 ----
[CW] collect: return: 25.36204, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.03778, qf2_loss: 0.03822, policy_loss: -50.81356, policy_entropy: 4.10278, alpha: 0.14910, time: 32.50787
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    80 ----
[CW] collect: return: 22.44819, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.05362, qf2_loss: 0.05409, policy_loss: -50.85293, policy_entropy: 4.10084, alpha: 0.14580, time: 32.52394
[CW] eval: return: 25.81509, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    81 ----
[CW] collect: return: 27.69844, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.06338, qf2_loss: 0.06390, policy_loss: -50.88865, policy_entropy: 4.10140, alpha: 0.14257, time: 32.49672
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    82 ----
[CW] collect: return: 24.99759, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.04227, qf2_loss: 0.04285, policy_loss: -50.91398, policy_entropy: 4.10101, alpha: 0.13941, time: 32.43273
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    83 ----
[CW] collect: return: 26.57647, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.08028, qf2_loss: 0.08114, policy_loss: -50.93456, policy_entropy: 4.10162, alpha: 0.13632, time: 32.36369
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    84 ----
[CW] collect: return: 24.85967, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.01121, qf2_loss: 0.01139, policy_loss: -50.95125, policy_entropy: 4.09987, alpha: 0.13331, time: 32.41967
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    85 ----
[CW] collect: return: 25.03475, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.04504, qf2_loss: 0.04555, policy_loss: -50.95756, policy_entropy: 4.10074, alpha: 0.13036, time: 32.65488
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    86 ----
[CW] collect: return: 32.66519, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.05770, qf2_loss: 0.05840, policy_loss: -50.96678, policy_entropy: 4.10131, alpha: 0.12747, time: 32.68838
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    87 ----
[CW] collect: return: 29.88004, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.02660, qf2_loss: 0.02691, policy_loss: -50.96541, policy_entropy: 4.10101, alpha: 0.12465, time: 32.44485
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    88 ----
[CW] collect: return: 25.07680, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.05239, qf2_loss: 0.05280, policy_loss: -50.95315, policy_entropy: 4.10211, alpha: 0.12189, time: 32.64319
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    89 ----
[CW] collect: return: 25.47071, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.03694, qf2_loss: 0.03757, policy_loss: -50.94604, policy_entropy: 4.10122, alpha: 0.11919, time: 32.46440
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    90 ----
[CW] collect: return: 27.93101, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.04342, qf2_loss: 0.04389, policy_loss: -50.93144, policy_entropy: 4.10034, alpha: 0.11655, time: 32.62817
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    91 ----
[CW] collect: return: 24.48006, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.04317, qf2_loss: 0.04369, policy_loss: -50.91218, policy_entropy: 4.10165, alpha: 0.11398, time: 32.60433
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    92 ----
[CW] collect: return: 24.80777, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.04393, qf2_loss: 0.04461, policy_loss: -50.88092, policy_entropy: 4.10189, alpha: 0.11145, time: 32.75592
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    93 ----
[CW] collect: return: 28.23802, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.06961, qf2_loss: 0.07059, policy_loss: -50.85157, policy_entropy: 4.10090, alpha: 0.10899, time: 32.42499
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    94 ----
[CW] collect: return: 23.96055, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.01504, qf2_loss: 0.01535, policy_loss: -50.81856, policy_entropy: 4.10071, alpha: 0.10658, time: 32.72190
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    95 ----
[CW] collect: return: 22.77986, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.03250, qf2_loss: 0.03298, policy_loss: -50.77885, policy_entropy: 4.10179, alpha: 0.10422, time: 32.51832
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    96 ----
[CW] collect: return: 24.85073, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.05125, qf2_loss: 0.05172, policy_loss: -50.73424, policy_entropy: 4.10171, alpha: 0.10192, time: 32.64458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    97 ----
[CW] collect: return: 25.54064, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.04176, qf2_loss: 0.04235, policy_loss: -50.68782, policy_entropy: 4.10152, alpha: 0.09966, time: 32.71820
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    98 ----
[CW] collect: return: 25.36439, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.03269, qf2_loss: 0.03331, policy_loss: -50.64197, policy_entropy: 4.10132, alpha: 0.09746, time: 32.50158
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    99 ----
[CW] collect: return: 24.68119, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.03823, qf2_loss: 0.03849, policy_loss: -50.57694, policy_entropy: 4.10217, alpha: 0.09530, time: 32.61765
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   100 ----
[CW] collect: return: 30.86502, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.04308, qf2_loss: 0.04358, policy_loss: -50.52100, policy_entropy: 4.10164, alpha: 0.09320, time: 32.57709
[CW] eval: return: 24.68664, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   101 ----
[CW] collect: return: 23.66858, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.02949, qf2_loss: 0.02982, policy_loss: -50.46181, policy_entropy: 4.10218, alpha: 0.09113, time: 33.00427
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   102 ----
[CW] collect: return: 24.37318, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.04893, qf2_loss: 0.04961, policy_loss: -50.39008, policy_entropy: 4.09995, alpha: 0.08912, time: 33.05248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   103 ----
[CW] collect: return: 24.74903, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.03966, qf2_loss: 0.04028, policy_loss: -50.32223, policy_entropy: 4.10118, alpha: 0.08715, time: 35.81959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   104 ----
[CW] collect: return: 23.19970, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.03138, qf2_loss: 0.03164, policy_loss: -50.25133, policy_entropy: 4.09991, alpha: 0.08522, time: 41.80121
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   105 ----
[CW] collect: return: 22.09909, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.07566, qf2_loss: 0.07669, policy_loss: -50.17235, policy_entropy: 4.10013, alpha: 0.08334, time: 32.92380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   106 ----
[CW] collect: return: 24.16469, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.00821, qf2_loss: 0.00838, policy_loss: -50.09845, policy_entropy: 4.10047, alpha: 0.08149, time: 33.12332
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   107 ----
[CW] collect: return: 23.54608, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.02671, qf2_loss: 0.02705, policy_loss: -50.01424, policy_entropy: 4.10140, alpha: 0.07969, time: 32.88067
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   108 ----
[CW] collect: return: 20.71213, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.03865, qf2_loss: 0.03908, policy_loss: -49.92632, policy_entropy: 4.10008, alpha: 0.07793, time: 32.88198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   109 ----
[CW] collect: return: 28.65810, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.04719, qf2_loss: 0.04762, policy_loss: -49.83820, policy_entropy: 4.10002, alpha: 0.07621, time: 32.99463
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   110 ----
[CW] collect: return: 20.09795, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.02305, qf2_loss: 0.02368, policy_loss: -49.74731, policy_entropy: 4.10063, alpha: 0.07452, time: 33.11028
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   111 ----
[CW] collect: return: 24.81494, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.02765, qf2_loss: 0.02778, policy_loss: -49.65303, policy_entropy: 4.10020, alpha: 0.07287, time: 33.23582
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   112 ----
[CW] collect: return: 23.52724, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.05677, qf2_loss: 0.05713, policy_loss: -49.55938, policy_entropy: 4.10083, alpha: 0.07126, time: 32.82254
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   113 ----
[CW] collect: return: 26.21111, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.01963, qf2_loss: 0.01995, policy_loss: -49.45749, policy_entropy: 4.09887, alpha: 0.06968, time: 33.00226
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   114 ----
[CW] collect: return: 26.52102, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.02481, qf2_loss: 0.02524, policy_loss: -49.36032, policy_entropy: 4.10036, alpha: 0.06814, time: 33.09075
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   115 ----
[CW] collect: return: 24.44021, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.04005, qf2_loss: 0.04065, policy_loss: -49.25834, policy_entropy: 4.10038, alpha: 0.06664, time: 33.13264
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   116 ----
[CW] collect: return: 23.21931, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.03190, qf2_loss: 0.03286, policy_loss: -49.15014, policy_entropy: 4.10022, alpha: 0.06516, time: 33.01326
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   117 ----
[CW] collect: return: 25.02694, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.03203, qf2_loss: 0.03215, policy_loss: -49.04265, policy_entropy: 4.09998, alpha: 0.06372, time: 32.97354
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   118 ----
[CW] collect: return: 23.34241, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.03896, qf2_loss: 0.03938, policy_loss: -48.93488, policy_entropy: 4.10120, alpha: 0.06231, time: 32.96228
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   119 ----
[CW] collect: return: 25.14347, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.02196, qf2_loss: 0.02239, policy_loss: -48.82431, policy_entropy: 4.10103, alpha: 0.06093, time: 32.71749
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   120 ----
[CW] collect: return: 24.84858, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.03603, qf2_loss: 0.03668, policy_loss: -48.72002, policy_entropy: 4.09963, alpha: 0.05959, time: 33.19672
[CW] eval: return: 24.46491, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   121 ----
[CW] collect: return: 24.79339, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.02894, qf2_loss: 0.02936, policy_loss: -48.59404, policy_entropy: 4.10029, alpha: 0.05827, time: 32.55580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   122 ----
[CW] collect: return: 26.33748, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.03208, qf2_loss: 0.03242, policy_loss: -48.47902, policy_entropy: 4.10090, alpha: 0.05698, time: 32.57344
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   123 ----
[CW] collect: return: 23.98590, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.04840, qf2_loss: 0.04935, policy_loss: -48.35740, policy_entropy: 4.10018, alpha: 0.05572, time: 32.70569
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   124 ----
[CW] collect: return: 23.10765, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.01611, qf2_loss: 0.01630, policy_loss: -48.23728, policy_entropy: 4.09947, alpha: 0.05449, time: 32.61346
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   125 ----
[CW] collect: return: 26.79751, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.03238, qf2_loss: 0.03288, policy_loss: -48.11515, policy_entropy: 4.10115, alpha: 0.05328, time: 32.62106
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   126 ----
[CW] collect: return: 24.13103, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.03875, qf2_loss: 0.03963, policy_loss: -47.99369, policy_entropy: 4.09990, alpha: 0.05210, time: 32.60386
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   127 ----
[CW] collect: return: 24.22550, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.02499, qf2_loss: 0.02515, policy_loss: -47.86630, policy_entropy: 4.10003, alpha: 0.05095, time: 32.63542
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   128 ----
[CW] collect: return: 26.47273, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.02222, qf2_loss: 0.02258, policy_loss: -47.74429, policy_entropy: 4.10096, alpha: 0.04983, time: 32.90904
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   129 ----
[CW] collect: return: 24.23622, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.04377, qf2_loss: 0.04411, policy_loss: -47.61384, policy_entropy: 4.09998, alpha: 0.04872, time: 32.44321
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   130 ----
[CW] collect: return: 23.15495, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.01823, qf2_loss: 0.01858, policy_loss: -47.48285, policy_entropy: 4.09797, alpha: 0.04765, time: 32.56396
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   131 ----
[CW] collect: return: 28.64377, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.03544, qf2_loss: 0.03620, policy_loss: -47.34801, policy_entropy: 4.09858, alpha: 0.04659, time: 32.59553
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   132 ----
[CW] collect: return: 20.53709, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.02431, qf2_loss: 0.02473, policy_loss: -47.22112, policy_entropy: 4.09851, alpha: 0.04556, time: 32.64077
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   133 ----
[CW] collect: return: 39.91882, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.02756, qf2_loss: 0.02802, policy_loss: -47.09081, policy_entropy: 4.09912, alpha: 0.04456, time: 32.80916
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   134 ----
[CW] collect: return: 25.08890, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.02810, qf2_loss: 0.02869, policy_loss: -46.95495, policy_entropy: 4.09926, alpha: 0.04357, time: 32.66103
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   135 ----
[CW] collect: return: 30.23211, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.02788, qf2_loss: 0.02818, policy_loss: -46.81648, policy_entropy: 4.09806, alpha: 0.04261, time: 32.78150
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   136 ----
[CW] collect: return: 24.90881, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.03161, qf2_loss: 0.03237, policy_loss: -46.68103, policy_entropy: 4.09793, alpha: 0.04167, time: 32.60651
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   137 ----
[CW] collect: return: 21.45078, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.03517, qf2_loss: 0.03572, policy_loss: -46.54182, policy_entropy: 4.09724, alpha: 0.04074, time: 32.99608
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   138 ----
[CW] collect: return: 26.79854, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.02058, qf2_loss: 0.02095, policy_loss: -46.40518, policy_entropy: 4.09854, alpha: 0.03984, time: 32.67026
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   139 ----
[CW] collect: return: 22.32076, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.02395, qf2_loss: 0.02432, policy_loss: -46.26780, policy_entropy: 4.10053, alpha: 0.03896, time: 33.11341
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   140 ----
[CW] collect: return: 22.77017, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.04303, qf2_loss: 0.04349, policy_loss: -46.12382, policy_entropy: 4.09937, alpha: 0.03810, time: 32.59576
[CW] eval: return: 25.39701, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   141 ----
[CW] collect: return: 24.44349, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.01596, qf2_loss: 0.01617, policy_loss: -45.97937, policy_entropy: 4.09843, alpha: 0.03726, time: 32.92377
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   142 ----
[CW] collect: return: 26.23270, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.02463, qf2_loss: 0.02519, policy_loss: -45.84107, policy_entropy: 4.09866, alpha: 0.03643, time: 32.58288
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   143 ----
[CW] collect: return: 25.65959, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.02851, qf2_loss: 0.02911, policy_loss: -45.69693, policy_entropy: 4.09900, alpha: 0.03563, time: 32.67716
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   144 ----
[CW] collect: return: 28.31969, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.02881, qf2_loss: 0.02927, policy_loss: -45.55263, policy_entropy: 4.09783, alpha: 0.03484, time: 32.62403
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   145 ----
[CW] collect: return: 24.18016, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.01980, qf2_loss: 0.02002, policy_loss: -45.40522, policy_entropy: 4.09747, alpha: 0.03407, time: 32.70493
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   146 ----
[CW] collect: return: 23.66383, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.02563, qf2_loss: 0.02611, policy_loss: -45.26005, policy_entropy: 4.09672, alpha: 0.03332, time: 32.80160
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   147 ----
[CW] collect: return: 23.61324, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.02931, qf2_loss: 0.02962, policy_loss: -45.11472, policy_entropy: 4.09707, alpha: 0.03258, time: 32.87833
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   148 ----
[CW] collect: return: 24.92849, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.03069, qf2_loss: 0.03114, policy_loss: -44.96712, policy_entropy: 4.09738, alpha: 0.03186, time: 32.65948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   149 ----
[CW] collect: return: 23.69990, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.02507, qf2_loss: 0.02544, policy_loss: -44.82428, policy_entropy: 4.09790, alpha: 0.03116, time: 32.91336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   150 ----
[CW] collect: return: 25.75781, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.02027, qf2_loss: 0.02046, policy_loss: -44.67962, policy_entropy: 4.09802, alpha: 0.03047, time: 32.52997
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   151 ----
[CW] collect: return: 25.23664, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.03131, qf2_loss: 0.03176, policy_loss: -44.52815, policy_entropy: 4.09809, alpha: 0.02979, time: 32.78329
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   152 ----
[CW] collect: return: 26.05878, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.02629, qf2_loss: 0.02682, policy_loss: -44.37844, policy_entropy: 4.09694, alpha: 0.02913, time: 32.75681
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   153 ----
[CW] collect: return: 25.09742, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.02356, qf2_loss: 0.02382, policy_loss: -44.22931, policy_entropy: 4.09441, alpha: 0.02849, time: 32.47745
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   154 ----
[CW] collect: return: 22.05605, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.02166, qf2_loss: 0.02196, policy_loss: -44.08015, policy_entropy: 4.09341, alpha: 0.02786, time: 32.69737
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   155 ----
[CW] collect: return: 22.54074, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.02824, qf2_loss: 0.02824, policy_loss: -43.92761, policy_entropy: 4.09289, alpha: 0.02725, time: 32.57271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   156 ----
[CW] collect: return: 26.52952, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.03429, qf2_loss: 0.03472, policy_loss: -43.78313, policy_entropy: 4.09390, alpha: 0.02664, time: 32.56687
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   157 ----
[CW] collect: return: 24.65552, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.01953, qf2_loss: 0.02015, policy_loss: -43.63141, policy_entropy: 4.09229, alpha: 0.02605, time: 32.65229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   158 ----
[CW] collect: return: 24.10879, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.01909, qf2_loss: 0.01930, policy_loss: -43.48129, policy_entropy: 4.09371, alpha: 0.02548, time: 32.67578
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   159 ----
[CW] collect: return: 27.19873, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.03547, qf2_loss: 0.03629, policy_loss: -43.32788, policy_entropy: 4.08958, alpha: 0.02492, time: 32.50368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   160 ----
[CW] collect: return: 27.70683, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.01394, qf2_loss: 0.01389, policy_loss: -43.17697, policy_entropy: 4.09405, alpha: 0.02437, time: 32.39034
[CW] eval: return: 26.20336, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   161 ----
[CW] collect: return: 26.95975, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.02542, qf2_loss: 0.02574, policy_loss: -43.02794, policy_entropy: 4.09226, alpha: 0.02383, time: 32.55738
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   162 ----
[CW] collect: return: 30.89722, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.01853, qf2_loss: 0.01874, policy_loss: -42.87503, policy_entropy: 4.09145, alpha: 0.02330, time: 32.53777
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   163 ----
[CW] collect: return: 29.67708, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.02836, qf2_loss: 0.02900, policy_loss: -42.72273, policy_entropy: 4.09085, alpha: 0.02279, time: 32.40539
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   164 ----
[CW] collect: return: 23.46965, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.02454, qf2_loss: 0.02495, policy_loss: -42.57346, policy_entropy: 4.08858, alpha: 0.02228, time: 32.57354
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   165 ----
[CW] collect: return: 26.22594, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.01885, qf2_loss: 0.01886, policy_loss: -42.42093, policy_entropy: 4.08718, alpha: 0.02179, time: 32.55549
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   166 ----
[CW] collect: return: 23.25796, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.03154, qf2_loss: 0.03214, policy_loss: -42.26729, policy_entropy: 4.08656, alpha: 0.02131, time: 32.60125
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   167 ----
[CW] collect: return: 26.46521, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.01905, qf2_loss: 0.01922, policy_loss: -42.11202, policy_entropy: 4.08475, alpha: 0.02084, time: 32.67080
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   168 ----
[CW] collect: return: 31.95527, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.02324, qf2_loss: 0.02372, policy_loss: -41.96079, policy_entropy: 4.08717, alpha: 0.02038, time: 32.46029
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   169 ----
[CW] collect: return: 25.29355, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.02515, qf2_loss: 0.02551, policy_loss: -41.80884, policy_entropy: 4.09130, alpha: 0.01993, time: 32.43358
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   170 ----
[CW] collect: return: 32.28834, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.02448, qf2_loss: 0.02503, policy_loss: -41.65699, policy_entropy: 4.08935, alpha: 0.01949, time: 32.47187
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   171 ----
[CW] collect: return: 24.18436, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.02675, qf2_loss: 0.02715, policy_loss: -41.50485, policy_entropy: 4.08641, alpha: 0.01906, time: 32.75794
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   172 ----
[CW] collect: return: 27.31227, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.02302, qf2_loss: 0.02328, policy_loss: -41.35305, policy_entropy: 4.08404, alpha: 0.01863, time: 32.52549
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   173 ----
[CW] collect: return: 24.62962, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.01925, qf2_loss: 0.01985, policy_loss: -41.19860, policy_entropy: 4.08624, alpha: 0.01822, time: 32.83780
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   174 ----
[CW] collect: return: 22.89640, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.02345, qf2_loss: 0.02380, policy_loss: -41.05418, policy_entropy: 4.08412, alpha: 0.01782, time: 32.54507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   175 ----
[CW] collect: return: 23.69219, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.02541, qf2_loss: 0.02596, policy_loss: -40.89560, policy_entropy: 4.08066, alpha: 0.01743, time: 32.59012
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   176 ----
[CW] collect: return: 23.59909, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.01685, qf2_loss: 0.01660, policy_loss: -40.73655, policy_entropy: 4.07587, alpha: 0.01704, time: 32.69384
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   177 ----
[CW] collect: return: 24.38657, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.02946, qf2_loss: 0.03027, policy_loss: -40.58552, policy_entropy: 4.08375, alpha: 0.01666, time: 32.45381
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   178 ----
[CW] collect: return: 27.24484, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.02054, qf2_loss: 0.02090, policy_loss: -40.43778, policy_entropy: 4.08184, alpha: 0.01630, time: 32.51285
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   179 ----
[CW] collect: return: 26.39264, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.01696, qf2_loss: 0.01712, policy_loss: -40.27701, policy_entropy: 4.07710, alpha: 0.01594, time: 32.68503
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   180 ----
[CW] collect: return: 24.12447, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.02984, qf2_loss: 0.03058, policy_loss: -40.12717, policy_entropy: 4.07721, alpha: 0.01558, time: 32.69224
[CW] eval: return: 25.00681, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   181 ----
[CW] collect: return: 26.24282, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.01703, qf2_loss: 0.01724, policy_loss: -39.97332, policy_entropy: 4.06060, alpha: 0.01524, time: 32.49898
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   182 ----
[CW] collect: return: 25.73196, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.02047, qf2_loss: 0.02079, policy_loss: -39.81595, policy_entropy: 4.05111, alpha: 0.01490, time: 32.42604
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   183 ----
[CW] collect: return: 24.42789, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.02542, qf2_loss: 0.02600, policy_loss: -39.67518, policy_entropy: 4.05920, alpha: 0.01458, time: 32.70763
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   184 ----
[CW] collect: return: 23.09984, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.02218, qf2_loss: 0.02257, policy_loss: -39.51858, policy_entropy: 4.03732, alpha: 0.01425, time: 32.52095
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   185 ----
[CW] collect: return: 33.71641, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.02275, qf2_loss: 0.02316, policy_loss: -39.36201, policy_entropy: 4.01828, alpha: 0.01394, time: 32.75379
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   186 ----
[CW] collect: return: 18.04123, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.02271, qf2_loss: 0.02330, policy_loss: -39.21889, policy_entropy: 4.00074, alpha: 0.01363, time: 32.50846
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   187 ----
[CW] collect: return: 21.02408, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.02527, qf2_loss: 0.02564, policy_loss: -39.07324, policy_entropy: 3.92642, alpha: 0.01334, time: 32.62253
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   188 ----
[CW] collect: return: 21.69561, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.02029, qf2_loss: 0.02079, policy_loss: -38.92197, policy_entropy: 3.93028, alpha: 0.01304, time: 32.52912
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   189 ----
[CW] collect: return: 28.00578, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.01623, qf2_loss: 0.01610, policy_loss: -38.76334, policy_entropy: 3.87797, alpha: 0.01276, time: 32.83905
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   190 ----
[CW] collect: return: 27.71207, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.02364, qf2_loss: 0.02419, policy_loss: -38.61384, policy_entropy: 3.87880, alpha: 0.01248, time: 32.74963
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   191 ----
[CW] collect: return: 19.62245, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.03071, qf2_loss: 0.03169, policy_loss: -38.46726, policy_entropy: 3.74832, alpha: 0.01221, time: 32.67643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   192 ----
[CW] collect: return: 25.78764, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.02059, qf2_loss: 0.02053, policy_loss: -38.32040, policy_entropy: 3.78189, alpha: 0.01195, time: 32.94992
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   193 ----
[CW] collect: return: 25.04747, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.01986, qf2_loss: 0.02040, policy_loss: -38.17196, policy_entropy: 3.65707, alpha: 0.01169, time: 32.73563
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   194 ----
[CW] collect: return: 24.88024, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.02955, qf2_loss: 0.02985, policy_loss: -38.02118, policy_entropy: 3.65263, alpha: 0.01144, time: 33.08286
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   195 ----
[CW] collect: return: 18.90029, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.01724, qf2_loss: 0.01703, policy_loss: -37.87639, policy_entropy: 3.59593, alpha: 0.01119, time: 32.35598
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   196 ----
[CW] collect: return: 25.60063, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.01990, qf2_loss: 0.02021, policy_loss: -37.72934, policy_entropy: 3.52912, alpha: 0.01096, time: 32.94316
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   197 ----
[CW] collect: return: 26.21683, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.02607, qf2_loss: 0.02690, policy_loss: -37.58306, policy_entropy: 3.34992, alpha: 0.01072, time: 32.69138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   198 ----
[CW] collect: return: 15.03894, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.02154, qf2_loss: 0.02208, policy_loss: -37.43134, policy_entropy: 3.46582, alpha: 0.01050, time: 32.75742
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   199 ----
[CW] collect: return: 25.52331, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.01939, qf2_loss: 0.02035, policy_loss: -37.28489, policy_entropy: 3.32547, alpha: 0.01027, time: 32.74435
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   200 ----
[CW] collect: return: 25.82975, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.03674, qf2_loss: 0.03770, policy_loss: -37.13836, policy_entropy: 3.54988, alpha: 0.01006, time: 32.67350
[CW] eval: return: 22.64109, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   201 ----
[CW] collect: return: 24.18675, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.01875, qf2_loss: 0.01899, policy_loss: -36.99255, policy_entropy: 3.05266, alpha: 0.00985, time: 32.78284
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   202 ----
[CW] collect: return: 27.61463, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.01733, qf2_loss: 0.01743, policy_loss: -36.84724, policy_entropy: 3.41447, alpha: 0.00964, time: 32.63708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   203 ----
[CW] collect: return: 22.27518, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.03068, qf2_loss: 0.03183, policy_loss: -36.69448, policy_entropy: 3.31979, alpha: 0.00943, time: 32.57314
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   204 ----
[CW] collect: return: 25.38030, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.01944, qf2_loss: 0.02002, policy_loss: -36.55165, policy_entropy: 2.93109, alpha: 0.00923, time: 32.64076
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   205 ----
[CW] collect: return: 15.53474, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.02591, qf2_loss: 0.02676, policy_loss: -36.40778, policy_entropy: 3.17146, alpha: 0.00904, time: 32.57470
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   206 ----
[CW] collect: return: 25.41813, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.01796, qf2_loss: 0.01890, policy_loss: -36.26018, policy_entropy: 3.16845, alpha: 0.00885, time: 32.65399
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   207 ----
[CW] collect: return: 26.06186, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.02685, qf2_loss: 0.02682, policy_loss: -36.11763, policy_entropy: 3.00755, alpha: 0.00866, time: 35.38868
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   208 ----
[CW] collect: return: 26.13164, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.03397, qf2_loss: 0.03535, policy_loss: -35.97397, policy_entropy: 0.62874, alpha: 0.00850, time: 33.05133
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   209 ----
[CW] collect: return: 35.26966, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.02037, qf2_loss: 0.02087, policy_loss: -35.84165, policy_entropy: 2.09524, alpha: 0.00836, time: 34.46844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   210 ----
[CW] collect: return: 25.09135, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.01698, qf2_loss: 0.01682, policy_loss: -35.71479, policy_entropy: 1.60016, alpha: 0.00820, time: 33.96955
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   211 ----
[CW] collect: return: 33.32151, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.04184, qf2_loss: 0.04328, policy_loss: -35.59200, policy_entropy: 1.50724, alpha: 0.00805, time: 32.97346
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   212 ----
[CW] collect: return: 25.11247, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.02304, qf2_loss: 0.02381, policy_loss: -35.46234, policy_entropy: 1.15407, alpha: 0.00790, time: 32.67127
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   213 ----
[CW] collect: return: 32.12515, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.03463, qf2_loss: 0.03406, policy_loss: -35.34064, policy_entropy: 0.66132, alpha: 0.00777, time: 32.81349
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   214 ----
[CW] collect: return: 24.01005, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 0.03349, qf2_loss: 0.03446, policy_loss: -35.22434, policy_entropy: 0.16634, alpha: 0.00764, time: 32.58344
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   215 ----
[CW] collect: return: 28.45411, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.03125, qf2_loss: 0.03211, policy_loss: -35.09661, policy_entropy: 0.32895, alpha: 0.00752, time: 32.63275
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   216 ----
[CW] collect: return: 54.53033, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.03382, qf2_loss: 0.03409, policy_loss: -34.97796, policy_entropy: 0.84115, alpha: 0.00739, time: 32.84485
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   217 ----
[CW] collect: return: 39.05865, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.03809, qf2_loss: 0.03813, policy_loss: -34.87973, policy_entropy: -0.37491, alpha: 0.00726, time: 32.40619
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   218 ----
[CW] collect: return: 43.24362, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.03326, qf2_loss: 0.03373, policy_loss: -34.77464, policy_entropy: -1.08766, alpha: 0.00716, time: 32.89689
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   219 ----
[CW] collect: return: 22.95516, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.04508, qf2_loss: 0.04590, policy_loss: -34.68118, policy_entropy: -1.11774, alpha: 0.00706, time: 32.62782
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   220 ----
[CW] collect: return: 45.04328, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.03217, qf2_loss: 0.03238, policy_loss: -34.58153, policy_entropy: -3.10222, alpha: 0.00697, time: 32.61909
[CW] eval: return: 27.92289, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   221 ----
[CW] collect: return: 9.39819, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.05239, qf2_loss: 0.05420, policy_loss: -34.48207, policy_entropy: -2.28511, alpha: 0.00691, time: 32.93586
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   222 ----
[CW] collect: return: 33.92589, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.03652, qf2_loss: 0.03703, policy_loss: -34.38461, policy_entropy: -1.16936, alpha: 0.00682, time: 32.81031
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   223 ----
[CW] collect: return: 24.56656, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.04657, qf2_loss: 0.04795, policy_loss: -34.29554, policy_entropy: -1.34816, alpha: 0.00672, time: 32.50939
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   224 ----
[CW] collect: return: 23.14837, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.04595, qf2_loss: 0.04631, policy_loss: -34.20704, policy_entropy: -1.58809, alpha: 0.00662, time: 32.84443
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   225 ----
[CW] collect: return: 24.84378, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.05547, qf2_loss: 0.05634, policy_loss: -34.11385, policy_entropy: -1.40939, alpha: 0.00652, time: 32.57100
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   226 ----
[CW] collect: return: 10.00925, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.03559, qf2_loss: 0.03584, policy_loss: -34.04327, policy_entropy: -3.54163, alpha: 0.00643, time: 32.82315
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   227 ----
[CW] collect: return: 10.20463, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.04548, qf2_loss: 0.04585, policy_loss: -33.94651, policy_entropy: -2.78652, alpha: 0.00638, time: 32.49021
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   228 ----
[CW] collect: return: 23.03380, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 0.05051, qf2_loss: 0.05135, policy_loss: -33.88570, policy_entropy: -2.03039, alpha: 0.00630, time: 32.81146
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   229 ----
[CW] collect: return: 56.05937, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.05634, qf2_loss: 0.05667, policy_loss: -33.82985, policy_entropy: -3.19587, alpha: 0.00622, time: 32.50626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   230 ----
[CW] collect: return: 43.21250, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.04980, qf2_loss: 0.05059, policy_loss: -33.76047, policy_entropy: -3.74501, alpha: 0.00616, time: 32.75730
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   231 ----
[CW] collect: return: 22.72805, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 0.03976, qf2_loss: 0.04006, policy_loss: -33.68949, policy_entropy: -2.97451, alpha: 0.00609, time: 32.82861
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   232 ----
[CW] collect: return: 58.70959, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.03423, qf2_loss: 0.03436, policy_loss: -33.63893, policy_entropy: -3.60617, alpha: 0.00602, time: 32.68574
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   233 ----
[CW] collect: return: 52.95623, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.05201, qf2_loss: 0.05234, policy_loss: -33.55643, policy_entropy: -3.10459, alpha: 0.00596, time: 32.94445
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   234 ----
[CW] collect: return: 37.96939, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.04344, qf2_loss: 0.04328, policy_loss: -33.50032, policy_entropy: -2.70938, alpha: 0.00588, time: 32.54178
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   235 ----
[CW] collect: return: 4.99555, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.07759, qf2_loss: 0.07934, policy_loss: -33.38945, policy_entropy: -2.49376, alpha: 0.00580, time: 32.78973
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   236 ----
[CW] collect: return: 43.37631, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.03876, qf2_loss: 0.03866, policy_loss: -33.26706, policy_entropy: -1.43116, alpha: 0.00569, time: 32.56819
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   237 ----
[CW] collect: return: 20.90617, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 0.04219, qf2_loss: 0.04231, policy_loss: -33.14862, policy_entropy: -1.79369, alpha: 0.00558, time: 32.87633
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   238 ----
[CW] collect: return: 13.57093, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 0.04318, qf2_loss: 0.04308, policy_loss: -33.05346, policy_entropy: -3.52180, alpha: 0.00548, time: 32.71716
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   239 ----
[CW] collect: return: 83.30333, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 0.04738, qf2_loss: 0.04819, policy_loss: -32.98795, policy_entropy: -4.16752, alpha: 0.00543, time: 32.75686
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   240 ----
[CW] collect: return: 50.34826, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.04058, qf2_loss: 0.04138, policy_loss: -32.87627, policy_entropy: -4.41904, alpha: 0.00538, time: 32.94521
[CW] eval: return: 37.88494, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   241 ----
[CW] collect: return: 20.36007, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 0.05480, qf2_loss: 0.05400, policy_loss: -32.78174, policy_entropy: -4.70560, alpha: 0.00534, time: 32.78090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   242 ----
[CW] collect: return: 18.14382, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 0.05642, qf2_loss: 0.05736, policy_loss: -32.68936, policy_entropy: -4.28516, alpha: 0.00531, time: 32.78963
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   243 ----
[CW] collect: return: 41.65095, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 0.03878, qf2_loss: 0.03894, policy_loss: -32.59437, policy_entropy: -2.89186, alpha: 0.00524, time: 33.46606
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   244 ----
[CW] collect: return: 27.78261, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 0.03360, qf2_loss: 0.03307, policy_loss: -32.51432, policy_entropy: -3.30188, alpha: 0.00515, time: 32.50809
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   245 ----
[CW] collect: return: 27.29123, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 0.03804, qf2_loss: 0.03824, policy_loss: -32.42219, policy_entropy: -2.20381, alpha: 0.00506, time: 32.70584
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   246 ----
[CW] collect: return: 27.69127, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 0.03955, qf2_loss: 0.03884, policy_loss: -32.29653, policy_entropy: -1.73717, alpha: 0.00495, time: 33.06105
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   247 ----
[CW] collect: return: 19.13723, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 0.03825, qf2_loss: 0.03905, policy_loss: -32.19514, policy_entropy: -0.70685, alpha: 0.00481, time: 32.46967
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   248 ----
[CW] collect: return: 21.59808, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 0.04006, qf2_loss: 0.04052, policy_loss: -32.08713, policy_entropy: -0.36518, alpha: 0.00467, time: 32.86093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   249 ----
[CW] collect: return: 102.54595, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 0.03094, qf2_loss: 0.03125, policy_loss: -32.05473, policy_entropy: -1.45766, alpha: 0.00454, time: 32.68125
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   250 ----
[CW] collect: return: 41.30807, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 0.03742, qf2_loss: 0.03736, policy_loss: -31.91985, policy_entropy: -1.55925, alpha: 0.00444, time: 32.60851
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   251 ----
[CW] collect: return: 45.84956, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 0.03299, qf2_loss: 0.03307, policy_loss: -31.86991, policy_entropy: -2.44094, alpha: 0.00434, time: 32.68812
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   252 ----
[CW] collect: return: 48.21384, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 0.05421, qf2_loss: 0.05430, policy_loss: -31.81130, policy_entropy: -2.52501, alpha: 0.00426, time: 33.03448
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   253 ----
[CW] collect: return: 45.30616, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 0.03923, qf2_loss: 0.03938, policy_loss: -31.78137, policy_entropy: -3.07310, alpha: 0.00418, time: 32.66396
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   254 ----
[CW] collect: return: 45.58482, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 0.07457, qf2_loss: 0.07655, policy_loss: -31.67310, policy_entropy: -3.26976, alpha: 0.00412, time: 32.83748
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   255 ----
[CW] collect: return: 40.74946, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 0.03921, qf2_loss: 0.03929, policy_loss: -31.62878, policy_entropy: -4.57181, alpha: 0.00407, time: 32.69462
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   256 ----
[CW] collect: return: 46.76736, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 0.05151, qf2_loss: 0.05140, policy_loss: -31.59380, policy_entropy: -4.04819, alpha: 0.00404, time: 32.75725
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   257 ----
[CW] collect: return: 91.32171, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 0.04455, qf2_loss: 0.04446, policy_loss: -31.52419, policy_entropy: -4.75086, alpha: 0.00399, time: 32.82023
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   258 ----
[CW] collect: return: 60.36187, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 0.06827, qf2_loss: 0.06893, policy_loss: -31.46119, policy_entropy: -6.16714, alpha: 0.00398, time: 32.80903
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   259 ----
[CW] collect: return: 81.27863, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 0.05767, qf2_loss: 0.05729, policy_loss: -31.40091, policy_entropy: -7.43012, alpha: 0.00399, time: 32.69090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   260 ----
[CW] collect: return: 42.80782, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 0.05141, qf2_loss: 0.05240, policy_loss: -31.36548, policy_entropy: -6.87791, alpha: 0.00403, time: 32.48109
[CW] eval: return: 35.20879, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   261 ----
[CW] collect: return: 44.21852, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 0.04948, qf2_loss: 0.04996, policy_loss: -31.22768, policy_entropy: -5.53794, alpha: 0.00404, time: 32.73029
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   262 ----
[CW] collect: return: 35.84275, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 0.04642, qf2_loss: 0.04608, policy_loss: -31.16089, policy_entropy: -5.58514, alpha: 0.00403, time: 32.63859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   263 ----
[CW] collect: return: 37.59511, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 0.05978, qf2_loss: 0.06085, policy_loss: -31.02507, policy_entropy: -3.62467, alpha: 0.00399, time: 32.54248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   264 ----
[CW] collect: return: 45.07388, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 0.04283, qf2_loss: 0.04273, policy_loss: -31.00972, policy_entropy: -1.59531, alpha: 0.00389, time: 32.78852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   265 ----
[CW] collect: return: 23.37315, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 0.05576, qf2_loss: 0.05561, policy_loss: -30.92792, policy_entropy: -5.32851, alpha: 0.00379, time: 32.66333
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   266 ----
[CW] collect: return: 22.07707, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 0.06706, qf2_loss: 0.06798, policy_loss: -30.87603, policy_entropy: -2.93826, alpha: 0.00375, time: 32.74678
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   267 ----
[CW] collect: return: 24.10479, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 0.04319, qf2_loss: 0.04278, policy_loss: -30.79498, policy_entropy: -3.62179, alpha: 0.00368, time: 32.66028
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   268 ----
[CW] collect: return: 24.03057, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 0.04591, qf2_loss: 0.04525, policy_loss: -30.71864, policy_entropy: -2.91243, alpha: 0.00361, time: 32.68932
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   269 ----
[CW] collect: return: 36.60598, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 0.04783, qf2_loss: 0.04864, policy_loss: -30.73218, policy_entropy: -1.53126, alpha: 0.00350, time: 32.99141
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   270 ----
[CW] collect: return: 23.67998, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 0.05393, qf2_loss: 0.05586, policy_loss: -30.59331, policy_entropy: -2.09574, alpha: 0.00340, time: 32.79724
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   271 ----
[CW] collect: return: 7.84037, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 0.04376, qf2_loss: 0.04439, policy_loss: -30.55748, policy_entropy: -3.37970, alpha: 0.00332, time: 33.04588
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   272 ----
[CW] collect: return: 44.44099, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 0.04815, qf2_loss: 0.04902, policy_loss: -30.50636, policy_entropy: -3.07257, alpha: 0.00325, time: 32.54900
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   273 ----
[CW] collect: return: 23.14757, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 0.06679, qf2_loss: 0.06651, policy_loss: -30.41031, policy_entropy: -2.96237, alpha: 0.00318, time: 33.10535
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   274 ----
[CW] collect: return: 24.87127, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 0.04264, qf2_loss: 0.04374, policy_loss: -30.29985, policy_entropy: -5.18122, alpha: 0.00313, time: 32.43347
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   275 ----
[CW] collect: return: 22.83922, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 0.04713, qf2_loss: 0.04813, policy_loss: -30.23170, policy_entropy: -6.49185, alpha: 0.00313, time: 32.82857
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   276 ----
[CW] collect: return: 32.35273, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 0.04701, qf2_loss: 0.04733, policy_loss: -30.20500, policy_entropy: -5.09976, alpha: 0.00313, time: 32.84175
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   277 ----
[CW] collect: return: 21.86287, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 0.05256, qf2_loss: 0.05257, policy_loss: -30.08813, policy_entropy: -6.09836, alpha: 0.00311, time: 32.75000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   278 ----
[CW] collect: return: 4.21588, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 0.06210, qf2_loss: 0.06301, policy_loss: -30.04151, policy_entropy: -4.60767, alpha: 0.00311, time: 32.79588
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   279 ----
[CW] collect: return: 44.19839, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 0.03905, qf2_loss: 0.03912, policy_loss: -29.94955, policy_entropy: -3.09309, alpha: 0.00304, time: 32.82380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   280 ----
[CW] collect: return: 23.44702, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 0.04344, qf2_loss: 0.04414, policy_loss: -29.91098, policy_entropy: -3.83229, alpha: 0.00298, time: 33.13123
[CW] eval: return: 34.69359, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   281 ----
[CW] collect: return: 20.98597, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 0.04211, qf2_loss: 0.04180, policy_loss: -29.82584, policy_entropy: -4.28793, alpha: 0.00293, time: 33.01938
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   282 ----
[CW] collect: return: 44.30411, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 0.05828, qf2_loss: 0.05810, policy_loss: -29.71914, policy_entropy: -4.15552, alpha: 0.00288, time: 32.47758
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   283 ----
[CW] collect: return: 23.39334, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 0.03849, qf2_loss: 0.03932, policy_loss: -29.69149, policy_entropy: -4.49297, alpha: 0.00284, time: 33.07308
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   284 ----
[CW] collect: return: 43.73333, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 0.03260, qf2_loss: 0.03279, policy_loss: -29.48242, policy_entropy: -4.49820, alpha: 0.00280, time: 32.68692
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   285 ----
[CW] collect: return: 44.35738, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 0.03719, qf2_loss: 0.03699, policy_loss: -29.47748, policy_entropy: -4.10188, alpha: 0.00276, time: 32.85580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   286 ----
[CW] collect: return: 23.33153, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 0.03784, qf2_loss: 0.03814, policy_loss: -29.33165, policy_entropy: -4.72844, alpha: 0.00271, time: 32.51371
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   287 ----
[CW] collect: return: 27.22570, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 0.03528, qf2_loss: 0.03574, policy_loss: -29.28116, policy_entropy: -5.18169, alpha: 0.00268, time: 32.76605
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   288 ----
[CW] collect: return: 23.55743, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 0.03763, qf2_loss: 0.03781, policy_loss: -29.27006, policy_entropy: -4.61489, alpha: 0.00266, time: 33.07292
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   289 ----
[CW] collect: return: 45.03385, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 0.05425, qf2_loss: 0.05558, policy_loss: -29.20427, policy_entropy: -4.86025, alpha: 0.00262, time: 32.88197
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   290 ----
[CW] collect: return: 45.38777, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 0.04658, qf2_loss: 0.04590, policy_loss: -29.04587, policy_entropy: -5.68251, alpha: 0.00260, time: 32.83291
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   291 ----
[CW] collect: return: 55.88968, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 0.03166, qf2_loss: 0.03140, policy_loss: -28.99216, policy_entropy: -5.48745, alpha: 0.00259, time: 33.31519
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   292 ----
[CW] collect: return: 42.70427, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 0.02749, qf2_loss: 0.02803, policy_loss: -28.95182, policy_entropy: -5.37938, alpha: 0.00257, time: 32.67031
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   293 ----
[CW] collect: return: 47.62666, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 0.04030, qf2_loss: 0.04008, policy_loss: -28.74577, policy_entropy: -4.84915, alpha: 0.00255, time: 32.64896
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   294 ----
[CW] collect: return: 32.57315, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 0.04383, qf2_loss: 0.04456, policy_loss: -28.74243, policy_entropy: -5.50628, alpha: 0.00252, time: 33.06127
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   295 ----
[CW] collect: return: 56.74824, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 0.04576, qf2_loss: 0.04541, policy_loss: -28.67236, policy_entropy: -5.55951, alpha: 0.00250, time: 32.65615
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   296 ----
[CW] collect: return: 44.27441, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 0.02902, qf2_loss: 0.02953, policy_loss: -28.52277, policy_entropy: -6.31814, alpha: 0.00250, time: 33.00060
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   297 ----
[CW] collect: return: 21.27352, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 0.04211, qf2_loss: 0.04161, policy_loss: -28.50050, policy_entropy: -5.69754, alpha: 0.00251, time: 32.54345
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   298 ----
[CW] collect: return: 48.09415, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 0.03635, qf2_loss: 0.03661, policy_loss: -28.36961, policy_entropy: -5.68183, alpha: 0.00249, time: 32.95246
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   299 ----
[CW] collect: return: 90.98606, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 0.04358, qf2_loss: 0.04520, policy_loss: -28.39145, policy_entropy: -6.06696, alpha: 0.00249, time: 32.61954
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   300 ----
[CW] collect: return: 22.01905, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 0.03752, qf2_loss: 0.03743, policy_loss: -28.30779, policy_entropy: -6.29628, alpha: 0.00249, time: 32.92937
[CW] eval: return: 37.01580, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   301 ----
[CW] collect: return: 22.48135, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 0.03257, qf2_loss: 0.03205, policy_loss: -28.21234, policy_entropy: -4.63664, alpha: 0.00248, time: 32.64724
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   302 ----
[CW] collect: return: 44.40099, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 0.04143, qf2_loss: 0.04220, policy_loss: -28.11689, policy_entropy: -5.84771, alpha: 0.00244, time: 32.65475
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   303 ----
[CW] collect: return: 84.39666, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 0.03205, qf2_loss: 0.03183, policy_loss: -28.05253, policy_entropy: -5.24568, alpha: 0.00242, time: 32.74807
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   304 ----
[CW] collect: return: 120.49793, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 0.03992, qf2_loss: 0.04072, policy_loss: -28.03656, policy_entropy: -7.06648, alpha: 0.00243, time: 32.78530
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   305 ----
[CW] collect: return: 36.20695, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 0.06430, qf2_loss: 0.06434, policy_loss: -27.94792, policy_entropy: -7.04884, alpha: 0.00247, time: 32.77980
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   306 ----
[CW] collect: return: 27.85077, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 0.04193, qf2_loss: 0.04284, policy_loss: -27.93257, policy_entropy: -6.52824, alpha: 0.00251, time: 32.84643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   307 ----
[CW] collect: return: 96.80618, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 0.04162, qf2_loss: 0.04145, policy_loss: -27.89070, policy_entropy: -6.57205, alpha: 0.00254, time: 32.66310
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   308 ----
[CW] collect: return: 43.72433, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 0.03612, qf2_loss: 0.03651, policy_loss: -27.71513, policy_entropy: -6.75072, alpha: 0.00257, time: 32.72291
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   309 ----
[CW] collect: return: 102.75320, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 0.04618, qf2_loss: 0.04684, policy_loss: -27.71615, policy_entropy: -6.87801, alpha: 0.00261, time: 32.76983
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   310 ----
[CW] collect: return: 94.57488, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 0.05407, qf2_loss: 0.05388, policy_loss: -27.70608, policy_entropy: -7.20375, alpha: 0.00267, time: 32.98671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   311 ----
[CW] collect: return: 99.87085, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 0.05320, qf2_loss: 0.05407, policy_loss: -27.68352, policy_entropy: -6.71260, alpha: 0.00273, time: 32.46421
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   312 ----
[CW] collect: return: 39.41609, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 0.05408, qf2_loss: 0.05424, policy_loss: -27.56456, policy_entropy: -6.50027, alpha: 0.00276, time: 32.83638
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   313 ----
[CW] collect: return: 53.07896, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 0.05495, qf2_loss: 0.05613, policy_loss: -27.46900, policy_entropy: -7.16570, alpha: 0.00281, time: 33.51319
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   314 ----
[CW] collect: return: 65.23635, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 0.06402, qf2_loss: 0.06459, policy_loss: -27.47452, policy_entropy: -7.15898, alpha: 0.00290, time: 32.86529
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   315 ----
[CW] collect: return: 56.53351, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 0.05827, qf2_loss: 0.05831, policy_loss: -27.40799, policy_entropy: -7.28909, alpha: 0.00299, time: 34.87519
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   316 ----
[CW] collect: return: 55.26467, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 0.05224, qf2_loss: 0.05146, policy_loss: -27.35355, policy_entropy: -7.09322, alpha: 0.00308, time: 32.69152
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   317 ----
[CW] collect: return: 57.60406, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 0.06103, qf2_loss: 0.06234, policy_loss: -27.29257, policy_entropy: -7.07285, alpha: 0.00316, time: 32.94580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   318 ----
[CW] collect: return: 47.77053, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 0.06367, qf2_loss: 0.06435, policy_loss: -27.22713, policy_entropy: -6.99797, alpha: 0.00325, time: 32.78947
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   319 ----
[CW] collect: return: 47.26839, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 0.05565, qf2_loss: 0.05571, policy_loss: -27.22625, policy_entropy: -5.46004, alpha: 0.00329, time: 32.62600
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   320 ----
[CW] collect: return: 113.01923, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 0.06782, qf2_loss: 0.06890, policy_loss: -27.12747, policy_entropy: -4.95883, alpha: 0.00322, time: 32.92283
[CW] eval: return: 63.54117, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   321 ----
[CW] collect: return: 68.56281, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 0.06088, qf2_loss: 0.06213, policy_loss: -27.19102, policy_entropy: -5.06930, alpha: 0.00313, time: 32.84491
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   322 ----
[CW] collect: return: 49.68361, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 0.06756, qf2_loss: 0.06787, policy_loss: -27.15600, policy_entropy: -6.09225, alpha: 0.00309, time: 32.83953
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   323 ----
[CW] collect: return: 101.92615, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 0.06614, qf2_loss: 0.06567, policy_loss: -27.06345, policy_entropy: -7.80874, alpha: 0.00316, time: 32.68093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   324 ----
[CW] collect: return: 145.20245, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 0.07231, qf2_loss: 0.07374, policy_loss: -26.95715, policy_entropy: -5.63005, alpha: 0.00325, time: 32.89731
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   325 ----
[CW] collect: return: 49.23847, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 0.08135, qf2_loss: 0.08071, policy_loss: -26.92353, policy_entropy: -5.38329, alpha: 0.00319, time: 32.69910
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   326 ----
[CW] collect: return: 76.97807, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 0.07527, qf2_loss: 0.07712, policy_loss: -26.87097, policy_entropy: -5.68478, alpha: 0.00316, time: 32.98355
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   327 ----
[CW] collect: return: 53.22737, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 0.07825, qf2_loss: 0.07810, policy_loss: -26.85795, policy_entropy: -6.80379, alpha: 0.00315, time: 32.90938
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   328 ----
[CW] collect: return: 68.80516, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 0.07077, qf2_loss: 0.07051, policy_loss: -26.84619, policy_entropy: -5.15192, alpha: 0.00319, time: 32.90745
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   329 ----
[CW] collect: return: 96.33441, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 0.09053, qf2_loss: 0.09145, policy_loss: -26.76421, policy_entropy: -5.12037, alpha: 0.00312, time: 32.66447
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   330 ----
[CW] collect: return: 45.79650, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 0.07313, qf2_loss: 0.07286, policy_loss: -26.74840, policy_entropy: -5.14396, alpha: 0.00304, time: 32.88871
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   331 ----
[CW] collect: return: 46.19316, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 0.08356, qf2_loss: 0.08550, policy_loss: -26.70581, policy_entropy: -5.36867, alpha: 0.00300, time: 32.64837
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   332 ----
[CW] collect: return: 50.40438, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 0.08081, qf2_loss: 0.08132, policy_loss: -26.67945, policy_entropy: -4.75675, alpha: 0.00293, time: 32.71079
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   333 ----
[CW] collect: return: 116.14603, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 0.09048, qf2_loss: 0.09165, policy_loss: -26.54961, policy_entropy: -5.63309, alpha: 0.00287, time: 32.69410
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   334 ----
[CW] collect: return: 58.73367, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 0.08530, qf2_loss: 0.08554, policy_loss: -26.57054, policy_entropy: -5.58069, alpha: 0.00284, time: 32.77773
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   335 ----
[CW] collect: return: 78.04461, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 0.09519, qf2_loss: 0.09615, policy_loss: -26.52460, policy_entropy: -6.23311, alpha: 0.00284, time: 33.00282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   336 ----
[CW] collect: return: 130.25715, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 0.08896, qf2_loss: 0.08955, policy_loss: -26.43862, policy_entropy: -5.86002, alpha: 0.00284, time: 32.69721
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   337 ----
[CW] collect: return: 64.50837, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 0.07756, qf2_loss: 0.07939, policy_loss: -26.34831, policy_entropy: -5.98611, alpha: 0.00284, time: 32.90950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   338 ----
[CW] collect: return: 58.41210, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 0.08722, qf2_loss: 0.08843, policy_loss: -26.29671, policy_entropy: -7.14575, alpha: 0.00286, time: 32.97342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   339 ----
[CW] collect: return: 132.49512, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 0.08522, qf2_loss: 0.08520, policy_loss: -26.33561, policy_entropy: -7.61537, alpha: 0.00297, time: 32.87353
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   340 ----
[CW] collect: return: 100.69046, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 0.09600, qf2_loss: 0.09663, policy_loss: -26.34882, policy_entropy: -6.04047, alpha: 0.00305, time: 32.84535
[CW] eval: return: 97.24570, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   341 ----
[CW] collect: return: 73.38830, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 0.09205, qf2_loss: 0.09300, policy_loss: -26.33055, policy_entropy: -6.75945, alpha: 0.00307, time: 33.08541
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   342 ----
[CW] collect: return: 101.35557, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 0.09112, qf2_loss: 0.09202, policy_loss: -26.38330, policy_entropy: -8.85621, alpha: 0.00320, time: 33.12938
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   343 ----
[CW] collect: return: 81.95831, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 0.09850, qf2_loss: 0.09864, policy_loss: -26.32947, policy_entropy: -7.77436, alpha: 0.00340, time: 33.11706
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   344 ----
[CW] collect: return: 133.94304, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 0.10171, qf2_loss: 0.10045, policy_loss: -26.29591, policy_entropy: -8.00172, alpha: 0.00353, time: 33.00653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   345 ----
[CW] collect: return: 101.42517, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 0.11127, qf2_loss: 0.11407, policy_loss: -26.33008, policy_entropy: -8.09457, alpha: 0.00370, time: 32.56946
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   346 ----
[CW] collect: return: 121.30740, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 0.10038, qf2_loss: 0.10208, policy_loss: -26.18047, policy_entropy: -8.00157, alpha: 0.00385, time: 32.78534
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   347 ----
[CW] collect: return: 112.19496, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 0.09938, qf2_loss: 0.10161, policy_loss: -26.24812, policy_entropy: -8.39069, alpha: 0.00403, time: 32.73815
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   348 ----
[CW] collect: return: 115.26698, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 0.08900, qf2_loss: 0.08988, policy_loss: -26.30013, policy_entropy: -8.12157, alpha: 0.00422, time: 33.00742
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   349 ----
[CW] collect: return: 10.20194, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 0.08989, qf2_loss: 0.09069, policy_loss: -26.11881, policy_entropy: -7.86764, alpha: 0.00439, time: 32.68072
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   350 ----
[CW] collect: return: 112.48415, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 0.08636, qf2_loss: 0.08719, policy_loss: -26.27159, policy_entropy: -7.94664, alpha: 0.00456, time: 32.72476
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   351 ----
[CW] collect: return: 124.82758, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 0.08233, qf2_loss: 0.08377, policy_loss: -26.15048, policy_entropy: -7.83521, alpha: 0.00474, time: 33.01937
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   352 ----
[CW] collect: return: 117.97633, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 0.08627, qf2_loss: 0.08696, policy_loss: -26.15700, policy_entropy: -7.34001, alpha: 0.00488, time: 32.81200
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   353 ----
[CW] collect: return: 110.83334, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 0.08238, qf2_loss: 0.08322, policy_loss: -26.15800, policy_entropy: -7.36147, alpha: 0.00501, time: 33.21948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   354 ----
[CW] collect: return: 117.81193, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 0.07857, qf2_loss: 0.07904, policy_loss: -26.13127, policy_entropy: -6.99118, alpha: 0.00514, time: 32.74244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   355 ----
[CW] collect: return: 117.90270, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 0.08437, qf2_loss: 0.08471, policy_loss: -26.19223, policy_entropy: -6.80523, alpha: 0.00524, time: 32.77865
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   356 ----
[CW] collect: return: 118.32396, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 0.07075, qf2_loss: 0.07181, policy_loss: -26.11676, policy_entropy: -6.45271, alpha: 0.00531, time: 33.28508
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   357 ----
[CW] collect: return: 119.08960, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 0.09502, qf2_loss: 0.09600, policy_loss: -25.99174, policy_entropy: -6.48039, alpha: 0.00538, time: 32.99497
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   358 ----
[CW] collect: return: 120.50787, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 0.08345, qf2_loss: 0.08278, policy_loss: -26.03208, policy_entropy: -6.33021, alpha: 0.00542, time: 32.57694
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   359 ----
[CW] collect: return: 99.47006, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 0.08886, qf2_loss: 0.08989, policy_loss: -26.11360, policy_entropy: -5.71178, alpha: 0.00543, time: 32.78702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   360 ----
[CW] collect: return: 102.25155, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 0.08792, qf2_loss: 0.08808, policy_loss: -26.01507, policy_entropy: -6.35408, alpha: 0.00542, time: 32.97730
[CW] eval: return: 101.47392, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   361 ----
[CW] collect: return: 110.58213, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 0.08643, qf2_loss: 0.08739, policy_loss: -26.00299, policy_entropy: -6.39879, alpha: 0.00550, time: 32.95690
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   362 ----
[CW] collect: return: 126.63870, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 0.08123, qf2_loss: 0.08166, policy_loss: -26.12655, policy_entropy: -6.47410, alpha: 0.00554, time: 32.88912
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   363 ----
[CW] collect: return: 120.84142, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 0.08642, qf2_loss: 0.08689, policy_loss: -26.12424, policy_entropy: -7.19092, alpha: 0.00567, time: 32.98491
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   364 ----
[CW] collect: return: 96.48407, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 0.11529, qf2_loss: 0.11464, policy_loss: -26.11364, policy_entropy: -7.24728, alpha: 0.00586, time: 32.92733
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   365 ----
[CW] collect: return: 113.82058, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 0.09244, qf2_loss: 0.09275, policy_loss: -26.18400, policy_entropy: -7.40362, alpha: 0.00606, time: 32.72379
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   366 ----
[CW] collect: return: 117.50903, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 0.09304, qf2_loss: 0.09316, policy_loss: -26.09111, policy_entropy: -7.26478, alpha: 0.00627, time: 32.71102
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   367 ----
[CW] collect: return: 121.81146, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 0.09020, qf2_loss: 0.08965, policy_loss: -26.18926, policy_entropy: -7.37260, alpha: 0.00651, time: 33.22822
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   368 ----
[CW] collect: return: 107.43880, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 0.09612, qf2_loss: 0.09737, policy_loss: -26.16272, policy_entropy: -6.59519, alpha: 0.00669, time: 32.81870
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   369 ----
[CW] collect: return: 110.09654, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 0.09209, qf2_loss: 0.09251, policy_loss: -26.21770, policy_entropy: -6.99764, alpha: 0.00680, time: 32.88085
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   370 ----
[CW] collect: return: 106.27998, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 0.08487, qf2_loss: 0.08680, policy_loss: -26.16772, policy_entropy: -6.91127, alpha: 0.00699, time: 32.67611
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   371 ----
[CW] collect: return: 14.74040, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 0.08430, qf2_loss: 0.08460, policy_loss: -26.21976, policy_entropy: -6.17022, alpha: 0.00711, time: 33.11928
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   372 ----
[CW] collect: return: 100.65914, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 0.08614, qf2_loss: 0.08709, policy_loss: -26.21162, policy_entropy: -5.91947, alpha: 0.00710, time: 32.61533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   373 ----
[CW] collect: return: 112.92853, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 0.09412, qf2_loss: 0.09317, policy_loss: -26.22729, policy_entropy: -5.98518, alpha: 0.00710, time: 32.84686
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   374 ----
[CW] collect: return: 68.49482, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 0.10163, qf2_loss: 0.10388, policy_loss: -26.20577, policy_entropy: -6.18095, alpha: 0.00711, time: 32.58949
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   375 ----
[CW] collect: return: 101.13668, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 0.08219, qf2_loss: 0.08381, policy_loss: -26.19606, policy_entropy: -6.39893, alpha: 0.00718, time: 32.87891
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   376 ----
[CW] collect: return: 114.14818, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 0.09009, qf2_loss: 0.09027, policy_loss: -26.28940, policy_entropy: -6.27986, alpha: 0.00725, time: 32.69861
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   377 ----
[CW] collect: return: 99.96929, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 0.08869, qf2_loss: 0.09060, policy_loss: -26.31736, policy_entropy: -6.00151, alpha: 0.00729, time: 32.87800
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   378 ----
[CW] collect: return: 23.18353, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 0.07193, qf2_loss: 0.07205, policy_loss: -26.34549, policy_entropy: -5.64265, alpha: 0.00724, time: 33.02806
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   379 ----
[CW] collect: return: 22.23354, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 0.07711, qf2_loss: 0.08009, policy_loss: -26.30754, policy_entropy: -5.14509, alpha: 0.00714, time: 32.64449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   380 ----
[CW] collect: return: 84.79285, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 0.07710, qf2_loss: 0.07662, policy_loss: -26.31767, policy_entropy: -5.50962, alpha: 0.00695, time: 33.11665
[CW] eval: return: 69.42816, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   381 ----
[CW] collect: return: 78.90800, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 0.07187, qf2_loss: 0.07189, policy_loss: -26.30538, policy_entropy: -5.66797, alpha: 0.00686, time: 32.86756
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   382 ----
[CW] collect: return: 148.68132, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 0.06670, qf2_loss: 0.06615, policy_loss: -26.27999, policy_entropy: -5.38121, alpha: 0.00677, time: 32.82675
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   383 ----
[CW] collect: return: 139.59347, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 0.06874, qf2_loss: 0.07026, policy_loss: -26.34552, policy_entropy: -5.60078, alpha: 0.00665, time: 32.75827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   384 ----
[CW] collect: return: 138.67831, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 0.07083, qf2_loss: 0.07281, policy_loss: -26.30329, policy_entropy: -5.40459, alpha: 0.00657, time: 36.22571
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   385 ----
[CW] collect: return: 165.30157, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 0.06493, qf2_loss: 0.06435, policy_loss: -26.31791, policy_entropy: -5.47265, alpha: 0.00644, time: 33.14893
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   386 ----
[CW] collect: return: 145.52557, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 0.05857, qf2_loss: 0.05935, policy_loss: -26.27170, policy_entropy: -5.91477, alpha: 0.00639, time: 33.17317
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   387 ----
[CW] collect: return: 132.12932, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 0.06412, qf2_loss: 0.06577, policy_loss: -26.34387, policy_entropy: -5.37294, alpha: 0.00632, time: 33.27523
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   388 ----
[CW] collect: return: 161.09682, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 0.06859, qf2_loss: 0.06917, policy_loss: -26.34191, policy_entropy: -5.75538, alpha: 0.00621, time: 33.10859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   389 ----
[CW] collect: return: 154.56259, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 0.06113, qf2_loss: 0.06161, policy_loss: -26.32492, policy_entropy: -5.33775, alpha: 0.00611, time: 33.28633
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   390 ----
[CW] collect: return: 153.70682, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 0.05887, qf2_loss: 0.06047, policy_loss: -26.32389, policy_entropy: -6.06874, alpha: 0.00607, time: 33.14760
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   391 ----
[CW] collect: return: 129.69672, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 0.09167, qf2_loss: 0.09176, policy_loss: -26.33147, policy_entropy: -5.83635, alpha: 0.00606, time: 32.99908
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   392 ----
[CW] collect: return: 129.58391, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 0.07001, qf2_loss: 0.06934, policy_loss: -26.36674, policy_entropy: -5.99674, alpha: 0.00604, time: 33.10363
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   393 ----
[CW] collect: return: 147.51126, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 0.06008, qf2_loss: 0.06108, policy_loss: -26.34068, policy_entropy: -5.72646, alpha: 0.00600, time: 33.12710
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   394 ----
[CW] collect: return: 117.70050, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 0.06473, qf2_loss: 0.06567, policy_loss: -26.36844, policy_entropy: -6.11435, alpha: 0.00601, time: 32.69668
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   395 ----
[CW] collect: return: 131.36369, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 0.05923, qf2_loss: 0.05930, policy_loss: -26.32407, policy_entropy: -5.56237, alpha: 0.00596, time: 33.01610
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   396 ----
[CW] collect: return: 112.60518, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 0.06596, qf2_loss: 0.06682, policy_loss: -26.40789, policy_entropy: -5.93381, alpha: 0.00591, time: 32.83146
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   397 ----
[CW] collect: return: 118.34130, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 0.06777, qf2_loss: 0.06790, policy_loss: -26.37820, policy_entropy: -5.68512, alpha: 0.00586, time: 33.02388
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   398 ----
[CW] collect: return: 101.98618, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 0.06154, qf2_loss: 0.06246, policy_loss: -26.41105, policy_entropy: -5.51660, alpha: 0.00580, time: 33.01301
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   399 ----
[CW] collect: return: 137.81065, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 0.05767, qf2_loss: 0.05828, policy_loss: -26.36702, policy_entropy: -5.18058, alpha: 0.00566, time: 33.46133
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   400 ----
[CW] collect: return: 131.53337, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 0.05723, qf2_loss: 0.05761, policy_loss: -26.38090, policy_entropy: -6.00165, alpha: 0.00558, time: 34.12802
[CW] eval: return: 127.75783, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   401 ----
[CW] collect: return: 152.95484, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 0.06205, qf2_loss: 0.06275, policy_loss: -26.45310, policy_entropy: -5.94409, alpha: 0.00555, time: 33.07395
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   402 ----
[CW] collect: return: 77.08078, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 0.05788, qf2_loss: 0.05699, policy_loss: -26.43324, policy_entropy: -5.71277, alpha: 0.00554, time: 32.82646
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   403 ----
[CW] collect: return: 120.25205, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 0.05239, qf2_loss: 0.05310, policy_loss: -26.45107, policy_entropy: -5.72162, alpha: 0.00546, time: 32.89662
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   404 ----
[CW] collect: return: 106.44125, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 0.06182, qf2_loss: 0.06286, policy_loss: -26.42407, policy_entropy: -5.83312, alpha: 0.00541, time: 32.96538
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   405 ----
[CW] collect: return: 141.11495, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 0.06623, qf2_loss: 0.06580, policy_loss: -26.42216, policy_entropy: -5.64066, alpha: 0.00538, time: 32.94414
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   406 ----
[CW] collect: return: 124.12226, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 0.05195, qf2_loss: 0.05321, policy_loss: -26.40730, policy_entropy: -5.29248, alpha: 0.00524, time: 32.88382
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   407 ----
[CW] collect: return: 128.81973, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 0.05241, qf2_loss: 0.05197, policy_loss: -26.42005, policy_entropy: -6.02568, alpha: 0.00518, time: 32.97876
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   408 ----
[CW] collect: return: 99.66138, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 0.05073, qf2_loss: 0.05099, policy_loss: -26.37881, policy_entropy: -5.48427, alpha: 0.00515, time: 32.84219
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   409 ----
[CW] collect: return: 57.62863, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 0.04917, qf2_loss: 0.04931, policy_loss: -26.37113, policy_entropy: -5.53794, alpha: 0.00505, time: 32.86937
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   410 ----
[CW] collect: return: 126.87480, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 0.05532, qf2_loss: 0.05553, policy_loss: -26.39667, policy_entropy: -5.35167, alpha: 0.00494, time: 32.76290
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   411 ----
[CW] collect: return: 138.37600, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 0.05163, qf2_loss: 0.05164, policy_loss: -26.39705, policy_entropy: -5.54274, alpha: 0.00485, time: 33.24776
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   412 ----
[CW] collect: return: 110.32193, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 0.04494, qf2_loss: 0.04574, policy_loss: -26.36162, policy_entropy: -5.45460, alpha: 0.00476, time: 32.73409
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   413 ----
[CW] collect: return: 121.70847, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 0.04532, qf2_loss: 0.04481, policy_loss: -26.37418, policy_entropy: -5.60032, alpha: 0.00467, time: 32.77256
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   414 ----
[CW] collect: return: 134.40035, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 0.04613, qf2_loss: 0.04582, policy_loss: -26.32498, policy_entropy: -5.77243, alpha: 0.00461, time: 32.71842
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   415 ----
[CW] collect: return: 121.64357, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 0.04476, qf2_loss: 0.04510, policy_loss: -26.31600, policy_entropy: -5.63626, alpha: 0.00456, time: 32.95143
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   416 ----
[CW] collect: return: 133.69419, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 0.04938, qf2_loss: 0.05091, policy_loss: -26.34373, policy_entropy: -5.73893, alpha: 0.00450, time: 33.12337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   417 ----
[CW] collect: return: 138.80160, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 0.04450, qf2_loss: 0.04438, policy_loss: -26.32498, policy_entropy: -6.08454, alpha: 0.00449, time: 32.92337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   418 ----
[CW] collect: return: 142.52792, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 0.04131, qf2_loss: 0.04198, policy_loss: -26.28350, policy_entropy: -5.68198, alpha: 0.00447, time: 36.08998
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   419 ----
[CW] collect: return: 126.64665, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 0.03669, qf2_loss: 0.03693, policy_loss: -26.27323, policy_entropy: -5.66832, alpha: 0.00442, time: 32.63943
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   420 ----
[CW] collect: return: 125.37358, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 0.04183, qf2_loss: 0.04234, policy_loss: -26.25009, policy_entropy: -5.51118, alpha: 0.00433, time: 33.21557
[CW] eval: return: 86.42311, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   421 ----
[CW] collect: return: 71.16044, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 0.04512, qf2_loss: 0.04580, policy_loss: -26.26706, policy_entropy: -5.63672, alpha: 0.00425, time: 32.83805
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   422 ----
[CW] collect: return: 128.82253, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 0.04238, qf2_loss: 0.04266, policy_loss: -26.27049, policy_entropy: -5.84952, alpha: 0.00420, time: 32.61918
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   423 ----
[CW] collect: return: 141.81746, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 0.04308, qf2_loss: 0.04305, policy_loss: -26.27888, policy_entropy: -5.94835, alpha: 0.00419, time: 33.14176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   424 ----
[CW] collect: return: 158.14173, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 0.03999, qf2_loss: 0.04050, policy_loss: -26.24912, policy_entropy: -6.11263, alpha: 0.00420, time: 32.99993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   425 ----
[CW] collect: return: 80.25920, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 0.04170, qf2_loss: 0.04173, policy_loss: -26.23675, policy_entropy: -6.36857, alpha: 0.00423, time: 32.82151
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   426 ----
[CW] collect: return: 129.91019, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 0.04290, qf2_loss: 0.04261, policy_loss: -26.26916, policy_entropy: -6.42518, alpha: 0.00431, time: 32.72899
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   427 ----
[CW] collect: return: 138.71091, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 0.05826, qf2_loss: 0.06044, policy_loss: -26.24199, policy_entropy: -6.58292, alpha: 0.00441, time: 32.84194
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   428 ----
[CW] collect: return: 152.52248, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 0.04117, qf2_loss: 0.04124, policy_loss: -26.22823, policy_entropy: -6.27013, alpha: 0.00451, time: 32.74094
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   429 ----
[CW] collect: return: 86.27123, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 0.04052, qf2_loss: 0.04060, policy_loss: -26.23714, policy_entropy: -6.12954, alpha: 0.00455, time: 32.86540
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   430 ----
[CW] collect: return: 145.79083, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 0.04494, qf2_loss: 0.04630, policy_loss: -26.21622, policy_entropy: -6.02318, alpha: 0.00458, time: 33.15430
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   431 ----
[CW] collect: return: 120.79652, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 0.04557, qf2_loss: 0.04461, policy_loss: -26.21474, policy_entropy: -6.14117, alpha: 0.00458, time: 33.04484
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   432 ----
[CW] collect: return: 121.70983, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 0.04107, qf2_loss: 0.04098, policy_loss: -26.23585, policy_entropy: -5.98417, alpha: 0.00461, time: 32.92653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   433 ----
[CW] collect: return: 137.43691, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 0.04377, qf2_loss: 0.04495, policy_loss: -26.19373, policy_entropy: -5.88782, alpha: 0.00460, time: 33.34675
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   434 ----
[CW] collect: return: 137.48840, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 0.04680, qf2_loss: 0.04605, policy_loss: -26.23196, policy_entropy: -6.02822, alpha: 0.00458, time: 32.87951
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   435 ----
[CW] collect: return: 137.40564, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 0.05380, qf2_loss: 0.05394, policy_loss: -26.19321, policy_entropy: -5.66606, alpha: 0.00455, time: 32.97813
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   436 ----
[CW] collect: return: 137.32232, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 0.06939, qf2_loss: 0.07032, policy_loss: -26.14449, policy_entropy: -5.97208, alpha: 0.00450, time: 32.79221
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   437 ----
[CW] collect: return: 129.35053, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 0.05137, qf2_loss: 0.05295, policy_loss: -26.17286, policy_entropy: -6.07211, alpha: 0.00450, time: 32.90494
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   438 ----
[CW] collect: return: 154.62067, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 0.04344, qf2_loss: 0.04409, policy_loss: -26.13560, policy_entropy: -5.90710, alpha: 0.00451, time: 33.01043
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   439 ----
[CW] collect: return: 145.06771, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 0.04539, qf2_loss: 0.04593, policy_loss: -26.09968, policy_entropy: -5.83764, alpha: 0.00449, time: 33.15494
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   440 ----
[CW] collect: return: 145.16340, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 0.04901, qf2_loss: 0.04819, policy_loss: -26.18470, policy_entropy: -5.97784, alpha: 0.00444, time: 33.19457
[CW] eval: return: 143.12165, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   441 ----
[CW] collect: return: 137.18375, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 0.04277, qf2_loss: 0.04382, policy_loss: -26.13547, policy_entropy: -5.76961, alpha: 0.00443, time: 32.96933
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   442 ----
[CW] collect: return: 135.88211, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 0.04264, qf2_loss: 0.04348, policy_loss: -26.14668, policy_entropy: -5.77688, alpha: 0.00436, time: 32.73063
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   443 ----
[CW] collect: return: 132.11582, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 0.04172, qf2_loss: 0.04151, policy_loss: -26.13469, policy_entropy: -5.92619, alpha: 0.00433, time: 32.96972
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   444 ----
[CW] collect: return: 127.74379, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 0.05811, qf2_loss: 0.05794, policy_loss: -26.10742, policy_entropy: -5.99979, alpha: 0.00431, time: 32.74881
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   445 ----
[CW] collect: return: 146.58600, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 0.07054, qf2_loss: 0.07249, policy_loss: -26.09162, policy_entropy: -5.52864, alpha: 0.00424, time: 32.91192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   446 ----
[CW] collect: return: 160.52332, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 0.04606, qf2_loss: 0.04683, policy_loss: -26.07922, policy_entropy: -5.92619, alpha: 0.00419, time: 32.86013
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   447 ----
[CW] collect: return: 131.98598, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 0.04344, qf2_loss: 0.04322, policy_loss: -26.07235, policy_entropy: -5.95017, alpha: 0.00417, time: 33.10347
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   448 ----
[CW] collect: return: 152.33157, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 0.04437, qf2_loss: 0.04384, policy_loss: -26.08662, policy_entropy: -6.12142, alpha: 0.00417, time: 32.80210
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   449 ----
[CW] collect: return: 153.99055, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 0.05033, qf2_loss: 0.05131, policy_loss: -26.07914, policy_entropy: -5.72801, alpha: 0.00417, time: 32.81842
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   450 ----
[CW] collect: return: 156.11209, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 0.06768, qf2_loss: 0.06770, policy_loss: -26.11449, policy_entropy: -5.86729, alpha: 0.00411, time: 32.68205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   451 ----
[CW] collect: return: 142.17507, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 0.05186, qf2_loss: 0.05175, policy_loss: -26.03421, policy_entropy: -6.04859, alpha: 0.00410, time: 32.94037
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   452 ----
[CW] collect: return: 133.07504, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 0.05416, qf2_loss: 0.05423, policy_loss: -26.05694, policy_entropy: -6.26783, alpha: 0.00413, time: 32.89219
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   453 ----
[CW] collect: return: 148.88084, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 0.04798, qf2_loss: 0.04848, policy_loss: -26.05477, policy_entropy: -6.06176, alpha: 0.00419, time: 32.90082
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   454 ----
[CW] collect: return: 156.97195, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 0.06596, qf2_loss: 0.06690, policy_loss: -26.08017, policy_entropy: -6.14081, alpha: 0.00420, time: 33.42632
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   455 ----
[CW] collect: return: 153.62303, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 0.04841, qf2_loss: 0.04847, policy_loss: -26.08808, policy_entropy: -6.07120, alpha: 0.00422, time: 33.15471
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   456 ----
[CW] collect: return: 140.86743, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 0.04880, qf2_loss: 0.04846, policy_loss: -26.06607, policy_entropy: -5.92304, alpha: 0.00423, time: 33.16754
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   457 ----
[CW] collect: return: 161.80404, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 0.06433, qf2_loss: 0.06435, policy_loss: -26.06587, policy_entropy: -6.08744, alpha: 0.00421, time: 33.57934
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   458 ----
[CW] collect: return: 151.34074, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 0.05262, qf2_loss: 0.05233, policy_loss: -26.09959, policy_entropy: -6.15879, alpha: 0.00428, time: 32.96516
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   459 ----
[CW] collect: return: 145.36455, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 0.05012, qf2_loss: 0.05078, policy_loss: -26.09694, policy_entropy: -6.00962, alpha: 0.00429, time: 32.87804
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   460 ----
[CW] collect: return: 151.60241, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 0.05102, qf2_loss: 0.05104, policy_loss: -26.05611, policy_entropy: -6.18852, alpha: 0.00432, time: 33.00506
[CW] eval: return: 153.07983, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   461 ----
[CW] collect: return: 139.14626, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 0.04780, qf2_loss: 0.04809, policy_loss: -26.07594, policy_entropy: -6.12231, alpha: 0.00435, time: 32.97433
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   462 ----
[CW] collect: return: 147.84109, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 0.04942, qf2_loss: 0.05049, policy_loss: -26.07268, policy_entropy: -5.99299, alpha: 0.00440, time: 32.89668
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   463 ----
[CW] collect: return: 153.64244, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 0.05606, qf2_loss: 0.05684, policy_loss: -26.10696, policy_entropy: -5.79168, alpha: 0.00436, time: 33.11033
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   464 ----
[CW] collect: return: 162.29254, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 0.05256, qf2_loss: 0.05067, policy_loss: -26.09597, policy_entropy: -5.80019, alpha: 0.00430, time: 32.78628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   465 ----
[CW] collect: return: 149.07138, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 0.05161, qf2_loss: 0.05238, policy_loss: -26.09350, policy_entropy: -6.04497, alpha: 0.00427, time: 32.79368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   466 ----
[CW] collect: return: 159.94433, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 0.06043, qf2_loss: 0.06142, policy_loss: -26.09424, policy_entropy: -6.15710, alpha: 0.00430, time: 32.89133
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   467 ----
[CW] collect: return: 154.21455, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 0.06405, qf2_loss: 0.06385, policy_loss: -26.13386, policy_entropy: -6.16701, alpha: 0.00436, time: 33.03410
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   468 ----
[CW] collect: return: 144.03721, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 0.05076, qf2_loss: 0.05058, policy_loss: -26.13777, policy_entropy: -5.98075, alpha: 0.00438, time: 32.84517
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   469 ----
[CW] collect: return: 142.34016, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 0.05041, qf2_loss: 0.05099, policy_loss: -26.15919, policy_entropy: -6.08944, alpha: 0.00438, time: 33.20316
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   470 ----
[CW] collect: return: 162.51107, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 0.05335, qf2_loss: 0.05364, policy_loss: -26.13416, policy_entropy: -6.03281, alpha: 0.00440, time: 33.09996
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   471 ----
[CW] collect: return: 149.73899, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 0.05649, qf2_loss: 0.05696, policy_loss: -26.18358, policy_entropy: -5.87270, alpha: 0.00440, time: 33.12923
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   472 ----
[CW] collect: return: 158.15393, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 0.05242, qf2_loss: 0.05241, policy_loss: -26.17873, policy_entropy: -5.94760, alpha: 0.00436, time: 32.79018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   473 ----
[CW] collect: return: 155.84418, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 0.05696, qf2_loss: 0.05772, policy_loss: -26.17629, policy_entropy: -5.86824, alpha: 0.00431, time: 33.26925
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   474 ----
[CW] collect: return: 161.20012, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 0.05332, qf2_loss: 0.05319, policy_loss: -26.16352, policy_entropy: -5.82377, alpha: 0.00429, time: 32.75196
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   475 ----
[CW] collect: return: 165.62926, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 0.05151, qf2_loss: 0.05187, policy_loss: -26.18072, policy_entropy: -5.84774, alpha: 0.00423, time: 33.08484
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   476 ----
[CW] collect: return: 161.29104, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 0.04990, qf2_loss: 0.04968, policy_loss: -26.22598, policy_entropy: -5.83509, alpha: 0.00416, time: 33.24395
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   477 ----
[CW] collect: return: 163.97084, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 0.05175, qf2_loss: 0.05185, policy_loss: -26.23985, policy_entropy: -6.14755, alpha: 0.00418, time: 33.19390
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   478 ----
[CW] collect: return: 164.24253, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 0.05960, qf2_loss: 0.05864, policy_loss: -26.18869, policy_entropy: -6.01185, alpha: 0.00420, time: 32.88228
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   479 ----
[CW] collect: return: 157.20532, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 0.06113, qf2_loss: 0.06197, policy_loss: -26.22716, policy_entropy: -5.86618, alpha: 0.00418, time: 33.01773
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   480 ----
[CW] collect: return: 162.27832, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 0.05485, qf2_loss: 0.05432, policy_loss: -26.25245, policy_entropy: -5.70401, alpha: 0.00410, time: 33.03748
[CW] eval: return: 152.94167, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   481 ----
[CW] collect: return: 149.81016, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 0.05416, qf2_loss: 0.05482, policy_loss: -26.31855, policy_entropy: -6.00418, alpha: 0.00406, time: 32.83833
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   482 ----
[CW] collect: return: 156.40039, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 0.05204, qf2_loss: 0.05241, policy_loss: -26.30365, policy_entropy: -6.10046, alpha: 0.00407, time: 32.94839
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   483 ----
[CW] collect: return: 162.05581, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 0.05237, qf2_loss: 0.05232, policy_loss: -26.28283, policy_entropy: -6.04222, alpha: 0.00410, time: 32.91003
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   484 ----
[CW] collect: return: 161.23447, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 0.05288, qf2_loss: 0.05271, policy_loss: -26.28991, policy_entropy: -5.89243, alpha: 0.00410, time: 33.08592
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   485 ----
[CW] collect: return: 160.51945, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 0.05157, qf2_loss: 0.05284, policy_loss: -26.34853, policy_entropy: -6.14069, alpha: 0.00409, time: 33.01186
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   486 ----
[CW] collect: return: 162.92764, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 0.05364, qf2_loss: 0.05346, policy_loss: -26.34447, policy_entropy: -6.06108, alpha: 0.00413, time: 33.10456
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   487 ----
[CW] collect: return: 159.15655, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 0.05249, qf2_loss: 0.05192, policy_loss: -26.34787, policy_entropy: -6.12110, alpha: 0.00415, time: 33.14244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   488 ----
[CW] collect: return: 153.95044, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 0.05499, qf2_loss: 0.05445, policy_loss: -26.32741, policy_entropy: -5.92846, alpha: 0.00416, time: 32.99475
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   489 ----
[CW] collect: return: 145.15429, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 0.06728, qf2_loss: 0.06931, policy_loss: -26.40417, policy_entropy: -6.02462, alpha: 0.00415, time: 33.92537
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   490 ----
[CW] collect: return: 162.41641, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 0.05874, qf2_loss: 0.05914, policy_loss: -26.43788, policy_entropy: -6.16258, alpha: 0.00417, time: 32.87425
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   491 ----
[CW] collect: return: 165.82343, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 0.05056, qf2_loss: 0.05080, policy_loss: -26.42470, policy_entropy: -6.25566, alpha: 0.00426, time: 33.11987
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   492 ----
[CW] collect: return: 162.79266, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 0.05016, qf2_loss: 0.05027, policy_loss: -26.44949, policy_entropy: -6.22335, alpha: 0.00435, time: 32.82931
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   493 ----
[CW] collect: return: 152.33078, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 0.05041, qf2_loss: 0.04985, policy_loss: -26.42856, policy_entropy: -6.18966, alpha: 0.00441, time: 32.96207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   494 ----
[CW] collect: return: 160.33084, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 0.05636, qf2_loss: 0.05669, policy_loss: -26.44586, policy_entropy: -6.10468, alpha: 0.00446, time: 32.94628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   495 ----
[CW] collect: return: 167.61150, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 0.05584, qf2_loss: 0.05552, policy_loss: -26.46158, policy_entropy: -6.11167, alpha: 0.00451, time: 33.12251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   496 ----
[CW] collect: return: 161.87289, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 0.06073, qf2_loss: 0.06131, policy_loss: -26.50141, policy_entropy: -6.20468, alpha: 0.00457, time: 33.05692
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   497 ----
[CW] collect: return: 155.97825, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 0.06190, qf2_loss: 0.06172, policy_loss: -26.47892, policy_entropy: -5.97433, alpha: 0.00462, time: 32.96449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   498 ----
[CW] collect: return: 149.46406, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 0.05984, qf2_loss: 0.06057, policy_loss: -26.50935, policy_entropy: -5.87287, alpha: 0.00458, time: 33.04693
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   499 ----
[CW] collect: return: 161.73728, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 0.05721, qf2_loss: 0.05689, policy_loss: -26.53063, policy_entropy: -5.94228, alpha: 0.00455, time: 33.06172
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   500 ----
[CW] collect: return: 155.38767, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 0.05877, qf2_loss: 0.05877, policy_loss: -26.56502, policy_entropy: -5.95685, alpha: 0.00453, time: 32.98498
[CW] eval: return: 152.18956, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   501 ----
[CW] collect: return: 140.14722, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 0.05466, qf2_loss: 0.05381, policy_loss: -26.52285, policy_entropy: -5.86119, alpha: 0.00449, time: 33.09826
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   502 ----
[CW] collect: return: 157.51060, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 0.05156, qf2_loss: 0.05283, policy_loss: -26.63618, policy_entropy: -6.16163, alpha: 0.00448, time: 33.03789
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   503 ----
[CW] collect: return: 162.65479, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 0.05809, qf2_loss: 0.05732, policy_loss: -26.63054, policy_entropy: -6.06704, alpha: 0.00455, time: 32.89605
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   504 ----
[CW] collect: return: 146.28455, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 0.05347, qf2_loss: 0.05328, policy_loss: -26.63989, policy_entropy: -5.85465, alpha: 0.00453, time: 32.95781
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   505 ----
[CW] collect: return: 143.65492, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 0.06162, qf2_loss: 0.06180, policy_loss: -26.63331, policy_entropy: -6.07213, alpha: 0.00449, time: 32.75077
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   506 ----
[CW] collect: return: 163.90831, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 0.06302, qf2_loss: 0.06305, policy_loss: -26.68684, policy_entropy: -6.02332, alpha: 0.00455, time: 33.36077
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   507 ----
[CW] collect: return: 138.07222, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 0.05353, qf2_loss: 0.05416, policy_loss: -26.66040, policy_entropy: -5.88875, alpha: 0.00451, time: 32.83474
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   508 ----
[CW] collect: return: 157.72212, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 0.05200, qf2_loss: 0.05172, policy_loss: -26.68147, policy_entropy: -5.89200, alpha: 0.00448, time: 32.89114
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   509 ----
[CW] collect: return: 158.69184, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 0.05594, qf2_loss: 0.05597, policy_loss: -26.70190, policy_entropy: -6.10085, alpha: 0.00445, time: 32.90830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   510 ----
[CW] collect: return: 174.73295, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 0.05247, qf2_loss: 0.05209, policy_loss: -26.73156, policy_entropy: -5.99885, alpha: 0.00448, time: 32.93396
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   511 ----
[CW] collect: return: 159.49718, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 0.07120, qf2_loss: 0.07185, policy_loss: -26.78712, policy_entropy: -6.09805, alpha: 0.00450, time: 33.04635
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   512 ----
[CW] collect: return: 158.62051, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 0.05792, qf2_loss: 0.05858, policy_loss: -26.73860, policy_entropy: -6.08338, alpha: 0.00455, time: 33.20537
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   513 ----
[CW] collect: return: 155.23212, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 0.06335, qf2_loss: 0.06180, policy_loss: -26.80784, policy_entropy: -5.95813, alpha: 0.00454, time: 33.15578
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   514 ----
[CW] collect: return: 128.27696, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 0.05934, qf2_loss: 0.05945, policy_loss: -26.78666, policy_entropy: -5.95036, alpha: 0.00453, time: 32.97345
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   515 ----
[CW] collect: return: 163.82977, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 0.06577, qf2_loss: 0.06738, policy_loss: -26.85097, policy_entropy: -5.91398, alpha: 0.00449, time: 33.04316
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   516 ----
[CW] collect: return: 161.65474, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 0.06266, qf2_loss: 0.06170, policy_loss: -26.85527, policy_entropy: -5.98886, alpha: 0.00448, time: 32.82722
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   517 ----
[CW] collect: return: 160.34440, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 0.08328, qf2_loss: 0.08235, policy_loss: -26.87619, policy_entropy: -6.04428, alpha: 0.00448, time: 33.04311
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   518 ----
[CW] collect: return: 169.74816, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 0.06092, qf2_loss: 0.06124, policy_loss: -26.85727, policy_entropy: -6.29144, alpha: 0.00453, time: 32.76242
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   519 ----
[CW] collect: return: 163.96614, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 0.04942, qf2_loss: 0.05014, policy_loss: -26.92578, policy_entropy: -6.21874, alpha: 0.00464, time: 33.21081
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   520 ----
[CW] collect: return: 171.23435, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 0.05239, qf2_loss: 0.05234, policy_loss: -26.94427, policy_entropy: -5.79605, alpha: 0.00468, time: 32.85697
[CW] eval: return: 158.60236, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   521 ----
[CW] collect: return: 158.94204, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 0.05505, qf2_loss: 0.05506, policy_loss: -26.91899, policy_entropy: -5.73975, alpha: 0.00457, time: 32.99700
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   522 ----
[CW] collect: return: 166.20045, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 0.05547, qf2_loss: 0.05471, policy_loss: -27.00327, policy_entropy: -6.02144, alpha: 0.00451, time: 35.30198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   523 ----
[CW] collect: return: 163.00943, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 0.06024, qf2_loss: 0.06074, policy_loss: -27.00469, policy_entropy: -6.00173, alpha: 0.00452, time: 32.71495
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   524 ----
[CW] collect: return: 161.60693, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 0.06399, qf2_loss: 0.06363, policy_loss: -26.97983, policy_entropy: -5.98076, alpha: 0.00454, time: 33.07472
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   525 ----
[CW] collect: return: 148.02758, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 0.05869, qf2_loss: 0.05961, policy_loss: -27.01569, policy_entropy: -5.79229, alpha: 0.00448, time: 32.91189
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   526 ----
[CW] collect: return: 167.39833, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 0.06423, qf2_loss: 0.06494, policy_loss: -26.99548, policy_entropy: -6.00788, alpha: 0.00444, time: 32.88913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   527 ----
[CW] collect: return: 162.22252, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 0.04745, qf2_loss: 0.04755, policy_loss: -27.05794, policy_entropy: -5.92441, alpha: 0.00444, time: 33.02910
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   528 ----
[CW] collect: return: 149.21925, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 0.04992, qf2_loss: 0.04984, policy_loss: -27.05548, policy_entropy: -5.79871, alpha: 0.00438, time: 32.84389
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   529 ----
[CW] collect: return: 166.13063, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 0.06121, qf2_loss: 0.05990, policy_loss: -27.08026, policy_entropy: -6.04089, alpha: 0.00436, time: 32.60684
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   530 ----
[CW] collect: return: 160.83813, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 0.07045, qf2_loss: 0.07009, policy_loss: -27.05890, policy_entropy: -5.76129, alpha: 0.00435, time: 32.88547
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   531 ----
[CW] collect: return: 155.43464, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 0.05035, qf2_loss: 0.05042, policy_loss: -27.08050, policy_entropy: -6.02386, alpha: 0.00430, time: 32.92528
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   532 ----
[CW] collect: return: 167.55889, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 0.06532, qf2_loss: 0.06656, policy_loss: -27.10822, policy_entropy: -5.85602, alpha: 0.00428, time: 32.63943
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   533 ----
[CW] collect: return: 168.42778, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 0.05029, qf2_loss: 0.05070, policy_loss: -27.15942, policy_entropy: -6.05409, alpha: 0.00426, time: 32.98643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   534 ----
[CW] collect: return: 165.33718, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 0.05088, qf2_loss: 0.05077, policy_loss: -27.09435, policy_entropy: -5.90435, alpha: 0.00424, time: 32.82625
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   535 ----
[CW] collect: return: 158.59522, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 0.05193, qf2_loss: 0.05162, policy_loss: -27.15783, policy_entropy: -6.34340, alpha: 0.00427, time: 33.28070
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   536 ----
[CW] collect: return: 148.58384, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 0.06188, qf2_loss: 0.06250, policy_loss: -27.13467, policy_entropy: -6.16554, alpha: 0.00440, time: 32.75976
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   537 ----
[CW] collect: return: 167.48549, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 0.05477, qf2_loss: 0.05376, policy_loss: -27.20352, policy_entropy: -6.05877, alpha: 0.00440, time: 32.77341
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   538 ----
[CW] collect: return: 169.49530, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 0.06117, qf2_loss: 0.06276, policy_loss: -27.19657, policy_entropy: -5.81688, alpha: 0.00441, time: 32.82292
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   539 ----
[CW] collect: return: 154.11908, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 0.05601, qf2_loss: 0.05582, policy_loss: -27.24969, policy_entropy: -5.87424, alpha: 0.00435, time: 32.84738
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   540 ----
[CW] collect: return: 163.14761, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 0.05958, qf2_loss: 0.05861, policy_loss: -27.26102, policy_entropy: -5.98448, alpha: 0.00432, time: 32.86149
[CW] eval: return: 167.67565, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   541 ----
[CW] collect: return: 176.28996, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 0.05497, qf2_loss: 0.05583, policy_loss: -27.24672, policy_entropy: -6.04347, alpha: 0.00432, time: 33.15026
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   542 ----
[CW] collect: return: 162.00715, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 0.04939, qf2_loss: 0.04986, policy_loss: -27.26314, policy_entropy: -5.97414, alpha: 0.00433, time: 33.08975
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   543 ----
[CW] collect: return: 167.29744, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 0.06726, qf2_loss: 0.06724, policy_loss: -27.29786, policy_entropy: -5.88105, alpha: 0.00430, time: 33.06219
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   544 ----
[CW] collect: return: 150.90731, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 0.05084, qf2_loss: 0.05046, policy_loss: -27.29194, policy_entropy: -6.01379, alpha: 0.00426, time: 32.94334
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   545 ----
[CW] collect: return: 155.84072, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 0.04804, qf2_loss: 0.04767, policy_loss: -27.29799, policy_entropy: -6.03840, alpha: 0.00429, time: 32.53490
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   546 ----
[CW] collect: return: 147.80567, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 0.05354, qf2_loss: 0.05315, policy_loss: -27.33130, policy_entropy: -6.00761, alpha: 0.00429, time: 33.35593
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   547 ----
[CW] collect: return: 160.07477, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 0.07408, qf2_loss: 0.07416, policy_loss: -27.31403, policy_entropy: -6.21896, alpha: 0.00433, time: 32.54859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   548 ----
[CW] collect: return: 167.94961, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 0.05335, qf2_loss: 0.05295, policy_loss: -27.35969, policy_entropy: -5.81634, alpha: 0.00435, time: 33.16456
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   549 ----
[CW] collect: return: 163.83452, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 0.05639, qf2_loss: 0.05675, policy_loss: -27.37564, policy_entropy: -6.16622, alpha: 0.00433, time: 32.76590
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   550 ----
[CW] collect: return: 163.14613, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 0.05418, qf2_loss: 0.05379, policy_loss: -27.39554, policy_entropy: -6.09114, alpha: 0.00438, time: 32.68930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   551 ----
[CW] collect: return: 168.87034, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 0.06395, qf2_loss: 0.06311, policy_loss: -27.46845, policy_entropy: -5.99366, alpha: 0.00440, time: 32.95858
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   552 ----
[CW] collect: return: 159.00420, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 0.05985, qf2_loss: 0.06094, policy_loss: -27.40409, policy_entropy: -6.03927, alpha: 0.00441, time: 33.01114
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   553 ----
[CW] collect: return: 158.76437, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 0.05455, qf2_loss: 0.05394, policy_loss: -27.44128, policy_entropy: -5.88440, alpha: 0.00440, time: 32.93277
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   554 ----
[CW] collect: return: 177.56051, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 0.05056, qf2_loss: 0.05017, policy_loss: -27.48096, policy_entropy: -5.91977, alpha: 0.00436, time: 32.84074
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   555 ----
[CW] collect: return: 164.96466, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 0.04827, qf2_loss: 0.04792, policy_loss: -27.42706, policy_entropy: -5.94715, alpha: 0.00434, time: 33.02053
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   556 ----
[CW] collect: return: 166.45956, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 0.05410, qf2_loss: 0.05434, policy_loss: -27.49863, policy_entropy: -6.07297, alpha: 0.00433, time: 32.99768
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   557 ----
[CW] collect: return: 159.32421, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 0.05519, qf2_loss: 0.05546, policy_loss: -27.48713, policy_entropy: -5.93693, alpha: 0.00433, time: 32.76242
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   558 ----
[CW] collect: return: 167.21064, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 0.05746, qf2_loss: 0.05765, policy_loss: -27.57010, policy_entropy: -6.02409, alpha: 0.00431, time: 32.89048
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   559 ----
[CW] collect: return: 156.08310, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 0.05782, qf2_loss: 0.05745, policy_loss: -27.52640, policy_entropy: -5.95661, alpha: 0.00433, time: 32.84041
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   560 ----
[CW] collect: return: 172.37990, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 0.05859, qf2_loss: 0.05819, policy_loss: -27.49547, policy_entropy: -6.06612, alpha: 0.00434, time: 32.95549
[CW] eval: return: 160.36747, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   561 ----
[CW] collect: return: 167.61829, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 0.05659, qf2_loss: 0.05660, policy_loss: -27.49072, policy_entropy: -5.98785, alpha: 0.00434, time: 33.20471
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   562 ----
[CW] collect: return: 163.13163, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 0.05567, qf2_loss: 0.05618, policy_loss: -27.54964, policy_entropy: -6.08978, alpha: 0.00437, time: 32.84452
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   563 ----
[CW] collect: return: 165.71845, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 0.05748, qf2_loss: 0.05680, policy_loss: -27.54416, policy_entropy: -6.01040, alpha: 0.00439, time: 32.93664
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   564 ----
[CW] collect: return: 157.53864, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 0.05394, qf2_loss: 0.05278, policy_loss: -27.55402, policy_entropy: -5.86888, alpha: 0.00436, time: 32.81149
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   565 ----
[CW] collect: return: 162.61898, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 0.05420, qf2_loss: 0.05480, policy_loss: -27.59281, policy_entropy: -6.08571, alpha: 0.00434, time: 32.87972
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   566 ----
[CW] collect: return: 166.96884, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 0.05570, qf2_loss: 0.05592, policy_loss: -27.61858, policy_entropy: -6.07839, alpha: 0.00438, time: 32.63156
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   567 ----
[CW] collect: return: 171.65053, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 0.05150, qf2_loss: 0.05151, policy_loss: -27.59835, policy_entropy: -6.03351, alpha: 0.00438, time: 33.08934
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   568 ----
[CW] collect: return: 167.33365, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 0.05690, qf2_loss: 0.05757, policy_loss: -27.57701, policy_entropy: -6.00747, alpha: 0.00441, time: 32.77719
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   569 ----
[CW] collect: return: 166.68169, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 0.06983, qf2_loss: 0.07017, policy_loss: -27.71017, policy_entropy: -6.03241, alpha: 0.00440, time: 32.63299
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   570 ----
[CW] collect: return: 152.96539, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 0.05319, qf2_loss: 0.05358, policy_loss: -27.70926, policy_entropy: -6.20722, alpha: 0.00444, time: 32.70598
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   571 ----
[CW] collect: return: 168.70142, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 0.05537, qf2_loss: 0.05459, policy_loss: -27.71423, policy_entropy: -5.79517, alpha: 0.00447, time: 33.05076
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   572 ----
[CW] collect: return: 154.78863, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 0.06213, qf2_loss: 0.06247, policy_loss: -27.69983, policy_entropy: -6.32199, alpha: 0.00446, time: 33.09715
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   573 ----
[CW] collect: return: 156.25953, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 0.09615, qf2_loss: 0.09479, policy_loss: -27.71897, policy_entropy: -6.18631, alpha: 0.00456, time: 32.99438
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   574 ----
[CW] collect: return: 162.22938, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 0.06171, qf2_loss: 0.06164, policy_loss: -27.80568, policy_entropy: -6.15216, alpha: 0.00463, time: 33.43942
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   575 ----
[CW] collect: return: 164.38036, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 0.05274, qf2_loss: 0.05275, policy_loss: -27.76774, policy_entropy: -6.09443, alpha: 0.00469, time: 33.01416
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   576 ----
[CW] collect: return: 166.86928, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 0.05323, qf2_loss: 0.05337, policy_loss: -27.74830, policy_entropy: -6.21785, alpha: 0.00473, time: 33.09583
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   577 ----
[CW] collect: return: 166.46948, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 0.05748, qf2_loss: 0.05691, policy_loss: -27.76448, policy_entropy: -5.80846, alpha: 0.00476, time: 33.26021
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   578 ----
[CW] collect: return: 170.56551, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 0.07126, qf2_loss: 0.07235, policy_loss: -27.85860, policy_entropy: -5.97908, alpha: 0.00468, time: 33.17429
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   579 ----
[CW] collect: return: 157.81447, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 0.06595, qf2_loss: 0.06582, policy_loss: -27.82603, policy_entropy: -6.12839, alpha: 0.00471, time: 32.93780
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   580 ----
[CW] collect: return: 144.21332, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 0.06095, qf2_loss: 0.05984, policy_loss: -27.84620, policy_entropy: -6.06714, alpha: 0.00478, time: 33.35637
[CW] eval: return: 162.19422, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   581 ----
[CW] collect: return: 150.80851, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 0.05594, qf2_loss: 0.05597, policy_loss: -27.88018, policy_entropy: -6.27387, alpha: 0.00479, time: 32.89152
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   582 ----
[CW] collect: return: 167.41853, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 0.05908, qf2_loss: 0.05998, policy_loss: -27.87589, policy_entropy: -6.08126, alpha: 0.00493, time: 32.75949
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   583 ----
[CW] collect: return: 165.24970, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 0.05702, qf2_loss: 0.05601, policy_loss: -27.91947, policy_entropy: -5.91795, alpha: 0.00489, time: 32.86391
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   584 ----
[CW] collect: return: 166.06903, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 0.05888, qf2_loss: 0.05777, policy_loss: -27.92952, policy_entropy: -6.08115, alpha: 0.00490, time: 32.70886
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   585 ----
[CW] collect: return: 167.67047, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 0.06451, qf2_loss: 0.06578, policy_loss: -27.91455, policy_entropy: -6.12041, alpha: 0.00495, time: 32.72586
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   586 ----
[CW] collect: return: 159.37227, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 0.06460, qf2_loss: 0.06529, policy_loss: -27.97474, policy_entropy: -6.15313, alpha: 0.00499, time: 32.92841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   587 ----
[CW] collect: return: 160.93848, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 0.06631, qf2_loss: 0.06657, policy_loss: -27.99490, policy_entropy: -5.86641, alpha: 0.00501, time: 33.33527
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   588 ----
[CW] collect: return: 169.65262, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 0.05945, qf2_loss: 0.05842, policy_loss: -27.99288, policy_entropy: -5.78540, alpha: 0.00493, time: 32.74513
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   589 ----
[CW] collect: return: 167.00816, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 0.06717, qf2_loss: 0.06797, policy_loss: -28.02817, policy_entropy: -6.01777, alpha: 0.00488, time: 33.59885
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   590 ----
[CW] collect: return: 164.29324, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 0.06042, qf2_loss: 0.06121, policy_loss: -28.07586, policy_entropy: -6.08310, alpha: 0.00491, time: 33.13759
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   591 ----
[CW] collect: return: 174.28671, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 0.07437, qf2_loss: 0.07345, policy_loss: -28.13239, policy_entropy: -6.26276, alpha: 0.00497, time: 33.44056
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   592 ----
[CW] collect: return: 166.93330, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 0.05665, qf2_loss: 0.05558, policy_loss: -28.13563, policy_entropy: -5.81282, alpha: 0.00501, time: 32.88698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   593 ----
[CW] collect: return: 170.26659, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 0.06132, qf2_loss: 0.06202, policy_loss: -28.13279, policy_entropy: -5.94215, alpha: 0.00492, time: 33.17481
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   594 ----
[CW] collect: return: 167.07856, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 0.06164, qf2_loss: 0.06101, policy_loss: -28.11826, policy_entropy: -6.02722, alpha: 0.00495, time: 33.83190
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   595 ----
[CW] collect: return: 161.10231, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 0.06112, qf2_loss: 0.06285, policy_loss: -28.20250, policy_entropy: -5.93490, alpha: 0.00493, time: 33.05015
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   596 ----
[CW] collect: return: 167.17163, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 0.06174, qf2_loss: 0.06127, policy_loss: -28.22464, policy_entropy: -5.98229, alpha: 0.00491, time: 33.21335
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   597 ----
[CW] collect: return: 161.98668, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 0.06797, qf2_loss: 0.06686, policy_loss: -28.18893, policy_entropy: -5.98568, alpha: 0.00491, time: 32.96619
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   598 ----
[CW] collect: return: 156.57220, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 0.07086, qf2_loss: 0.06931, policy_loss: -28.21719, policy_entropy: -6.15554, alpha: 0.00492, time: 33.35257
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   599 ----
[CW] collect: return: 170.37836, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 0.06937, qf2_loss: 0.07135, policy_loss: -28.32265, policy_entropy: -6.09313, alpha: 0.00498, time: 33.01040
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   600 ----
[CW] collect: return: 166.77276, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 0.06852, qf2_loss: 0.06731, policy_loss: -28.27276, policy_entropy: -6.00788, alpha: 0.00501, time: 32.93444
[CW] eval: return: 171.79089, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   601 ----
[CW] collect: return: 166.70119, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 0.06386, qf2_loss: 0.06414, policy_loss: -28.31929, policy_entropy: -6.28880, alpha: 0.00507, time: 33.01307
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   602 ----
[CW] collect: return: 167.05912, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 0.06475, qf2_loss: 0.06487, policy_loss: -28.29156, policy_entropy: -6.17418, alpha: 0.00516, time: 32.99140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   603 ----
[CW] collect: return: 170.03630, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 0.05957, qf2_loss: 0.05905, policy_loss: -28.34792, policy_entropy: -6.14891, alpha: 0.00523, time: 32.83260
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   604 ----
[CW] collect: return: 187.42086, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 0.06474, qf2_loss: 0.06437, policy_loss: -28.36431, policy_entropy: -6.04610, alpha: 0.00527, time: 33.17241
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   605 ----
[CW] collect: return: 172.13002, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 0.06631, qf2_loss: 0.06592, policy_loss: -28.35258, policy_entropy: -6.09493, alpha: 0.00530, time: 33.07866
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   606 ----
[CW] collect: return: 168.49914, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 0.06382, qf2_loss: 0.06398, policy_loss: -28.39975, policy_entropy: -5.96231, alpha: 0.00534, time: 33.06728
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   607 ----
[CW] collect: return: 172.04195, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 0.06701, qf2_loss: 0.06597, policy_loss: -28.45243, policy_entropy: -5.78932, alpha: 0.00527, time: 32.80002
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   608 ----
[CW] collect: return: 168.77561, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 0.06747, qf2_loss: 0.06833, policy_loss: -28.52185, policy_entropy: -5.87205, alpha: 0.00518, time: 32.81259
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   609 ----
[CW] collect: return: 166.25952, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 0.06850, qf2_loss: 0.06831, policy_loss: -28.49069, policy_entropy: -6.08638, alpha: 0.00517, time: 32.87279
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   610 ----
[CW] collect: return: 171.80977, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 0.08323, qf2_loss: 0.08146, policy_loss: -28.41592, policy_entropy: -6.12214, alpha: 0.00521, time: 33.10695
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   611 ----
[CW] collect: return: 181.02207, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 0.07701, qf2_loss: 0.07876, policy_loss: -28.52612, policy_entropy: -6.30459, alpha: 0.00530, time: 33.00121
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   612 ----
[CW] collect: return: 173.08944, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 0.07580, qf2_loss: 0.07394, policy_loss: -28.52088, policy_entropy: -5.97353, alpha: 0.00537, time: 33.18002
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   613 ----
[CW] collect: return: 168.25279, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 0.06173, qf2_loss: 0.06203, policy_loss: -28.51843, policy_entropy: -5.95166, alpha: 0.00537, time: 32.89951
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   614 ----
[CW] collect: return: 178.69715, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 0.05912, qf2_loss: 0.05850, policy_loss: -28.55607, policy_entropy: -5.92081, alpha: 0.00535, time: 32.91803
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   615 ----
[CW] collect: return: 171.89689, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 0.06963, qf2_loss: 0.06981, policy_loss: -28.62418, policy_entropy: -6.03705, alpha: 0.00532, time: 32.81539
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   616 ----
[CW] collect: return: 163.54031, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 0.06323, qf2_loss: 0.06351, policy_loss: -28.60848, policy_entropy: -5.99494, alpha: 0.00532, time: 32.95895
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   617 ----
[CW] collect: return: 165.40954, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 0.06994, qf2_loss: 0.07040, policy_loss: -28.70054, policy_entropy: -6.13594, alpha: 0.00535, time: 32.87893
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   618 ----
[CW] collect: return: 174.38223, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 0.08627, qf2_loss: 0.08529, policy_loss: -28.69672, policy_entropy: -6.29508, alpha: 0.00544, time: 32.86018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   619 ----
[CW] collect: return: 160.33669, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 0.06839, qf2_loss: 0.06766, policy_loss: -28.71534, policy_entropy: -5.99254, alpha: 0.00552, time: 33.00737
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   620 ----
[CW] collect: return: 172.23830, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 0.06569, qf2_loss: 0.06678, policy_loss: -28.69716, policy_entropy: -5.71095, alpha: 0.00546, time: 33.01814
[CW] eval: return: 170.16793, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   621 ----
[CW] collect: return: 170.54673, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 0.07152, qf2_loss: 0.07143, policy_loss: -28.72856, policy_entropy: -5.98802, alpha: 0.00538, time: 32.70884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   622 ----
[CW] collect: return: 169.83062, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 0.06718, qf2_loss: 0.06720, policy_loss: -28.77033, policy_entropy: -6.14949, alpha: 0.00541, time: 32.68323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   623 ----
[CW] collect: return: 164.24435, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 0.07600, qf2_loss: 0.07579, policy_loss: -28.78615, policy_entropy: -6.04918, alpha: 0.00548, time: 33.11077
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   624 ----
[CW] collect: return: 173.04638, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 0.07839, qf2_loss: 0.07995, policy_loss: -28.85980, policy_entropy: -6.08457, alpha: 0.00548, time: 32.73474
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   625 ----
[CW] collect: return: 172.78431, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 0.07512, qf2_loss: 0.07223, policy_loss: -28.84758, policy_entropy: -5.86736, alpha: 0.00548, time: 32.76866
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   626 ----
[CW] collect: return: 164.44429, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 0.08627, qf2_loss: 0.08677, policy_loss: -28.88026, policy_entropy: -5.88567, alpha: 0.00541, time: 32.79230
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   627 ----
[CW] collect: return: 173.52539, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 0.07871, qf2_loss: 0.07877, policy_loss: -28.95267, policy_entropy: -6.30264, alpha: 0.00545, time: 32.67907
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   628 ----
[CW] collect: return: 168.60392, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 0.07785, qf2_loss: 0.07773, policy_loss: -28.93838, policy_entropy: -6.15780, alpha: 0.00555, time: 36.10229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   629 ----
[CW] collect: return: 171.48377, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 0.07299, qf2_loss: 0.07207, policy_loss: -28.94820, policy_entropy: -6.12794, alpha: 0.00563, time: 32.76351
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   630 ----
[CW] collect: return: 170.33533, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 0.07234, qf2_loss: 0.07199, policy_loss: -29.02029, policy_entropy: -6.00755, alpha: 0.00564, time: 32.75345
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   631 ----
[CW] collect: return: 168.97156, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 0.07190, qf2_loss: 0.07217, policy_loss: -29.02480, policy_entropy: -5.93897, alpha: 0.00561, time: 33.08455
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   632 ----
[CW] collect: return: 178.24648, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 0.07604, qf2_loss: 0.07693, policy_loss: -29.07613, policy_entropy: -5.94716, alpha: 0.00560, time: 32.59875
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   633 ----
[CW] collect: return: 173.56357, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 0.06908, qf2_loss: 0.06790, policy_loss: -29.11456, policy_entropy: -6.04637, alpha: 0.00559, time: 33.15710
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   634 ----
[CW] collect: return: 168.95746, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 0.07808, qf2_loss: 0.07789, policy_loss: -29.14761, policy_entropy: -6.09225, alpha: 0.00563, time: 32.72563
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   635 ----
[CW] collect: return: 167.13964, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 0.08747, qf2_loss: 0.08678, policy_loss: -29.19121, policy_entropy: -5.84532, alpha: 0.00564, time: 33.00295
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   636 ----
[CW] collect: return: 173.51853, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 0.07355, qf2_loss: 0.07353, policy_loss: -29.19426, policy_entropy: -5.85739, alpha: 0.00556, time: 32.86318
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   637 ----
[CW] collect: return: 170.57258, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 0.07690, qf2_loss: 0.07709, policy_loss: -29.27155, policy_entropy: -5.99266, alpha: 0.00553, time: 32.89169
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   638 ----
[CW] collect: return: 172.54332, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 0.08856, qf2_loss: 0.08890, policy_loss: -29.27544, policy_entropy: -6.11114, alpha: 0.00554, time: 32.85501
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   639 ----
[CW] collect: return: 163.10285, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 0.07046, qf2_loss: 0.07041, policy_loss: -29.29900, policy_entropy: -6.00825, alpha: 0.00557, time: 34.10069
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   640 ----
[CW] collect: return: 174.40600, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 0.08310, qf2_loss: 0.08276, policy_loss: -29.28962, policy_entropy: -6.14430, alpha: 0.00561, time: 33.21887
[CW] eval: return: 174.21388, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   641 ----
[CW] collect: return: 178.19076, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 0.06811, qf2_loss: 0.06807, policy_loss: -29.38105, policy_entropy: -6.03097, alpha: 0.00565, time: 33.06126
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   642 ----
[CW] collect: return: 173.79547, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 0.07281, qf2_loss: 0.07083, policy_loss: -29.38055, policy_entropy: -5.95284, alpha: 0.00565, time: 33.10693
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   643 ----
[CW] collect: return: 174.38857, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 0.08142, qf2_loss: 0.08286, policy_loss: -29.41504, policy_entropy: -6.04601, alpha: 0.00562, time: 33.27979
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   644 ----
[CW] collect: return: 176.08884, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 0.07542, qf2_loss: 0.07550, policy_loss: -29.41002, policy_entropy: -6.08859, alpha: 0.00565, time: 33.25378
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   645 ----
[CW] collect: return: 166.10361, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 0.07517, qf2_loss: 0.07521, policy_loss: -29.43948, policy_entropy: -6.12590, alpha: 0.00571, time: 33.00213
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   646 ----
[CW] collect: return: 180.24803, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 0.08935, qf2_loss: 0.08929, policy_loss: -29.44762, policy_entropy: -6.08122, alpha: 0.00576, time: 32.83507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   647 ----
[CW] collect: return: 168.98676, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 0.07212, qf2_loss: 0.07149, policy_loss: -29.53538, policy_entropy: -5.90585, alpha: 0.00575, time: 33.07753
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   648 ----
[CW] collect: return: 169.89471, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 0.08582, qf2_loss: 0.08536, policy_loss: -29.60774, policy_entropy: -6.12859, alpha: 0.00577, time: 32.85299
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   649 ----
[CW] collect: return: 162.26923, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 0.07213, qf2_loss: 0.07177, policy_loss: -29.56637, policy_entropy: -5.80583, alpha: 0.00577, time: 33.05927
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   650 ----
[CW] collect: return: 178.14981, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 0.07370, qf2_loss: 0.07438, policy_loss: -29.56271, policy_entropy: -6.03770, alpha: 0.00572, time: 33.10281
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   651 ----
[CW] collect: return: 166.62489, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 0.07047, qf2_loss: 0.07059, policy_loss: -29.67701, policy_entropy: -5.64580, alpha: 0.00566, time: 33.02761
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   652 ----
[CW] collect: return: 173.75394, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 0.07541, qf2_loss: 0.07509, policy_loss: -29.62314, policy_entropy: -5.76635, alpha: 0.00555, time: 33.07809
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   653 ----
[CW] collect: return: 172.57544, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 0.07500, qf2_loss: 0.07294, policy_loss: -29.67197, policy_entropy: -5.75862, alpha: 0.00545, time: 32.75150
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   654 ----
[CW] collect: return: 175.31095, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 0.08678, qf2_loss: 0.08779, policy_loss: -29.66669, policy_entropy: -5.97002, alpha: 0.00542, time: 32.93607
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   655 ----
[CW] collect: return: 179.89976, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 0.07281, qf2_loss: 0.07105, policy_loss: -29.75312, policy_entropy: -5.94744, alpha: 0.00538, time: 32.81832
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   656 ----
[CW] collect: return: 172.60103, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 0.06798, qf2_loss: 0.06841, policy_loss: -29.68022, policy_entropy: -5.93780, alpha: 0.00537, time: 32.80600
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   657 ----
[CW] collect: return: 174.88449, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 0.06794, qf2_loss: 0.06811, policy_loss: -29.75863, policy_entropy: -6.05200, alpha: 0.00535, time: 33.32404
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   658 ----
[CW] collect: return: 160.51827, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 0.07123, qf2_loss: 0.07107, policy_loss: -29.79545, policy_entropy: -5.94802, alpha: 0.00536, time: 32.93624
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   659 ----
[CW] collect: return: 173.87965, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 0.07297, qf2_loss: 0.07154, policy_loss: -29.81755, policy_entropy: -6.05406, alpha: 0.00535, time: 33.09176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   660 ----
[CW] collect: return: 187.37080, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 0.06610, qf2_loss: 0.06545, policy_loss: -29.82098, policy_entropy: -5.95246, alpha: 0.00538, time: 32.93158
[CW] eval: return: 176.82670, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   661 ----
[CW] collect: return: 173.78800, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 0.07033, qf2_loss: 0.07165, policy_loss: -29.83358, policy_entropy: -6.02763, alpha: 0.00537, time: 33.75235
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   662 ----
[CW] collect: return: 170.53035, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 0.06855, qf2_loss: 0.06787, policy_loss: -29.89422, policy_entropy: -6.17771, alpha: 0.00539, time: 33.60294
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   663 ----
[CW] collect: return: 185.36000, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 0.08194, qf2_loss: 0.08302, policy_loss: -29.91575, policy_entropy: -6.30167, alpha: 0.00547, time: 33.59380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   664 ----
[CW] collect: return: 189.04629, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 0.07618, qf2_loss: 0.07284, policy_loss: -29.96532, policy_entropy: -5.86659, alpha: 0.00553, time: 33.40611
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   665 ----
[CW] collect: return: 179.29964, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 0.06789, qf2_loss: 0.06793, policy_loss: -30.00488, policy_entropy: -6.04003, alpha: 0.00552, time: 33.08376
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   666 ----
[CW] collect: return: 179.98424, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 0.06526, qf2_loss: 0.06530, policy_loss: -29.97262, policy_entropy: -5.94031, alpha: 0.00551, time: 32.85325
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   667 ----
[CW] collect: return: 180.53550, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 0.06841, qf2_loss: 0.06774, policy_loss: -29.98168, policy_entropy: -5.96658, alpha: 0.00549, time: 33.00491
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   668 ----
[CW] collect: return: 191.31867, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 0.06775, qf2_loss: 0.06757, policy_loss: -29.97680, policy_entropy: -5.83424, alpha: 0.00546, time: 33.01046
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   669 ----
[CW] collect: return: 174.76844, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 0.07100, qf2_loss: 0.07170, policy_loss: -30.04992, policy_entropy: -5.81167, alpha: 0.00538, time: 33.09459
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   670 ----
[CW] collect: return: 205.59187, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 0.06866, qf2_loss: 0.06822, policy_loss: -30.07550, policy_entropy: -6.04699, alpha: 0.00534, time: 33.10304
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   671 ----
[CW] collect: return: 192.74283, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 0.06857, qf2_loss: 0.06741, policy_loss: -30.04897, policy_entropy: -5.93990, alpha: 0.00535, time: 33.14603
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   672 ----
[CW] collect: return: 197.18323, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 0.08153, qf2_loss: 0.08152, policy_loss: -30.10265, policy_entropy: -5.81657, alpha: 0.00530, time: 33.03124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   673 ----
[CW] collect: return: 193.86743, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 0.07617, qf2_loss: 0.07501, policy_loss: -30.15883, policy_entropy: -6.17834, alpha: 0.00529, time: 33.11698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   674 ----
[CW] collect: return: 205.66776, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 0.06643, qf2_loss: 0.06684, policy_loss: -30.18110, policy_entropy: -5.80768, alpha: 0.00531, time: 33.02881
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   675 ----
[CW] collect: return: 177.95109, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 0.07634, qf2_loss: 0.07675, policy_loss: -30.19978, policy_entropy: -6.02775, alpha: 0.00528, time: 33.06974
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   676 ----
[CW] collect: return: 181.83974, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 0.07599, qf2_loss: 0.07578, policy_loss: -30.19289, policy_entropy: -5.93944, alpha: 0.00528, time: 32.99988
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   677 ----
[CW] collect: return: 192.70057, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 0.06720, qf2_loss: 0.06733, policy_loss: -30.23385, policy_entropy: -6.25773, alpha: 0.00529, time: 33.12013
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   678 ----
[CW] collect: return: 165.35251, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 0.08333, qf2_loss: 0.08374, policy_loss: -30.21782, policy_entropy: -6.02012, alpha: 0.00534, time: 32.94398
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   679 ----
[CW] collect: return: 206.91114, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 0.07296, qf2_loss: 0.07091, policy_loss: -30.27289, policy_entropy: -6.44481, alpha: 0.00541, time: 33.11447
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   680 ----
[CW] collect: return: 203.54600, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 0.07187, qf2_loss: 0.07204, policy_loss: -30.27126, policy_entropy: -6.35563, alpha: 0.00557, time: 33.03193
[CW] eval: return: 187.55202, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   681 ----
[CW] collect: return: 203.50079, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 0.07728, qf2_loss: 0.07622, policy_loss: -30.39499, policy_entropy: -6.23264, alpha: 0.00570, time: 33.12459
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   682 ----
[CW] collect: return: 169.73596, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 0.07729, qf2_loss: 0.07922, policy_loss: -30.33834, policy_entropy: -6.08060, alpha: 0.00576, time: 32.96911
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   683 ----
[CW] collect: return: 197.90765, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 0.07022, qf2_loss: 0.06987, policy_loss: -30.39469, policy_entropy: -5.92034, alpha: 0.00576, time: 32.99938
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   684 ----
[CW] collect: return: 197.50528, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 0.08145, qf2_loss: 0.08090, policy_loss: -30.36604, policy_entropy: -5.96227, alpha: 0.00574, time: 32.97595
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   685 ----
[CW] collect: return: 189.26916, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 0.06998, qf2_loss: 0.06949, policy_loss: -30.44139, policy_entropy: -6.11716, alpha: 0.00573, time: 32.97815
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   686 ----
[CW] collect: return: 186.98257, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 0.07692, qf2_loss: 0.07603, policy_loss: -30.43083, policy_entropy: -6.10245, alpha: 0.00579, time: 32.85004
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   687 ----
[CW] collect: return: 195.44819, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 0.07428, qf2_loss: 0.07416, policy_loss: -30.45482, policy_entropy: -6.15939, alpha: 0.00586, time: 33.15778
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   688 ----
[CW] collect: return: 191.38768, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 0.07987, qf2_loss: 0.08074, policy_loss: -30.48697, policy_entropy: -6.08332, alpha: 0.00589, time: 33.16097
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   689 ----
[CW] collect: return: 188.31573, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 0.06813, qf2_loss: 0.06815, policy_loss: -30.55241, policy_entropy: -6.02398, alpha: 0.00593, time: 33.20178
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   690 ----
[CW] collect: return: 184.99612, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 0.07093, qf2_loss: 0.06962, policy_loss: -30.57505, policy_entropy: -5.78588, alpha: 0.00588, time: 33.18885
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   691 ----
[CW] collect: return: 212.95669, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 0.07547, qf2_loss: 0.07480, policy_loss: -30.57036, policy_entropy: -6.03393, alpha: 0.00584, time: 33.17141
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   692 ----
[CW] collect: return: 200.43938, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 0.08078, qf2_loss: 0.08327, policy_loss: -30.56247, policy_entropy: -5.92515, alpha: 0.00583, time: 33.19555
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   693 ----
[CW] collect: return: 201.38925, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 0.07299, qf2_loss: 0.07116, policy_loss: -30.57235, policy_entropy: -6.00019, alpha: 0.00581, time: 33.10663
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   694 ----
[CW] collect: return: 200.19057, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 0.06993, qf2_loss: 0.06946, policy_loss: -30.63428, policy_entropy: -6.01839, alpha: 0.00581, time: 33.08193
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   695 ----
[CW] collect: return: 194.00187, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 0.08728, qf2_loss: 0.08632, policy_loss: -30.58704, policy_entropy: -6.08202, alpha: 0.00583, time: 33.11807
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   696 ----
[CW] collect: return: 188.06725, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 0.08942, qf2_loss: 0.08826, policy_loss: -30.71084, policy_entropy: -6.11562, alpha: 0.00589, time: 33.17038
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   697 ----
[CW] collect: return: 203.10131, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 0.06399, qf2_loss: 0.06437, policy_loss: -30.69762, policy_entropy: -6.00775, alpha: 0.00592, time: 33.21794
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   698 ----
[CW] collect: return: 202.85606, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 0.07145, qf2_loss: 0.07183, policy_loss: -30.78025, policy_entropy: -5.95229, alpha: 0.00590, time: 33.10822
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   699 ----
[CW] collect: return: 196.86714, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 0.07000, qf2_loss: 0.06943, policy_loss: -30.75908, policy_entropy: -6.07806, alpha: 0.00592, time: 33.22949
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   700 ----
[CW] collect: return: 185.04255, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 0.07327, qf2_loss: 0.07384, policy_loss: -30.73553, policy_entropy: -5.93266, alpha: 0.00592, time: 33.17063
[CW] eval: return: 200.19412, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   701 ----
[CW] collect: return: 207.33276, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 0.07134, qf2_loss: 0.07089, policy_loss: -30.78780, policy_entropy: -6.08396, alpha: 0.00592, time: 33.15709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   702 ----
[CW] collect: return: 207.13707, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 0.06848, qf2_loss: 0.06794, policy_loss: -30.80911, policy_entropy: -6.00182, alpha: 0.00593, time: 33.17831
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   703 ----
[CW] collect: return: 201.50890, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 0.06817, qf2_loss: 0.06751, policy_loss: -30.82524, policy_entropy: -5.95817, alpha: 0.00594, time: 32.92381
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   704 ----
[CW] collect: return: 222.65352, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 0.07931, qf2_loss: 0.07992, policy_loss: -30.82449, policy_entropy: -5.81661, alpha: 0.00588, time: 33.07414
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   705 ----
[CW] collect: return: 216.94945, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 0.07563, qf2_loss: 0.07482, policy_loss: -30.87571, policy_entropy: -5.94259, alpha: 0.00583, time: 33.04594
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   706 ----
[CW] collect: return: 194.26495, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 0.07658, qf2_loss: 0.07765, policy_loss: -30.83758, policy_entropy: -5.99810, alpha: 0.00581, time: 33.01960
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   707 ----
[CW] collect: return: 207.11391, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 0.06648, qf2_loss: 0.06611, policy_loss: -30.93863, policy_entropy: -6.11906, alpha: 0.00584, time: 33.11010
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   708 ----
[CW] collect: return: 206.84181, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 0.07649, qf2_loss: 0.07636, policy_loss: -30.94680, policy_entropy: -5.99604, alpha: 0.00586, time: 33.21297
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   709 ----
[CW] collect: return: 200.41453, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 0.07374, qf2_loss: 0.07291, policy_loss: -31.01161, policy_entropy: -6.19011, alpha: 0.00589, time: 33.16327
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   710 ----
[CW] collect: return: 212.67264, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 0.07352, qf2_loss: 0.07327, policy_loss: -30.96211, policy_entropy: -6.02416, alpha: 0.00594, time: 33.17402
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   711 ----
[CW] collect: return: 213.95362, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 0.08085, qf2_loss: 0.08247, policy_loss: -30.93157, policy_entropy: -6.11721, alpha: 0.00598, time: 33.13986
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   712 ----
[CW] collect: return: 232.93206, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 0.07563, qf2_loss: 0.07431, policy_loss: -30.97084, policy_entropy: -6.07960, alpha: 0.00602, time: 33.38612
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   713 ----
[CW] collect: return: 214.36976, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 0.08871, qf2_loss: 0.08881, policy_loss: -30.99563, policy_entropy: -6.04316, alpha: 0.00605, time: 33.14346
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   714 ----
[CW] collect: return: 219.95840, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 0.07361, qf2_loss: 0.07417, policy_loss: -30.99247, policy_entropy: -5.96291, alpha: 0.00603, time: 33.20288
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   715 ----
[CW] collect: return: 223.00051, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 0.07229, qf2_loss: 0.07197, policy_loss: -31.08615, policy_entropy: -5.96817, alpha: 0.00604, time: 33.00932
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   716 ----
[CW] collect: return: 210.25772, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 0.06880, qf2_loss: 0.06887, policy_loss: -31.07012, policy_entropy: -6.06701, alpha: 0.00604, time: 33.18279
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   717 ----
[CW] collect: return: 204.46268, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 0.07189, qf2_loss: 0.07105, policy_loss: -31.06782, policy_entropy: -5.97768, alpha: 0.00607, time: 33.00440
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   718 ----
[CW] collect: return: 204.47382, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 0.07397, qf2_loss: 0.07433, policy_loss: -31.11009, policy_entropy: -6.11496, alpha: 0.00607, time: 33.11015
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   719 ----
[CW] collect: return: 215.23781, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 0.08142, qf2_loss: 0.08057, policy_loss: -31.16206, policy_entropy: -5.96122, alpha: 0.00609, time: 33.13104
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   720 ----
[CW] collect: return: 230.58861, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 0.08408, qf2_loss: 0.08322, policy_loss: -31.14412, policy_entropy: -5.81924, alpha: 0.00605, time: 33.28288
[CW] eval: return: 204.56530, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   721 ----
[CW] collect: return: 202.24019, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 0.07207, qf2_loss: 0.07160, policy_loss: -31.18502, policy_entropy: -6.06111, alpha: 0.00601, time: 33.04080
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   722 ----
[CW] collect: return: 219.12335, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 0.07448, qf2_loss: 0.07354, policy_loss: -31.22344, policy_entropy: -6.07446, alpha: 0.00604, time: 32.94140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   723 ----
[CW] collect: return: 174.89593, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 0.07379, qf2_loss: 0.07372, policy_loss: -31.19463, policy_entropy: -6.03497, alpha: 0.00607, time: 33.01918
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   724 ----
[CW] collect: return: 217.11309, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 0.08760, qf2_loss: 0.08944, policy_loss: -31.24850, policy_entropy: -6.06381, alpha: 0.00608, time: 33.08412
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   725 ----
[CW] collect: return: 200.68885, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 0.07549, qf2_loss: 0.07522, policy_loss: -31.32133, policy_entropy: -6.30501, alpha: 0.00618, time: 33.10164
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   726 ----
[CW] collect: return: 222.83501, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 0.08226, qf2_loss: 0.08062, policy_loss: -31.26000, policy_entropy: -6.00798, alpha: 0.00625, time: 33.12885
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   727 ----
[CW] collect: return: 220.52346, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 0.07904, qf2_loss: 0.07840, policy_loss: -31.31386, policy_entropy: -6.01092, alpha: 0.00625, time: 33.30401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   728 ----
[CW] collect: return: 203.05006, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 0.08507, qf2_loss: 0.08403, policy_loss: -31.40108, policy_entropy: -6.07459, alpha: 0.00628, time: 33.16626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   729 ----
[CW] collect: return: 206.25741, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 0.08150, qf2_loss: 0.08115, policy_loss: -31.38388, policy_entropy: -5.97079, alpha: 0.00628, time: 33.10528
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   730 ----
[CW] collect: return: 226.86620, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 0.07713, qf2_loss: 0.07679, policy_loss: -31.44857, policy_entropy: -5.92866, alpha: 0.00629, time: 33.01117
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   731 ----
[CW] collect: return: 223.27337, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 0.08598, qf2_loss: 0.08725, policy_loss: -31.40905, policy_entropy: -5.96976, alpha: 0.00624, time: 33.08719
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   732 ----
[CW] collect: return: 225.27874, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 0.09444, qf2_loss: 0.09604, policy_loss: -31.43959, policy_entropy: -6.01887, alpha: 0.00625, time: 33.03064
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   733 ----
[CW] collect: return: 222.30445, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 0.08878, qf2_loss: 0.08710, policy_loss: -31.46268, policy_entropy: -6.08583, alpha: 0.00626, time: 33.12935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   734 ----
[CW] collect: return: 222.03554, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 0.07718, qf2_loss: 0.07679, policy_loss: -31.45859, policy_entropy: -6.21494, alpha: 0.00631, time: 33.16029
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   735 ----
[CW] collect: return: 194.42647, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 0.07919, qf2_loss: 0.07833, policy_loss: -31.52640, policy_entropy: -6.08059, alpha: 0.00639, time: 33.12472
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   736 ----
[CW] collect: return: 221.93031, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 0.08880, qf2_loss: 0.08883, policy_loss: -31.53151, policy_entropy: -6.09140, alpha: 0.00643, time: 32.96414
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   737 ----
[CW] collect: return: 223.31704, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 0.08627, qf2_loss: 0.08659, policy_loss: -31.54303, policy_entropy: -6.23550, alpha: 0.00652, time: 33.17642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   738 ----
[CW] collect: return: 236.51736, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 0.07968, qf2_loss: 0.08000, policy_loss: -31.54353, policy_entropy: -6.10802, alpha: 0.00664, time: 32.91836
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   739 ----
[CW] collect: return: 215.23895, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 0.07845, qf2_loss: 0.07856, policy_loss: -31.62096, policy_entropy: -5.98084, alpha: 0.00666, time: 33.03961
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   740 ----
[CW] collect: return: 226.98394, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 0.08841, qf2_loss: 0.08680, policy_loss: -31.69656, policy_entropy: -5.86604, alpha: 0.00661, time: 33.08406
[CW] eval: return: 214.39507, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   741 ----
[CW] collect: return: 221.74041, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 0.08518, qf2_loss: 0.08495, policy_loss: -31.67140, policy_entropy: -5.90018, alpha: 0.00655, time: 32.92271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   742 ----
[CW] collect: return: 194.31741, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 0.08018, qf2_loss: 0.08115, policy_loss: -31.72216, policy_entropy: -5.94753, alpha: 0.00653, time: 32.99891
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   743 ----
[CW] collect: return: 231.20663, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 0.08181, qf2_loss: 0.08169, policy_loss: -31.69654, policy_entropy: -5.81926, alpha: 0.00645, time: 32.92593
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   744 ----
[CW] collect: return: 209.70483, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 0.09953, qf2_loss: 0.09905, policy_loss: -31.74026, policy_entropy: -6.07412, alpha: 0.00641, time: 33.06421
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   745 ----
[CW] collect: return: 194.02179, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 0.07969, qf2_loss: 0.07953, policy_loss: -31.73905, policy_entropy: -6.16399, alpha: 0.00646, time: 32.65948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   746 ----
[CW] collect: return: 203.71028, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 0.09063, qf2_loss: 0.09046, policy_loss: -31.77521, policy_entropy: -6.01544, alpha: 0.00652, time: 32.88349
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   747 ----
[CW] collect: return: 215.32375, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 0.08742, qf2_loss: 0.08721, policy_loss: -31.84472, policy_entropy: -5.91385, alpha: 0.00652, time: 33.00136
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   748 ----
[CW] collect: return: 211.07002, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 0.08436, qf2_loss: 0.08497, policy_loss: -31.82567, policy_entropy: -6.00191, alpha: 0.00648, time: 32.96247
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   749 ----
[CW] collect: return: 210.59708, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 0.07731, qf2_loss: 0.07765, policy_loss: -31.86075, policy_entropy: -5.90526, alpha: 0.00646, time: 33.00713
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   750 ----
[CW] collect: return: 224.16382, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 0.08483, qf2_loss: 0.08414, policy_loss: -31.82308, policy_entropy: -5.92155, alpha: 0.00643, time: 32.78526
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   751 ----
[CW] collect: return: 219.29949, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 0.08191, qf2_loss: 0.08097, policy_loss: -31.94384, policy_entropy: -5.90949, alpha: 0.00638, time: 32.89755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   752 ----
[CW] collect: return: 215.76981, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 0.08654, qf2_loss: 0.08717, policy_loss: -31.92718, policy_entropy: -5.78695, alpha: 0.00632, time: 32.88751
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   753 ----
[CW] collect: return: 217.63030, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 0.08509, qf2_loss: 0.08429, policy_loss: -31.94172, policy_entropy: -5.81250, alpha: 0.00621, time: 33.00502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   754 ----
[CW] collect: return: 235.06160, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 0.08592, qf2_loss: 0.08442, policy_loss: -32.01983, policy_entropy: -6.09089, alpha: 0.00617, time: 32.84658
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   755 ----
[CW] collect: return: 184.11782, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 0.08003, qf2_loss: 0.07990, policy_loss: -32.05271, policy_entropy: -6.07093, alpha: 0.00622, time: 32.87891
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   756 ----
[CW] collect: return: 222.65617, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 0.08538, qf2_loss: 0.08477, policy_loss: -32.00111, policy_entropy: -5.96443, alpha: 0.00621, time: 33.01009
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   757 ----
[CW] collect: return: 231.16937, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 0.09156, qf2_loss: 0.09016, policy_loss: -32.08350, policy_entropy: -6.09277, alpha: 0.00624, time: 33.14208
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   758 ----
[CW] collect: return: 222.93400, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 0.08747, qf2_loss: 0.08740, policy_loss: -32.07777, policy_entropy: -6.02279, alpha: 0.00628, time: 33.12730
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   759 ----
[CW] collect: return: 200.83280, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 0.08714, qf2_loss: 0.08660, policy_loss: -32.18453, policy_entropy: -6.02229, alpha: 0.00627, time: 32.98545
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   760 ----
[CW] collect: return: 216.32798, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 0.08493, qf2_loss: 0.08211, policy_loss: -32.14052, policy_entropy: -5.99131, alpha: 0.00628, time: 32.85858
[CW] eval: return: 224.59736, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   761 ----
[CW] collect: return: 221.22545, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 0.08402, qf2_loss: 0.08453, policy_loss: -32.21130, policy_entropy: -6.03569, alpha: 0.00629, time: 32.93917
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   762 ----
[CW] collect: return: 233.17190, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 0.08272, qf2_loss: 0.08384, policy_loss: -32.16383, policy_entropy: -6.04224, alpha: 0.00630, time: 33.05909
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   763 ----
[CW] collect: return: 205.58466, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 0.08462, qf2_loss: 0.08516, policy_loss: -32.24560, policy_entropy: -6.09661, alpha: 0.00635, time: 32.79996
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   764 ----
[CW] collect: return: 221.64862, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 0.08390, qf2_loss: 0.08331, policy_loss: -32.27274, policy_entropy: -6.08828, alpha: 0.00637, time: 32.92944
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   765 ----
[CW] collect: return: 223.25383, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 0.08803, qf2_loss: 0.08891, policy_loss: -32.31520, policy_entropy: -6.07236, alpha: 0.00645, time: 32.73233
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   766 ----
[CW] collect: return: 216.81018, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 0.09091, qf2_loss: 0.08965, policy_loss: -32.23386, policy_entropy: -6.08253, alpha: 0.00650, time: 33.11337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   767 ----
[CW] collect: return: 234.61840, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 0.10001, qf2_loss: 0.09897, policy_loss: -32.34448, policy_entropy: -5.87871, alpha: 0.00648, time: 33.25331
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   768 ----
[CW] collect: return: 212.69517, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 0.08724, qf2_loss: 0.08668, policy_loss: -32.38842, policy_entropy: -6.08562, alpha: 0.00646, time: 33.78462
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   769 ----
[CW] collect: return: 235.29881, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 0.08956, qf2_loss: 0.08900, policy_loss: -32.34574, policy_entropy: -6.11621, alpha: 0.00650, time: 36.57800
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   770 ----
[CW] collect: return: 214.25434, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 0.07885, qf2_loss: 0.07872, policy_loss: -32.41087, policy_entropy: -6.19164, alpha: 0.00660, time: 33.57852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   771 ----
[CW] collect: return: 223.19932, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 0.09334, qf2_loss: 0.09202, policy_loss: -32.45207, policy_entropy: -6.02324, alpha: 0.00668, time: 33.57608
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   772 ----
[CW] collect: return: 238.54567, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 0.11217, qf2_loss: 0.11227, policy_loss: -32.50655, policy_entropy: -5.93350, alpha: 0.00665, time: 33.62750
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   773 ----
[CW] collect: return: 247.45317, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 0.08825, qf2_loss: 0.08802, policy_loss: -32.50709, policy_entropy: -6.02142, alpha: 0.00666, time: 33.79844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   774 ----
[CW] collect: return: 229.77883, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 0.10494, qf2_loss: 0.10397, policy_loss: -32.50255, policy_entropy: -5.97024, alpha: 0.00665, time: 33.72648
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   775 ----
[CW] collect: return: 239.33214, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 0.08423, qf2_loss: 0.08307, policy_loss: -32.51118, policy_entropy: -6.08185, alpha: 0.00665, time: 33.61533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   776 ----
[CW] collect: return: 221.70251, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 0.08790, qf2_loss: 0.08828, policy_loss: -32.58948, policy_entropy: -5.96317, alpha: 0.00667, time: 33.60190
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   777 ----
[CW] collect: return: 238.78531, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 0.09467, qf2_loss: 0.09412, policy_loss: -32.57434, policy_entropy: -5.96600, alpha: 0.00665, time: 33.65573
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   778 ----
[CW] collect: return: 201.17887, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 0.10946, qf2_loss: 0.11018, policy_loss: -32.57410, policy_entropy: -6.12377, alpha: 0.00668, time: 33.74316
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   779 ----
[CW] collect: return: 228.37893, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 0.09124, qf2_loss: 0.08979, policy_loss: -32.65451, policy_entropy: -6.23318, alpha: 0.00679, time: 33.58494
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   780 ----
[CW] collect: return: 236.95255, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 0.09257, qf2_loss: 0.09134, policy_loss: -32.72740, policy_entropy: -6.00384, alpha: 0.00686, time: 33.60533
[CW] eval: return: 228.54543, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   781 ----
[CW] collect: return: 213.65591, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 0.08886, qf2_loss: 0.08920, policy_loss: -32.73591, policy_entropy: -5.96697, alpha: 0.00685, time: 33.70691
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   782 ----
[CW] collect: return: 227.56124, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 0.09211, qf2_loss: 0.09214, policy_loss: -32.78491, policy_entropy: -5.95661, alpha: 0.00683, time: 33.62966
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   783 ----
[CW] collect: return: 219.76416, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 0.09937, qf2_loss: 0.09869, policy_loss: -32.74298, policy_entropy: -5.86169, alpha: 0.00677, time: 33.61757
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   784 ----
[CW] collect: return: 210.29943, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 0.10105, qf2_loss: 0.09940, policy_loss: -32.79854, policy_entropy: -6.09810, alpha: 0.00675, time: 33.66000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   785 ----
[CW] collect: return: 221.94284, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 0.09785, qf2_loss: 0.09767, policy_loss: -32.86399, policy_entropy: -6.02283, alpha: 0.00679, time: 33.57636
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   786 ----
[CW] collect: return: 234.30781, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 0.09520, qf2_loss: 0.09500, policy_loss: -32.87103, policy_entropy: -5.90501, alpha: 0.00679, time: 33.76293
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   787 ----
[CW] collect: return: 240.01268, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 0.10680, qf2_loss: 0.10533, policy_loss: -32.94190, policy_entropy: -5.87565, alpha: 0.00670, time: 33.80467
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   788 ----
[CW] collect: return: 248.61927, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 0.10502, qf2_loss: 0.10599, policy_loss: -32.93988, policy_entropy: -5.97365, alpha: 0.00669, time: 33.92410
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   789 ----
[CW] collect: return: 229.86401, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 0.08520, qf2_loss: 0.08431, policy_loss: -32.90373, policy_entropy: -5.99750, alpha: 0.00667, time: 33.68817
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   790 ----
[CW] collect: return: 216.76071, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 0.08697, qf2_loss: 0.08559, policy_loss: -32.96668, policy_entropy: -5.95517, alpha: 0.00665, time: 33.76871
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   791 ----
[CW] collect: return: 242.41204, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 0.10131, qf2_loss: 0.10161, policy_loss: -32.97601, policy_entropy: -6.03233, alpha: 0.00662, time: 33.89429
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   792 ----
[CW] collect: return: 242.53217, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 0.09653, qf2_loss: 0.09728, policy_loss: -32.99209, policy_entropy: -5.95229, alpha: 0.00664, time: 33.68305
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   793 ----
[CW] collect: return: 230.90432, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 0.10029, qf2_loss: 0.09935, policy_loss: -32.96599, policy_entropy: -6.02202, alpha: 0.00666, time: 33.65663
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   794 ----
[CW] collect: return: 234.91289, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 0.09387, qf2_loss: 0.09337, policy_loss: -33.03853, policy_entropy: -6.04827, alpha: 0.00663, time: 33.58934
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   795 ----
[CW] collect: return: 239.40948, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 0.08998, qf2_loss: 0.08930, policy_loss: -33.08128, policy_entropy: -6.01985, alpha: 0.00668, time: 33.66192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   796 ----
[CW] collect: return: 223.96459, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 0.08937, qf2_loss: 0.08898, policy_loss: -33.06452, policy_entropy: -6.00469, alpha: 0.00668, time: 33.89889
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   797 ----
[CW] collect: return: 250.24654, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 0.08971, qf2_loss: 0.08932, policy_loss: -33.15868, policy_entropy: -6.11906, alpha: 0.00671, time: 33.75813
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   798 ----
[CW] collect: return: 218.08409, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 0.08947, qf2_loss: 0.08828, policy_loss: -33.15132, policy_entropy: -6.05419, alpha: 0.00674, time: 33.70064
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   799 ----
[CW] collect: return: 234.43450, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 0.09575, qf2_loss: 0.09542, policy_loss: -33.13677, policy_entropy: -5.96330, alpha: 0.00676, time: 33.64204
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   800 ----
[CW] collect: return: 207.91197, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 0.10407, qf2_loss: 0.10573, policy_loss: -33.21358, policy_entropy: -5.96658, alpha: 0.00676, time: 33.75817
[CW] eval: return: 233.71855, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   801 ----
[CW] collect: return: 220.07331, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 0.09110, qf2_loss: 0.09047, policy_loss: -33.17079, policy_entropy: -5.90020, alpha: 0.00669, time: 33.64711
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   802 ----
[CW] collect: return: 240.39019, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 0.10057, qf2_loss: 0.09807, policy_loss: -33.26582, policy_entropy: -6.08014, alpha: 0.00668, time: 33.60711
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   803 ----
[CW] collect: return: 257.08311, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 0.10297, qf2_loss: 0.10428, policy_loss: -33.24889, policy_entropy: -6.12728, alpha: 0.00675, time: 33.58247
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   804 ----
[CW] collect: return: 235.61474, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 0.08636, qf2_loss: 0.08566, policy_loss: -33.30442, policy_entropy: -6.13382, alpha: 0.00683, time: 33.52888
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   805 ----
[CW] collect: return: 251.37143, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 0.08324, qf2_loss: 0.08232, policy_loss: -33.30991, policy_entropy: -6.12995, alpha: 0.00690, time: 33.61875
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   806 ----
[CW] collect: return: 249.57360, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 0.09661, qf2_loss: 0.09568, policy_loss: -33.30444, policy_entropy: -6.07927, alpha: 0.00695, time: 33.56145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   807 ----
[CW] collect: return: 251.02149, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 0.09941, qf2_loss: 0.09860, policy_loss: -33.34621, policy_entropy: -6.04348, alpha: 0.00700, time: 33.65195
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   808 ----
[CW] collect: return: 229.43035, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 0.09646, qf2_loss: 0.09520, policy_loss: -33.31869, policy_entropy: -6.09953, alpha: 0.00704, time: 33.63820
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   809 ----
[CW] collect: return: 243.75053, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 0.08741, qf2_loss: 0.08799, policy_loss: -33.37089, policy_entropy: -5.92068, alpha: 0.00705, time: 33.65894
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   810 ----
[CW] collect: return: 242.62002, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 0.09373, qf2_loss: 0.09384, policy_loss: -33.37822, policy_entropy: -5.99933, alpha: 0.00702, time: 33.68529
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   811 ----
[CW] collect: return: 228.71765, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 0.09488, qf2_loss: 0.09474, policy_loss: -33.41311, policy_entropy: -6.04772, alpha: 0.00705, time: 33.50697
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   812 ----
[CW] collect: return: 235.44733, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 0.08825, qf2_loss: 0.08723, policy_loss: -33.44032, policy_entropy: -5.96036, alpha: 0.00704, time: 33.52919
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   813 ----
[CW] collect: return: 217.07065, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 0.09158, qf2_loss: 0.09110, policy_loss: -33.48866, policy_entropy: -5.84567, alpha: 0.00701, time: 33.46904
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   814 ----
[CW] collect: return: 254.56345, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 0.09472, qf2_loss: 0.09508, policy_loss: -33.51810, policy_entropy: -5.97994, alpha: 0.00694, time: 33.60027
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   815 ----
[CW] collect: return: 249.72193, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 0.09192, qf2_loss: 0.09137, policy_loss: -33.56653, policy_entropy: -6.00443, alpha: 0.00693, time: 33.39958
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   816 ----
[CW] collect: return: 235.48181, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 0.09214, qf2_loss: 0.09175, policy_loss: -33.52702, policy_entropy: -6.07011, alpha: 0.00696, time: 33.51007
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   817 ----
[CW] collect: return: 235.96545, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 0.09370, qf2_loss: 0.09377, policy_loss: -33.55637, policy_entropy: -6.01231, alpha: 0.00697, time: 33.36175
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   818 ----
[CW] collect: return: 251.39087, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 0.09455, qf2_loss: 0.09343, policy_loss: -33.61405, policy_entropy: -6.09586, alpha: 0.00699, time: 33.46085
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   819 ----
[CW] collect: return: 241.91988, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 0.09589, qf2_loss: 0.09515, policy_loss: -33.64477, policy_entropy: -5.96650, alpha: 0.00704, time: 33.30606
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   820 ----
[CW] collect: return: 234.15903, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 0.10413, qf2_loss: 0.10332, policy_loss: -33.63111, policy_entropy: -6.09207, alpha: 0.00704, time: 33.41642
[CW] eval: return: 238.23295, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   821 ----
[CW] collect: return: 241.65050, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 0.09462, qf2_loss: 0.09482, policy_loss: -33.65793, policy_entropy: -5.97708, alpha: 0.00707, time: 33.34215
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   822 ----
[CW] collect: return: 247.74540, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 0.10188, qf2_loss: 0.10171, policy_loss: -33.71153, policy_entropy: -6.01568, alpha: 0.00707, time: 33.52219
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   823 ----
[CW] collect: return: 259.04084, steps: 1000.00000, total_steps: 829000.00000
[CW] train: qf1_loss: 0.09989, qf2_loss: 0.09866, policy_loss: -33.73222, policy_entropy: -5.97582, alpha: 0.00706, time: 33.39254
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   824 ----
[CW] collect: return: 237.46519, steps: 1000.00000, total_steps: 830000.00000
[CW] train: qf1_loss: 0.09745, qf2_loss: 0.09659, policy_loss: -33.73002, policy_entropy: -5.91020, alpha: 0.00701, time: 33.27268
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   825 ----
[CW] collect: return: 239.15200, steps: 1000.00000, total_steps: 831000.00000
[CW] train: qf1_loss: 0.10808, qf2_loss: 0.10844, policy_loss: -33.75577, policy_entropy: -5.94515, alpha: 0.00700, time: 33.45357
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   826 ----
[CW] collect: return: 237.45446, steps: 1000.00000, total_steps: 832000.00000
[CW] train: qf1_loss: 0.09906, qf2_loss: 0.09778, policy_loss: -33.81134, policy_entropy: -5.91284, alpha: 0.00697, time: 33.36825
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   827 ----
[CW] collect: return: 249.66744, steps: 1000.00000, total_steps: 833000.00000
[CW] train: qf1_loss: 0.10153, qf2_loss: 0.10212, policy_loss: -33.85262, policy_entropy: -5.82027, alpha: 0.00688, time: 33.53140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   828 ----
[CW] collect: return: 242.74405, steps: 1000.00000, total_steps: 834000.00000
[CW] train: qf1_loss: 0.10375, qf2_loss: 0.10409, policy_loss: -33.88285, policy_entropy: -5.96755, alpha: 0.00681, time: 33.36974
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   829 ----
[CW] collect: return: 257.52590, steps: 1000.00000, total_steps: 835000.00000
[CW] train: qf1_loss: 0.09971, qf2_loss: 0.09833, policy_loss: -33.88912, policy_entropy: -6.03442, alpha: 0.00680, time: 33.35306
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   830 ----
[CW] collect: return: 239.81559, steps: 1000.00000, total_steps: 836000.00000
[CW] train: qf1_loss: 0.11085, qf2_loss: 0.10954, policy_loss: -33.88990, policy_entropy: -6.15854, alpha: 0.00687, time: 33.39454
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   831 ----
[CW] collect: return: 228.86727, steps: 1000.00000, total_steps: 837000.00000
[CW] train: qf1_loss: 0.09666, qf2_loss: 0.09618, policy_loss: -33.98727, policy_entropy: -6.07333, alpha: 0.00692, time: 33.39836
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   832 ----
[CW] collect: return: 236.61257, steps: 1000.00000, total_steps: 838000.00000
[CW] train: qf1_loss: 0.09926, qf2_loss: 0.09867, policy_loss: -33.95681, policy_entropy: -6.01369, alpha: 0.00696, time: 33.35593
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   833 ----
[CW] collect: return: 258.26059, steps: 1000.00000, total_steps: 839000.00000
[CW] train: qf1_loss: 0.09944, qf2_loss: 0.09849, policy_loss: -33.97092, policy_entropy: -6.02594, alpha: 0.00696, time: 33.40022
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   834 ----
[CW] collect: return: 249.03555, steps: 1000.00000, total_steps: 840000.00000
[CW] train: qf1_loss: 0.11022, qf2_loss: 0.11025, policy_loss: -34.03099, policy_entropy: -6.11544, alpha: 0.00698, time: 33.38247
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   835 ----
[CW] collect: return: 245.11840, steps: 1000.00000, total_steps: 841000.00000
[CW] train: qf1_loss: 0.11746, qf2_loss: 0.11682, policy_loss: -34.05297, policy_entropy: -5.89619, alpha: 0.00701, time: 33.32724
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   836 ----
[CW] collect: return: 254.76904, steps: 1000.00000, total_steps: 842000.00000
[CW] train: qf1_loss: 0.09553, qf2_loss: 0.09495, policy_loss: -34.14314, policy_entropy: -5.97308, alpha: 0.00696, time: 33.33841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   837 ----
[CW] collect: return: 217.32732, steps: 1000.00000, total_steps: 843000.00000
[CW] train: qf1_loss: 0.09436, qf2_loss: 0.09361, policy_loss: -34.13685, policy_entropy: -6.00266, alpha: 0.00695, time: 33.45689
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   838 ----
[CW] collect: return: 243.21829, steps: 1000.00000, total_steps: 844000.00000
[CW] train: qf1_loss: 0.09466, qf2_loss: 0.09360, policy_loss: -34.11374, policy_entropy: -5.78192, alpha: 0.00692, time: 33.47250
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   839 ----
[CW] collect: return: 238.41526, steps: 1000.00000, total_steps: 845000.00000
[CW] train: qf1_loss: 0.10159, qf2_loss: 0.10116, policy_loss: -34.13823, policy_entropy: -5.86479, alpha: 0.00680, time: 33.37074
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   840 ----
[CW] collect: return: 235.09900, steps: 1000.00000, total_steps: 846000.00000
[CW] train: qf1_loss: 0.09810, qf2_loss: 0.09814, policy_loss: -34.11119, policy_entropy: -5.95169, alpha: 0.00675, time: 33.35682
[CW] eval: return: 240.48623, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   841 ----
[CW] collect: return: 243.34087, steps: 1000.00000, total_steps: 847000.00000
[CW] train: qf1_loss: 0.10358, qf2_loss: 0.10282, policy_loss: -34.14762, policy_entropy: -6.04121, alpha: 0.00674, time: 33.42521
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   842 ----
[CW] collect: return: 248.87412, steps: 1000.00000, total_steps: 848000.00000
[CW] train: qf1_loss: 0.10033, qf2_loss: 0.10028, policy_loss: -34.28663, policy_entropy: -6.07393, alpha: 0.00677, time: 33.27735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   843 ----
[CW] collect: return: 254.29466, steps: 1000.00000, total_steps: 849000.00000
[CW] train: qf1_loss: 0.11238, qf2_loss: 0.11281, policy_loss: -34.24159, policy_entropy: -5.99909, alpha: 0.00683, time: 33.23450
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   844 ----
[CW] collect: return: 237.08971, steps: 1000.00000, total_steps: 850000.00000
[CW] train: qf1_loss: 0.09657, qf2_loss: 0.09811, policy_loss: -34.25617, policy_entropy: -5.85576, alpha: 0.00678, time: 33.25501
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   845 ----
[CW] collect: return: 253.56631, steps: 1000.00000, total_steps: 851000.00000
[CW] train: qf1_loss: 0.10763, qf2_loss: 0.10602, policy_loss: -34.28837, policy_entropy: -5.97950, alpha: 0.00671, time: 33.37928
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   846 ----
[CW] collect: return: 227.57548, steps: 1000.00000, total_steps: 852000.00000
[CW] train: qf1_loss: 0.09883, qf2_loss: 0.09922, policy_loss: -34.34096, policy_entropy: -6.12220, alpha: 0.00674, time: 33.29427
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   847 ----
[CW] collect: return: 219.52806, steps: 1000.00000, total_steps: 853000.00000
[CW] train: qf1_loss: 0.10732, qf2_loss: 0.10691, policy_loss: -34.37853, policy_entropy: -6.09036, alpha: 0.00681, time: 33.37571
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   848 ----
[CW] collect: return: 248.55846, steps: 1000.00000, total_steps: 854000.00000
[CW] train: qf1_loss: 0.10261, qf2_loss: 0.10230, policy_loss: -34.31608, policy_entropy: -6.02693, alpha: 0.00684, time: 33.16218
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   849 ----
[CW] collect: return: 231.56912, steps: 1000.00000, total_steps: 855000.00000
[CW] train: qf1_loss: 0.10561, qf2_loss: 0.10588, policy_loss: -34.45081, policy_entropy: -6.03229, alpha: 0.00688, time: 33.36062
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   850 ----
[CW] collect: return: 241.51230, steps: 1000.00000, total_steps: 856000.00000
[CW] train: qf1_loss: 0.09886, qf2_loss: 0.09734, policy_loss: -34.38599, policy_entropy: -6.05331, alpha: 0.00687, time: 33.16533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   851 ----
[CW] collect: return: 241.68470, steps: 1000.00000, total_steps: 857000.00000
[CW] train: qf1_loss: 0.10601, qf2_loss: 0.10573, policy_loss: -34.47583, policy_entropy: -6.02046, alpha: 0.00690, time: 33.58824
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   852 ----
[CW] collect: return: 237.25878, steps: 1000.00000, total_steps: 858000.00000
[CW] train: qf1_loss: 0.10504, qf2_loss: 0.10477, policy_loss: -34.54007, policy_entropy: -6.02960, alpha: 0.00691, time: 33.14667
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   853 ----
[CW] collect: return: 237.88932, steps: 1000.00000, total_steps: 859000.00000
[CW] train: qf1_loss: 0.11025, qf2_loss: 0.10994, policy_loss: -34.55504, policy_entropy: -5.99440, alpha: 0.00693, time: 33.39370
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   854 ----
[CW] collect: return: 255.52796, steps: 1000.00000, total_steps: 860000.00000
[CW] train: qf1_loss: 0.10761, qf2_loss: 0.10626, policy_loss: -34.50670, policy_entropy: -5.97812, alpha: 0.00692, time: 33.27585
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   855 ----
[CW] collect: return: 260.30193, steps: 1000.00000, total_steps: 861000.00000
[CW] train: qf1_loss: 0.10314, qf2_loss: 0.10224, policy_loss: -34.59108, policy_entropy: -6.02711, alpha: 0.00690, time: 33.32072
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   856 ----
[CW] collect: return: 252.17185, steps: 1000.00000, total_steps: 862000.00000
[CW] train: qf1_loss: 0.10565, qf2_loss: 0.10385, policy_loss: -34.60009, policy_entropy: -6.02209, alpha: 0.00691, time: 33.19551
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   857 ----
[CW] collect: return: 225.74153, steps: 1000.00000, total_steps: 863000.00000
[CW] train: qf1_loss: 0.10267, qf2_loss: 0.10284, policy_loss: -34.60609, policy_entropy: -6.07560, alpha: 0.00693, time: 33.31593
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   858 ----
[CW] collect: return: 240.91030, steps: 1000.00000, total_steps: 864000.00000
[CW] train: qf1_loss: 0.10667, qf2_loss: 0.10506, policy_loss: -34.73334, policy_entropy: -6.14876, alpha: 0.00700, time: 33.10290
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   859 ----
[CW] collect: return: 261.76854, steps: 1000.00000, total_steps: 865000.00000
[CW] train: qf1_loss: 0.11230, qf2_loss: 0.11168, policy_loss: -34.69723, policy_entropy: -6.12815, alpha: 0.00708, time: 33.17893
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   860 ----
[CW] collect: return: 239.10307, steps: 1000.00000, total_steps: 866000.00000
[CW] train: qf1_loss: 0.11281, qf2_loss: 0.11289, policy_loss: -34.77841, policy_entropy: -6.13291, alpha: 0.00716, time: 33.22554
[CW] eval: return: 249.30715, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   861 ----
[CW] collect: return: 267.84212, steps: 1000.00000, total_steps: 867000.00000
[CW] train: qf1_loss: 0.11203, qf2_loss: 0.11095, policy_loss: -34.76380, policy_entropy: -6.02767, alpha: 0.00722, time: 33.11538
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   862 ----
[CW] collect: return: 246.37305, steps: 1000.00000, total_steps: 868000.00000
[CW] train: qf1_loss: 0.10526, qf2_loss: 0.10362, policy_loss: -34.73110, policy_entropy: -5.88656, alpha: 0.00719, time: 33.14082
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   863 ----
[CW] collect: return: 243.83455, steps: 1000.00000, total_steps: 869000.00000
[CW] train: qf1_loss: 0.09915, qf2_loss: 0.09889, policy_loss: -34.76665, policy_entropy: -5.84220, alpha: 0.00712, time: 33.18208
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   864 ----
[CW] collect: return: 213.25589, steps: 1000.00000, total_steps: 870000.00000
[CW] train: qf1_loss: 0.10528, qf2_loss: 0.10437, policy_loss: -34.85485, policy_entropy: -5.97728, alpha: 0.00706, time: 33.14204
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   865 ----
[CW] collect: return: 225.12380, steps: 1000.00000, total_steps: 871000.00000
[CW] train: qf1_loss: 0.11511, qf2_loss: 0.11548, policy_loss: -34.81432, policy_entropy: -6.05921, alpha: 0.00706, time: 33.31656
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   866 ----
[CW] collect: return: 268.87510, steps: 1000.00000, total_steps: 872000.00000
[CW] train: qf1_loss: 0.11793, qf2_loss: 0.11800, policy_loss: -34.91045, policy_entropy: -5.90213, alpha: 0.00706, time: 33.31278
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   867 ----
[CW] collect: return: 243.52471, steps: 1000.00000, total_steps: 873000.00000
[CW] train: qf1_loss: 0.10526, qf2_loss: 0.10587, policy_loss: -34.92007, policy_entropy: -6.12538, alpha: 0.00704, time: 33.32249
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   868 ----
[CW] collect: return: 228.61915, steps: 1000.00000, total_steps: 874000.00000
[CW] train: qf1_loss: 0.11810, qf2_loss: 0.11918, policy_loss: -34.94501, policy_entropy: -6.09822, alpha: 0.00712, time: 33.23864
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   869 ----
[CW] collect: return: 258.66587, steps: 1000.00000, total_steps: 875000.00000
[CW] train: qf1_loss: 0.12114, qf2_loss: 0.12308, policy_loss: -34.97873, policy_entropy: -5.97569, alpha: 0.00714, time: 33.25538
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   870 ----
[CW] collect: return: 268.65806, steps: 1000.00000, total_steps: 876000.00000
[CW] train: qf1_loss: 0.10665, qf2_loss: 0.10491, policy_loss: -35.08412, policy_entropy: -6.03747, alpha: 0.00714, time: 33.16765
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   871 ----
[CW] collect: return: 258.35115, steps: 1000.00000, total_steps: 877000.00000
[CW] train: qf1_loss: 0.11582, qf2_loss: 0.11401, policy_loss: -35.03095, policy_entropy: -6.08910, alpha: 0.00717, time: 34.75304
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   872 ----
[CW] collect: return: 270.24282, steps: 1000.00000, total_steps: 878000.00000
[CW] train: qf1_loss: 0.13036, qf2_loss: 0.12685, policy_loss: -35.03206, policy_entropy: -5.91365, alpha: 0.00720, time: 33.63214
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   873 ----
[CW] collect: return: 272.45122, steps: 1000.00000, total_steps: 879000.00000
[CW] train: qf1_loss: 0.11416, qf2_loss: 0.11535, policy_loss: -35.10929, policy_entropy: -5.95831, alpha: 0.00713, time: 37.13171
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   874 ----
[CW] collect: return: 257.77380, steps: 1000.00000, total_steps: 880000.00000
[CW] train: qf1_loss: 0.10269, qf2_loss: 0.10211, policy_loss: -35.06394, policy_entropy: -6.00133, alpha: 0.00714, time: 33.84701
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   875 ----
[CW] collect: return: 251.86259, steps: 1000.00000, total_steps: 881000.00000
[CW] train: qf1_loss: 0.10832, qf2_loss: 0.10769, policy_loss: -35.08555, policy_entropy: -6.14380, alpha: 0.00717, time: 33.66781
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   876 ----
[CW] collect: return: 265.05150, steps: 1000.00000, total_steps: 882000.00000
[CW] train: qf1_loss: 0.11077, qf2_loss: 0.11069, policy_loss: -35.12595, policy_entropy: -6.07186, alpha: 0.00726, time: 33.62363
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   877 ----
[CW] collect: return: 248.68103, steps: 1000.00000, total_steps: 883000.00000
[CW] train: qf1_loss: 0.11614, qf2_loss: 0.11460, policy_loss: -35.16134, policy_entropy: -5.98765, alpha: 0.00724, time: 33.61542
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   878 ----
[CW] collect: return: 251.70994, steps: 1000.00000, total_steps: 884000.00000
[CW] train: qf1_loss: 0.11457, qf2_loss: 0.11315, policy_loss: -35.32289, policy_entropy: -6.10773, alpha: 0.00729, time: 33.70900
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   879 ----
[CW] collect: return: 282.22888, steps: 1000.00000, total_steps: 885000.00000
[CW] train: qf1_loss: 0.10865, qf2_loss: 0.10842, policy_loss: -35.19310, policy_entropy: -5.97435, alpha: 0.00730, time: 33.38074
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   880 ----
[CW] collect: return: 245.89473, steps: 1000.00000, total_steps: 886000.00000
[CW] train: qf1_loss: 0.10491, qf2_loss: 0.10414, policy_loss: -35.33660, policy_entropy: -6.00612, alpha: 0.00732, time: 33.45666
[CW] eval: return: 257.49456, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   881 ----
[CW] collect: return: 260.44588, steps: 1000.00000, total_steps: 887000.00000
[CW] train: qf1_loss: 0.10494, qf2_loss: 0.10438, policy_loss: -35.37097, policy_entropy: -5.94691, alpha: 0.00729, time: 33.40010
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   882 ----
[CW] collect: return: 261.90189, steps: 1000.00000, total_steps: 888000.00000
[CW] train: qf1_loss: 0.12076, qf2_loss: 0.11960, policy_loss: -35.42430, policy_entropy: -5.94314, alpha: 0.00725, time: 33.29222
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   883 ----
[CW] collect: return: 260.66647, steps: 1000.00000, total_steps: 889000.00000
[CW] train: qf1_loss: 0.11325, qf2_loss: 0.11225, policy_loss: -35.46436, policy_entropy: -6.09168, alpha: 0.00726, time: 33.24433
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   884 ----
[CW] collect: return: 280.12012, steps: 1000.00000, total_steps: 890000.00000
[CW] train: qf1_loss: 0.10974, qf2_loss: 0.10953, policy_loss: -35.35000, policy_entropy: -5.90801, alpha: 0.00726, time: 33.28523
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   885 ----
[CW] collect: return: 254.00548, steps: 1000.00000, total_steps: 891000.00000
[CW] train: qf1_loss: 0.10886, qf2_loss: 0.10776, policy_loss: -35.47722, policy_entropy: -5.98569, alpha: 0.00722, time: 33.33458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   886 ----
[CW] collect: return: 257.83762, steps: 1000.00000, total_steps: 892000.00000
[CW] train: qf1_loss: 0.11891, qf2_loss: 0.11870, policy_loss: -35.49938, policy_entropy: -6.05922, alpha: 0.00725, time: 33.33855
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   887 ----
[CW] collect: return: 275.81850, steps: 1000.00000, total_steps: 893000.00000
[CW] train: qf1_loss: 0.11781, qf2_loss: 0.11733, policy_loss: -35.46636, policy_entropy: -6.10396, alpha: 0.00731, time: 33.16118
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   888 ----
[CW] collect: return: 272.75597, steps: 1000.00000, total_steps: 894000.00000
[CW] train: qf1_loss: 0.10752, qf2_loss: 0.10628, policy_loss: -35.48332, policy_entropy: -6.01409, alpha: 0.00734, time: 33.00771
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   889 ----
[CW] collect: return: 276.08137, steps: 1000.00000, total_steps: 895000.00000
[CW] train: qf1_loss: 0.11775, qf2_loss: 0.11831, policy_loss: -35.50255, policy_entropy: -6.10559, alpha: 0.00735, time: 33.00644
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   890 ----
[CW] collect: return: 242.15354, steps: 1000.00000, total_steps: 896000.00000
[CW] train: qf1_loss: 0.12397, qf2_loss: 0.12219, policy_loss: -35.49634, policy_entropy: -6.06028, alpha: 0.00741, time: 32.97519
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   891 ----
[CW] collect: return: 256.00191, steps: 1000.00000, total_steps: 897000.00000
[CW] train: qf1_loss: 0.11637, qf2_loss: 0.11606, policy_loss: -35.57679, policy_entropy: -6.11815, alpha: 0.00748, time: 33.17461
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   892 ----
[CW] collect: return: 261.81023, steps: 1000.00000, total_steps: 898000.00000
[CW] train: qf1_loss: 0.11381, qf2_loss: 0.11419, policy_loss: -35.68434, policy_entropy: -6.26912, alpha: 0.00758, time: 33.10855
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   893 ----
[CW] collect: return: 240.55151, steps: 1000.00000, total_steps: 899000.00000
[CW] train: qf1_loss: 0.14219, qf2_loss: 0.14092, policy_loss: -35.74528, policy_entropy: -6.05490, alpha: 0.00770, time: 33.12291
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   894 ----
[CW] collect: return: 266.06056, steps: 1000.00000, total_steps: 900000.00000
[CW] train: qf1_loss: 0.12432, qf2_loss: 0.12329, policy_loss: -35.71199, policy_entropy: -6.20211, alpha: 0.00775, time: 33.01183
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   895 ----
[CW] collect: return: 241.23831, steps: 1000.00000, total_steps: 901000.00000
[CW] train: qf1_loss: 0.11513, qf2_loss: 0.11345, policy_loss: -35.73053, policy_entropy: -5.97356, alpha: 0.00785, time: 33.10316
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   896 ----
[CW] collect: return: 256.12750, steps: 1000.00000, total_steps: 902000.00000
[CW] train: qf1_loss: 0.12247, qf2_loss: 0.12175, policy_loss: -35.79644, policy_entropy: -6.03957, alpha: 0.00785, time: 33.11774
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   897 ----
[CW] collect: return: 267.37032, steps: 1000.00000, total_steps: 903000.00000
[CW] train: qf1_loss: 0.11850, qf2_loss: 0.11824, policy_loss: -35.78869, policy_entropy: -5.97347, alpha: 0.00786, time: 33.16785
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   898 ----
[CW] collect: return: 263.39710, steps: 1000.00000, total_steps: 904000.00000
[CW] train: qf1_loss: 0.11360, qf2_loss: 0.11362, policy_loss: -35.81770, policy_entropy: -5.93017, alpha: 0.00782, time: 33.10357
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   899 ----
[CW] collect: return: 266.74788, steps: 1000.00000, total_steps: 905000.00000
[CW] train: qf1_loss: 0.11652, qf2_loss: 0.11501, policy_loss: -35.80750, policy_entropy: -5.81807, alpha: 0.00774, time: 33.18776
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   900 ----
[CW] collect: return: 274.54804, steps: 1000.00000, total_steps: 906000.00000
[CW] train: qf1_loss: 0.11963, qf2_loss: 0.11929, policy_loss: -35.87360, policy_entropy: -5.77191, alpha: 0.00762, time: 32.99611
[CW] eval: return: 229.56836, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   901 ----
[CW] collect: return: 250.33132, steps: 1000.00000, total_steps: 907000.00000
[CW] train: qf1_loss: 0.12466, qf2_loss: 0.12162, policy_loss: -35.90151, policy_entropy: -5.98107, alpha: 0.00751, time: 33.29900
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   902 ----
[CW] collect: return: 243.94994, steps: 1000.00000, total_steps: 908000.00000
[CW] train: qf1_loss: 0.13987, qf2_loss: 0.13992, policy_loss: -35.86013, policy_entropy: -6.17118, alpha: 0.00757, time: 32.90623
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   903 ----
[CW] collect: return: 252.11139, steps: 1000.00000, total_steps: 909000.00000
[CW] train: qf1_loss: 0.12874, qf2_loss: 0.12788, policy_loss: -35.94027, policy_entropy: -6.23439, alpha: 0.00770, time: 32.97024
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   904 ----
[CW] collect: return: 254.03115, steps: 1000.00000, total_steps: 910000.00000
[CW] train: qf1_loss: 0.11985, qf2_loss: 0.12012, policy_loss: -35.96474, policy_entropy: -5.96096, alpha: 0.00778, time: 33.06017
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   905 ----
[CW] collect: return: 250.78753, steps: 1000.00000, total_steps: 911000.00000
[CW] train: qf1_loss: 0.11539, qf2_loss: 0.11428, policy_loss: -36.09466, policy_entropy: -6.07801, alpha: 0.00777, time: 33.03221
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   906 ----
[CW] collect: return: 250.44775, steps: 1000.00000, total_steps: 912000.00000
[CW] train: qf1_loss: 0.11746, qf2_loss: 0.11659, policy_loss: -36.05529, policy_entropy: -6.14917, alpha: 0.00783, time: 33.06007
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   907 ----
[CW] collect: return: 260.79746, steps: 1000.00000, total_steps: 913000.00000
[CW] train: qf1_loss: 0.11886, qf2_loss: 0.11920, policy_loss: -36.08394, policy_entropy: -6.01883, alpha: 0.00790, time: 33.06958
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   908 ----
[CW] collect: return: 273.97972, steps: 1000.00000, total_steps: 914000.00000
[CW] train: qf1_loss: 0.13185, qf2_loss: 0.12978, policy_loss: -36.04536, policy_entropy: -5.98931, alpha: 0.00790, time: 33.07614
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   909 ----
[CW] collect: return: 259.90788, steps: 1000.00000, total_steps: 915000.00000
[CW] train: qf1_loss: 0.13613, qf2_loss: 0.13490, policy_loss: -36.14180, policy_entropy: -6.05336, alpha: 0.00789, time: 33.13236
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   910 ----
[CW] collect: return: 268.07869, steps: 1000.00000, total_steps: 916000.00000
[CW] train: qf1_loss: 0.13881, qf2_loss: 0.13933, policy_loss: -36.19731, policy_entropy: -6.20524, alpha: 0.00797, time: 33.50165
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   911 ----
[CW] collect: return: 249.18410, steps: 1000.00000, total_steps: 917000.00000
[CW] train: qf1_loss: 0.12046, qf2_loss: 0.12069, policy_loss: -36.21108, policy_entropy: -6.09356, alpha: 0.00808, time: 33.28938
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   912 ----
[CW] collect: return: 249.64202, steps: 1000.00000, total_steps: 918000.00000
[CW] train: qf1_loss: 0.12685, qf2_loss: 0.12549, policy_loss: -36.22174, policy_entropy: -5.93288, alpha: 0.00809, time: 33.27586
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   913 ----
[CW] collect: return: 236.60478, steps: 1000.00000, total_steps: 919000.00000
[CW] train: qf1_loss: 0.13157, qf2_loss: 0.12792, policy_loss: -36.27483, policy_entropy: -5.96875, alpha: 0.00807, time: 33.35389
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   914 ----
[CW] collect: return: 283.12649, steps: 1000.00000, total_steps: 920000.00000
[CW] train: qf1_loss: 0.12088, qf2_loss: 0.12097, policy_loss: -36.31440, policy_entropy: -5.94777, alpha: 0.00804, time: 33.20656
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   915 ----
[CW] collect: return: 260.04371, steps: 1000.00000, total_steps: 921000.00000
[CW] train: qf1_loss: 0.12109, qf2_loss: 0.12080, policy_loss: -36.33237, policy_entropy: -6.14226, alpha: 0.00805, time: 33.21786
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   916 ----
[CW] collect: return: 256.59433, steps: 1000.00000, total_steps: 922000.00000
[CW] train: qf1_loss: 0.11682, qf2_loss: 0.11645, policy_loss: -36.36407, policy_entropy: -6.07756, alpha: 0.00814, time: 33.20220
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   917 ----
[CW] collect: return: 245.59010, steps: 1000.00000, total_steps: 923000.00000
[CW] train: qf1_loss: 0.13509, qf2_loss: 0.13492, policy_loss: -36.38482, policy_entropy: -6.18207, alpha: 0.00820, time: 33.24524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   918 ----
[CW] collect: return: 269.12795, steps: 1000.00000, total_steps: 924000.00000
[CW] train: qf1_loss: 0.12186, qf2_loss: 0.12074, policy_loss: -36.47121, policy_entropy: -6.08161, alpha: 0.00833, time: 33.18130
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   919 ----
[CW] collect: return: 241.07318, steps: 1000.00000, total_steps: 925000.00000
[CW] train: qf1_loss: 0.15956, qf2_loss: 0.16117, policy_loss: -36.52030, policy_entropy: -5.91593, alpha: 0.00833, time: 33.23247
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   920 ----
[CW] collect: return: 281.01149, steps: 1000.00000, total_steps: 926000.00000
[CW] train: qf1_loss: 0.13105, qf2_loss: 0.12958, policy_loss: -36.51271, policy_entropy: -5.92980, alpha: 0.00825, time: 33.22884
[CW] eval: return: 259.61650, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   921 ----
[CW] collect: return: 268.34395, steps: 1000.00000, total_steps: 927000.00000
[CW] train: qf1_loss: 0.14829, qf2_loss: 0.14643, policy_loss: -36.45490, policy_entropy: -6.06033, alpha: 0.00826, time: 33.13734
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   922 ----
[CW] collect: return: 255.17715, steps: 1000.00000, total_steps: 928000.00000
[CW] train: qf1_loss: 0.11964, qf2_loss: 0.11731, policy_loss: -36.54652, policy_entropy: -6.00955, alpha: 0.00829, time: 32.89877
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   923 ----
[CW] collect: return: 258.03794, steps: 1000.00000, total_steps: 929000.00000
[CW] train: qf1_loss: 0.12380, qf2_loss: 0.12229, policy_loss: -36.49134, policy_entropy: -5.99063, alpha: 0.00827, time: 32.91918
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   924 ----
[CW] collect: return: 261.37539, steps: 1000.00000, total_steps: 930000.00000
[CW] train: qf1_loss: 0.12332, qf2_loss: 0.12332, policy_loss: -36.58912, policy_entropy: -6.07667, alpha: 0.00830, time: 33.05275
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   925 ----
[CW] collect: return: 247.61299, steps: 1000.00000, total_steps: 931000.00000
[CW] train: qf1_loss: 0.12266, qf2_loss: 0.12248, policy_loss: -36.67033, policy_entropy: -5.96368, alpha: 0.00832, time: 33.09140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   926 ----
[CW] collect: return: 264.44252, steps: 1000.00000, total_steps: 932000.00000
[CW] train: qf1_loss: 0.12210, qf2_loss: 0.12166, policy_loss: -36.63236, policy_entropy: -5.89768, alpha: 0.00828, time: 33.40589
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   927 ----
[CW] collect: return: 245.55988, steps: 1000.00000, total_steps: 933000.00000
[CW] train: qf1_loss: 0.12640, qf2_loss: 0.12483, policy_loss: -36.63324, policy_entropy: -6.05044, alpha: 0.00827, time: 33.19449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   928 ----
[CW] collect: return: 267.81119, steps: 1000.00000, total_steps: 934000.00000
[CW] train: qf1_loss: 0.12126, qf2_loss: 0.12089, policy_loss: -36.79535, policy_entropy: -5.89806, alpha: 0.00824, time: 33.32865
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   929 ----
[CW] collect: return: 265.19596, steps: 1000.00000, total_steps: 935000.00000
[CW] train: qf1_loss: 0.14573, qf2_loss: 0.14479, policy_loss: -36.80789, policy_entropy: -5.85440, alpha: 0.00817, time: 33.05605
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   930 ----
[CW] collect: return: 265.17587, steps: 1000.00000, total_steps: 936000.00000
[CW] train: qf1_loss: 0.14128, qf2_loss: 0.14139, policy_loss: -36.78390, policy_entropy: -6.07101, alpha: 0.00811, time: 33.29071
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   931 ----
[CW] collect: return: 283.98722, steps: 1000.00000, total_steps: 937000.00000
[CW] train: qf1_loss: 0.13112, qf2_loss: 0.13010, policy_loss: -36.89698, policy_entropy: -6.17394, alpha: 0.00819, time: 33.26270
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   932 ----
[CW] collect: return: 258.79981, steps: 1000.00000, total_steps: 938000.00000
[CW] train: qf1_loss: 0.12281, qf2_loss: 0.12101, policy_loss: -36.92033, policy_entropy: -5.98684, alpha: 0.00829, time: 33.12524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   933 ----
[CW] collect: return: 259.62480, steps: 1000.00000, total_steps: 939000.00000
[CW] train: qf1_loss: 0.12277, qf2_loss: 0.12168, policy_loss: -36.90545, policy_entropy: -6.09943, alpha: 0.00830, time: 33.29834
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   934 ----
[CW] collect: return: 282.98204, steps: 1000.00000, total_steps: 940000.00000
[CW] train: qf1_loss: 0.12102, qf2_loss: 0.12009, policy_loss: -36.89287, policy_entropy: -5.96479, alpha: 0.00833, time: 33.28046
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   935 ----
[CW] collect: return: 252.34008, steps: 1000.00000, total_steps: 941000.00000
[CW] train: qf1_loss: 0.14024, qf2_loss: 0.13964, policy_loss: -37.07565, policy_entropy: -6.14372, alpha: 0.00833, time: 33.19023
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   936 ----
[CW] collect: return: 272.04985, steps: 1000.00000, total_steps: 942000.00000
[CW] train: qf1_loss: 0.13143, qf2_loss: 0.12997, policy_loss: -37.02765, policy_entropy: -5.92312, alpha: 0.00837, time: 33.06250
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   937 ----
[CW] collect: return: 266.38087, steps: 1000.00000, total_steps: 943000.00000
[CW] train: qf1_loss: 0.14590, qf2_loss: 0.14463, policy_loss: -37.00417, policy_entropy: -6.06258, alpha: 0.00837, time: 32.88426
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   938 ----
[CW] collect: return: 264.39535, steps: 1000.00000, total_steps: 944000.00000
[CW] train: qf1_loss: 0.12573, qf2_loss: 0.12490, policy_loss: -37.05925, policy_entropy: -6.11716, alpha: 0.00844, time: 33.28305
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   939 ----
[CW] collect: return: 257.25856, steps: 1000.00000, total_steps: 945000.00000
[CW] train: qf1_loss: 0.14252, qf2_loss: 0.14107, policy_loss: -37.06540, policy_entropy: -6.00536, alpha: 0.00848, time: 33.11234
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   940 ----
[CW] collect: return: 271.80760, steps: 1000.00000, total_steps: 946000.00000
[CW] train: qf1_loss: 0.13305, qf2_loss: 0.13339, policy_loss: -37.03657, policy_entropy: -6.02700, alpha: 0.00850, time: 33.38677
[CW] eval: return: 267.13351, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   941 ----
[CW] collect: return: 269.69214, steps: 1000.00000, total_steps: 947000.00000
[CW] train: qf1_loss: 0.12450, qf2_loss: 0.12572, policy_loss: -37.16924, policy_entropy: -6.08464, alpha: 0.00852, time: 33.31904
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   942 ----
[CW] collect: return: 245.16708, steps: 1000.00000, total_steps: 948000.00000
[CW] train: qf1_loss: 0.13748, qf2_loss: 0.13447, policy_loss: -37.26905, policy_entropy: -6.04327, alpha: 0.00857, time: 33.27249
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   943 ----
[CW] collect: return: 266.36012, steps: 1000.00000, total_steps: 949000.00000
[CW] train: qf1_loss: 0.13958, qf2_loss: 0.14035, policy_loss: -37.22640, policy_entropy: -5.97291, alpha: 0.00860, time: 33.18875
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   944 ----
[CW] collect: return: 280.07572, steps: 1000.00000, total_steps: 950000.00000
[CW] train: qf1_loss: 0.14183, qf2_loss: 0.14106, policy_loss: -37.30244, policy_entropy: -5.96135, alpha: 0.00855, time: 33.26749
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   945 ----
[CW] collect: return: 274.34279, steps: 1000.00000, total_steps: 951000.00000
[CW] train: qf1_loss: 0.13275, qf2_loss: 0.13167, policy_loss: -37.23654, policy_entropy: -5.97466, alpha: 0.00855, time: 33.35281
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   946 ----
[CW] collect: return: 253.07765, steps: 1000.00000, total_steps: 952000.00000
[CW] train: qf1_loss: 0.12668, qf2_loss: 0.12621, policy_loss: -37.30692, policy_entropy: -6.03131, alpha: 0.00854, time: 33.28368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   947 ----
[CW] collect: return: 267.61749, steps: 1000.00000, total_steps: 953000.00000
[CW] train: qf1_loss: 0.13831, qf2_loss: 0.13678, policy_loss: -37.35186, policy_entropy: -5.98450, alpha: 0.00854, time: 33.20629
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   948 ----
[CW] collect: return: 245.02848, steps: 1000.00000, total_steps: 954000.00000
[CW] train: qf1_loss: 0.15439, qf2_loss: 0.15568, policy_loss: -37.30824, policy_entropy: -6.01731, alpha: 0.00854, time: 33.04769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   949 ----
[CW] collect: return: 272.29148, steps: 1000.00000, total_steps: 955000.00000
[CW] train: qf1_loss: 0.13199, qf2_loss: 0.13316, policy_loss: -37.44270, policy_entropy: -5.99854, alpha: 0.00853, time: 32.97913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   950 ----
[CW] collect: return: 271.50739, steps: 1000.00000, total_steps: 956000.00000
[CW] train: qf1_loss: 0.13623, qf2_loss: 0.13276, policy_loss: -37.41835, policy_entropy: -5.92302, alpha: 0.00853, time: 33.19044
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   951 ----
[CW] collect: return: 284.73026, steps: 1000.00000, total_steps: 957000.00000
[CW] train: qf1_loss: 0.13456, qf2_loss: 0.13284, policy_loss: -37.40772, policy_entropy: -6.13005, alpha: 0.00851, time: 33.05916
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   952 ----
[CW] collect: return: 262.90578, steps: 1000.00000, total_steps: 958000.00000
[CW] train: qf1_loss: 0.15301, qf2_loss: 0.15616, policy_loss: -37.51927, policy_entropy: -6.09623, alpha: 0.00860, time: 33.18751
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   953 ----
[CW] collect: return: 275.96717, steps: 1000.00000, total_steps: 959000.00000
[CW] train: qf1_loss: 0.13855, qf2_loss: 0.13637, policy_loss: -37.58861, policy_entropy: -6.10518, alpha: 0.00867, time: 33.03695
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   954 ----
[CW] collect: return: 277.14920, steps: 1000.00000, total_steps: 960000.00000
[CW] train: qf1_loss: 0.13637, qf2_loss: 0.13577, policy_loss: -37.57397, policy_entropy: -6.04640, alpha: 0.00871, time: 33.23316
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   955 ----
[CW] collect: return: 273.63201, steps: 1000.00000, total_steps: 961000.00000
[CW] train: qf1_loss: 0.14047, qf2_loss: 0.14025, policy_loss: -37.55919, policy_entropy: -6.02082, alpha: 0.00875, time: 34.05672
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   956 ----
[CW] collect: return: 292.65054, steps: 1000.00000, total_steps: 962000.00000
[CW] train: qf1_loss: 0.16565, qf2_loss: 0.16221, policy_loss: -37.66589, policy_entropy: -5.96777, alpha: 0.00875, time: 33.07900
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   957 ----
[CW] collect: return: 286.10998, steps: 1000.00000, total_steps: 963000.00000
[CW] train: qf1_loss: 0.13454, qf2_loss: 0.13435, policy_loss: -37.77235, policy_entropy: -6.04724, alpha: 0.00876, time: 33.06792
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   958 ----
[CW] collect: return: 266.06644, steps: 1000.00000, total_steps: 964000.00000
[CW] train: qf1_loss: 0.13121, qf2_loss: 0.13018, policy_loss: -37.76150, policy_entropy: -6.02146, alpha: 0.00875, time: 33.13745
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   959 ----
[CW] collect: return: 263.04742, steps: 1000.00000, total_steps: 965000.00000
[CW] train: qf1_loss: 0.12792, qf2_loss: 0.12830, policy_loss: -37.74396, policy_entropy: -5.90520, alpha: 0.00876, time: 33.44134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   960 ----
[CW] collect: return: 279.11225, steps: 1000.00000, total_steps: 966000.00000
[CW] train: qf1_loss: 0.14089, qf2_loss: 0.13877, policy_loss: -37.81316, policy_entropy: -6.01888, alpha: 0.00872, time: 33.38817
[CW] eval: return: 269.45112, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   961 ----
[CW] collect: return: 284.97295, steps: 1000.00000, total_steps: 967000.00000
[CW] train: qf1_loss: 0.13132, qf2_loss: 0.13150, policy_loss: -37.81880, policy_entropy: -5.93692, alpha: 0.00872, time: 33.26034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   962 ----
[CW] collect: return: 273.70726, steps: 1000.00000, total_steps: 968000.00000
[CW] train: qf1_loss: 0.15348, qf2_loss: 0.15268, policy_loss: -37.86432, policy_entropy: -6.02135, alpha: 0.00872, time: 33.14534
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   963 ----
[CW] collect: return: 285.55501, steps: 1000.00000, total_steps: 969000.00000
[CW] train: qf1_loss: 0.13358, qf2_loss: 0.13251, policy_loss: -37.95863, policy_entropy: -6.02188, alpha: 0.00872, time: 33.24561
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   964 ----
[CW] collect: return: 278.93086, steps: 1000.00000, total_steps: 970000.00000
[CW] train: qf1_loss: 0.15252, qf2_loss: 0.15219, policy_loss: -37.95709, policy_entropy: -6.05720, alpha: 0.00875, time: 33.32499
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   965 ----
[CW] collect: return: 275.58822, steps: 1000.00000, total_steps: 971000.00000
[CW] train: qf1_loss: 0.13729, qf2_loss: 0.13535, policy_loss: -37.96602, policy_entropy: -5.93156, alpha: 0.00874, time: 33.32392
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   966 ----
[CW] collect: return: 275.82211, steps: 1000.00000, total_steps: 972000.00000
[CW] train: qf1_loss: 0.13585, qf2_loss: 0.13633, policy_loss: -37.99910, policy_entropy: -5.99328, alpha: 0.00872, time: 33.31300
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   967 ----
[CW] collect: return: 250.70448, steps: 1000.00000, total_steps: 973000.00000
[CW] train: qf1_loss: 0.16173, qf2_loss: 0.15772, policy_loss: -37.94108, policy_entropy: -5.93805, alpha: 0.00871, time: 33.54714
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   968 ----
[CW] collect: return: 244.75198, steps: 1000.00000, total_steps: 974000.00000
[CW] train: qf1_loss: 0.14197, qf2_loss: 0.14076, policy_loss: -38.09253, policy_entropy: -5.98896, alpha: 0.00867, time: 33.53527
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   969 ----
[CW] collect: return: 260.44882, steps: 1000.00000, total_steps: 975000.00000
[CW] train: qf1_loss: 0.15429, qf2_loss: 0.15508, policy_loss: -38.10357, policy_entropy: -5.99118, alpha: 0.00867, time: 33.45197
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   970 ----
[CW] collect: return: 287.70459, steps: 1000.00000, total_steps: 976000.00000
[CW] train: qf1_loss: 0.13173, qf2_loss: 0.13083, policy_loss: -38.05584, policy_entropy: -5.84561, alpha: 0.00863, time: 33.22817
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   971 ----
[CW] collect: return: 263.07004, steps: 1000.00000, total_steps: 977000.00000
[CW] train: qf1_loss: 0.13935, qf2_loss: 0.13786, policy_loss: -38.17998, policy_entropy: -5.95359, alpha: 0.00854, time: 33.11649
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   972 ----
[CW] collect: return: 269.01447, steps: 1000.00000, total_steps: 978000.00000
[CW] train: qf1_loss: 0.13379, qf2_loss: 0.13218, policy_loss: -38.14089, policy_entropy: -6.05222, alpha: 0.00854, time: 33.11666
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   973 ----
[CW] collect: return: 260.18450, steps: 1000.00000, total_steps: 979000.00000
[CW] train: qf1_loss: 0.14854, qf2_loss: 0.14720, policy_loss: -38.23785, policy_entropy: -6.17727, alpha: 0.00861, time: 33.22768
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   974 ----
[CW] collect: return: 247.55160, steps: 1000.00000, total_steps: 980000.00000
[CW] train: qf1_loss: 0.14225, qf2_loss: 0.14182, policy_loss: -38.17659, policy_entropy: -6.03208, alpha: 0.00869, time: 33.17777
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   975 ----
[CW] collect: return: 262.69291, steps: 1000.00000, total_steps: 981000.00000
[CW] train: qf1_loss: 0.14245, qf2_loss: 0.14065, policy_loss: -38.31024, policy_entropy: -6.07711, alpha: 0.00871, time: 33.15166
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   976 ----
[CW] collect: return: 266.45165, steps: 1000.00000, total_steps: 982000.00000
[CW] train: qf1_loss: 0.14469, qf2_loss: 0.14510, policy_loss: -38.25421, policy_entropy: -6.11669, alpha: 0.00878, time: 33.24144
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   977 ----
[CW] collect: return: 276.46359, steps: 1000.00000, total_steps: 983000.00000
[CW] train: qf1_loss: 0.14652, qf2_loss: 0.14665, policy_loss: -38.40237, policy_entropy: -6.11717, alpha: 0.00886, time: 33.55854
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   978 ----
[CW] collect: return: 260.54067, steps: 1000.00000, total_steps: 984000.00000
[CW] train: qf1_loss: 0.14400, qf2_loss: 0.14329, policy_loss: -38.42317, policy_entropy: -6.02964, alpha: 0.00892, time: 33.18705
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   979 ----
[CW] collect: return: 259.68682, steps: 1000.00000, total_steps: 985000.00000
[CW] train: qf1_loss: 0.14252, qf2_loss: 0.14221, policy_loss: -38.33586, policy_entropy: -5.78203, alpha: 0.00887, time: 33.14644
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   980 ----
[CW] collect: return: 265.42888, steps: 1000.00000, total_steps: 986000.00000
[CW] train: qf1_loss: 0.13576, qf2_loss: 0.13496, policy_loss: -38.43999, policy_entropy: -5.94764, alpha: 0.00875, time: 33.07266
[CW] eval: return: 275.31395, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   981 ----
[CW] collect: return: 271.92917, steps: 1000.00000, total_steps: 987000.00000
[CW] train: qf1_loss: 0.12572, qf2_loss: 0.12504, policy_loss: -38.52656, policy_entropy: -5.92788, alpha: 0.00873, time: 33.22889
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   982 ----
[CW] collect: return: 289.14592, steps: 1000.00000, total_steps: 988000.00000
[CW] train: qf1_loss: 0.15895, qf2_loss: 0.15723, policy_loss: -38.42959, policy_entropy: -5.95794, alpha: 0.00867, time: 33.13735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   983 ----
[CW] collect: return: 278.10616, steps: 1000.00000, total_steps: 989000.00000
[CW] train: qf1_loss: 0.13644, qf2_loss: 0.13556, policy_loss: -38.48870, policy_entropy: -5.98698, alpha: 0.00866, time: 33.22304
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   984 ----
[CW] collect: return: 274.64408, steps: 1000.00000, total_steps: 990000.00000
[CW] train: qf1_loss: 0.13884, qf2_loss: 0.13839, policy_loss: -38.52253, policy_entropy: -5.95743, alpha: 0.00863, time: 33.20813
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   985 ----
[CW] collect: return: 278.02139, steps: 1000.00000, total_steps: 991000.00000
[CW] train: qf1_loss: 0.13869, qf2_loss: 0.13664, policy_loss: -38.57792, policy_entropy: -6.04675, alpha: 0.00862, time: 33.23844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   986 ----
[CW] collect: return: 276.33905, steps: 1000.00000, total_steps: 992000.00000
[CW] train: qf1_loss: 0.14195, qf2_loss: 0.14170, policy_loss: -38.65847, policy_entropy: -5.95805, alpha: 0.00863, time: 33.38419
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   987 ----
[CW] collect: return: 287.89145, steps: 1000.00000, total_steps: 993000.00000
[CW] train: qf1_loss: 0.14582, qf2_loss: 0.14418, policy_loss: -38.63474, policy_entropy: -6.06385, alpha: 0.00864, time: 33.26855
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   988 ----
[CW] collect: return: 250.30187, steps: 1000.00000, total_steps: 994000.00000
[CW] train: qf1_loss: 0.13704, qf2_loss: 0.13450, policy_loss: -38.65880, policy_entropy: -6.07333, alpha: 0.00871, time: 32.90642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   989 ----
[CW] collect: return: 255.69028, steps: 1000.00000, total_steps: 995000.00000
[CW] train: qf1_loss: 0.13157, qf2_loss: 0.13020, policy_loss: -38.72337, policy_entropy: -6.04870, alpha: 0.00875, time: 32.96987
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   990 ----
[CW] collect: return: 265.89440, steps: 1000.00000, total_steps: 996000.00000
[CW] train: qf1_loss: 0.13355, qf2_loss: 0.13402, policy_loss: -38.71734, policy_entropy: -6.01276, alpha: 0.00877, time: 33.03601
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   991 ----
[CW] collect: return: 256.89642, steps: 1000.00000, total_steps: 997000.00000
[CW] train: qf1_loss: 0.13426, qf2_loss: 0.13454, policy_loss: -38.84243, policy_entropy: -6.04808, alpha: 0.00878, time: 33.27251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   992 ----
[CW] collect: return: 276.29296, steps: 1000.00000, total_steps: 998000.00000
[CW] train: qf1_loss: 0.14172, qf2_loss: 0.14016, policy_loss: -38.86588, policy_entropy: -6.00257, alpha: 0.00880, time: 33.20294
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   993 ----
[CW] collect: return: 273.88801, steps: 1000.00000, total_steps: 999000.00000
[CW] train: qf1_loss: 0.14207, qf2_loss: 0.14076, policy_loss: -38.90495, policy_entropy: -6.03459, alpha: 0.00884, time: 33.01358
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   994 ----
[CW] collect: return: 259.03274, steps: 1000.00000, total_steps: 1000000.00000
[CW] train: qf1_loss: 0.13218, qf2_loss: 0.13266, policy_loss: -38.74864, policy_entropy: -5.94760, alpha: 0.00880, time: 32.98113
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   995 ----
[CW] collect: return: 278.72931, steps: 1000.00000, total_steps: 1001000.00000
[CW] train: qf1_loss: 0.13851, qf2_loss: 0.13668, policy_loss: -38.80429, policy_entropy: -6.06974, alpha: 0.00879, time: 33.12816
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   996 ----
[CW] collect: return: 285.40963, steps: 1000.00000, total_steps: 1002000.00000
[CW] train: qf1_loss: 0.14685, qf2_loss: 0.14574, policy_loss: -38.88730, policy_entropy: -6.05520, alpha: 0.00886, time: 33.03808
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   997 ----
[CW] collect: return: 260.34545, steps: 1000.00000, total_steps: 1003000.00000
[CW] train: qf1_loss: 0.14145, qf2_loss: 0.13978, policy_loss: -38.93684, policy_entropy: -6.01499, alpha: 0.00890, time: 33.29553
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   998 ----
[CW] collect: return: 279.69621, steps: 1000.00000, total_steps: 1004000.00000
[CW] train: qf1_loss: 0.14751, qf2_loss: 0.14671, policy_loss: -38.95646, policy_entropy: -5.98305, alpha: 0.00887, time: 33.30750
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   999 ----
[CW] collect: return: 266.04894, steps: 1000.00000, total_steps: 1005000.00000
[CW] train: qf1_loss: 0.15407, qf2_loss: 0.15202, policy_loss: -38.97905, policy_entropy: -5.95888, alpha: 0.00888, time: 33.16599
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:  1000 ----
[CW] collect: return: 280.86778, steps: 1000.00000, total_steps: 1006000.00000
[CW] train: qf1_loss: 0.13756, qf2_loss: 0.13838, policy_loss: -38.99692, policy_entropy: -5.99164, alpha: 0.00884, time: 33.34709
[CW] eval: return: 278.99587, steps: 1000.00000
[CW] ---------------------------

============================= JOB FEEDBACK =============================

NodeName=uc2n905
Job ID: 21913911
Array Job ID: 21913911_0
Cluster: uc2
User/Group: uprnr/stud
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 4
CPU Utilized: 10:25:35
CPU Efficiency: 25.05% of 1-17:37:04 core-walltime
Job Wall-clock time: 10:24:16
Memory Utilized: 9.49 GB
Memory Efficiency: 16.19% of 58.59 GB
