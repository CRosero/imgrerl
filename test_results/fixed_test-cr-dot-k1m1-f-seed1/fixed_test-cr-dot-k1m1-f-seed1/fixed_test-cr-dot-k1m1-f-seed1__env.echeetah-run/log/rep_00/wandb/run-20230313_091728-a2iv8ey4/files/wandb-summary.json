{"collect/return": 413.6221267655492, "collect/steps": 1000.0, "collect/total_steps": 679000.0, "train/qf1_loss": 7.54521653175354, "train/qf2_loss": 7.751370794773102, "train/policy_loss": -92.97518630981445, "train/policy_entropy": -6.5471997785568234, "train/alpha": 0.020193715989589692, "train/time": 51.28512144088745, "eval/return": 402.60184838835266, "eval/steps": 1000.0, "_timestamp": 1678731320.917225, "_runtime": 35872.31698393822, "_step": 673}