Hostname: uc2n509.localdomain
Found slurm config: SLURM
Seeting default slurm config
/home/kit/stud/uprnr/imgrerl/test_results/fixed_test-cr-dot-k1m1-f-seed1/fixed_test-cr-dot-k1m1-f-seed1/fixed_test-cr-dot-k1m1-f-seed1__env.echeetah-run/log/rep_00
[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
False
params: 
 {'env': {'env': 'cheetah-run'}} 

additionalVars: 
 {'seed': 1, 'agent': {'image_augmentation_K': 1, 'image_augmentation_M': 1, 'image_augmentation_type': <AugmentationType.DIFFERENT_OVER_TIME: 3>, 'image_augmentation_actor_critic_same_aug': False}}
conf_dict: 
 --------Config-------- 
seed: 1
cuda_id: 0
Subconfig: env
	env: cheetah-run
	obs_type: image
Subconfig: agent
	algo_name: sac
	encoder: lstm
	action_embedding_size: 32
	observ_embedding_size: 0
	reward_embedding_size: 32
	rnn_hidden_size: 512
	dqn_layers: [512, 512, 512]
	policy_layers: [512, 512, 512]
	lr: 0.0003
	gamma: 0.99
	tau: 0.005
	entropy_alpha: 1.0
	image_augmentation_type: AugmentationType.DIFFERENT_OVER_TIME
	image_augmentation_K: 1
	image_augmentation_M: 1
	image_augmentation_actor_critic_same_aug: False
Subconfig: rl
	sampled_seq_len: 64
	buffer_size: 1000000.0
	batch_size: 32
	rollouts_per_iter: 1
	num_updates_per_iter: 100
	num_init_rollouts: 5
Subconfig: eval
	interval: 20
	num_rollouts: 20
---------------------- 
 
Init feature extractor:  6 ;  32 ;  <function relu at 0x14ee111027a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x14ee111027a0>
Init feature extractor:  6 ;  164 ;  <function relu at 0x14ee111027a0>
Critic: 
Critic_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (current_shortcut_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=164, bias=True)
  )
  (qf1): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
  (qf2): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
)
Init feature extractor:  6 ;  32 ;  <function relu at 0x14ee111027a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x14ee111027a0>
Actor: 
Actor_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (policy): TanhGaussianPolicy(
    (fc0): Linear(in_features=612, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=6, bias=True)
    (last_fc_log_std): Linear(in_features=512, out_features=6, bias=True)
  )
)
buffer RAM usage: 11.48 GB
Collecting train stats
[CW] ---- Iteration:     0 ----
[CW] collect: return: 17.77195, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.66348, qf2_loss: 1.65843, policy_loss: -7.85940, policy_entropy: 4.09694, alpha: 0.98504, time: 56.79975
[CW] eval: return: 14.85823, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     1 ----
[CW] collect: return: 13.50068, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.09013, qf2_loss: 0.09039, policy_loss: -8.53084, policy_entropy: 4.10074, alpha: 0.95626, time: 50.29544
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     2 ----
[CW] collect: return: 9.92688, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.08338, qf2_loss: 0.08352, policy_loss: -9.22191, policy_entropy: 4.09961, alpha: 0.92871, time: 50.10545
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     3 ----
[CW] collect: return: 7.80108, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.07674, qf2_loss: 0.07677, policy_loss: -10.14317, policy_entropy: 4.10167, alpha: 0.90231, time: 50.78075
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     4 ----
[CW] collect: return: 13.48492, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06949, qf2_loss: 0.06947, policy_loss: -11.18759, policy_entropy: 4.10131, alpha: 0.87698, time: 50.47099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     5 ----
[CW] collect: return: 5.28313, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06363, qf2_loss: 0.06372, policy_loss: -12.31348, policy_entropy: 4.10158, alpha: 0.85267, time: 50.41051
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     6 ----
[CW] collect: return: 11.00853, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.07165, qf2_loss: 0.07215, policy_loss: -13.48300, policy_entropy: 4.10111, alpha: 0.82930, time: 50.51596
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     7 ----
[CW] collect: return: 9.99047, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.08048, qf2_loss: 0.08031, policy_loss: -14.68558, policy_entropy: 4.10101, alpha: 0.80683, time: 50.44126
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     8 ----
[CW] collect: return: 22.08755, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.08647, qf2_loss: 0.08565, policy_loss: -15.88833, policy_entropy: 4.10121, alpha: 0.78519, time: 50.50100
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     9 ----
[CW] collect: return: 13.31531, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.08790, qf2_loss: 0.08682, policy_loss: -17.09679, policy_entropy: 4.10007, alpha: 0.76435, time: 50.61742
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    10 ----
[CW] collect: return: 4.32741, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.09307, qf2_loss: 0.09177, policy_loss: -18.28420, policy_entropy: 4.10081, alpha: 0.74426, time: 50.51844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    11 ----
[CW] collect: return: 18.93062, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.10520, qf2_loss: 0.10345, policy_loss: -19.45528, policy_entropy: 4.10070, alpha: 0.72488, time: 50.55034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    12 ----
[CW] collect: return: 14.92404, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.10553, qf2_loss: 0.10384, policy_loss: -20.60593, policy_entropy: 4.10074, alpha: 0.70617, time: 50.53934
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    13 ----
[CW] collect: return: 25.83633, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.08469, qf2_loss: 0.08365, policy_loss: -21.73258, policy_entropy: 4.09935, alpha: 0.68809, time: 50.46801
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    14 ----
[CW] collect: return: 31.05044, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.11407, qf2_loss: 0.11217, policy_loss: -22.84161, policy_entropy: 4.09963, alpha: 0.67062, time: 50.40356
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    15 ----
[CW] collect: return: 9.90469, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.11581, qf2_loss: 0.11396, policy_loss: -23.90457, policy_entropy: 4.09998, alpha: 0.65372, time: 50.53088
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    16 ----
[CW] collect: return: 12.62945, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.10028, qf2_loss: 0.09903, policy_loss: -24.94319, policy_entropy: 4.09954, alpha: 0.63737, time: 50.42007
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    17 ----
[CW] collect: return: 19.36041, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.13779, qf2_loss: 0.13554, policy_loss: -25.96211, policy_entropy: 4.09762, alpha: 0.62154, time: 50.51886
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    18 ----
[CW] collect: return: 9.17979, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.09879, qf2_loss: 0.09785, policy_loss: -26.94200, policy_entropy: 4.09806, alpha: 0.60620, time: 50.55415
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    19 ----
[CW] collect: return: 19.07781, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.12939, qf2_loss: 0.12766, policy_loss: -27.90339, policy_entropy: 4.09692, alpha: 0.59133, time: 50.72090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    20 ----
[CW] collect: return: 12.69386, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.16443, qf2_loss: 0.16190, policy_loss: -28.82904, policy_entropy: 4.09703, alpha: 0.57692, time: 50.72185
[CW] eval: return: 13.50651, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    21 ----
[CW] collect: return: 14.27617, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.09445, qf2_loss: 0.09387, policy_loss: -29.72765, policy_entropy: 4.09568, alpha: 0.56293, time: 50.75018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    22 ----
[CW] collect: return: 7.36705, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.17360, qf2_loss: 0.17108, policy_loss: -30.60318, policy_entropy: 4.09465, alpha: 0.54936, time: 50.67488
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    23 ----
[CW] collect: return: 15.99370, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.10658, qf2_loss: 0.10584, policy_loss: -31.45388, policy_entropy: 4.09494, alpha: 0.53618, time: 50.54529
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    24 ----
[CW] collect: return: 5.29294, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.15740, qf2_loss: 0.15547, policy_loss: -32.27017, policy_entropy: 4.09459, alpha: 0.52338, time: 50.61400
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    25 ----
[CW] collect: return: 11.43579, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.14558, qf2_loss: 0.14401, policy_loss: -33.07554, policy_entropy: 4.09382, alpha: 0.51094, time: 50.72325
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    26 ----
[CW] collect: return: 17.04853, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.14223, qf2_loss: 0.14069, policy_loss: -33.84882, policy_entropy: 4.09297, alpha: 0.49885, time: 50.81080
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    27 ----
[CW] collect: return: 11.62482, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.13916, qf2_loss: 0.13777, policy_loss: -34.60214, policy_entropy: 4.09174, alpha: 0.48709, time: 50.77455
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    28 ----
[CW] collect: return: 16.84954, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.16520, qf2_loss: 0.16320, policy_loss: -35.33397, policy_entropy: 4.09128, alpha: 0.47566, time: 50.62484
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    29 ----
[CW] collect: return: 8.19686, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.23786, qf2_loss: 0.23510, policy_loss: -36.03589, policy_entropy: 4.08864, alpha: 0.46454, time: 50.55610
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    30 ----
[CW] collect: return: 18.46113, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.09362, qf2_loss: 0.09332, policy_loss: -36.72928, policy_entropy: 4.08860, alpha: 0.45371, time: 50.68919
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    31 ----
[CW] collect: return: 7.65596, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.16319, qf2_loss: 0.16188, policy_loss: -37.40128, policy_entropy: 4.08657, alpha: 0.44317, time: 50.63129
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    32 ----
[CW] collect: return: 8.74002, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.13771, qf2_loss: 0.13654, policy_loss: -38.01874, policy_entropy: 4.08828, alpha: 0.43291, time: 50.87501
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    33 ----
[CW] collect: return: 14.51251, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.16853, qf2_loss: 0.16679, policy_loss: -38.65455, policy_entropy: 4.08621, alpha: 0.42291, time: 50.78365
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    34 ----
[CW] collect: return: 11.85619, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.17432, qf2_loss: 0.17270, policy_loss: -39.26581, policy_entropy: 4.08351, alpha: 0.41318, time: 50.85873
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    35 ----
[CW] collect: return: 23.82610, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.15844, qf2_loss: 0.15725, policy_loss: -39.83104, policy_entropy: 4.08444, alpha: 0.40369, time: 50.87063
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    36 ----
[CW] collect: return: 22.54285, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.16520, qf2_loss: 0.16392, policy_loss: -40.40027, policy_entropy: 4.08439, alpha: 0.39444, time: 50.93301
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    37 ----
[CW] collect: return: 17.06427, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.16125, qf2_loss: 0.16026, policy_loss: -40.94842, policy_entropy: 4.08436, alpha: 0.38542, time: 50.84797
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    38 ----
[CW] collect: return: 10.03417, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.19444, qf2_loss: 0.19316, policy_loss: -41.45931, policy_entropy: 4.08394, alpha: 0.37663, time: 50.87040
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    39 ----
[CW] collect: return: 7.59879, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.19145, qf2_loss: 0.19036, policy_loss: -41.96101, policy_entropy: 4.08509, alpha: 0.36805, time: 50.73621
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    40 ----
[CW] collect: return: 16.27798, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.12970, qf2_loss: 0.13016, policy_loss: -42.45034, policy_entropy: 4.08414, alpha: 0.35969, time: 50.88358
[CW] eval: return: 14.23874, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    41 ----
[CW] collect: return: 9.26228, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.18264, qf2_loss: 0.18250, policy_loss: -42.90745, policy_entropy: 4.08537, alpha: 0.35153, time: 51.36805
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    42 ----
[CW] collect: return: 13.74237, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.17014, qf2_loss: 0.17081, policy_loss: -43.35117, policy_entropy: 4.08651, alpha: 0.34357, time: 50.73532
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    43 ----
[CW] collect: return: 8.93942, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.14917, qf2_loss: 0.15067, policy_loss: -43.78812, policy_entropy: 4.08764, alpha: 0.33580, time: 50.77038
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    44 ----
[CW] collect: return: 10.71441, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.21434, qf2_loss: 0.21459, policy_loss: -44.19480, policy_entropy: 4.08861, alpha: 0.32821, time: 50.98593
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    45 ----
[CW] collect: return: 13.78254, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.13376, qf2_loss: 0.13572, policy_loss: -44.58757, policy_entropy: 4.08833, alpha: 0.32081, time: 50.76065
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    46 ----
[CW] collect: return: 13.87632, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.18342, qf2_loss: 0.18498, policy_loss: -44.98589, policy_entropy: 4.09092, alpha: 0.31358, time: 50.79300
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    47 ----
[CW] collect: return: 16.96317, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.17467, qf2_loss: 0.17652, policy_loss: -45.36018, policy_entropy: 4.09226, alpha: 0.30652, time: 50.36243
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    48 ----
[CW] collect: return: 21.97757, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.11487, qf2_loss: 0.11667, policy_loss: -45.71883, policy_entropy: 4.09207, alpha: 0.29963, time: 50.43051
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    49 ----
[CW] collect: return: 10.22756, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.17770, qf2_loss: 0.17798, policy_loss: -46.06450, policy_entropy: 4.09200, alpha: 0.29290, time: 50.42228
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    50 ----
[CW] collect: return: 26.28612, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.16648, qf2_loss: 0.16725, policy_loss: -46.39807, policy_entropy: 4.08831, alpha: 0.28633, time: 50.47891
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    51 ----
[CW] collect: return: 21.92309, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.18110, qf2_loss: 0.18092, policy_loss: -46.71540, policy_entropy: 4.08550, alpha: 0.27992, time: 50.47647
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    52 ----
[CW] collect: return: 16.30683, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.11939, qf2_loss: 0.12003, policy_loss: -47.02803, policy_entropy: 4.08165, alpha: 0.27366, time: 50.38139
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    53 ----
[CW] collect: return: 22.63999, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.20819, qf2_loss: 0.20720, policy_loss: -47.33018, policy_entropy: 4.07865, alpha: 0.26754, time: 50.51510
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    54 ----
[CW] collect: return: 18.15144, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.15906, qf2_loss: 0.15835, policy_loss: -47.59453, policy_entropy: 4.07602, alpha: 0.26157, time: 50.52219
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    55 ----
[CW] collect: return: 17.27957, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.14719, qf2_loss: 0.14706, policy_loss: -47.86982, policy_entropy: 4.07102, alpha: 0.25573, time: 50.50632
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    56 ----
[CW] collect: return: 10.96499, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.16976, qf2_loss: 0.16913, policy_loss: -48.13718, policy_entropy: 4.06957, alpha: 0.25003, time: 55.37036
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    57 ----
[CW] collect: return: 30.09515, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.18539, qf2_loss: 0.18460, policy_loss: -48.37357, policy_entropy: 4.06601, alpha: 0.24446, time: 50.63276
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    58 ----
[CW] collect: return: 19.04794, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.13780, qf2_loss: 0.13780, policy_loss: -48.62237, policy_entropy: 4.06140, alpha: 0.23903, time: 50.45559
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    59 ----
[CW] collect: return: 10.31478, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.16310, qf2_loss: 0.16255, policy_loss: -48.84467, policy_entropy: 4.05606, alpha: 0.23371, time: 50.34909
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    60 ----
[CW] collect: return: 13.57334, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.17809, qf2_loss: 0.17725, policy_loss: -49.05603, policy_entropy: 4.04971, alpha: 0.22852, time: 50.41901
[CW] eval: return: 20.24377, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    61 ----
[CW] collect: return: 26.71824, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.16525, qf2_loss: 0.16465, policy_loss: -49.27571, policy_entropy: 4.03806, alpha: 0.22345, time: 50.54944
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    62 ----
[CW] collect: return: 29.81189, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.16431, qf2_loss: 0.16382, policy_loss: -49.46319, policy_entropy: 4.03599, alpha: 0.21850, time: 50.37798
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    63 ----
[CW] collect: return: 24.68457, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.20043, qf2_loss: 0.19936, policy_loss: -49.65616, policy_entropy: 4.02850, alpha: 0.21366, time: 50.35670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    64 ----
[CW] collect: return: 16.42459, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.15188, qf2_loss: 0.15156, policy_loss: -49.82246, policy_entropy: 4.01649, alpha: 0.20893, time: 50.50896
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    65 ----
[CW] collect: return: 22.49401, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.18226, qf2_loss: 0.18133, policy_loss: -49.99339, policy_entropy: 4.00537, alpha: 0.20431, time: 50.24695
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    66 ----
[CW] collect: return: 31.28017, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.14314, qf2_loss: 0.14317, policy_loss: -50.15419, policy_entropy: 3.98929, alpha: 0.19980, time: 50.31339
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    67 ----
[CW] collect: return: 14.52827, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.20713, qf2_loss: 0.20577, policy_loss: -50.30908, policy_entropy: 3.98103, alpha: 0.19539, time: 50.30786
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    68 ----
[CW] collect: return: 26.84149, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.25027, qf2_loss: 0.24872, policy_loss: -50.46492, policy_entropy: 3.95924, alpha: 0.19109, time: 50.23739
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    69 ----
[CW] collect: return: 31.31142, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.16175, qf2_loss: 0.16160, policy_loss: -50.59857, policy_entropy: 3.93219, alpha: 0.18689, time: 50.37571
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    70 ----
[CW] collect: return: 24.68105, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.16382, qf2_loss: 0.16395, policy_loss: -50.74086, policy_entropy: 3.90315, alpha: 0.18279, time: 50.52387
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    71 ----
[CW] collect: return: 31.35180, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.20746, qf2_loss: 0.20654, policy_loss: -50.85526, policy_entropy: 3.87936, alpha: 0.17878, time: 50.42059
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    72 ----
[CW] collect: return: 31.57269, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.19953, qf2_loss: 0.19952, policy_loss: -50.96827, policy_entropy: 3.86112, alpha: 0.17487, time: 50.45496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    73 ----
[CW] collect: return: 31.47593, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.18756, qf2_loss: 0.18870, policy_loss: -51.08808, policy_entropy: 3.80037, alpha: 0.17106, time: 50.46082
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    74 ----
[CW] collect: return: 46.85216, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.20207, qf2_loss: 0.20349, policy_loss: -51.21524, policy_entropy: 3.75056, alpha: 0.16735, time: 50.29947
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    75 ----
[CW] collect: return: 67.81006, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.25766, qf2_loss: 0.25867, policy_loss: -51.33938, policy_entropy: 3.64258, alpha: 0.16373, time: 50.47180
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    76 ----
[CW] collect: return: 64.02753, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.20167, qf2_loss: 0.20553, policy_loss: -51.49367, policy_entropy: 3.50726, alpha: 0.16024, time: 50.43523
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    77 ----
[CW] collect: return: 75.05840, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.28960, qf2_loss: 0.29282, policy_loss: -51.61476, policy_entropy: 3.33081, alpha: 0.15685, time: 50.35744
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    78 ----
[CW] collect: return: 54.23407, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.23879, qf2_loss: 0.24461, policy_loss: -51.77226, policy_entropy: 3.08994, alpha: 0.15360, time: 50.41863
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    79 ----
[CW] collect: return: 66.64061, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.37496, qf2_loss: 0.37771, policy_loss: -51.91040, policy_entropy: 2.83645, alpha: 0.15049, time: 50.38018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    80 ----
[CW] collect: return: 78.78160, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.26030, qf2_loss: 0.26583, policy_loss: -52.10687, policy_entropy: 2.41253, alpha: 0.14750, time: 50.36666
[CW] eval: return: 63.49954, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    81 ----
[CW] collect: return: 100.80474, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.27553, qf2_loss: 0.28070, policy_loss: -52.37180, policy_entropy: 1.88443, alpha: 0.14473, time: 52.25882
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    82 ----
[CW] collect: return: 93.73424, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.40093, qf2_loss: 0.40543, policy_loss: -52.67949, policy_entropy: 1.48331, alpha: 0.14213, time: 50.37423
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    83 ----
[CW] collect: return: 41.36999, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.33796, qf2_loss: 0.34349, policy_loss: -52.99998, policy_entropy: 1.07191, alpha: 0.13968, time: 50.65865
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    84 ----
[CW] collect: return: 61.45280, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.29982, qf2_loss: 0.30443, policy_loss: -53.34274, policy_entropy: 0.73833, alpha: 0.13735, time: 50.30792
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    85 ----
[CW] collect: return: 108.28645, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.34741, qf2_loss: 0.35036, policy_loss: -53.69343, policy_entropy: 0.57219, alpha: 0.13511, time: 50.30774
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    86 ----
[CW] collect: return: 54.93351, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.32119, qf2_loss: 0.32365, policy_loss: -54.10382, policy_entropy: 0.48529, alpha: 0.13291, time: 50.55798
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    87 ----
[CW] collect: return: 91.77571, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.29530, qf2_loss: 0.29763, policy_loss: -54.48972, policy_entropy: 0.36736, alpha: 0.13073, time: 50.39867
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    88 ----
[CW] collect: return: 87.71637, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.37053, qf2_loss: 0.37128, policy_loss: -54.89504, policy_entropy: 0.27459, alpha: 0.12858, time: 50.27417
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    89 ----
[CW] collect: return: 160.21792, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.31836, qf2_loss: 0.31976, policy_loss: -55.28046, policy_entropy: 0.25001, alpha: 0.12646, time: 50.53935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    90 ----
[CW] collect: return: 61.95299, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.35523, qf2_loss: 0.35518, policy_loss: -55.65479, policy_entropy: 0.19580, alpha: 0.12435, time: 50.40729
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    91 ----
[CW] collect: return: 80.96275, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.31375, qf2_loss: 0.31524, policy_loss: -56.06269, policy_entropy: 0.12567, alpha: 0.12225, time: 50.36855
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    92 ----
[CW] collect: return: 33.15530, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.31910, qf2_loss: 0.31936, policy_loss: -56.37803, policy_entropy: 0.17070, alpha: 0.12017, time: 50.53200
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    93 ----
[CW] collect: return: 65.60083, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.30272, qf2_loss: 0.30310, policy_loss: -56.73426, policy_entropy: 0.14341, alpha: 0.11808, time: 50.29837
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    94 ----
[CW] collect: return: 28.78468, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.38009, qf2_loss: 0.37854, policy_loss: -56.94889, policy_entropy: 0.26726, alpha: 0.11599, time: 50.63775
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    95 ----
[CW] collect: return: 58.54283, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.35404, qf2_loss: 0.35425, policy_loss: -57.27871, policy_entropy: 0.31036, alpha: 0.11387, time: 50.31034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    96 ----
[CW] collect: return: 28.74344, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.29758, qf2_loss: 0.29673, policy_loss: -57.48196, policy_entropy: 0.44832, alpha: 0.11174, time: 50.43859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    97 ----
[CW] collect: return: 110.92885, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.34767, qf2_loss: 0.34643, policy_loss: -57.69052, policy_entropy: 0.56525, alpha: 0.10959, time: 50.20970
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    98 ----
[CW] collect: return: 42.68150, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.34833, qf2_loss: 0.34874, policy_loss: -57.82986, policy_entropy: 0.74482, alpha: 0.10742, time: 50.01485
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    99 ----
[CW] collect: return: 90.34976, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.27481, qf2_loss: 0.27804, policy_loss: -57.98548, policy_entropy: 0.89741, alpha: 0.10521, time: 50.09666
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   100 ----
[CW] collect: return: 62.06514, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.34322, qf2_loss: 0.34282, policy_loss: -58.05858, policy_entropy: 1.02145, alpha: 0.10299, time: 50.27336
[CW] eval: return: 78.85643, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   101 ----
[CW] collect: return: 30.93650, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.31249, qf2_loss: 0.31225, policy_loss: -58.14114, policy_entropy: 1.03594, alpha: 0.10080, time: 50.05278
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   102 ----
[CW] collect: return: 83.08401, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.30927, qf2_loss: 0.30909, policy_loss: -58.16307, policy_entropy: 1.19100, alpha: 0.09864, time: 50.09599
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   103 ----
[CW] collect: return: 26.25759, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.24848, qf2_loss: 0.24781, policy_loss: -58.16427, policy_entropy: 1.34317, alpha: 0.09645, time: 50.09990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   104 ----
[CW] collect: return: 54.71298, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.25018, qf2_loss: 0.24974, policy_loss: -58.14799, policy_entropy: 1.60482, alpha: 0.09428, time: 50.16910
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   105 ----
[CW] collect: return: 53.92155, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.25832, qf2_loss: 0.25769, policy_loss: -58.20064, policy_entropy: 1.91714, alpha: 0.09207, time: 50.15436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   106 ----
[CW] collect: return: 59.84672, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.14773, qf2_loss: 0.14908, policy_loss: -58.11978, policy_entropy: 2.27270, alpha: 0.08984, time: 50.18575
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   107 ----
[CW] collect: return: 47.97148, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.15909, qf2_loss: 0.16089, policy_loss: -58.09114, policy_entropy: 2.55085, alpha: 0.08759, time: 50.45535
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   108 ----
[CW] collect: return: 46.99837, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.19576, qf2_loss: 0.19670, policy_loss: -57.97690, policy_entropy: 2.78733, alpha: 0.08536, time: 50.33188
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   109 ----
[CW] collect: return: 36.46231, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.22504, qf2_loss: 0.22642, policy_loss: -57.84481, policy_entropy: 3.00296, alpha: 0.08317, time: 50.15924
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   110 ----
[CW] collect: return: 20.39227, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.10591, qf2_loss: 0.10662, policy_loss: -57.73057, policy_entropy: 3.18805, alpha: 0.08100, time: 50.17051
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   111 ----
[CW] collect: return: 27.19169, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.13651, qf2_loss: 0.13622, policy_loss: -57.69189, policy_entropy: 3.30245, alpha: 0.07890, time: 50.22642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   112 ----
[CW] collect: return: 11.02522, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.15461, qf2_loss: 0.15433, policy_loss: -57.55533, policy_entropy: 3.36117, alpha: 0.07685, time: 50.39790
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   113 ----
[CW] collect: return: 16.46297, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.17741, qf2_loss: 0.17706, policy_loss: -57.53362, policy_entropy: 3.31922, alpha: 0.07489, time: 50.59798
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   114 ----
[CW] collect: return: 30.33250, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.27680, qf2_loss: 0.27361, policy_loss: -57.41288, policy_entropy: 3.25296, alpha: 0.07300, time: 50.58800
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   115 ----
[CW] collect: return: 28.73446, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.10912, qf2_loss: 0.10856, policy_loss: -57.25426, policy_entropy: 3.25693, alpha: 0.07119, time: 50.75971
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   116 ----
[CW] collect: return: 48.06023, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.13940, qf2_loss: 0.13882, policy_loss: -57.11308, policy_entropy: 3.20050, alpha: 0.06945, time: 50.49011
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   117 ----
[CW] collect: return: 23.53296, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.14384, qf2_loss: 0.14334, policy_loss: -57.10850, policy_entropy: 2.98831, alpha: 0.06778, time: 50.34477
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   118 ----
[CW] collect: return: 61.41690, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.15007, qf2_loss: 0.14991, policy_loss: -57.03044, policy_entropy: 2.71183, alpha: 0.06620, time: 50.36078
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   119 ----
[CW] collect: return: 111.23722, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.23403, qf2_loss: 0.23291, policy_loss: -56.92606, policy_entropy: 2.31897, alpha: 0.06472, time: 50.26064
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   120 ----
[CW] collect: return: 32.18903, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.22280, qf2_loss: 0.22155, policy_loss: -56.88861, policy_entropy: 1.66564, alpha: 0.06336, time: 50.44615
[CW] eval: return: 55.02537, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   121 ----
[CW] collect: return: 53.58172, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.26359, qf2_loss: 0.26198, policy_loss: -56.82215, policy_entropy: 1.29809, alpha: 0.06210, time: 50.28538
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   122 ----
[CW] collect: return: 55.07337, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.26225, qf2_loss: 0.26133, policy_loss: -56.74084, policy_entropy: 0.72350, alpha: 0.06093, time: 52.29449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   123 ----
[CW] collect: return: 70.43360, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.25006, qf2_loss: 0.25021, policy_loss: -56.87454, policy_entropy: 0.15887, alpha: 0.05987, time: 50.22052
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   124 ----
[CW] collect: return: 79.35895, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.38204, qf2_loss: 0.37920, policy_loss: -56.76286, policy_entropy: 0.09775, alpha: 0.05886, time: 50.60604
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   125 ----
[CW] collect: return: 88.39434, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.26484, qf2_loss: 0.26430, policy_loss: -56.92516, policy_entropy: -0.45683, alpha: 0.05790, time: 50.26252
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   126 ----
[CW] collect: return: 57.48992, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.29717, qf2_loss: 0.29800, policy_loss: -56.96366, policy_entropy: -0.63584, alpha: 0.05699, time: 50.05028
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   127 ----
[CW] collect: return: 131.92326, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.36203, qf2_loss: 0.36100, policy_loss: -56.97121, policy_entropy: -0.70205, alpha: 0.05610, time: 50.16975
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   128 ----
[CW] collect: return: 75.03282, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.34551, qf2_loss: 0.34762, policy_loss: -57.13702, policy_entropy: -0.85850, alpha: 0.05524, time: 50.39176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   129 ----
[CW] collect: return: 57.28952, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.34822, qf2_loss: 0.35165, policy_loss: -57.14366, policy_entropy: -0.86725, alpha: 0.05437, time: 50.21208
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   130 ----
[CW] collect: return: 87.53459, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.36868, qf2_loss: 0.37177, policy_loss: -57.23298, policy_entropy: -1.02070, alpha: 0.05352, time: 50.16307
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   131 ----
[CW] collect: return: 76.33085, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.39408, qf2_loss: 0.39634, policy_loss: -57.30748, policy_entropy: -0.98728, alpha: 0.05267, time: 50.24027
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   132 ----
[CW] collect: return: 91.57641, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.37527, qf2_loss: 0.37823, policy_loss: -57.36757, policy_entropy: -1.15546, alpha: 0.05184, time: 50.22020
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   133 ----
[CW] collect: return: 53.79419, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.53556, qf2_loss: 0.53762, policy_loss: -57.44658, policy_entropy: -1.17375, alpha: 0.05102, time: 50.16905
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   134 ----
[CW] collect: return: 32.75189, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.28941, qf2_loss: 0.29293, policy_loss: -57.54373, policy_entropy: -1.25636, alpha: 0.05020, time: 50.10136
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   135 ----
[CW] collect: return: 47.89060, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.27919, qf2_loss: 0.28158, policy_loss: -57.52795, policy_entropy: -1.17522, alpha: 0.04937, time: 50.19342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   136 ----
[CW] collect: return: 64.72241, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.33044, qf2_loss: 0.33327, policy_loss: -57.57169, policy_entropy: -1.31829, alpha: 0.04856, time: 50.03798
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   137 ----
[CW] collect: return: 36.20659, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.34539, qf2_loss: 0.34849, policy_loss: -57.75864, policy_entropy: -1.42650, alpha: 0.04776, time: 50.13757
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   138 ----
[CW] collect: return: 89.10918, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.38285, qf2_loss: 0.38355, policy_loss: -57.79160, policy_entropy: -1.39997, alpha: 0.04697, time: 50.45517
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   139 ----
[CW] collect: return: 95.72962, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.44946, qf2_loss: 0.44688, policy_loss: -57.74014, policy_entropy: -1.38828, alpha: 0.04617, time: 50.49658
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   140 ----
[CW] collect: return: 50.27445, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.31155, qf2_loss: 0.31384, policy_loss: -57.60125, policy_entropy: -1.37290, alpha: 0.04538, time: 50.21520
[CW] eval: return: 80.44797, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   141 ----
[CW] collect: return: 59.64438, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.41251, qf2_loss: 0.41503, policy_loss: -57.65480, policy_entropy: -1.28615, alpha: 0.04457, time: 50.39351
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   142 ----
[CW] collect: return: 41.22996, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.37227, qf2_loss: 0.37370, policy_loss: -57.64068, policy_entropy: -1.46222, alpha: 0.04379, time: 50.19861
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   143 ----
[CW] collect: return: 54.32044, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.60405, qf2_loss: 0.60441, policy_loss: -57.66465, policy_entropy: -1.36117, alpha: 0.04300, time: 50.16327
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   144 ----
[CW] collect: return: 80.59411, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.32883, qf2_loss: 0.33195, policy_loss: -57.79446, policy_entropy: -1.53239, alpha: 0.04223, time: 50.05939
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   145 ----
[CW] collect: return: 136.11035, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.35513, qf2_loss: 0.35940, policy_loss: -57.77662, policy_entropy: -1.51402, alpha: 0.04147, time: 50.59019
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   146 ----
[CW] collect: return: 89.95482, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.42347, qf2_loss: 0.42343, policy_loss: -57.99685, policy_entropy: -1.63478, alpha: 0.04073, time: 50.63973
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   147 ----
[CW] collect: return: 28.60496, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.36520, qf2_loss: 0.36956, policy_loss: -57.88214, policy_entropy: -1.72085, alpha: 0.04000, time: 50.84283
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   148 ----
[CW] collect: return: 61.30683, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.37396, qf2_loss: 0.37816, policy_loss: -57.73144, policy_entropy: -1.57770, alpha: 0.03927, time: 50.46595
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   149 ----
[CW] collect: return: 93.40446, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.38458, qf2_loss: 0.38841, policy_loss: -57.81771, policy_entropy: -1.80566, alpha: 0.03856, time: 50.44381
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   150 ----
[CW] collect: return: 53.90853, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.42981, qf2_loss: 0.43712, policy_loss: -57.89417, policy_entropy: -1.86938, alpha: 0.03787, time: 50.49735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   151 ----
[CW] collect: return: 27.13658, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.52157, qf2_loss: 0.52558, policy_loss: -57.74479, policy_entropy: -1.74632, alpha: 0.03718, time: 50.60484
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   152 ----
[CW] collect: return: 110.44044, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.49509, qf2_loss: 0.49711, policy_loss: -57.89139, policy_entropy: -2.10201, alpha: 0.03651, time: 50.38477
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   153 ----
[CW] collect: return: 148.06494, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.42413, qf2_loss: 0.42946, policy_loss: -58.05153, policy_entropy: -2.16687, alpha: 0.03587, time: 50.20646
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   154 ----
[CW] collect: return: 116.13693, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.63402, qf2_loss: 0.64042, policy_loss: -58.18503, policy_entropy: -2.33039, alpha: 0.03527, time: 50.29500
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   155 ----
[CW] collect: return: 95.43628, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.47509, qf2_loss: 0.48086, policy_loss: -58.16449, policy_entropy: -2.43332, alpha: 0.03467, time: 50.59069
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   156 ----
[CW] collect: return: 148.60742, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.56120, qf2_loss: 0.56735, policy_loss: -58.29203, policy_entropy: -2.49590, alpha: 0.03410, time: 51.19411
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   157 ----
[CW] collect: return: 76.24472, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.45777, qf2_loss: 0.46568, policy_loss: -58.27335, policy_entropy: -2.61406, alpha: 0.03354, time: 52.04858
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   158 ----
[CW] collect: return: 64.98481, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.50422, qf2_loss: 0.51303, policy_loss: -58.34547, policy_entropy: -2.87806, alpha: 0.03300, time: 50.33495
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   159 ----
[CW] collect: return: 233.32856, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.70614, qf2_loss: 0.70962, policy_loss: -58.62323, policy_entropy: -3.11499, alpha: 0.03250, time: 50.53975
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   160 ----
[CW] collect: return: 173.50623, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.68929, qf2_loss: 0.69679, policy_loss: -58.43702, policy_entropy: -3.13461, alpha: 0.03203, time: 50.50114
[CW] eval: return: 102.86553, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   161 ----
[CW] collect: return: 66.24838, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.57537, qf2_loss: 0.58813, policy_loss: -58.85136, policy_entropy: -3.40208, alpha: 0.03158, time: 50.29827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   162 ----
[CW] collect: return: 100.56686, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.78109, qf2_loss: 0.78749, policy_loss: -58.91458, policy_entropy: -3.63204, alpha: 0.03115, time: 50.20123
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   163 ----
[CW] collect: return: 94.52646, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.68165, qf2_loss: 0.69168, policy_loss: -59.27643, policy_entropy: -3.69419, alpha: 0.03075, time: 50.21492
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   164 ----
[CW] collect: return: 58.61088, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.66629, qf2_loss: 0.67368, policy_loss: -59.08542, policy_entropy: -3.47637, alpha: 0.03033, time: 50.13094
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   165 ----
[CW] collect: return: 89.50294, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.60635, qf2_loss: 0.61760, policy_loss: -59.53984, policy_entropy: -3.77267, alpha: 0.02991, time: 49.93275
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   166 ----
[CW] collect: return: 106.36738, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.80134, qf2_loss: 0.80950, policy_loss: -59.43381, policy_entropy: -3.90748, alpha: 0.02953, time: 53.03570
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   167 ----
[CW] collect: return: 172.00767, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.77234, qf2_loss: 0.78476, policy_loss: -59.78928, policy_entropy: -4.13243, alpha: 0.02916, time: 49.94053
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   168 ----
[CW] collect: return: 127.10052, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.71145, qf2_loss: 0.72684, policy_loss: -59.85825, policy_entropy: -4.33365, alpha: 0.02884, time: 50.72421
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   169 ----
[CW] collect: return: 97.75303, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.79799, qf2_loss: 0.81125, policy_loss: -59.97156, policy_entropy: -4.30237, alpha: 0.02853, time: 50.73835
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   170 ----
[CW] collect: return: 116.45305, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.77685, qf2_loss: 0.78786, policy_loss: -59.92178, policy_entropy: -4.23959, alpha: 0.02821, time: 50.85708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   171 ----
[CW] collect: return: 145.54491, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.82928, qf2_loss: 0.83796, policy_loss: -60.26970, policy_entropy: -4.14454, alpha: 0.02785, time: 50.94907
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   172 ----
[CW] collect: return: 53.32136, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.84130, qf2_loss: 0.85463, policy_loss: -60.20877, policy_entropy: -4.24196, alpha: 0.02749, time: 50.88671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   173 ----
[CW] collect: return: 71.90821, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.77347, qf2_loss: 0.78432, policy_loss: -60.56419, policy_entropy: -4.22983, alpha: 0.02713, time: 50.66627
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   174 ----
[CW] collect: return: 200.79293, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.66338, qf2_loss: 0.67535, policy_loss: -60.85090, policy_entropy: -4.29868, alpha: 0.02678, time: 50.87777
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   175 ----
[CW] collect: return: 81.37236, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.74444, qf2_loss: 0.75743, policy_loss: -60.58213, policy_entropy: -4.20143, alpha: 0.02642, time: 51.08336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   176 ----
[CW] collect: return: 61.96123, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.78380, qf2_loss: 0.79419, policy_loss: -60.76657, policy_entropy: -4.18085, alpha: 0.02605, time: 50.85241
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   177 ----
[CW] collect: return: 114.69056, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.83215, qf2_loss: 0.84848, policy_loss: -60.71101, policy_entropy: -4.09743, alpha: 0.02566, time: 50.80836
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   178 ----
[CW] collect: return: 180.18272, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.73681, qf2_loss: 0.75638, policy_loss: -61.03966, policy_entropy: -4.25143, alpha: 0.02527, time: 50.89882
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   179 ----
[CW] collect: return: 103.00250, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.88592, qf2_loss: 0.90346, policy_loss: -61.22044, policy_entropy: -4.34714, alpha: 0.02490, time: 50.72468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   180 ----
[CW] collect: return: 107.93290, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.76771, qf2_loss: 0.78354, policy_loss: -61.44926, policy_entropy: -4.34529, alpha: 0.02456, time: 50.70696
[CW] eval: return: 117.11358, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   181 ----
[CW] collect: return: 176.62697, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.99624, qf2_loss: 1.01382, policy_loss: -61.37896, policy_entropy: -4.38580, alpha: 0.02419, time: 50.91728
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   182 ----
[CW] collect: return: 94.49418, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.88846, qf2_loss: 0.90733, policy_loss: -61.56296, policy_entropy: -4.49646, alpha: 0.02386, time: 51.05247
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   183 ----
[CW] collect: return: 138.81527, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.79591, qf2_loss: 0.81404, policy_loss: -61.69849, policy_entropy: -4.59030, alpha: 0.02353, time: 50.70028
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   184 ----
[CW] collect: return: 78.06886, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.85591, qf2_loss: 0.87479, policy_loss: -61.41029, policy_entropy: -4.27654, alpha: 0.02320, time: 50.66150
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   185 ----
[CW] collect: return: 72.64484, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 1.19790, qf2_loss: 1.21500, policy_loss: -61.70165, policy_entropy: -4.29855, alpha: 0.02280, time: 50.66206
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   186 ----
[CW] collect: return: 45.33238, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.98382, qf2_loss: 1.00231, policy_loss: -61.92767, policy_entropy: -4.53023, alpha: 0.02244, time: 50.76068
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   187 ----
[CW] collect: return: 139.81527, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.70367, qf2_loss: 0.72859, policy_loss: -61.99441, policy_entropy: -4.50074, alpha: 0.02212, time: 50.70012
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   188 ----
[CW] collect: return: 105.76348, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.77010, qf2_loss: 0.78942, policy_loss: -62.03401, policy_entropy: -4.53325, alpha: 0.02178, time: 50.93714
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   189 ----
[CW] collect: return: 157.02588, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.87654, qf2_loss: 0.89736, policy_loss: -62.19325, policy_entropy: -4.56445, alpha: 0.02144, time: 50.80799
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   190 ----
[CW] collect: return: 156.86624, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.87237, qf2_loss: 0.89018, policy_loss: -62.03881, policy_entropy: -4.45417, alpha: 0.02111, time: 50.90698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   191 ----
[CW] collect: return: 70.20197, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.92252, qf2_loss: 0.94126, policy_loss: -62.11936, policy_entropy: -4.66149, alpha: 0.02077, time: 50.82883
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   192 ----
[CW] collect: return: 94.12485, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.78883, qf2_loss: 0.81198, policy_loss: -62.38814, policy_entropy: -4.77773, alpha: 0.02048, time: 50.80224
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   193 ----
[CW] collect: return: 119.78728, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 1.11260, qf2_loss: 1.12597, policy_loss: -62.55091, policy_entropy: -4.84684, alpha: 0.02021, time: 50.84712
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   194 ----
[CW] collect: return: 137.82570, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 1.01662, qf2_loss: 1.03934, policy_loss: -62.05669, policy_entropy: -4.97345, alpha: 0.01995, time: 50.90698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   195 ----
[CW] collect: return: 137.26286, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.78711, qf2_loss: 0.80988, policy_loss: -62.28715, policy_entropy: -4.79161, alpha: 0.01968, time: 51.10326
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   196 ----
[CW] collect: return: 124.51273, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 1.00616, qf2_loss: 1.02100, policy_loss: -62.91993, policy_entropy: -4.82147, alpha: 0.01939, time: 53.14073
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   197 ----
[CW] collect: return: 119.16811, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 1.27565, qf2_loss: 1.30147, policy_loss: -62.80864, policy_entropy: -5.08944, alpha: 0.01913, time: 51.07627
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   198 ----
[CW] collect: return: 75.44930, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.86720, qf2_loss: 0.88571, policy_loss: -62.88411, policy_entropy: -5.28142, alpha: 0.01892, time: 50.92329
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   199 ----
[CW] collect: return: 94.37978, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.98223, qf2_loss: 0.99798, policy_loss: -62.26488, policy_entropy: -5.21465, alpha: 0.01874, time: 51.00489
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   200 ----
[CW] collect: return: 78.15937, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 1.71751, qf2_loss: 1.74187, policy_loss: -63.00143, policy_entropy: -5.29249, alpha: 0.01854, time: 50.77380
[CW] eval: return: 138.85438, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   201 ----
[CW] collect: return: 120.72308, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.89196, qf2_loss: 0.91461, policy_loss: -62.71057, policy_entropy: -5.17400, alpha: 0.01835, time: 51.23518
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   202 ----
[CW] collect: return: 126.84576, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.77473, qf2_loss: 0.80071, policy_loss: -62.85773, policy_entropy: -5.21975, alpha: 0.01813, time: 51.08162
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   203 ----
[CW] collect: return: 87.61973, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.83549, qf2_loss: 0.85782, policy_loss: -62.79666, policy_entropy: -5.39022, alpha: 0.01792, time: 50.97311
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   204 ----
[CW] collect: return: 81.45255, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.83480, qf2_loss: 0.86448, policy_loss: -62.57942, policy_entropy: -5.55808, alpha: 0.01777, time: 50.69937
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   205 ----
[CW] collect: return: 53.47583, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 1.21353, qf2_loss: 1.23985, policy_loss: -63.09543, policy_entropy: -5.74961, alpha: 0.01767, time: 50.68580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   206 ----
[CW] collect: return: 54.52328, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.93453, qf2_loss: 0.96190, policy_loss: -63.21573, policy_entropy: -5.92489, alpha: 0.01762, time: 50.73499
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   207 ----
[CW] collect: return: 182.91604, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.87145, qf2_loss: 0.90189, policy_loss: -63.42683, policy_entropy: -5.70605, alpha: 0.01758, time: 50.95393
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   208 ----
[CW] collect: return: 146.65185, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.98847, qf2_loss: 1.02003, policy_loss: -63.60701, policy_entropy: -5.65797, alpha: 0.01748, time: 50.77692
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   209 ----
[CW] collect: return: 42.90607, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 1.25113, qf2_loss: 1.28173, policy_loss: -63.57983, policy_entropy: -5.74286, alpha: 0.01738, time: 50.67631
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   210 ----
[CW] collect: return: 93.82107, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 1.10673, qf2_loss: 1.13774, policy_loss: -63.46139, policy_entropy: -5.48109, alpha: 0.01724, time: 50.66232
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   211 ----
[CW] collect: return: 133.12424, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 1.00935, qf2_loss: 1.03820, policy_loss: -63.82721, policy_entropy: -5.64975, alpha: 0.01707, time: 50.70381
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   212 ----
[CW] collect: return: 58.99802, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 1.97351, qf2_loss: 1.99874, policy_loss: -63.37850, policy_entropy: -5.53245, alpha: 0.01692, time: 50.79668
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   213 ----
[CW] collect: return: 137.73479, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 1.15942, qf2_loss: 1.19029, policy_loss: -63.80830, policy_entropy: -5.60814, alpha: 0.01677, time: 50.54833
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   214 ----
[CW] collect: return: 178.63408, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 1.06069, qf2_loss: 1.09536, policy_loss: -64.16633, policy_entropy: -5.62089, alpha: 0.01662, time: 51.00190
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   215 ----
[CW] collect: return: 66.60923, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 1.01079, qf2_loss: 1.04531, policy_loss: -64.30787, policy_entropy: -5.73410, alpha: 0.01650, time: 50.86267
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   216 ----
[CW] collect: return: 104.50393, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 1.14011, qf2_loss: 1.17241, policy_loss: -64.19850, policy_entropy: -5.53585, alpha: 0.01635, time: 50.52171
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   217 ----
[CW] collect: return: 106.19691, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 1.11153, qf2_loss: 1.15018, policy_loss: -63.83122, policy_entropy: -5.67621, alpha: 0.01619, time: 50.94799
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   218 ----
[CW] collect: return: 216.18099, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 1.24614, qf2_loss: 1.28144, policy_loss: -63.94068, policy_entropy: -5.71912, alpha: 0.01605, time: 50.60333
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   219 ----
[CW] collect: return: 63.38926, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 1.26576, qf2_loss: 1.29873, policy_loss: -64.47337, policy_entropy: -5.77427, alpha: 0.01596, time: 50.48884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   220 ----
[CW] collect: return: 78.44482, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 1.24327, qf2_loss: 1.28141, policy_loss: -64.00135, policy_entropy: -5.67380, alpha: 0.01584, time: 50.81723
[CW] eval: return: 131.10453, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   221 ----
[CW] collect: return: 77.57544, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 1.51071, qf2_loss: 1.55547, policy_loss: -64.20784, policy_entropy: -5.75971, alpha: 0.01571, time: 51.72949
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   222 ----
[CW] collect: return: 83.79641, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 1.30982, qf2_loss: 1.34530, policy_loss: -64.27017, policy_entropy: -6.01461, alpha: 0.01566, time: 50.49240
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   223 ----
[CW] collect: return: 109.45507, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 1.26310, qf2_loss: 1.29235, policy_loss: -65.34955, policy_entropy: -6.28297, alpha: 0.01572, time: 50.64002
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   224 ----
[CW] collect: return: 30.46045, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 1.19500, qf2_loss: 1.23441, policy_loss: -64.37005, policy_entropy: -6.02905, alpha: 0.01582, time: 50.63706
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   225 ----
[CW] collect: return: 83.74770, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 1.50598, qf2_loss: 1.54340, policy_loss: -64.74244, policy_entropy: -5.90079, alpha: 0.01579, time: 50.40101
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   226 ----
[CW] collect: return: 34.97756, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 1.28947, qf2_loss: 1.32921, policy_loss: -64.81653, policy_entropy: -4.99512, alpha: 0.01555, time: 50.81410
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   227 ----
[CW] collect: return: 68.05961, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 1.16908, qf2_loss: 1.20507, policy_loss: -64.77831, policy_entropy: -5.29599, alpha: 0.01507, time: 50.68956
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   228 ----
[CW] collect: return: 100.02047, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 1.23342, qf2_loss: 1.26925, policy_loss: -64.67101, policy_entropy: -5.74595, alpha: 0.01484, time: 50.82147
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   229 ----
[CW] collect: return: 156.01434, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 1.46150, qf2_loss: 1.50026, policy_loss: -64.86154, policy_entropy: -5.66169, alpha: 0.01473, time: 50.84078
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   230 ----
[CW] collect: return: 72.84317, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 3.51194, qf2_loss: 3.59426, policy_loss: -65.45084, policy_entropy: -5.81083, alpha: 0.01456, time: 50.96399
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   231 ----
[CW] collect: return: 78.96806, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 1.42490, qf2_loss: 1.46263, policy_loss: -64.67605, policy_entropy: -5.73758, alpha: 0.01448, time: 50.85782
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   232 ----
[CW] collect: return: 186.38301, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 1.11186, qf2_loss: 1.13961, policy_loss: -65.47773, policy_entropy: -5.74730, alpha: 0.01432, time: 50.97697
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   233 ----
[CW] collect: return: 49.05966, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 1.01280, qf2_loss: 1.04226, policy_loss: -65.57683, policy_entropy: -5.69656, alpha: 0.01420, time: 50.91759
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   234 ----
[CW] collect: return: 107.92074, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 1.15862, qf2_loss: 1.18339, policy_loss: -64.96935, policy_entropy: -5.62741, alpha: 0.01402, time: 50.97170
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   235 ----
[CW] collect: return: 130.73126, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 1.20330, qf2_loss: 1.23152, policy_loss: -64.81884, policy_entropy: -5.75810, alpha: 0.01384, time: 50.89913
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   236 ----
[CW] collect: return: 106.66605, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 1.21112, qf2_loss: 1.25203, policy_loss: -65.94356, policy_entropy: -5.87191, alpha: 0.01376, time: 50.81827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   237 ----
[CW] collect: return: 68.14190, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 1.46403, qf2_loss: 1.49788, policy_loss: -65.55112, policy_entropy: -5.75875, alpha: 0.01363, time: 51.03827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   238 ----
[CW] collect: return: 119.10259, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 1.46671, qf2_loss: 1.49704, policy_loss: -65.90094, policy_entropy: -6.83573, alpha: 0.01378, time: 50.84904
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   239 ----
[CW] collect: return: 26.64318, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 1.37609, qf2_loss: 1.40807, policy_loss: -66.06158, policy_entropy: -6.72861, alpha: 0.01420, time: 51.73534
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   240 ----
[CW] collect: return: 157.65719, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 1.41816, qf2_loss: 1.45334, policy_loss: -66.26286, policy_entropy: -6.61868, alpha: 0.01460, time: 50.83069
[CW] eval: return: 100.64623, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   241 ----
[CW] collect: return: 60.53859, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 1.59126, qf2_loss: 1.62645, policy_loss: -65.95922, policy_entropy: -6.32382, alpha: 0.01489, time: 50.97038
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   242 ----
[CW] collect: return: 64.81701, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 1.54449, qf2_loss: 1.58914, policy_loss: -65.58850, policy_entropy: -6.18241, alpha: 0.01508, time: 50.63590
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   243 ----
[CW] collect: return: 133.54187, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 1.45569, qf2_loss: 1.49038, policy_loss: -66.00849, policy_entropy: -6.18254, alpha: 0.01520, time: 50.87145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   244 ----
[CW] collect: return: 138.29698, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 1.59791, qf2_loss: 1.62146, policy_loss: -66.76701, policy_entropy: -6.67112, alpha: 0.01546, time: 50.66422
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   245 ----
[CW] collect: return: 115.71935, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 1.63469, qf2_loss: 1.67230, policy_loss: -66.50446, policy_entropy: -6.30459, alpha: 0.01589, time: 50.71610
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   246 ----
[CW] collect: return: 93.33504, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 2.42102, qf2_loss: 2.47594, policy_loss: -66.23417, policy_entropy: -6.11081, alpha: 0.01607, time: 50.91062
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   247 ----
[CW] collect: return: 86.20613, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 1.51061, qf2_loss: 1.54765, policy_loss: -66.27174, policy_entropy: -6.11940, alpha: 0.01610, time: 50.73755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   248 ----
[CW] collect: return: 152.87221, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 1.40757, qf2_loss: 1.45139, policy_loss: -66.38873, policy_entropy: -6.19889, alpha: 0.01627, time: 50.69849
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   249 ----
[CW] collect: return: 190.85960, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 1.81922, qf2_loss: 1.87369, policy_loss: -66.52425, policy_entropy: -5.86616, alpha: 0.01635, time: 50.65778
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   250 ----
[CW] collect: return: 80.80056, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 1.69014, qf2_loss: 1.71696, policy_loss: -66.51500, policy_entropy: -5.96278, alpha: 0.01621, time: 50.44994
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   251 ----
[CW] collect: return: 223.54730, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 1.58752, qf2_loss: 1.62539, policy_loss: -65.88863, policy_entropy: -5.79571, alpha: 0.01608, time: 50.74413
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   252 ----
[CW] collect: return: 132.40715, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 1.58346, qf2_loss: 1.62558, policy_loss: -67.65240, policy_entropy: -6.07750, alpha: 0.01605, time: 50.35793
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   253 ----
[CW] collect: return: 107.66176, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 1.70078, qf2_loss: 1.73789, policy_loss: -67.38460, policy_entropy: -5.91423, alpha: 0.01606, time: 50.53510
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   254 ----
[CW] collect: return: 72.00062, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 2.09624, qf2_loss: 2.14856, policy_loss: -67.08926, policy_entropy: -5.60936, alpha: 0.01588, time: 50.54683
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   255 ----
[CW] collect: return: 57.90792, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 1.59609, qf2_loss: 1.64690, policy_loss: -67.32531, policy_entropy: -6.05955, alpha: 0.01565, time: 50.44119
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   256 ----
[CW] collect: return: 172.96339, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 1.53852, qf2_loss: 1.58162, policy_loss: -67.78891, policy_entropy: -6.27220, alpha: 0.01577, time: 50.60186
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   257 ----
[CW] collect: return: 24.09740, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 1.76715, qf2_loss: 1.81265, policy_loss: -66.91953, policy_entropy: -6.26315, alpha: 0.01607, time: 50.73634
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   258 ----
[CW] collect: return: 106.38630, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 1.56416, qf2_loss: 1.59811, policy_loss: -66.83424, policy_entropy: -6.16748, alpha: 0.01626, time: 50.63610
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   259 ----
[CW] collect: return: 102.40495, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 1.60812, qf2_loss: 1.66413, policy_loss: -67.17131, policy_entropy: -6.11215, alpha: 0.01640, time: 51.13447
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   260 ----
[CW] collect: return: 262.27045, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 1.95013, qf2_loss: 2.00253, policy_loss: -67.58830, policy_entropy: -6.00878, alpha: 0.01643, time: 50.46046
[CW] eval: return: 151.83885, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   261 ----
[CW] collect: return: 184.85635, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 1.96532, qf2_loss: 2.00463, policy_loss: -67.14826, policy_entropy: -6.00448, alpha: 0.01644, time: 50.56583
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   262 ----
[CW] collect: return: 204.95668, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 2.58349, qf2_loss: 2.64651, policy_loss: -67.76691, policy_entropy: -5.99065, alpha: 0.01646, time: 50.47574
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   263 ----
[CW] collect: return: 102.33114, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 1.86792, qf2_loss: 1.88854, policy_loss: -67.53398, policy_entropy: -5.83749, alpha: 0.01633, time: 50.61961
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   264 ----
[CW] collect: return: 196.46853, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 1.53152, qf2_loss: 1.58086, policy_loss: -67.77461, policy_entropy: -5.86888, alpha: 0.01625, time: 50.62912
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   265 ----
[CW] collect: return: 195.86700, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 2.19769, qf2_loss: 2.24708, policy_loss: -67.68806, policy_entropy: -5.71378, alpha: 0.01611, time: 50.53652
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   266 ----
[CW] collect: return: 201.78782, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 2.64089, qf2_loss: 2.69685, policy_loss: -68.14579, policy_entropy: -5.87634, alpha: 0.01585, time: 52.24275
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   267 ----
[CW] collect: return: 222.15999, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 2.29746, qf2_loss: 2.35649, policy_loss: -68.21014, policy_entropy: -5.80244, alpha: 0.01573, time: 50.39335
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   268 ----
[CW] collect: return: 267.34009, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 1.80599, qf2_loss: 1.84840, policy_loss: -69.01267, policy_entropy: -5.67421, alpha: 0.01555, time: 50.60455
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   269 ----
[CW] collect: return: 225.30301, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 1.53119, qf2_loss: 1.58835, policy_loss: -68.18844, policy_entropy: -5.36330, alpha: 0.01525, time: 50.60546
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   270 ----
[CW] collect: return: 146.30876, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 1.80141, qf2_loss: 1.85349, policy_loss: -68.89223, policy_entropy: -5.56196, alpha: 0.01482, time: 50.36314
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   271 ----
[CW] collect: return: 241.00165, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 1.74602, qf2_loss: 1.80244, policy_loss: -69.46034, policy_entropy: -5.87610, alpha: 0.01462, time: 50.56560
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   272 ----
[CW] collect: return: 232.70416, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 1.93501, qf2_loss: 1.98261, policy_loss: -69.94474, policy_entropy: -6.03633, alpha: 0.01457, time: 50.52100
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   273 ----
[CW] collect: return: 153.29824, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 2.10366, qf2_loss: 2.16605, policy_loss: -69.66799, policy_entropy: -6.20629, alpha: 0.01466, time: 50.75494
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   274 ----
[CW] collect: return: 210.95009, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 3.00832, qf2_loss: 3.05237, policy_loss: -69.02877, policy_entropy: -5.89432, alpha: 0.01471, time: 50.74269
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   275 ----
[CW] collect: return: 24.73225, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 7.69877, qf2_loss: 7.75207, policy_loss: -70.04419, policy_entropy: -7.08408, alpha: 0.01488, time: 50.66587
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   276 ----
[CW] collect: return: 183.57263, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 1.99244, qf2_loss: 2.04038, policy_loss: -69.57175, policy_entropy: -6.15814, alpha: 0.01542, time: 50.74211
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   277 ----
[CW] collect: return: 247.25996, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 1.58906, qf2_loss: 1.64472, policy_loss: -70.74436, policy_entropy: -5.89837, alpha: 0.01537, time: 50.76388
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   278 ----
[CW] collect: return: 45.91333, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 1.63598, qf2_loss: 1.69465, policy_loss: -70.05747, policy_entropy: -6.02532, alpha: 0.01536, time: 50.62194
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   279 ----
[CW] collect: return: 161.53632, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 1.59654, qf2_loss: 1.65154, policy_loss: -70.37148, policy_entropy: -6.16180, alpha: 0.01542, time: 50.46190
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   280 ----
[CW] collect: return: 201.77616, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 1.76192, qf2_loss: 1.82401, policy_loss: -70.60185, policy_entropy: -6.20647, alpha: 0.01554, time: 50.45496
[CW] eval: return: 158.82004, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   281 ----
[CW] collect: return: 214.20978, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 2.13607, qf2_loss: 2.20236, policy_loss: -70.54444, policy_entropy: -6.04672, alpha: 0.01562, time: 50.49205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   282 ----
[CW] collect: return: 166.74849, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 1.91570, qf2_loss: 1.96980, policy_loss: -70.91436, policy_entropy: -5.77320, alpha: 0.01555, time: 50.55110
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   283 ----
[CW] collect: return: 209.33888, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 1.79791, qf2_loss: 1.86515, policy_loss: -71.48004, policy_entropy: -5.91870, alpha: 0.01547, time: 50.67378
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   284 ----
[CW] collect: return: 260.01048, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 1.84970, qf2_loss: 1.89987, policy_loss: -71.31193, policy_entropy: -5.98533, alpha: 0.01540, time: 50.69950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   285 ----
[CW] collect: return: 141.66257, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 2.15479, qf2_loss: 2.23167, policy_loss: -71.15248, policy_entropy: -5.96524, alpha: 0.01543, time: 50.54834
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   286 ----
[CW] collect: return: 206.41021, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 2.38254, qf2_loss: 2.46727, policy_loss: -71.98082, policy_entropy: -6.20834, alpha: 0.01543, time: 50.72510
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   287 ----
[CW] collect: return: 204.46657, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 2.99655, qf2_loss: 3.05647, policy_loss: -71.67286, policy_entropy: -6.34379, alpha: 0.01564, time: 50.50663
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   288 ----
[CW] collect: return: 125.64315, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 2.45353, qf2_loss: 2.52886, policy_loss: -72.53348, policy_entropy: -6.65772, alpha: 0.01602, time: 51.13138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   289 ----
[CW] collect: return: 36.24004, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 1.88914, qf2_loss: 1.95543, policy_loss: -72.77910, policy_entropy: -6.35631, alpha: 0.01638, time: 50.57485
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   290 ----
[CW] collect: return: 152.97877, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 1.95833, qf2_loss: 2.03211, policy_loss: -71.84283, policy_entropy: -6.01997, alpha: 0.01655, time: 50.36496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   291 ----
[CW] collect: return: 111.21849, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 2.12203, qf2_loss: 2.20289, policy_loss: -71.24724, policy_entropy: -6.17303, alpha: 0.01662, time: 50.73658
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   292 ----
[CW] collect: return: 248.52959, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 2.00419, qf2_loss: 2.07262, policy_loss: -72.55617, policy_entropy: -6.15738, alpha: 0.01672, time: 50.68741
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   293 ----
[CW] collect: return: 107.91831, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 2.46688, qf2_loss: 2.52638, policy_loss: -72.44486, policy_entropy: -6.19652, alpha: 0.01692, time: 50.71590
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   294 ----
[CW] collect: return: 239.93845, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 6.62187, qf2_loss: 6.74064, policy_loss: -72.01746, policy_entropy: -5.32956, alpha: 0.01679, time: 50.62594
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   295 ----
[CW] collect: return: 140.60135, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 2.49207, qf2_loss: 2.55482, policy_loss: -73.09633, policy_entropy: -5.02119, alpha: 0.01614, time: 50.65336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   296 ----
[CW] collect: return: 262.93735, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 2.03134, qf2_loss: 2.09430, policy_loss: -73.09969, policy_entropy: -5.73415, alpha: 0.01562, time: 50.96136
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   297 ----
[CW] collect: return: 221.57048, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 1.90304, qf2_loss: 1.97449, policy_loss: -73.32444, policy_entropy: -6.84230, alpha: 0.01581, time: 51.00461
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   298 ----
[CW] collect: return: 249.51648, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 1.91593, qf2_loss: 1.98543, policy_loss: -72.78379, policy_entropy: -6.72563, alpha: 0.01634, time: 50.85595
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   299 ----
[CW] collect: return: 243.65664, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 1.90665, qf2_loss: 1.97932, policy_loss: -72.57128, policy_entropy: -6.54402, alpha: 0.01680, time: 50.88734
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   300 ----
[CW] collect: return: 138.08413, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 2.89451, qf2_loss: 2.95893, policy_loss: -73.10692, policy_entropy: -6.45395, alpha: 0.01713, time: 50.75008
[CW] eval: return: 209.63241, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   301 ----
[CW] collect: return: 215.19429, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 2.24297, qf2_loss: 2.31907, policy_loss: -73.29057, policy_entropy: -6.47090, alpha: 0.01746, time: 50.94587
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   302 ----
[CW] collect: return: 271.53157, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 1.92147, qf2_loss: 2.01215, policy_loss: -73.58190, policy_entropy: -6.43491, alpha: 0.01787, time: 51.02829
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   303 ----
[CW] collect: return: 233.57463, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 1.81931, qf2_loss: 1.88812, policy_loss: -73.84699, policy_entropy: -6.41235, alpha: 0.01819, time: 51.29210
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   304 ----
[CW] collect: return: 58.69887, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 2.21141, qf2_loss: 2.29429, policy_loss: -74.28815, policy_entropy: -6.09472, alpha: 0.01847, time: 50.74156
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   305 ----
[CW] collect: return: 235.39509, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 2.36017, qf2_loss: 2.45047, policy_loss: -74.29767, policy_entropy: -6.12232, alpha: 0.01859, time: 50.83449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   306 ----
[CW] collect: return: 247.89479, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 2.22963, qf2_loss: 2.33610, policy_loss: -73.79251, policy_entropy: -5.94067, alpha: 0.01856, time: 50.74357
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   307 ----
[CW] collect: return: 233.89023, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 2.01516, qf2_loss: 2.10131, policy_loss: -73.54935, policy_entropy: -5.48062, alpha: 0.01843, time: 50.64322
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   308 ----
[CW] collect: return: 251.86395, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 2.09396, qf2_loss: 2.16407, policy_loss: -73.87529, policy_entropy: -5.26975, alpha: 0.01781, time: 50.62521
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   309 ----
[CW] collect: return: 180.11177, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 2.17787, qf2_loss: 2.26127, policy_loss: -73.77907, policy_entropy: -5.44464, alpha: 0.01727, time: 50.71389
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   310 ----
[CW] collect: return: 249.40779, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 2.23211, qf2_loss: 2.32035, policy_loss: -73.75292, policy_entropy: -5.68022, alpha: 0.01695, time: 50.47182
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   311 ----
[CW] collect: return: 68.25635, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 3.65334, qf2_loss: 3.74284, policy_loss: -74.56218, policy_entropy: -5.84930, alpha: 0.01681, time: 50.68614
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   312 ----
[CW] collect: return: 245.29922, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 2.53099, qf2_loss: 2.62329, policy_loss: -74.31758, policy_entropy: -5.72785, alpha: 0.01660, time: 50.88732
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   313 ----
[CW] collect: return: 216.91644, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 3.64402, qf2_loss: 3.79065, policy_loss: -74.27187, policy_entropy: -6.35446, alpha: 0.01654, time: 50.79925
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   314 ----
[CW] collect: return: 142.01952, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 2.27089, qf2_loss: 2.34105, policy_loss: -74.59065, policy_entropy: -6.29539, alpha: 0.01690, time: 50.93092
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   315 ----
[CW] collect: return: 25.40150, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 2.22409, qf2_loss: 2.33102, policy_loss: -74.66291, policy_entropy: -6.02605, alpha: 0.01695, time: 51.65643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   316 ----
[CW] collect: return: 249.55742, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 2.05734, qf2_loss: 2.13404, policy_loss: -74.86688, policy_entropy: -5.83231, alpha: 0.01696, time: 50.73312
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   317 ----
[CW] collect: return: 242.91132, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 2.16008, qf2_loss: 2.23954, policy_loss: -75.63340, policy_entropy: -5.91187, alpha: 0.01684, time: 50.97834
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   318 ----
[CW] collect: return: 207.85290, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 2.00054, qf2_loss: 2.09107, policy_loss: -75.12203, policy_entropy: -6.13787, alpha: 0.01687, time: 50.78166
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   319 ----
[CW] collect: return: 176.98345, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 2.10076, qf2_loss: 2.18678, policy_loss: -75.30669, policy_entropy: -5.78807, alpha: 0.01681, time: 50.57105
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   320 ----
[CW] collect: return: 239.85295, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 2.02422, qf2_loss: 2.12909, policy_loss: -74.66122, policy_entropy: -5.90310, alpha: 0.01673, time: 50.73235
[CW] eval: return: 196.17924, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   321 ----
[CW] collect: return: 224.62121, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 2.57952, qf2_loss: 2.67666, policy_loss: -74.92083, policy_entropy: -5.86757, alpha: 0.01664, time: 50.77961
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   322 ----
[CW] collect: return: 198.64044, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 2.73030, qf2_loss: 2.81219, policy_loss: -75.74427, policy_entropy: -6.08881, alpha: 0.01658, time: 50.61091
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   323 ----
[CW] collect: return: 192.17099, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 2.21903, qf2_loss: 2.30198, policy_loss: -75.07541, policy_entropy: -5.89926, alpha: 0.01658, time: 50.73893
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   324 ----
[CW] collect: return: 216.69545, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 1.90684, qf2_loss: 1.98307, policy_loss: -75.48116, policy_entropy: -5.93700, alpha: 0.01651, time: 50.43434
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   325 ----
[CW] collect: return: 137.12170, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 2.11953, qf2_loss: 2.22479, policy_loss: -75.84339, policy_entropy: -6.30866, alpha: 0.01662, time: 50.40727
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   326 ----
[CW] collect: return: 32.22349, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 3.53438, qf2_loss: 3.64750, policy_loss: -75.54278, policy_entropy: -6.50648, alpha: 0.01696, time: 50.76196
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   327 ----
[CW] collect: return: 187.73371, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 6.04297, qf2_loss: 6.10969, policy_loss: -74.91171, policy_entropy: -6.35614, alpha: 0.01726, time: 50.57890
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   328 ----
[CW] collect: return: 192.06699, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 3.33332, qf2_loss: 3.45070, policy_loss: -75.73097, policy_entropy: -6.57520, alpha: 0.01782, time: 50.80205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   329 ----
[CW] collect: return: 1.91110, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 2.14385, qf2_loss: 2.25269, policy_loss: -75.43032, policy_entropy: -6.21718, alpha: 0.01816, time: 50.41532
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   330 ----
[CW] collect: return: 142.14994, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 3.39104, qf2_loss: 3.49933, policy_loss: -75.42219, policy_entropy: -5.90415, alpha: 0.01825, time: 50.47053
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   331 ----
[CW] collect: return: 220.37966, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 2.04330, qf2_loss: 2.16063, policy_loss: -76.34272, policy_entropy: -5.85856, alpha: 0.01807, time: 50.56903
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   332 ----
[CW] collect: return: 64.61033, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 1.69060, qf2_loss: 1.76833, policy_loss: -75.87825, policy_entropy: -6.16049, alpha: 0.01812, time: 50.59350
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   333 ----
[CW] collect: return: 183.24338, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 1.71482, qf2_loss: 1.79737, policy_loss: -75.40973, policy_entropy: -6.05411, alpha: 0.01828, time: 50.75168
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   334 ----
[CW] collect: return: 254.99387, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 1.69165, qf2_loss: 1.79603, policy_loss: -75.35384, policy_entropy: -5.90595, alpha: 0.01823, time: 50.59833
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   335 ----
[CW] collect: return: 52.18587, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 1.76518, qf2_loss: 1.87530, policy_loss: -75.56823, policy_entropy: -6.04055, alpha: 0.01815, time: 50.85578
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   336 ----
[CW] collect: return: 225.50613, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 1.79670, qf2_loss: 1.89466, policy_loss: -76.46519, policy_entropy: -6.08335, alpha: 0.01829, time: 50.90034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   337 ----
[CW] collect: return: 173.88098, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 1.94316, qf2_loss: 2.07485, policy_loss: -76.09455, policy_entropy: -6.28996, alpha: 0.01844, time: 50.55051
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   338 ----
[CW] collect: return: 55.94398, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 1.86379, qf2_loss: 1.97179, policy_loss: -75.81019, policy_entropy: -6.07692, alpha: 0.01871, time: 50.41766
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   339 ----
[CW] collect: return: 243.28753, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 2.82022, qf2_loss: 2.91703, policy_loss: -76.34430, policy_entropy: -5.96112, alpha: 0.01877, time: 50.90549
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   340 ----
[CW] collect: return: 255.63061, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 7.12558, qf2_loss: 7.34986, policy_loss: -75.16514, policy_entropy: -5.75618, alpha: 0.01859, time: 50.77706
[CW] eval: return: 228.86710, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   341 ----
[CW] collect: return: 175.11001, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 4.12928, qf2_loss: 4.21767, policy_loss: -75.75203, policy_entropy: -5.44578, alpha: 0.01814, time: 51.95076
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   342 ----
[CW] collect: return: 50.52833, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 2.36408, qf2_loss: 2.46610, policy_loss: -76.51012, policy_entropy: -6.23797, alpha: 0.01790, time: 50.62820
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   343 ----
[CW] collect: return: 143.00503, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 1.78552, qf2_loss: 1.89110, policy_loss: -75.17977, policy_entropy: -6.32886, alpha: 0.01821, time: 50.36071
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   344 ----
[CW] collect: return: 209.86867, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 2.00156, qf2_loss: 2.10672, policy_loss: -75.32312, policy_entropy: -6.14204, alpha: 0.01853, time: 50.58596
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   345 ----
[CW] collect: return: 275.76021, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 3.14298, qf2_loss: 3.26516, policy_loss: -76.11544, policy_entropy: -6.20365, alpha: 0.01866, time: 50.75680
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   346 ----
[CW] collect: return: 165.71393, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 2.54742, qf2_loss: 2.67071, policy_loss: -75.62173, policy_entropy: -6.25721, alpha: 0.01899, time: 50.50691
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   347 ----
[CW] collect: return: 286.25975, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 1.85536, qf2_loss: 1.97118, policy_loss: -76.23741, policy_entropy: -6.07117, alpha: 0.01917, time: 50.61586
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   348 ----
[CW] collect: return: 94.92170, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 1.92060, qf2_loss: 2.01871, policy_loss: -76.25848, policy_entropy: -6.07560, alpha: 0.01929, time: 50.55038
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   349 ----
[CW] collect: return: 207.85215, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 2.05748, qf2_loss: 2.15824, policy_loss: -75.87670, policy_entropy: -6.00088, alpha: 0.01936, time: 50.44260
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   350 ----
[CW] collect: return: 303.80085, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 1.90612, qf2_loss: 2.01753, policy_loss: -75.60568, policy_entropy: -6.18810, alpha: 0.01943, time: 50.59633
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   351 ----
[CW] collect: return: 58.07877, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 1.92329, qf2_loss: 2.06534, policy_loss: -76.18314, policy_entropy: -6.20581, alpha: 0.01975, time: 50.44468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   352 ----
[CW] collect: return: 299.99935, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 2.07084, qf2_loss: 2.18870, policy_loss: -75.45575, policy_entropy: -5.94978, alpha: 0.01992, time: 50.38128
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   353 ----
[CW] collect: return: 88.87450, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 2.08963, qf2_loss: 2.20896, policy_loss: -76.03950, policy_entropy: -6.00028, alpha: 0.01992, time: 50.41279
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   354 ----
[CW] collect: return: 54.30223, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 2.39242, qf2_loss: 2.50644, policy_loss: -76.27024, policy_entropy: -5.93991, alpha: 0.01979, time: 50.47979
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   355 ----
[CW] collect: return: 115.94338, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 4.20836, qf2_loss: 4.30588, policy_loss: -75.18159, policy_entropy: -5.99652, alpha: 0.01970, time: 50.41937
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   356 ----
[CW] collect: return: 281.21786, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 3.61298, qf2_loss: 3.76497, policy_loss: -76.22590, policy_entropy: -6.23011, alpha: 0.01977, time: 50.73203
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   357 ----
[CW] collect: return: 95.52685, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 2.23943, qf2_loss: 2.35302, policy_loss: -75.96522, policy_entropy: -5.93387, alpha: 0.02017, time: 50.82857
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   358 ----
[CW] collect: return: 198.80338, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 2.01155, qf2_loss: 2.12338, policy_loss: -76.27980, policy_entropy: -5.34896, alpha: 0.01960, time: 50.82134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   359 ----
[CW] collect: return: 115.44191, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 2.55941, qf2_loss: 2.66830, policy_loss: -75.43438, policy_entropy: -5.53446, alpha: 0.01880, time: 51.12301
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   360 ----
[CW] collect: return: 223.32743, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 4.11585, qf2_loss: 4.25955, policy_loss: -76.27836, policy_entropy: -5.39925, alpha: 0.01831, time: 50.63518
[CW] eval: return: 199.14394, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   361 ----
[CW] collect: return: 259.81223, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 3.40994, qf2_loss: 3.53325, policy_loss: -76.00909, policy_entropy: -5.97693, alpha: 0.01797, time: 50.75406
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   362 ----
[CW] collect: return: 171.16493, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 2.20095, qf2_loss: 2.35276, policy_loss: -76.99780, policy_entropy: -6.63374, alpha: 0.01824, time: 50.56871
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   363 ----
[CW] collect: return: 117.53173, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 2.11447, qf2_loss: 2.23433, policy_loss: -76.34829, policy_entropy: -6.17451, alpha: 0.01867, time: 50.82034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   364 ----
[CW] collect: return: 234.88511, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 2.02539, qf2_loss: 2.15209, policy_loss: -76.95951, policy_entropy: -6.11473, alpha: 0.01883, time: 50.95674
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   365 ----
[CW] collect: return: 241.98772, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 2.19268, qf2_loss: 2.29597, policy_loss: -75.37872, policy_entropy: -5.63679, alpha: 0.01867, time: 50.80732
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   366 ----
[CW] collect: return: 289.21801, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 2.28511, qf2_loss: 2.42624, policy_loss: -76.01628, policy_entropy: -5.99867, alpha: 0.01850, time: 50.66127
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   367 ----
[CW] collect: return: 218.67149, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 2.32046, qf2_loss: 2.44866, policy_loss: -76.99524, policy_entropy: -6.34022, alpha: 0.01861, time: 50.67963
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   368 ----
[CW] collect: return: 315.24348, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 3.09180, qf2_loss: 3.23716, policy_loss: -76.91712, policy_entropy: -6.09644, alpha: 0.01888, time: 50.51307
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   369 ----
[CW] collect: return: 181.69255, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 3.63268, qf2_loss: 3.71989, policy_loss: -77.30499, policy_entropy: -6.40149, alpha: 0.01906, time: 50.50952
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   370 ----
[CW] collect: return: 323.66179, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 3.51130, qf2_loss: 3.63000, policy_loss: -77.29768, policy_entropy: -6.02968, alpha: 0.01939, time: 50.48918
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   371 ----
[CW] collect: return: 268.55400, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 2.02803, qf2_loss: 2.15766, policy_loss: -77.57538, policy_entropy: -6.43886, alpha: 0.01961, time: 50.81317
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   372 ----
[CW] collect: return: 195.62399, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 2.04167, qf2_loss: 2.14809, policy_loss: -77.45205, policy_entropy: -6.25521, alpha: 0.02004, time: 50.51121
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   373 ----
[CW] collect: return: 297.64166, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 2.02239, qf2_loss: 2.13546, policy_loss: -77.20698, policy_entropy: -6.17161, alpha: 0.02030, time: 50.32754
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   374 ----
[CW] collect: return: 109.95794, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 2.02322, qf2_loss: 2.12461, policy_loss: -77.78438, policy_entropy: -5.94562, alpha: 0.02042, time: 50.53309
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   375 ----
[CW] collect: return: 78.84513, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 2.49005, qf2_loss: 2.58131, policy_loss: -77.48788, policy_entropy: -5.86402, alpha: 0.02031, time: 50.65937
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   376 ----
[CW] collect: return: 219.35334, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 2.62517, qf2_loss: 2.76580, policy_loss: -76.94693, policy_entropy: -5.26859, alpha: 0.01986, time: 51.28834
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   377 ----
[CW] collect: return: 321.63497, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 2.36539, qf2_loss: 2.50028, policy_loss: -77.74424, policy_entropy: -5.22409, alpha: 0.01892, time: 51.68395
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   378 ----
[CW] collect: return: 326.69027, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 3.05288, qf2_loss: 3.14725, policy_loss: -79.02813, policy_entropy: -5.51728, alpha: 0.01832, time: 50.63971
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   379 ----
[CW] collect: return: 322.07473, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 2.38447, qf2_loss: 2.49715, policy_loss: -78.04501, policy_entropy: -5.29132, alpha: 0.01782, time: 50.59042
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   380 ----
[CW] collect: return: 347.84578, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 2.60077, qf2_loss: 2.73316, policy_loss: -77.93732, policy_entropy: -5.41276, alpha: 0.01730, time: 50.76673
[CW] eval: return: 239.57597, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   381 ----
[CW] collect: return: 169.94149, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 2.75082, qf2_loss: 2.86598, policy_loss: -78.27556, policy_entropy: -5.46438, alpha: 0.01690, time: 50.68826
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   382 ----
[CW] collect: return: 237.32738, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 2.43977, qf2_loss: 2.53125, policy_loss: -78.47459, policy_entropy: -6.01039, alpha: 0.01669, time: 50.49633
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   383 ----
[CW] collect: return: 229.32024, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 2.13635, qf2_loss: 2.22740, policy_loss: -78.51240, policy_entropy: -5.99706, alpha: 0.01670, time: 50.56905
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   384 ----
[CW] collect: return: 229.41597, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 2.48697, qf2_loss: 2.57996, policy_loss: -78.63000, policy_entropy: -6.05786, alpha: 0.01670, time: 50.60273
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   385 ----
[CW] collect: return: 153.93434, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 2.43131, qf2_loss: 2.53595, policy_loss: -78.93864, policy_entropy: -6.23135, alpha: 0.01682, time: 50.44235
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   386 ----
[CW] collect: return: 296.89739, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 4.05798, qf2_loss: 4.18863, policy_loss: -79.13111, policy_entropy: -6.05859, alpha: 0.01689, time: 50.69854
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   387 ----
[CW] collect: return: 55.13092, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 3.48893, qf2_loss: 3.58632, policy_loss: -78.83478, policy_entropy: -6.48441, alpha: 0.01717, time: 50.53683
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   388 ----
[CW] collect: return: 318.51431, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 2.74456, qf2_loss: 2.85567, policy_loss: -77.98749, policy_entropy: -6.18853, alpha: 0.01744, time: 50.52244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   389 ----
[CW] collect: return: 262.90681, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 4.89041, qf2_loss: 4.99720, policy_loss: -78.06831, policy_entropy: -6.12187, alpha: 0.01749, time: 50.66990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   390 ----
[CW] collect: return: 314.22502, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 2.89254, qf2_loss: 3.03388, policy_loss: -78.60115, policy_entropy: -5.92315, alpha: 0.01755, time: 50.38127
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   391 ----
[CW] collect: return: 301.98168, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 2.44985, qf2_loss: 2.56553, policy_loss: -79.13948, policy_entropy: -6.17326, alpha: 0.01756, time: 50.41466
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   392 ----
[CW] collect: return: 292.95245, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 2.13542, qf2_loss: 2.23407, policy_loss: -78.44417, policy_entropy: -6.20217, alpha: 0.01768, time: 50.40325
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   393 ----
[CW] collect: return: 267.56990, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 2.48250, qf2_loss: 2.57363, policy_loss: -79.06838, policy_entropy: -6.41953, alpha: 0.01801, time: 50.41265
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   394 ----
[CW] collect: return: 298.28621, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 4.31902, qf2_loss: 4.39139, policy_loss: -79.96263, policy_entropy: -6.54698, alpha: 0.01847, time: 50.56073
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   395 ----
[CW] collect: return: 276.17499, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 6.10510, qf2_loss: 6.44597, policy_loss: -78.01634, policy_entropy: -6.74892, alpha: 0.01900, time: 50.40989
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   396 ----
[CW] collect: return: 116.27829, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 3.33977, qf2_loss: 3.44776, policy_loss: -78.70946, policy_entropy: -6.56669, alpha: 0.01966, time: 50.29983
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   397 ----
[CW] collect: return: 262.84907, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 2.13581, qf2_loss: 2.23669, policy_loss: -78.77782, policy_entropy: -6.32023, alpha: 0.02010, time: 50.49830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   398 ----
[CW] collect: return: 255.85479, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 1.93210, qf2_loss: 2.02535, policy_loss: -78.98884, policy_entropy: -6.11718, alpha: 0.02041, time: 52.37445
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   399 ----
[CW] collect: return: 300.79956, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 2.10640, qf2_loss: 2.20805, policy_loss: -79.10018, policy_entropy: -5.96447, alpha: 0.02047, time: 50.16032
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   400 ----
[CW] collect: return: 117.27966, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 2.47851, qf2_loss: 2.58336, policy_loss: -79.75657, policy_entropy: -6.18261, alpha: 0.02048, time: 50.34130
[CW] eval: return: 221.85077, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   401 ----
[CW] collect: return: 303.67260, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 8.41144, qf2_loss: 8.49930, policy_loss: -79.57359, policy_entropy: -6.07349, alpha: 0.02062, time: 50.56086
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   402 ----
[CW] collect: return: 63.63348, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 4.40518, qf2_loss: 4.56128, policy_loss: -79.36982, policy_entropy: -6.21463, alpha: 0.02087, time: 50.29575
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   403 ----
[CW] collect: return: 328.46760, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 2.52588, qf2_loss: 2.63547, policy_loss: -79.04894, policy_entropy: -5.42864, alpha: 0.02072, time: 50.56406
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   404 ----
[CW] collect: return: 122.01569, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 2.72624, qf2_loss: 2.82966, policy_loss: -79.82843, policy_entropy: -5.56499, alpha: 0.02012, time: 50.67499
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   405 ----
[CW] collect: return: 320.20066, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 2.54291, qf2_loss: 2.64018, policy_loss: -79.30171, policy_entropy: -5.78079, alpha: 0.01971, time: 50.13704
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   406 ----
[CW] collect: return: 319.89370, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 2.15018, qf2_loss: 2.25099, policy_loss: -80.53274, policy_entropy: -5.80612, alpha: 0.01949, time: 50.55062
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   407 ----
[CW] collect: return: 125.25408, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 2.05672, qf2_loss: 2.16979, policy_loss: -79.76852, policy_entropy: -5.80201, alpha: 0.01932, time: 50.59647
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   408 ----
[CW] collect: return: 156.29786, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 2.12679, qf2_loss: 2.21779, policy_loss: -79.72331, policy_entropy: -5.85369, alpha: 0.01924, time: 50.50287
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   409 ----
[CW] collect: return: 111.01667, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 2.37210, qf2_loss: 2.48924, policy_loss: -79.82392, policy_entropy: -6.07013, alpha: 0.01911, time: 50.48672
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   410 ----
[CW] collect: return: 136.45552, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 2.51536, qf2_loss: 2.63372, policy_loss: -79.54267, policy_entropy: -5.79866, alpha: 0.01901, time: 50.48383
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   411 ----
[CW] collect: return: 286.39759, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 2.44957, qf2_loss: 2.59350, policy_loss: -79.81618, policy_entropy: -5.94361, alpha: 0.01893, time: 50.43963
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   412 ----
[CW] collect: return: 99.25230, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 5.28530, qf2_loss: 5.40610, policy_loss: -80.71140, policy_entropy: -5.97382, alpha: 0.01894, time: 50.51739
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   413 ----
[CW] collect: return: 106.33749, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 3.98625, qf2_loss: 4.15176, policy_loss: -80.09081, policy_entropy: -5.73227, alpha: 0.01878, time: 50.37618
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   414 ----
[CW] collect: return: 47.47369, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 3.22071, qf2_loss: 3.35622, policy_loss: -81.14905, policy_entropy: -6.04998, alpha: 0.01861, time: 50.21512
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   415 ----
[CW] collect: return: 142.63411, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 3.73980, qf2_loss: 3.80879, policy_loss: -79.49636, policy_entropy: -6.05504, alpha: 0.01869, time: 50.38194
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   416 ----
[CW] collect: return: 127.68932, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 2.42493, qf2_loss: 2.55533, policy_loss: -80.27243, policy_entropy: -6.05104, alpha: 0.01875, time: 50.32422
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   417 ----
[CW] collect: return: 194.06238, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 2.29770, qf2_loss: 2.41960, policy_loss: -80.36659, policy_entropy: -5.95424, alpha: 0.01877, time: 50.24052
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   418 ----
[CW] collect: return: 336.80345, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 2.46684, qf2_loss: 2.59265, policy_loss: -81.09487, policy_entropy: -5.98767, alpha: 0.01871, time: 50.36836
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   419 ----
[CW] collect: return: 287.24762, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 2.77223, qf2_loss: 2.89864, policy_loss: -80.37906, policy_entropy: -5.90926, alpha: 0.01868, time: 52.08565
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   420 ----
[CW] collect: return: 75.97929, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 3.29252, qf2_loss: 3.42692, policy_loss: -82.02641, policy_entropy: -5.97037, alpha: 0.01862, time: 50.28806
[CW] eval: return: 228.57875, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   421 ----
[CW] collect: return: 123.68457, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 2.57491, qf2_loss: 2.70223, policy_loss: -80.71262, policy_entropy: -5.96260, alpha: 0.01860, time: 50.94665
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   422 ----
[CW] collect: return: 303.03257, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 2.95801, qf2_loss: 3.08194, policy_loss: -79.86010, policy_entropy: -5.80755, alpha: 0.01852, time: 50.22349
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   423 ----
[CW] collect: return: 296.43672, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 2.92209, qf2_loss: 3.04142, policy_loss: -80.71879, policy_entropy: -6.18350, alpha: 0.01844, time: 50.27122
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   424 ----
[CW] collect: return: 111.98673, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 2.76165, qf2_loss: 2.88630, policy_loss: -81.02770, policy_entropy: -6.25662, alpha: 0.01870, time: 51.09305
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   425 ----
[CW] collect: return: 233.05497, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 2.71965, qf2_loss: 2.85204, policy_loss: -80.09650, policy_entropy: -6.01627, alpha: 0.01883, time: 50.19735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   426 ----
[CW] collect: return: 150.15180, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 2.55521, qf2_loss: 2.67375, policy_loss: -81.99646, policy_entropy: -5.88873, alpha: 0.01879, time: 50.39883
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   427 ----
[CW] collect: return: 208.19159, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 4.82184, qf2_loss: 5.02614, policy_loss: -80.63166, policy_entropy: -5.88818, alpha: 0.01865, time: 50.45312
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   428 ----
[CW] collect: return: 66.72034, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 6.39619, qf2_loss: 6.49662, policy_loss: -80.41992, policy_entropy: -5.45865, alpha: 0.01840, time: 50.44466
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   429 ----
[CW] collect: return: 278.57456, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 2.99159, qf2_loss: 3.11319, policy_loss: -80.43406, policy_entropy: -5.76505, alpha: 0.01800, time: 50.66323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   430 ----
[CW] collect: return: 331.16670, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 2.91986, qf2_loss: 3.06514, policy_loss: -80.31353, policy_entropy: -5.94452, alpha: 0.01788, time: 50.60462
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   431 ----
[CW] collect: return: 212.26351, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 4.45460, qf2_loss: 4.62976, policy_loss: -80.68676, policy_entropy: -5.90879, alpha: 0.01778, time: 50.49630
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   432 ----
[CW] collect: return: 232.23482, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 8.59421, qf2_loss: 8.82481, policy_loss: -81.07548, policy_entropy: -6.39722, alpha: 0.01792, time: 50.94041
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   433 ----
[CW] collect: return: 146.84855, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 4.57239, qf2_loss: 4.71529, policy_loss: -80.63937, policy_entropy: -6.25165, alpha: 0.01823, time: 50.61250
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   434 ----
[CW] collect: return: 169.92590, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 4.10668, qf2_loss: 4.27756, policy_loss: -80.63543, policy_entropy: -5.87968, alpha: 0.01830, time: 50.59630
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   435 ----
[CW] collect: return: 294.89688, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 3.00707, qf2_loss: 3.17992, policy_loss: -81.49540, policy_entropy: -5.56571, alpha: 0.01808, time: 50.69378
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   436 ----
[CW] collect: return: 275.67942, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 2.98415, qf2_loss: 3.13158, policy_loss: -82.03759, policy_entropy: -5.73984, alpha: 0.01775, time: 50.57258
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   437 ----
[CW] collect: return: 274.45249, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 2.50377, qf2_loss: 2.61423, policy_loss: -81.51034, policy_entropy: -5.84459, alpha: 0.01763, time: 50.85358
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   438 ----
[CW] collect: return: 110.73501, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 3.13645, qf2_loss: 3.27514, policy_loss: -81.38518, policy_entropy: -5.85436, alpha: 0.01746, time: 51.00624
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   439 ----
[CW] collect: return: 71.14658, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 2.68896, qf2_loss: 2.82541, policy_loss: -81.34273, policy_entropy: -5.96847, alpha: 0.01743, time: 50.49368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   440 ----
[CW] collect: return: 209.41402, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 2.80964, qf2_loss: 2.95423, policy_loss: -80.05698, policy_entropy: -5.96501, alpha: 0.01741, time: 50.73601
[CW] eval: return: 243.00691, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   441 ----
[CW] collect: return: 107.17116, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 2.67731, qf2_loss: 2.83925, policy_loss: -80.97019, policy_entropy: -6.10532, alpha: 0.01738, time: 50.95352
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   442 ----
[CW] collect: return: 247.76514, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 2.64655, qf2_loss: 2.77158, policy_loss: -81.11680, policy_entropy: -6.27577, alpha: 0.01755, time: 50.91863
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   443 ----
[CW] collect: return: 313.39166, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 3.71119, qf2_loss: 3.86938, policy_loss: -81.31683, policy_entropy: -5.89050, alpha: 0.01764, time: 50.61272
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   444 ----
[CW] collect: return: 155.22117, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 12.36784, qf2_loss: 12.57007, policy_loss: -81.75170, policy_entropy: -6.08658, alpha: 0.01757, time: 50.75828
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   445 ----
[CW] collect: return: 267.29475, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 4.67526, qf2_loss: 4.75561, policy_loss: -81.38162, policy_entropy: -6.43329, alpha: 0.01779, time: 50.52193
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   446 ----
[CW] collect: return: 268.18816, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 2.75561, qf2_loss: 2.90728, policy_loss: -82.90636, policy_entropy: -6.26101, alpha: 0.01807, time: 50.59032
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   447 ----
[CW] collect: return: 287.50200, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 2.66604, qf2_loss: 2.81080, policy_loss: -82.00825, policy_entropy: -6.01481, alpha: 0.01821, time: 50.98360
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   448 ----
[CW] collect: return: 34.80313, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 3.87891, qf2_loss: 3.99820, policy_loss: -81.99935, policy_entropy: -5.95858, alpha: 0.01815, time: 50.58364
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   449 ----
[CW] collect: return: 334.56896, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 2.71146, qf2_loss: 2.83314, policy_loss: -81.64652, policy_entropy: -5.80148, alpha: 0.01808, time: 50.75011
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   450 ----
[CW] collect: return: 254.30768, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 2.61693, qf2_loss: 2.73324, policy_loss: -81.86999, policy_entropy: -5.68294, alpha: 0.01790, time: 50.76670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   451 ----
[CW] collect: return: 308.32828, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 2.62777, qf2_loss: 2.76106, policy_loss: -82.14458, policy_entropy: -5.64486, alpha: 0.01766, time: 50.72163
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   452 ----
[CW] collect: return: 234.37372, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 2.47458, qf2_loss: 2.59292, policy_loss: -82.50578, policy_entropy: -6.05456, alpha: 0.01749, time: 50.37398
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   453 ----
[CW] collect: return: 33.49055, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 2.86299, qf2_loss: 2.97106, policy_loss: -81.69497, policy_entropy: -5.79841, alpha: 0.01748, time: 50.37396
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   454 ----
[CW] collect: return: 349.91083, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 2.88047, qf2_loss: 3.00324, policy_loss: -82.87393, policy_entropy: -6.03852, alpha: 0.01739, time: 50.48219
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   455 ----
[CW] collect: return: 311.08688, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 2.59168, qf2_loss: 2.74328, policy_loss: -83.19037, policy_entropy: -6.09430, alpha: 0.01742, time: 50.57253
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   456 ----
[CW] collect: return: 288.85017, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 2.78824, qf2_loss: 2.93270, policy_loss: -82.39682, policy_entropy: -5.99179, alpha: 0.01744, time: 50.91048
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   457 ----
[CW] collect: return: 149.71278, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 3.65143, qf2_loss: 3.81571, policy_loss: -83.17424, policy_entropy: -6.58048, alpha: 0.01768, time: 50.69095
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   458 ----
[CW] collect: return: 235.40895, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 4.22752, qf2_loss: 4.40835, policy_loss: -83.19637, policy_entropy: -6.29188, alpha: 0.01796, time: 50.68110
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   459 ----
[CW] collect: return: 25.44951, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 3.14611, qf2_loss: 3.31579, policy_loss: -82.13481, policy_entropy: -5.91984, alpha: 0.01813, time: 50.74534
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   460 ----
[CW] collect: return: 338.99051, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 3.77680, qf2_loss: 3.82890, policy_loss: -82.71871, policy_entropy: -5.94708, alpha: 0.01806, time: 50.75896
[CW] eval: return: 195.26932, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   461 ----
[CW] collect: return: 191.31499, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 4.24979, qf2_loss: 4.45456, policy_loss: -82.81699, policy_entropy: -5.97044, alpha: 0.01801, time: 50.56796
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   462 ----
[CW] collect: return: 234.44413, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 7.05448, qf2_loss: 7.26025, policy_loss: -82.00903, policy_entropy: -6.59438, alpha: 0.01815, time: 50.62266
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   463 ----
[CW] collect: return: 194.47044, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 4.99039, qf2_loss: 5.14910, policy_loss: -83.40992, policy_entropy: -6.48404, alpha: 0.01872, time: 50.59240
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   464 ----
[CW] collect: return: 294.44569, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 8.16704, qf2_loss: 8.37112, policy_loss: -83.95696, policy_entropy: -6.13236, alpha: 0.01887, time: 50.50271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   465 ----
[CW] collect: return: 265.71504, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 5.45545, qf2_loss: 5.72600, policy_loss: -83.30027, policy_entropy: -6.15127, alpha: 0.01903, time: 50.66736
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   466 ----
[CW] collect: return: 306.38446, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 3.31136, qf2_loss: 3.43835, policy_loss: -84.50676, policy_entropy: -6.08105, alpha: 0.01919, time: 50.92643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   467 ----
[CW] collect: return: 254.61969, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 2.84064, qf2_loss: 2.96593, policy_loss: -82.90152, policy_entropy: -5.78173, alpha: 0.01910, time: 50.71769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   468 ----
[CW] collect: return: 317.79826, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 2.69597, qf2_loss: 2.81468, policy_loss: -83.92863, policy_entropy: -5.85586, alpha: 0.01895, time: 50.56579
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   469 ----
[CW] collect: return: 144.63255, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 2.50791, qf2_loss: 2.63937, policy_loss: -83.32657, policy_entropy: -5.94507, alpha: 0.01886, time: 50.46193
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   470 ----
[CW] collect: return: 300.91401, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 6.91463, qf2_loss: 7.11427, policy_loss: -85.21113, policy_entropy: -5.94320, alpha: 0.01883, time: 50.63296
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   471 ----
[CW] collect: return: 83.55800, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 15.32184, qf2_loss: 15.44465, policy_loss: -82.22061, policy_entropy: -7.25864, alpha: 0.01919, time: 50.87830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   472 ----
[CW] collect: return: 180.60093, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 3.70038, qf2_loss: 3.92906, policy_loss: -82.77768, policy_entropy: -6.54818, alpha: 0.02001, time: 50.59824
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   473 ----
[CW] collect: return: 344.52970, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 3.39739, qf2_loss: 3.55748, policy_loss: -84.03311, policy_entropy: -6.18302, alpha: 0.02024, time: 50.44026
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   474 ----
[CW] collect: return: 155.42171, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 2.83219, qf2_loss: 2.96422, policy_loss: -82.95096, policy_entropy: -5.71902, alpha: 0.02024, time: 50.61542
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   475 ----
[CW] collect: return: 247.16923, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 2.77043, qf2_loss: 2.89770, policy_loss: -83.02348, policy_entropy: -5.88368, alpha: 0.02005, time: 50.67784
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   476 ----
[CW] collect: return: 24.69705, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 2.63325, qf2_loss: 2.75310, policy_loss: -84.97587, policy_entropy: -6.12200, alpha: 0.02002, time: 50.69242
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   477 ----
[CW] collect: return: 314.35805, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 2.71442, qf2_loss: 2.83525, policy_loss: -84.50678, policy_entropy: -6.25359, alpha: 0.02020, time: 50.55899
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   478 ----
[CW] collect: return: 310.21071, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 2.63042, qf2_loss: 2.77398, policy_loss: -84.65203, policy_entropy: -5.87118, alpha: 0.02028, time: 50.48413
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   479 ----
[CW] collect: return: 317.19747, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 2.74683, qf2_loss: 2.88399, policy_loss: -84.21149, policy_entropy: -6.11668, alpha: 0.02023, time: 50.63909
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   480 ----
[CW] collect: return: 274.61569, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 2.75838, qf2_loss: 2.85917, policy_loss: -84.45371, policy_entropy: -6.39954, alpha: 0.02041, time: 50.67507
[CW] eval: return: 220.55477, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   481 ----
[CW] collect: return: 266.52017, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 2.97566, qf2_loss: 3.15662, policy_loss: -84.52460, policy_entropy: -6.60362, alpha: 0.02085, time: 50.53196
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   482 ----
[CW] collect: return: 243.74166, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 3.44319, qf2_loss: 3.58235, policy_loss: -83.97328, policy_entropy: -6.56995, alpha: 0.02143, time: 50.37571
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   483 ----
[CW] collect: return: 56.12244, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 3.00033, qf2_loss: 3.13810, policy_loss: -85.22941, policy_entropy: -6.49051, alpha: 0.02198, time: 50.52105
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   484 ----
[CW] collect: return: 89.03467, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 10.90319, qf2_loss: 11.10438, policy_loss: -84.20864, policy_entropy: -6.80847, alpha: 0.02246, time: 50.76018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   485 ----
[CW] collect: return: 265.71428, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 4.23699, qf2_loss: 4.44010, policy_loss: -85.04654, policy_entropy: -6.52875, alpha: 0.02336, time: 50.48486
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   486 ----
[CW] collect: return: 293.26924, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 3.29689, qf2_loss: 3.44350, policy_loss: -85.12432, policy_entropy: -5.78933, alpha: 0.02355, time: 50.75238
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   487 ----
[CW] collect: return: 197.98633, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 3.00669, qf2_loss: 3.12893, policy_loss: -85.04389, policy_entropy: -5.83506, alpha: 0.02328, time: 51.11314
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   488 ----
[CW] collect: return: 261.91693, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 2.77652, qf2_loss: 2.88931, policy_loss: -85.98288, policy_entropy: -5.73850, alpha: 0.02307, time: 50.79525
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   489 ----
[CW] collect: return: 27.13832, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 2.93911, qf2_loss: 3.10136, policy_loss: -84.49677, policy_entropy: -5.67904, alpha: 0.02275, time: 50.85517
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   490 ----
[CW] collect: return: 21.23580, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 2.99767, qf2_loss: 3.17640, policy_loss: -85.56165, policy_entropy: -5.69576, alpha: 0.02244, time: 50.62198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   491 ----
[CW] collect: return: 24.72839, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 3.05235, qf2_loss: 3.18724, policy_loss: -85.29096, policy_entropy: -5.42290, alpha: 0.02203, time: 50.61865
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   492 ----
[CW] collect: return: 26.93857, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 3.16973, qf2_loss: 3.30409, policy_loss: -84.25665, policy_entropy: -5.08436, alpha: 0.02140, time: 52.75418
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   493 ----
[CW] collect: return: 20.37084, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 3.72403, qf2_loss: 3.83054, policy_loss: -84.66610, policy_entropy: -5.19726, alpha: 0.02059, time: 50.74415
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   494 ----
[CW] collect: return: 266.13456, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 4.66585, qf2_loss: 4.85236, policy_loss: -84.54968, policy_entropy: -4.99772, alpha: 0.01994, time: 50.61142
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   495 ----
[CW] collect: return: 35.23269, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 4.21644, qf2_loss: 4.47890, policy_loss: -85.60425, policy_entropy: -5.17936, alpha: 0.01928, time: 50.60807
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   496 ----
[CW] collect: return: 35.99715, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 3.91079, qf2_loss: 4.00545, policy_loss: -85.73993, policy_entropy: -5.27294, alpha: 0.01880, time: 50.54301
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   497 ----
[CW] collect: return: 218.08753, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 3.09999, qf2_loss: 3.28577, policy_loss: -84.85675, policy_entropy: -5.25223, alpha: 0.01841, time: 50.66326
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   498 ----
[CW] collect: return: 31.46790, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 2.98831, qf2_loss: 3.17343, policy_loss: -84.95046, policy_entropy: -5.12308, alpha: 0.01795, time: 50.74768
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   499 ----
[CW] collect: return: 57.82492, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 3.58927, qf2_loss: 3.72372, policy_loss: -85.06631, policy_entropy: -5.18892, alpha: 0.01748, time: 52.27636
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   500 ----
[CW] collect: return: 36.14707, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 3.34909, qf2_loss: 3.50334, policy_loss: -85.18435, policy_entropy: -5.09689, alpha: 0.01703, time: 50.92496
[CW] eval: return: 40.34006, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   501 ----
[CW] collect: return: 39.61086, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 3.52842, qf2_loss: 3.60012, policy_loss: -84.80491, policy_entropy: -5.14856, alpha: 0.01662, time: 50.88626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   502 ----
[CW] collect: return: 42.11133, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 4.33542, qf2_loss: 4.53320, policy_loss: -84.22858, policy_entropy: -5.17694, alpha: 0.01624, time: 50.71957
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   503 ----
[CW] collect: return: 28.52467, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 3.59013, qf2_loss: 3.72889, policy_loss: -84.09033, policy_entropy: -5.48137, alpha: 0.01592, time: 50.51057
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   504 ----
[CW] collect: return: 35.70132, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 4.68841, qf2_loss: 4.79588, policy_loss: -83.74113, policy_entropy: -5.60386, alpha: 0.01571, time: 51.42875
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   505 ----
[CW] collect: return: 30.59067, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 3.01063, qf2_loss: 3.18007, policy_loss: -84.25607, policy_entropy: -5.63954, alpha: 0.01557, time: 50.52255
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   506 ----
[CW] collect: return: 30.39317, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 2.59938, qf2_loss: 2.70746, policy_loss: -82.72400, policy_entropy: -5.59953, alpha: 0.01538, time: 50.51229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   507 ----
[CW] collect: return: 115.56567, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 2.86180, qf2_loss: 2.97290, policy_loss: -83.67589, policy_entropy: -5.57861, alpha: 0.01523, time: 50.74352
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   508 ----
[CW] collect: return: 32.87206, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 3.17135, qf2_loss: 3.25985, policy_loss: -83.54282, policy_entropy: -5.62895, alpha: 0.01503, time: 50.43212
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   509 ----
[CW] collect: return: 36.03352, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 2.69951, qf2_loss: 2.81881, policy_loss: -83.19780, policy_entropy: -5.71961, alpha: 0.01489, time: 50.41977
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   510 ----
[CW] collect: return: 33.93042, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 2.77139, qf2_loss: 2.87722, policy_loss: -82.04051, policy_entropy: -5.72755, alpha: 0.01477, time: 50.46682
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   511 ----
[CW] collect: return: 31.92456, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 3.00991, qf2_loss: 3.08837, policy_loss: -84.04469, policy_entropy: -5.85842, alpha: 0.01469, time: 50.55306
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   512 ----
[CW] collect: return: 229.98937, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 3.10068, qf2_loss: 3.21038, policy_loss: -83.28865, policy_entropy: -5.84825, alpha: 0.01459, time: 50.50132
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   513 ----
[CW] collect: return: 68.24698, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 3.96851, qf2_loss: 4.04273, policy_loss: -84.40350, policy_entropy: -6.19601, alpha: 0.01459, time: 50.56104
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   514 ----
[CW] collect: return: 39.59764, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 3.87543, qf2_loss: 4.01228, policy_loss: -83.16773, policy_entropy: -6.58578, alpha: 0.01481, time: 50.34883
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   515 ----
[CW] collect: return: 31.69063, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 2.81481, qf2_loss: 2.93126, policy_loss: -83.99346, policy_entropy: -6.19597, alpha: 0.01496, time: 50.33369
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   516 ----
[CW] collect: return: 147.47964, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 3.28903, qf2_loss: 3.45747, policy_loss: -82.49640, policy_entropy: -6.92142, alpha: 0.01522, time: 50.42860
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   517 ----
[CW] collect: return: 328.31738, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 4.06143, qf2_loss: 4.22904, policy_loss: -82.82866, policy_entropy: -6.27225, alpha: 0.01562, time: 50.34484
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   518 ----
[CW] collect: return: 32.35603, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 6.44489, qf2_loss: 6.63666, policy_loss: -83.50246, policy_entropy: -6.00993, alpha: 0.01573, time: 50.46253
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   519 ----
[CW] collect: return: 272.08670, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 4.48750, qf2_loss: 4.65176, policy_loss: -83.18805, policy_entropy: -6.01964, alpha: 0.01577, time: 50.48203
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   520 ----
[CW] collect: return: 240.14940, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 3.13944, qf2_loss: 3.25418, policy_loss: -83.46911, policy_entropy: -6.33226, alpha: 0.01582, time: 50.24241
[CW] eval: return: 226.54084, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   521 ----
[CW] collect: return: 279.24492, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 4.19379, qf2_loss: 4.33309, policy_loss: -83.47963, policy_entropy: -6.04494, alpha: 0.01589, time: 51.45134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   522 ----
[CW] collect: return: 229.21241, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 3.94952, qf2_loss: 4.12302, policy_loss: -83.47247, policy_entropy: -6.03627, alpha: 0.01594, time: 51.18467
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   523 ----
[CW] collect: return: 216.83517, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 3.70750, qf2_loss: 3.82868, policy_loss: -83.12692, policy_entropy: -6.23712, alpha: 0.01605, time: 50.58279
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   524 ----
[CW] collect: return: 317.05393, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 3.42395, qf2_loss: 3.55835, policy_loss: -83.22888, policy_entropy: -5.85219, alpha: 0.01606, time: 50.50016
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   525 ----
[CW] collect: return: 294.40604, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 2.89871, qf2_loss: 3.01767, policy_loss: -83.22627, policy_entropy: -6.09654, alpha: 0.01599, time: 50.68811
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   526 ----
[CW] collect: return: 43.10848, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 3.61785, qf2_loss: 3.67520, policy_loss: -82.24542, policy_entropy: -6.65279, alpha: 0.01626, time: 50.71750
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   527 ----
[CW] collect: return: 216.19452, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 3.34080, qf2_loss: 3.45133, policy_loss: -82.97677, policy_entropy: -6.88547, alpha: 0.01679, time: 50.95920
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   528 ----
[CW] collect: return: 313.28931, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 3.09971, qf2_loss: 3.24136, policy_loss: -83.56602, policy_entropy: -6.52944, alpha: 0.01730, time: 50.72984
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   529 ----
[CW] collect: return: 300.60956, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 3.12349, qf2_loss: 3.26796, policy_loss: -83.44035, policy_entropy: -6.92055, alpha: 0.01771, time: 50.43687
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   530 ----
[CW] collect: return: 325.53719, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 3.89071, qf2_loss: 3.99596, policy_loss: -83.11156, policy_entropy: -6.49208, alpha: 0.01829, time: 50.49955
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   531 ----
[CW] collect: return: 317.43681, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 5.00921, qf2_loss: 5.19892, policy_loss: -83.08684, policy_entropy: -6.19816, alpha: 0.01843, time: 50.39709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   532 ----
[CW] collect: return: 288.77094, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 4.65503, qf2_loss: 4.83496, policy_loss: -83.57813, policy_entropy: -7.24057, alpha: 0.01898, time: 51.01006
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   533 ----
[CW] collect: return: 271.24860, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 4.02065, qf2_loss: 4.21771, policy_loss: -83.19237, policy_entropy: -6.06068, alpha: 0.01957, time: 50.68436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   534 ----
[CW] collect: return: 303.02105, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 3.48543, qf2_loss: 3.62598, policy_loss: -83.69701, policy_entropy: -6.40895, alpha: 0.01973, time: 50.95042
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   535 ----
[CW] collect: return: 289.99986, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 3.12784, qf2_loss: 3.23323, policy_loss: -84.29355, policy_entropy: -5.38010, alpha: 0.01974, time: 51.18892
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   536 ----
[CW] collect: return: 35.01034, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 3.46393, qf2_loss: 3.52773, policy_loss: -84.52678, policy_entropy: -5.34105, alpha: 0.01921, time: 50.83584
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   537 ----
[CW] collect: return: 234.38256, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 3.57253, qf2_loss: 3.75475, policy_loss: -83.52225, policy_entropy: -5.52656, alpha: 0.01881, time: 50.78388
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   538 ----
[CW] collect: return: 37.07629, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 7.33535, qf2_loss: 7.41812, policy_loss: -84.22996, policy_entropy: -5.67834, alpha: 0.01849, time: 50.74899
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   539 ----
[CW] collect: return: 136.06505, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 4.44394, qf2_loss: 4.59104, policy_loss: -83.34545, policy_entropy: -6.47354, alpha: 0.01860, time: 50.52980
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   540 ----
[CW] collect: return: 349.90666, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 3.29341, qf2_loss: 3.40255, policy_loss: -83.79040, policy_entropy: -6.05400, alpha: 0.01873, time: 50.63216
[CW] eval: return: 163.09255, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   541 ----
[CW] collect: return: 344.52664, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 3.46945, qf2_loss: 3.62145, policy_loss: -82.83785, policy_entropy: -6.09900, alpha: 0.01878, time: 50.94736
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   542 ----
[CW] collect: return: 235.48377, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 3.32634, qf2_loss: 3.46519, policy_loss: -83.78747, policy_entropy: -5.67641, alpha: 0.01873, time: 50.66455
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   543 ----
[CW] collect: return: 224.87301, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 3.02508, qf2_loss: 3.10359, policy_loss: -84.10197, policy_entropy: -5.71927, alpha: 0.01854, time: 50.57685
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   544 ----
[CW] collect: return: 335.52311, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 3.14251, qf2_loss: 3.20704, policy_loss: -83.95285, policy_entropy: -5.95349, alpha: 0.01842, time: 50.65895
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   545 ----
[CW] collect: return: 354.26967, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 3.25644, qf2_loss: 3.37414, policy_loss: -83.69509, policy_entropy: -5.77614, alpha: 0.01831, time: 50.87083
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   546 ----
[CW] collect: return: 342.51161, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 3.35822, qf2_loss: 3.50688, policy_loss: -84.52111, policy_entropy: -5.75711, alpha: 0.01818, time: 50.76741
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   547 ----
[CW] collect: return: 393.58370, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 3.02089, qf2_loss: 3.15775, policy_loss: -84.55282, policy_entropy: -5.85161, alpha: 0.01801, time: 50.88107
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   548 ----
[CW] collect: return: 351.74466, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 3.47123, qf2_loss: 3.62008, policy_loss: -83.30783, policy_entropy: -5.85728, alpha: 0.01790, time: 50.88437
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   549 ----
[CW] collect: return: 368.42320, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 3.31369, qf2_loss: 3.47305, policy_loss: -83.61627, policy_entropy: -5.79085, alpha: 0.01779, time: 50.69350
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   550 ----
[CW] collect: return: 201.45483, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 3.92629, qf2_loss: 4.06061, policy_loss: -84.46563, policy_entropy: -5.79493, alpha: 0.01767, time: 50.46165
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   551 ----
[CW] collect: return: 340.42281, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 3.92970, qf2_loss: 4.09205, policy_loss: -85.23791, policy_entropy: -6.75085, alpha: 0.01783, time: 50.63789
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   552 ----
[CW] collect: return: 336.92203, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 4.26714, qf2_loss: 4.41651, policy_loss: -84.45041, policy_entropy: -5.91215, alpha: 0.01803, time: 50.49716
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   553 ----
[CW] collect: return: 166.96562, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 8.75604, qf2_loss: 9.03652, policy_loss: -84.06192, policy_entropy: -6.34981, alpha: 0.01803, time: 50.51470
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   554 ----
[CW] collect: return: 69.53236, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 5.42094, qf2_loss: 5.50490, policy_loss: -84.77886, policy_entropy: -6.02290, alpha: 0.01825, time: 50.57585
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   555 ----
[CW] collect: return: 5.09331, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 4.30836, qf2_loss: 4.44975, policy_loss: -86.37047, policy_entropy: -6.09330, alpha: 0.01831, time: 50.69578
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   556 ----
[CW] collect: return: 362.60177, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 3.35777, qf2_loss: 3.46276, policy_loss: -85.05331, policy_entropy: -6.07163, alpha: 0.01831, time: 50.43450
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   557 ----
[CW] collect: return: 8.71772, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 2.72173, qf2_loss: 2.80932, policy_loss: -84.79089, policy_entropy: -5.84148, alpha: 0.01836, time: 51.18633
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   558 ----
[CW] collect: return: 8.01130, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 2.54623, qf2_loss: 2.67680, policy_loss: -83.55502, policy_entropy: -6.01948, alpha: 0.01824, time: 50.87043
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   559 ----
[CW] collect: return: 143.64386, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 3.76868, qf2_loss: 3.88064, policy_loss: -85.63175, policy_entropy: -6.41529, alpha: 0.01834, time: 50.39279
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   560 ----
[CW] collect: return: 340.83423, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 5.04467, qf2_loss: 5.21888, policy_loss: -86.06089, policy_entropy: -6.19820, alpha: 0.01866, time: 50.48493
[CW] eval: return: 300.67749, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   561 ----
[CW] collect: return: 369.95245, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 3.04798, qf2_loss: 3.14905, policy_loss: -85.10281, policy_entropy: -6.17969, alpha: 0.01878, time: 50.97833
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   562 ----
[CW] collect: return: 345.59321, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 2.93110, qf2_loss: 3.01829, policy_loss: -87.01104, policy_entropy: -6.52430, alpha: 0.01899, time: 51.02784
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   563 ----
[CW] collect: return: 126.78855, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 3.10698, qf2_loss: 3.22444, policy_loss: -84.69050, policy_entropy: -6.42040, alpha: 0.01937, time: 50.77674
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   564 ----
[CW] collect: return: 27.13242, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 3.43600, qf2_loss: 3.54894, policy_loss: -84.56697, policy_entropy: -6.24167, alpha: 0.01969, time: 50.58732
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   565 ----
[CW] collect: return: 270.12040, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 3.21594, qf2_loss: 3.32781, policy_loss: -84.82399, policy_entropy: -6.38117, alpha: 0.01992, time: 50.71828
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   566 ----
[CW] collect: return: 329.89745, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 2.80030, qf2_loss: 2.91089, policy_loss: -86.21240, policy_entropy: -6.26144, alpha: 0.02026, time: 50.76928
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   567 ----
[CW] collect: return: 137.33072, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 2.85777, qf2_loss: 2.97315, policy_loss: -84.17143, policy_entropy: -6.00281, alpha: 0.02037, time: 50.92651
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   568 ----
[CW] collect: return: 205.37736, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 3.70889, qf2_loss: 3.83637, policy_loss: -85.96486, policy_entropy: -6.10427, alpha: 0.02037, time: 50.81315
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   569 ----
[CW] collect: return: 147.66494, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 3.36413, qf2_loss: 3.48442, policy_loss: -85.12115, policy_entropy: -6.04797, alpha: 0.02048, time: 51.01629
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   570 ----
[CW] collect: return: 240.27665, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 3.96197, qf2_loss: 4.11444, policy_loss: -85.96856, policy_entropy: -5.97330, alpha: 0.02046, time: 50.90907
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   571 ----
[CW] collect: return: 411.60505, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 11.11360, qf2_loss: 11.23648, policy_loss: -85.78347, policy_entropy: -5.90612, alpha: 0.02037, time: 51.03278
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   572 ----
[CW] collect: return: 322.94472, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 8.23100, qf2_loss: 8.41910, policy_loss: -85.03945, policy_entropy: -6.92344, alpha: 0.02072, time: 50.68582
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   573 ----
[CW] collect: return: 367.92057, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 3.58305, qf2_loss: 3.71836, policy_loss: -84.79688, policy_entropy: -6.12108, alpha: 0.02122, time: 50.69048
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   574 ----
[CW] collect: return: 147.15857, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 2.79290, qf2_loss: 2.91865, policy_loss: -85.94252, policy_entropy: -5.82845, alpha: 0.02121, time: 50.48640
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   575 ----
[CW] collect: return: 252.55371, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 2.69120, qf2_loss: 2.79873, policy_loss: -87.13780, policy_entropy: -6.37081, alpha: 0.02124, time: 50.58844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   576 ----
[CW] collect: return: 318.34849, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 2.91343, qf2_loss: 3.06657, policy_loss: -85.83125, policy_entropy: -6.09894, alpha: 0.02153, time: 50.58980
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   577 ----
[CW] collect: return: 48.56238, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 2.74436, qf2_loss: 2.83139, policy_loss: -85.86541, policy_entropy: -6.02718, alpha: 0.02151, time: 50.80698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   578 ----
[CW] collect: return: 220.71475, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 2.61416, qf2_loss: 2.70555, policy_loss: -86.87700, policy_entropy: -6.30216, alpha: 0.02167, time: 53.28673
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   579 ----
[CW] collect: return: 380.49241, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 2.75482, qf2_loss: 2.91354, policy_loss: -85.77862, policy_entropy: -5.82017, alpha: 0.02182, time: 50.67737
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   580 ----
[CW] collect: return: 288.37544, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 3.14996, qf2_loss: 3.30615, policy_loss: -87.13027, policy_entropy: -5.82206, alpha: 0.02155, time: 50.48496
[CW] eval: return: 265.80804, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   581 ----
[CW] collect: return: 371.41407, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 3.91990, qf2_loss: 4.04727, policy_loss: -86.69220, policy_entropy: -6.00469, alpha: 0.02144, time: 50.71240
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   582 ----
[CW] collect: return: 94.09613, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 3.39810, qf2_loss: 3.52537, policy_loss: -85.83732, policy_entropy: -5.57567, alpha: 0.02138, time: 50.47181
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   583 ----
[CW] collect: return: 175.05230, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 3.01183, qf2_loss: 3.16659, policy_loss: -86.83441, policy_entropy: -5.71064, alpha: 0.02102, time: 50.58471
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   584 ----
[CW] collect: return: 356.24595, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 3.60444, qf2_loss: 3.74327, policy_loss: -85.59888, policy_entropy: -5.52341, alpha: 0.02071, time: 50.67327
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   585 ----
[CW] collect: return: 43.84161, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 4.44346, qf2_loss: 4.55141, policy_loss: -86.53892, policy_entropy: -5.46000, alpha: 0.02029, time: 50.83884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   586 ----
[CW] collect: return: 363.16808, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 3.53634, qf2_loss: 3.66508, policy_loss: -87.43278, policy_entropy: -5.69942, alpha: 0.01993, time: 50.73344
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   587 ----
[CW] collect: return: 394.18616, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 3.25861, qf2_loss: 3.40809, policy_loss: -86.36156, policy_entropy: -5.79532, alpha: 0.01969, time: 50.82740
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   588 ----
[CW] collect: return: 407.72909, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 3.68620, qf2_loss: 3.76141, policy_loss: -85.84583, policy_entropy: -5.77250, alpha: 0.01961, time: 50.87191
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   589 ----
[CW] collect: return: 358.43860, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 5.29986, qf2_loss: 5.52892, policy_loss: -87.22766, policy_entropy: -5.96861, alpha: 0.01946, time: 50.80349
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   590 ----
[CW] collect: return: 289.43590, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 6.53252, qf2_loss: 6.67929, policy_loss: -87.02065, policy_entropy: -6.32081, alpha: 0.01958, time: 50.53878
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   591 ----
[CW] collect: return: 159.62933, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 3.90455, qf2_loss: 4.01009, policy_loss: -87.30560, policy_entropy: -6.35937, alpha: 0.01985, time: 50.61984
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   592 ----
[CW] collect: return: 119.67292, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 4.81988, qf2_loss: 4.93719, policy_loss: -86.98493, policy_entropy: -5.61045, alpha: 0.01994, time: 50.73675
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   593 ----
[CW] collect: return: 252.77225, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 3.14770, qf2_loss: 3.24352, policy_loss: -86.61774, policy_entropy: -5.54812, alpha: 0.01958, time: 50.51742
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   594 ----
[CW] collect: return: 390.96110, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 3.37512, qf2_loss: 3.53088, policy_loss: -87.84882, policy_entropy: -5.96989, alpha: 0.01938, time: 50.47006
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   595 ----
[CW] collect: return: 103.91670, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 2.98969, qf2_loss: 3.09288, policy_loss: -87.04540, policy_entropy: -5.94295, alpha: 0.01932, time: 50.94588
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   596 ----
[CW] collect: return: 200.67038, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 3.38073, qf2_loss: 3.50460, policy_loss: -86.83405, policy_entropy: -6.16996, alpha: 0.01939, time: 51.26769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   597 ----
[CW] collect: return: 378.08474, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 3.28580, qf2_loss: 3.44507, policy_loss: -86.83888, policy_entropy: -6.30083, alpha: 0.01953, time: 50.53912
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   598 ----
[CW] collect: return: 384.87547, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 4.60947, qf2_loss: 4.66503, policy_loss: -88.09536, policy_entropy: -6.20167, alpha: 0.01971, time: 50.72751
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   599 ----
[CW] collect: return: 387.93239, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 10.88476, qf2_loss: 11.21036, policy_loss: -87.80139, policy_entropy: -6.39406, alpha: 0.01989, time: 51.27064
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   600 ----
[CW] collect: return: 378.47799, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 9.00096, qf2_loss: 9.19553, policy_loss: -86.65208, policy_entropy: -6.77126, alpha: 0.02035, time: 50.47777
[CW] eval: return: 281.97757, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   601 ----
[CW] collect: return: 122.39834, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 3.63990, qf2_loss: 3.78384, policy_loss: -87.34932, policy_entropy: -6.51173, alpha: 0.02098, time: 51.05206
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   602 ----
[CW] collect: return: 116.56910, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 2.92835, qf2_loss: 3.00089, policy_loss: -86.97574, policy_entropy: -5.83659, alpha: 0.02119, time: 50.83924
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   603 ----
[CW] collect: return: 366.87838, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 2.84762, qf2_loss: 2.95523, policy_loss: -88.00553, policy_entropy: -5.82177, alpha: 0.02102, time: 51.00837
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   604 ----
[CW] collect: return: 266.93349, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 3.29284, qf2_loss: 3.37736, policy_loss: -87.64708, policy_entropy: -5.76407, alpha: 0.02081, time: 50.65844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   605 ----
[CW] collect: return: 130.69240, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 3.97994, qf2_loss: 4.17296, policy_loss: -87.37260, policy_entropy: -5.56481, alpha: 0.02054, time: 51.20628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   606 ----
[CW] collect: return: 427.31091, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 3.47541, qf2_loss: 3.59999, policy_loss: -86.84313, policy_entropy: -5.76230, alpha: 0.02026, time: 50.65237
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   607 ----
[CW] collect: return: 246.50061, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 3.12005, qf2_loss: 3.22982, policy_loss: -87.39309, policy_entropy: -5.48181, alpha: 0.01994, time: 50.76749
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   608 ----
[CW] collect: return: 286.37677, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 3.59951, qf2_loss: 3.68916, policy_loss: -88.14036, policy_entropy: -5.66694, alpha: 0.01961, time: 50.74331
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   609 ----
[CW] collect: return: 365.39768, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 3.99387, qf2_loss: 4.08119, policy_loss: -89.11158, policy_entropy: -5.83360, alpha: 0.01942, time: 50.64273
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   610 ----
[CW] collect: return: 74.28761, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 3.92958, qf2_loss: 4.08240, policy_loss: -88.49594, policy_entropy: -5.79827, alpha: 0.01926, time: 50.70144
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   611 ----
[CW] collect: return: 138.40042, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 4.99696, qf2_loss: 5.17793, policy_loss: -86.54383, policy_entropy: -5.81896, alpha: 0.01911, time: 50.67840
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   612 ----
[CW] collect: return: 151.13161, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 4.11349, qf2_loss: 4.35252, policy_loss: -89.22205, policy_entropy: -6.36325, alpha: 0.01916, time: 50.68124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   613 ----
[CW] collect: return: 164.28806, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 3.33685, qf2_loss: 3.47434, policy_loss: -88.82095, policy_entropy: -5.98584, alpha: 0.01931, time: 50.51380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   614 ----
[CW] collect: return: 215.80896, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 3.45829, qf2_loss: 3.57680, policy_loss: -87.70585, policy_entropy: -5.88181, alpha: 0.01926, time: 50.49767
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   615 ----
[CW] collect: return: 368.70645, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 4.32738, qf2_loss: 4.45990, policy_loss: -88.31971, policy_entropy: -5.87845, alpha: 0.01919, time: 50.69318
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   616 ----
[CW] collect: return: 372.48657, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 3.43230, qf2_loss: 3.56450, policy_loss: -87.97359, policy_entropy: -5.67645, alpha: 0.01898, time: 50.75157
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   617 ----
[CW] collect: return: 227.92692, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 3.45824, qf2_loss: 3.61732, policy_loss: -87.92681, policy_entropy: -5.87190, alpha: 0.01883, time: 50.40797
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   618 ----
[CW] collect: return: 385.30582, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 3.10639, qf2_loss: 3.24303, policy_loss: -87.75599, policy_entropy: -5.80917, alpha: 0.01873, time: 50.51779
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   619 ----
[CW] collect: return: 170.60132, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 2.82285, qf2_loss: 2.91885, policy_loss: -88.13861, policy_entropy: -5.99262, alpha: 0.01860, time: 50.53002
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   620 ----
[CW] collect: return: 423.28953, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 2.89894, qf2_loss: 3.02704, policy_loss: -87.74704, policy_entropy: -5.83985, alpha: 0.01858, time: 50.60756
[CW] eval: return: 262.86341, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   621 ----
[CW] collect: return: 377.96159, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 4.22750, qf2_loss: 4.39501, policy_loss: -88.23844, policy_entropy: -5.67197, alpha: 0.01839, time: 50.55755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   622 ----
[CW] collect: return: 274.57296, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 5.75868, qf2_loss: 5.92206, policy_loss: -86.51958, policy_entropy: -5.94963, alpha: 0.01816, time: 50.41500
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   623 ----
[CW] collect: return: 267.53583, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 5.54860, qf2_loss: 5.65714, policy_loss: -89.60528, policy_entropy: -6.56620, alpha: 0.01840, time: 51.14737
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   624 ----
[CW] collect: return: 328.36513, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 11.12590, qf2_loss: 11.38920, policy_loss: -88.00137, policy_entropy: -6.04854, alpha: 0.01866, time: 51.24933
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   625 ----
[CW] collect: return: 72.54635, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 5.76847, qf2_loss: 5.99357, policy_loss: -87.60727, policy_entropy: -6.16729, alpha: 0.01875, time: 51.14789
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   626 ----
[CW] collect: return: 388.52862, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 3.77596, qf2_loss: 3.95774, policy_loss: -89.25368, policy_entropy: -5.83605, alpha: 0.01872, time: 51.03207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   627 ----
[CW] collect: return: 422.62392, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 3.50280, qf2_loss: 3.63980, policy_loss: -88.60637, policy_entropy: -5.82007, alpha: 0.01859, time: 51.13862
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   628 ----
[CW] collect: return: 332.83180, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 3.25180, qf2_loss: 3.33487, policy_loss: -88.57527, policy_entropy: -5.88732, alpha: 0.01848, time: 51.13124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   629 ----
[CW] collect: return: 394.43333, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 2.80950, qf2_loss: 2.91496, policy_loss: -88.81936, policy_entropy: -5.94492, alpha: 0.01844, time: 51.06863
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   630 ----
[CW] collect: return: 364.83585, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 2.52702, qf2_loss: 2.63353, policy_loss: -88.48180, policy_entropy: -5.93292, alpha: 0.01836, time: 51.08926
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   631 ----
[CW] collect: return: 404.60379, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 2.80744, qf2_loss: 2.90286, policy_loss: -88.98531, policy_entropy: -5.82734, alpha: 0.01829, time: 51.05426
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   632 ----
[CW] collect: return: 374.10468, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 3.07481, qf2_loss: 3.17096, policy_loss: -88.97771, policy_entropy: -5.99195, alpha: 0.01822, time: 50.89860
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   633 ----
[CW] collect: return: 417.85439, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 2.97983, qf2_loss: 3.12862, policy_loss: -89.54046, policy_entropy: -5.98949, alpha: 0.01820, time: 50.85502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   634 ----
[CW] collect: return: 382.42057, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 2.84084, qf2_loss: 2.96508, policy_loss: -88.14833, policy_entropy: -5.85974, alpha: 0.01819, time: 50.97572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   635 ----
[CW] collect: return: 369.86580, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 2.96201, qf2_loss: 3.05310, policy_loss: -91.42372, policy_entropy: -6.24345, alpha: 0.01818, time: 51.12875
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   636 ----
[CW] collect: return: 401.14334, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 3.16925, qf2_loss: 3.29298, policy_loss: -90.02256, policy_entropy: -6.10260, alpha: 0.01835, time: 50.96943
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   637 ----
[CW] collect: return: 378.72563, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 3.18339, qf2_loss: 3.29628, policy_loss: -89.76066, policy_entropy: -6.05581, alpha: 0.01842, time: 50.87944
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   638 ----
[CW] collect: return: 410.61851, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 3.10393, qf2_loss: 3.18107, policy_loss: -89.95192, policy_entropy: -5.96551, alpha: 0.01846, time: 51.75829
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   639 ----
[CW] collect: return: 225.76174, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 3.51544, qf2_loss: 3.59468, policy_loss: -91.26032, policy_entropy: -5.94613, alpha: 0.01834, time: 51.09471
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   640 ----
[CW] collect: return: 414.56618, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 4.25518, qf2_loss: 4.35813, policy_loss: -90.67367, policy_entropy: -6.06997, alpha: 0.01837, time: 50.56101
[CW] eval: return: 361.74960, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   641 ----
[CW] collect: return: 388.36804, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 3.82718, qf2_loss: 3.93315, policy_loss: -89.01004, policy_entropy: -6.06724, alpha: 0.01840, time: 50.60538
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   642 ----
[CW] collect: return: 332.59632, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 6.91380, qf2_loss: 7.13404, policy_loss: -92.42499, policy_entropy: -6.40213, alpha: 0.01857, time: 51.18413
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   643 ----
[CW] collect: return: 397.55807, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 5.37796, qf2_loss: 5.48696, policy_loss: -89.42944, policy_entropy: -5.80012, alpha: 0.01873, time: 51.03125
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   644 ----
[CW] collect: return: 391.66582, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 6.44845, qf2_loss: 6.68390, policy_loss: -91.39152, policy_entropy: -6.13658, alpha: 0.01864, time: 51.13265
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   645 ----
[CW] collect: return: 388.93261, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 4.20482, qf2_loss: 4.35430, policy_loss: -89.10130, policy_entropy: -5.92018, alpha: 0.01869, time: 51.21543
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   646 ----
[CW] collect: return: 425.88743, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 3.33304, qf2_loss: 3.44929, policy_loss: -90.14039, policy_entropy: -5.90386, alpha: 0.01864, time: 52.25237
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   647 ----
[CW] collect: return: 392.77075, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 2.88856, qf2_loss: 2.99456, policy_loss: -90.20195, policy_entropy: -5.91765, alpha: 0.01852, time: 51.14143
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   648 ----
[CW] collect: return: 419.12871, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 3.18580, qf2_loss: 3.28249, policy_loss: -91.05854, policy_entropy: -6.01999, alpha: 0.01850, time: 51.26236
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   649 ----
[CW] collect: return: 388.64644, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 4.98832, qf2_loss: 5.07585, policy_loss: -91.45706, policy_entropy: -5.95336, alpha: 0.01854, time: 51.23708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   650 ----
[CW] collect: return: 245.26022, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 3.24608, qf2_loss: 3.30386, policy_loss: -90.71184, policy_entropy: -5.95014, alpha: 0.01848, time: 51.49043
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   651 ----
[CW] collect: return: 416.83206, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 3.07726, qf2_loss: 3.16932, policy_loss: -91.60385, policy_entropy: -5.98654, alpha: 0.01844, time: 51.37197
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   652 ----
[CW] collect: return: 387.89843, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 3.21489, qf2_loss: 3.29644, policy_loss: -91.64698, policy_entropy: -6.03661, alpha: 0.01845, time: 51.16953
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   653 ----
[CW] collect: return: 400.76385, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 3.25494, qf2_loss: 3.35322, policy_loss: -90.72107, policy_entropy: -6.08564, alpha: 0.01851, time: 51.20401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   654 ----
[CW] collect: return: 212.54480, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 3.02022, qf2_loss: 3.07913, policy_loss: -91.55645, policy_entropy: -6.19602, alpha: 0.01861, time: 51.39658
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   655 ----
[CW] collect: return: 398.32795, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 2.93670, qf2_loss: 3.00779, policy_loss: -92.11000, policy_entropy: -6.10122, alpha: 0.01877, time: 51.34836
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   656 ----
[CW] collect: return: 200.30888, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 3.56449, qf2_loss: 3.63374, policy_loss: -92.53450, policy_entropy: -5.94430, alpha: 0.01876, time: 51.19712
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   657 ----
[CW] collect: return: 442.71610, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 3.16116, qf2_loss: 3.26405, policy_loss: -92.14843, policy_entropy: -5.90261, alpha: 0.01872, time: 51.42906
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   658 ----
[CW] collect: return: 465.04748, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 2.81854, qf2_loss: 2.89705, policy_loss: -90.23069, policy_entropy: -5.72619, alpha: 0.01858, time: 51.68681
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   659 ----
[CW] collect: return: 242.97801, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 3.54785, qf2_loss: 3.65622, policy_loss: -91.81828, policy_entropy: -5.91774, alpha: 0.01841, time: 51.47993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   660 ----
[CW] collect: return: 415.96675, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 3.51559, qf2_loss: 3.65439, policy_loss: -91.17314, policy_entropy: -6.07785, alpha: 0.01839, time: 51.48372
[CW] eval: return: 402.60185, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   661 ----
[CW] collect: return: 438.19796, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 3.92658, qf2_loss: 3.98360, policy_loss: -91.26364, policy_entropy: -6.09476, alpha: 0.01846, time: 51.54782
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   662 ----
[CW] collect: return: 471.02164, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 4.65858, qf2_loss: 4.78386, policy_loss: -93.18537, policy_entropy: -6.03854, alpha: 0.01851, time: 51.51749
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   663 ----
[CW] collect: return: 436.75483, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 4.08845, qf2_loss: 4.16754, policy_loss: -93.61985, policy_entropy: -6.23014, alpha: 0.01862, time: 51.46929
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   664 ----
[CW] collect: return: 44.98469, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 3.14169, qf2_loss: 3.24667, policy_loss: -93.01294, policy_entropy: -6.19050, alpha: 0.01876, time: 51.29221
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   665 ----
[CW] collect: return: 315.81734, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 3.68871, qf2_loss: 3.76540, policy_loss: -93.43330, policy_entropy: -6.06430, alpha: 0.01890, time: 51.17814
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   666 ----
[CW] collect: return: 385.15833, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 5.93111, qf2_loss: 5.98019, policy_loss: -91.88118, policy_entropy: -5.84651, alpha: 0.01889, time: 51.80795
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   667 ----
[CW] collect: return: 459.64446, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 4.21867, qf2_loss: 4.33396, policy_loss: -94.03159, policy_entropy: -6.09744, alpha: 0.01882, time: 51.21172
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   668 ----
[CW] collect: return: 416.52861, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 3.42733, qf2_loss: 3.49735, policy_loss: -92.67011, policy_entropy: -5.85140, alpha: 0.01890, time: 51.14584
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   669 ----
[CW] collect: return: 434.47295, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 4.63931, qf2_loss: 4.69794, policy_loss: -94.25142, policy_entropy: -6.05605, alpha: 0.01877, time: 51.37170
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   670 ----
[CW] collect: return: 427.84626, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 4.00746, qf2_loss: 4.04374, policy_loss: -94.20816, policy_entropy: -6.32272, alpha: 0.01881, time: 51.34905
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   671 ----
[CW] collect: return: 133.07821, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 3.43658, qf2_loss: 3.51888, policy_loss: -93.31294, policy_entropy: -7.03305, alpha: 0.01943, time: 51.17947
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   672 ----
[CW] collect: return: 440.68894, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 4.69886, qf2_loss: 4.77034, policy_loss: -92.83180, policy_entropy: -6.15563, alpha: 0.01997, time: 51.14262
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   673 ----
[CW] collect: return: 413.62213, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 7.54522, qf2_loss: 7.75137, policy_loss: -92.97519, policy_entropy: -6.54720, alpha: 0.02019, time: 51.28512
[CW] ---------------------------

============================= JOB FEEDBACK =============================

NodeName=uc2n509
Job ID: 21912859
Array Job ID: 21912859_0
Cluster: uc2
User/Group: uprnr/stud
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 4
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-16:00:48 core-walltime
Job Wall-clock time: 10:00:12
Memory Utilized: 4.78 GB
Memory Efficiency: 8.16% of 58.59 GB
