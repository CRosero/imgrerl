{"collect/return": 728.4463433902783, "collect/steps": 1000.0, "collect/total_steps": 415000.0, "train/qf1_loss": 33.16035150527954, "train/qf2_loss": 33.3713630104065, "train/policy_loss": -355.1414547729492, "train/policy_entropy": -1.0053000485897063, "train/alpha": 0.5254013407230377, "train/time": 68.97896909713745, "eval/return": 708.768209042341, "eval/steps": 1000.0, "_timestamp": 1678749046.097083, "_runtime": 28634.519567012787, "_step": 409}