{"collect/return": 206.64628280326724, "collect/steps": 1000.0, "collect/total_steps": 787000.0, "train/qf1_loss": 0.11874215058982372, "train/qf2_loss": 0.11970328830182553, "train/policy_loss": -28.121415252685548, "train/policy_entropy": -6.041454358100891, "train/alpha": 0.0070175900170579555, "train/time": 51.23608064651489, "eval/return": 193.29126435494982, "eval/steps": 1000.0, "_timestamp": 1678755378.960618, "_runtime": 43186.74858498573, "_step": 781}