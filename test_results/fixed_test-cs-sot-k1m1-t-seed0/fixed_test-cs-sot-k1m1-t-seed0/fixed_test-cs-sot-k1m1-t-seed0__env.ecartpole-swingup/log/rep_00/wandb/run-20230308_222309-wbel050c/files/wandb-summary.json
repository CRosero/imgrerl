{"collect/return": 833.0007965076366, "collect/steps": 1000.0, "collect/total_steps": 546000.0, "train/qf1_loss": 39.25626989364624, "train/qf2_loss": 38.738718338012696, "train/policy_loss": -571.5189459228516, "train/policy_entropy": -1.00098672747612, "train/alpha": 0.4952185332775116, "train/time": 51.318063497543335, "eval/return": 834.8349014096384, "eval/steps": 1000.0, "_timestamp": 1678339235.2985675, "_runtime": 28645.410984516144, "_step": 540}