{"collect/return": 847.0594004190061, "collect/steps": 1000.0, "collect/total_steps": 837000.0, "train/qf1_loss": 16.40227746963501, "train/qf2_loss": 16.519911808967592, "train/policy_loss": -687.6638494873047, "train/policy_entropy": -0.9830216103792191, "train/alpha": 0.2841398406028748, "train/time": 32.93867611885071, "eval/return": 821.1659314386303, "eval/steps": 1000.0, "_timestamp": 1678385766.08418, "_runtime": 28663.713316202164, "_step": 831}