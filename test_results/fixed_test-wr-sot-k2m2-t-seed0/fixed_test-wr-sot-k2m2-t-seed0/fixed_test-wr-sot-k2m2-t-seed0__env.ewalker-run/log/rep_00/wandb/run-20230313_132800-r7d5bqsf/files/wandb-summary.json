{"collect/return": 153.8228583857417, "collect/steps": 1000.0, "collect/total_steps": 588000.0, "train/qf1_loss": 0.1270735939592123, "train/qf2_loss": 0.1261320086568594, "train/policy_loss": -26.244547576904296, "train/policy_entropy": -5.962228217124939, "train/alpha": 0.004265692010521889, "train/time": 69.75852179527283, "eval/return": 164.02961771191332, "eval/steps": 1000.0, "_timestamp": 1678753654.0685143, "_runtime": 43173.25404334068, "_step": 582}