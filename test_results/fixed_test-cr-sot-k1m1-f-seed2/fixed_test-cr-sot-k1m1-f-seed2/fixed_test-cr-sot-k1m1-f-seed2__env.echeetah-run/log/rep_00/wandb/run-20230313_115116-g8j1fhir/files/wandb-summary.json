{"collect/return": 589.7409326797351, "collect/steps": 1000.0, "collect/total_steps": 673000.0, "train/qf1_loss": 5.118543190956116, "train/qf2_loss": 5.04141021847725, "train/policy_loss": -188.47811782836914, "train/policy_entropy": -6.073796195983887, "train/alpha": 0.052862354144454005, "train/time": 52.466983795166016, "eval/return": 663.7898473122274, "eval/steps": 1000.0, "_timestamp": 1678740649.0128038, "_runtime": 35972.45780873299, "_step": 667}