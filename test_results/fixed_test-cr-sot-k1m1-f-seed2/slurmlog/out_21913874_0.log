Hostname: uc2n509.localdomain
Found slurm config: SLURM
Seeting default slurm config
/home/kit/stud/uprnr/imgrerl/test_results/fixed_test-cr-sot-k1m1-f-seed2/fixed_test-cr-sot-k1m1-f-seed2/fixed_test-cr-sot-k1m1-f-seed2__env.echeetah-run/log/rep_00
[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
False
params: 
 {'env': {'env': 'cheetah-run'}} 

additionalVars: 
 {'seed': 2, 'agent': {'image_augmentation_K': 1, 'image_augmentation_M': 1, 'image_augmentation_type': <AugmentationType.SAME_OVER_TIME: 2>, 'image_augmentation_actor_critic_same_aug': False}}
conf_dict: 
 --------Config-------- 
seed: 2
cuda_id: 0
Subconfig: env
	env: cheetah-run
	obs_type: image
Subconfig: agent
	algo_name: sac
	encoder: lstm
	action_embedding_size: 32
	observ_embedding_size: 0
	reward_embedding_size: 32
	rnn_hidden_size: 512
	dqn_layers: [512, 512, 512]
	policy_layers: [512, 512, 512]
	lr: 0.0003
	gamma: 0.99
	tau: 0.005
	entropy_alpha: 1.0
	image_augmentation_type: AugmentationType.SAME_OVER_TIME
	image_augmentation_K: 1
	image_augmentation_M: 1
	image_augmentation_actor_critic_same_aug: False
Subconfig: rl
	sampled_seq_len: 64
	buffer_size: 1000000.0
	batch_size: 32
	rollouts_per_iter: 1
	num_updates_per_iter: 100
	num_init_rollouts: 5
Subconfig: eval
	interval: 20
	num_rollouts: 20
---------------------- 
 
Init feature extractor:  6 ;  32 ;  <function relu at 0x153faaa027a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x153faaa027a0>
Init feature extractor:  6 ;  164 ;  <function relu at 0x153faaa027a0>
Critic: 
Critic_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (current_shortcut_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=164, bias=True)
  )
  (qf1): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
  (qf2): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
)
Init feature extractor:  6 ;  32 ;  <function relu at 0x153faaa027a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x153faaa027a0>
Actor: 
Actor_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (policy): TanhGaussianPolicy(
    (fc0): Linear(in_features=612, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=6, bias=True)
    (last_fc_log_std): Linear(in_features=512, out_features=6, bias=True)
  )
)
buffer RAM usage: 11.48 GB
Collecting train stats
[CW] ---- Iteration:     0 ----
[CW] collect: return: 15.25847, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.75380, qf2_loss: 1.74864, policy_loss: -7.82477, policy_entropy: 4.09824, alpha: 0.98504, time: 51.43569
[CW] eval: return: 14.72147, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     1 ----
[CW] collect: return: 7.67879, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.09145, qf2_loss: 0.09104, policy_loss: -8.52342, policy_entropy: 4.09991, alpha: 0.95626, time: 51.34285
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     2 ----
[CW] collect: return: 25.73558, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.08123, qf2_loss: 0.08112, policy_loss: -9.22400, policy_entropy: 4.10038, alpha: 0.92871, time: 51.09601
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     3 ----
[CW] collect: return: 11.55527, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.07449, qf2_loss: 0.07454, policy_loss: -10.14501, policy_entropy: 4.10157, alpha: 0.90231, time: 51.21730
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     4 ----
[CW] collect: return: 7.16303, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06695, qf2_loss: 0.06704, policy_loss: -11.17532, policy_entropy: 4.10035, alpha: 0.87698, time: 51.19713
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     5 ----
[CW] collect: return: 14.80203, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06355, qf2_loss: 0.06346, policy_loss: -12.27850, policy_entropy: 4.10201, alpha: 0.85267, time: 51.10910
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     6 ----
[CW] collect: return: 11.25319, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.07422, qf2_loss: 0.07309, policy_loss: -13.42723, policy_entropy: 4.10059, alpha: 0.82930, time: 51.22527
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     7 ----
[CW] collect: return: 19.09899, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.07298, qf2_loss: 0.07258, policy_loss: -14.63622, policy_entropy: 4.10101, alpha: 0.80683, time: 51.04333
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     8 ----
[CW] collect: return: 14.01475, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.10114, qf2_loss: 0.10105, policy_loss: -15.85463, policy_entropy: 4.10127, alpha: 0.78520, time: 51.07850
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     9 ----
[CW] collect: return: 6.14957, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.06440, qf2_loss: 0.06466, policy_loss: -17.06097, policy_entropy: 4.10091, alpha: 0.76436, time: 51.62532
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    10 ----
[CW] collect: return: 7.71624, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.09404, qf2_loss: 0.09442, policy_loss: -18.24457, policy_entropy: 4.10112, alpha: 0.74426, time: 51.30615
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    11 ----
[CW] collect: return: 14.70268, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.07399, qf2_loss: 0.07441, policy_loss: -19.41663, policy_entropy: 4.10060, alpha: 0.72488, time: 51.01829
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    12 ----
[CW] collect: return: 10.14809, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.09233, qf2_loss: 0.09288, policy_loss: -20.55733, policy_entropy: 4.10152, alpha: 0.70617, time: 51.26513
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    13 ----
[CW] collect: return: 11.68021, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.07689, qf2_loss: 0.07737, policy_loss: -21.67187, policy_entropy: 4.10098, alpha: 0.68809, time: 51.15383
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    14 ----
[CW] collect: return: 18.95500, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.09659, qf2_loss: 0.09705, policy_loss: -22.76762, policy_entropy: 4.10023, alpha: 0.67062, time: 51.12480
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    15 ----
[CW] collect: return: 21.81969, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.07860, qf2_loss: 0.07910, policy_loss: -23.83276, policy_entropy: 4.09992, alpha: 0.65372, time: 51.13204
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    16 ----
[CW] collect: return: 14.40067, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.09734, qf2_loss: 0.09790, policy_loss: -24.87290, policy_entropy: 4.09882, alpha: 0.63737, time: 51.16251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    17 ----
[CW] collect: return: 10.61255, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.09192, qf2_loss: 0.09242, policy_loss: -25.87641, policy_entropy: 4.09831, alpha: 0.62154, time: 51.33371
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    18 ----
[CW] collect: return: 25.07836, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.09135, qf2_loss: 0.09170, policy_loss: -26.87417, policy_entropy: 4.09800, alpha: 0.60620, time: 51.18479
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    19 ----
[CW] collect: return: 8.26595, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.10131, qf2_loss: 0.10187, policy_loss: -27.81341, policy_entropy: 4.09691, alpha: 0.59133, time: 50.96214
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    20 ----
[CW] collect: return: 10.80179, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.10034, qf2_loss: 0.10083, policy_loss: -28.73504, policy_entropy: 4.09782, alpha: 0.57692, time: 50.89765
[CW] eval: return: 12.63482, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    21 ----
[CW] collect: return: 15.00899, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.10697, qf2_loss: 0.10753, policy_loss: -29.63517, policy_entropy: 4.09721, alpha: 0.56293, time: 51.53243
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    22 ----
[CW] collect: return: 17.14032, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.12006, qf2_loss: 0.12039, policy_loss: -30.51544, policy_entropy: 4.09620, alpha: 0.54935, time: 53.23615
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    23 ----
[CW] collect: return: 7.48929, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.09229, qf2_loss: 0.09267, policy_loss: -31.36290, policy_entropy: 4.09537, alpha: 0.53618, time: 51.34673
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    24 ----
[CW] collect: return: 16.15403, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.11992, qf2_loss: 0.12045, policy_loss: -32.17868, policy_entropy: 4.09614, alpha: 0.52338, time: 51.37595
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    25 ----
[CW] collect: return: 12.80603, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.12507, qf2_loss: 0.12515, policy_loss: -32.97451, policy_entropy: 4.09436, alpha: 0.51094, time: 51.30801
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    26 ----
[CW] collect: return: 8.99122, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.09132, qf2_loss: 0.09130, policy_loss: -33.73752, policy_entropy: 4.09464, alpha: 0.49885, time: 51.22700
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    27 ----
[CW] collect: return: 8.76073, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.12201, qf2_loss: 0.12199, policy_loss: -34.48723, policy_entropy: 4.09364, alpha: 0.48709, time: 51.20459
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    28 ----
[CW] collect: return: 17.03854, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.13159, qf2_loss: 0.13178, policy_loss: -35.20660, policy_entropy: 4.09234, alpha: 0.47565, time: 51.37545
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    29 ----
[CW] collect: return: 12.00707, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.11542, qf2_loss: 0.11549, policy_loss: -35.91416, policy_entropy: 4.09191, alpha: 0.46453, time: 51.27472
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    30 ----
[CW] collect: return: 13.62963, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.12584, qf2_loss: 0.12573, policy_loss: -36.60040, policy_entropy: 4.09039, alpha: 0.45370, time: 51.12491
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    31 ----
[CW] collect: return: 15.78324, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.10723, qf2_loss: 0.10717, policy_loss: -37.25200, policy_entropy: 4.08931, alpha: 0.44316, time: 51.14800
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    32 ----
[CW] collect: return: 12.31020, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.17920, qf2_loss: 0.18041, policy_loss: -37.88351, policy_entropy: 4.09007, alpha: 0.43290, time: 51.14698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    33 ----
[CW] collect: return: 18.24815, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.09657, qf2_loss: 0.09657, policy_loss: -38.50129, policy_entropy: 4.09082, alpha: 0.42290, time: 51.17960
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    34 ----
[CW] collect: return: 15.02116, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.12083, qf2_loss: 0.12065, policy_loss: -39.10202, policy_entropy: 4.08870, alpha: 0.41316, time: 51.27042
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    35 ----
[CW] collect: return: 15.79755, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.15700, qf2_loss: 0.15748, policy_loss: -39.68151, policy_entropy: 4.08644, alpha: 0.40367, time: 50.88511
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    36 ----
[CW] collect: return: 18.62361, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.11265, qf2_loss: 0.11290, policy_loss: -40.23940, policy_entropy: 4.08397, alpha: 0.39442, time: 50.95362
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    37 ----
[CW] collect: return: 12.17487, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.14482, qf2_loss: 0.14521, policy_loss: -40.78682, policy_entropy: 4.08386, alpha: 0.38541, time: 51.08155
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    38 ----
[CW] collect: return: 15.34835, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.12378, qf2_loss: 0.12390, policy_loss: -41.28506, policy_entropy: 4.08160, alpha: 0.37662, time: 51.08648
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    39 ----
[CW] collect: return: 18.14214, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.15119, qf2_loss: 0.15167, policy_loss: -41.79777, policy_entropy: 4.08035, alpha: 0.36805, time: 51.09159
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    40 ----
[CW] collect: return: 16.42747, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.15234, qf2_loss: 0.15324, policy_loss: -42.28366, policy_entropy: 4.07985, alpha: 0.35969, time: 51.31334
[CW] eval: return: 16.51265, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    41 ----
[CW] collect: return: 24.66391, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.12931, qf2_loss: 0.12939, policy_loss: -42.76822, policy_entropy: 4.07340, alpha: 0.35153, time: 51.19797
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    42 ----
[CW] collect: return: 15.88213, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.13062, qf2_loss: 0.13055, policy_loss: -43.20570, policy_entropy: 4.07423, alpha: 0.34358, time: 51.11410
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    43 ----
[CW] collect: return: 27.53259, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.15463, qf2_loss: 0.15480, policy_loss: -43.66674, policy_entropy: 4.06843, alpha: 0.33582, time: 51.66616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    44 ----
[CW] collect: return: 24.67715, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.14722, qf2_loss: 0.14723, policy_loss: -44.06540, policy_entropy: 4.06614, alpha: 0.32825, time: 50.82336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    45 ----
[CW] collect: return: 19.67233, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.15452, qf2_loss: 0.15443, policy_loss: -44.48369, policy_entropy: 4.06068, alpha: 0.32086, time: 50.96084
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    46 ----
[CW] collect: return: 25.31910, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.17149, qf2_loss: 0.17144, policy_loss: -44.87001, policy_entropy: 4.06024, alpha: 0.31364, time: 51.34659
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    47 ----
[CW] collect: return: 27.23965, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.14951, qf2_loss: 0.14911, policy_loss: -45.26120, policy_entropy: 4.05370, alpha: 0.30660, time: 51.81841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    48 ----
[CW] collect: return: 22.60916, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.13750, qf2_loss: 0.13717, policy_loss: -45.61669, policy_entropy: 4.05148, alpha: 0.29973, time: 50.82414
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    49 ----
[CW] collect: return: 26.69195, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.20283, qf2_loss: 0.20359, policy_loss: -45.97017, policy_entropy: 4.04936, alpha: 0.29302, time: 50.86065
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    50 ----
[CW] collect: return: 10.95119, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.16020, qf2_loss: 0.15880, policy_loss: -46.29757, policy_entropy: 4.05095, alpha: 0.28646, time: 50.99251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    51 ----
[CW] collect: return: 7.55945, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.13622, qf2_loss: 0.13553, policy_loss: -46.59585, policy_entropy: 4.04533, alpha: 0.28006, time: 50.79246
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    52 ----
[CW] collect: return: 21.99531, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.18440, qf2_loss: 0.18438, policy_loss: -46.91514, policy_entropy: 4.04792, alpha: 0.27380, time: 50.82948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    53 ----
[CW] collect: return: 11.36876, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.14570, qf2_loss: 0.14405, policy_loss: -47.19510, policy_entropy: 4.03975, alpha: 0.26769, time: 51.09005
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    54 ----
[CW] collect: return: 25.66075, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.16665, qf2_loss: 0.16645, policy_loss: -47.47353, policy_entropy: 4.04534, alpha: 0.26173, time: 51.36637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    55 ----
[CW] collect: return: 11.57778, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.17794, qf2_loss: 0.17733, policy_loss: -47.74341, policy_entropy: 4.03588, alpha: 0.25589, time: 51.08517
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    56 ----
[CW] collect: return: 34.47096, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.17036, qf2_loss: 0.17036, policy_loss: -48.00896, policy_entropy: 4.02909, alpha: 0.25020, time: 51.34817
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    57 ----
[CW] collect: return: 31.17467, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.18345, qf2_loss: 0.18320, policy_loss: -48.25875, policy_entropy: 4.02982, alpha: 0.24464, time: 51.45840
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    58 ----
[CW] collect: return: 24.42839, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.14126, qf2_loss: 0.14045, policy_loss: -48.48966, policy_entropy: 4.01858, alpha: 0.23920, time: 51.33912
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    59 ----
[CW] collect: return: 32.35400, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.16812, qf2_loss: 0.16730, policy_loss: -48.73361, policy_entropy: 4.01423, alpha: 0.23389, time: 51.17999
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    60 ----
[CW] collect: return: 13.26420, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.19052, qf2_loss: 0.19097, policy_loss: -48.93860, policy_entropy: 4.00757, alpha: 0.22870, time: 51.41807
[CW] eval: return: 22.62574, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    61 ----
[CW] collect: return: 16.49279, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.17170, qf2_loss: 0.17105, policy_loss: -49.14482, policy_entropy: 3.99613, alpha: 0.22364, time: 51.20908
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    62 ----
[CW] collect: return: 30.88295, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.17214, qf2_loss: 0.17260, policy_loss: -49.33289, policy_entropy: 3.99407, alpha: 0.21868, time: 51.25271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    63 ----
[CW] collect: return: 26.90295, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.17736, qf2_loss: 0.17765, policy_loss: -49.54364, policy_entropy: 3.98141, alpha: 0.21385, time: 51.22854
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    64 ----
[CW] collect: return: 22.05821, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.17429, qf2_loss: 0.17438, policy_loss: -49.70924, policy_entropy: 3.97060, alpha: 0.20912, time: 51.38957
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    65 ----
[CW] collect: return: 44.05414, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.19011, qf2_loss: 0.18973, policy_loss: -49.89882, policy_entropy: 3.95767, alpha: 0.20450, time: 52.08437
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    66 ----
[CW] collect: return: 42.13970, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.18404, qf2_loss: 0.18371, policy_loss: -50.07360, policy_entropy: 3.93653, alpha: 0.20000, time: 51.15231
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    67 ----
[CW] collect: return: 36.37285, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.17449, qf2_loss: 0.17358, policy_loss: -50.21977, policy_entropy: 3.91487, alpha: 0.19560, time: 51.77843
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    68 ----
[CW] collect: return: 41.76475, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.22869, qf2_loss: 0.22867, policy_loss: -50.37419, policy_entropy: 3.89395, alpha: 0.19130, time: 51.08597
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    69 ----
[CW] collect: return: 27.15240, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.18030, qf2_loss: 0.18049, policy_loss: -50.51212, policy_entropy: 3.86740, alpha: 0.18710, time: 51.05596
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    70 ----
[CW] collect: return: 55.65561, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.21503, qf2_loss: 0.21391, policy_loss: -50.64520, policy_entropy: 3.83883, alpha: 0.18301, time: 51.27642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    71 ----
[CW] collect: return: 61.12666, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.18234, qf2_loss: 0.18083, policy_loss: -50.79494, policy_entropy: 3.76148, alpha: 0.17902, time: 51.12876
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    72 ----
[CW] collect: return: 54.44152, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.26254, qf2_loss: 0.26459, policy_loss: -50.94633, policy_entropy: 3.71095, alpha: 0.17514, time: 51.03754
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    73 ----
[CW] collect: return: 57.66429, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.22723, qf2_loss: 0.22846, policy_loss: -51.07388, policy_entropy: 3.57895, alpha: 0.17136, time: 51.30691
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    74 ----
[CW] collect: return: 69.31212, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.19970, qf2_loss: 0.19912, policy_loss: -51.21558, policy_entropy: 3.41653, alpha: 0.16772, time: 51.05071
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    75 ----
[CW] collect: return: 68.21383, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.28509, qf2_loss: 0.28746, policy_loss: -51.38492, policy_entropy: 3.21717, alpha: 0.16420, time: 50.72369
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    76 ----
[CW] collect: return: 31.68845, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.30178, qf2_loss: 0.30165, policy_loss: -51.55379, policy_entropy: 2.96959, alpha: 0.16082, time: 50.87309
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    77 ----
[CW] collect: return: 78.07966, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.23888, qf2_loss: 0.23723, policy_loss: -51.75174, policy_entropy: 2.67337, alpha: 0.15760, time: 50.86985
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    78 ----
[CW] collect: return: 71.25298, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.23881, qf2_loss: 0.23700, policy_loss: -51.94528, policy_entropy: 2.45768, alpha: 0.15450, time: 50.93465
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    79 ----
[CW] collect: return: 81.16165, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.30026, qf2_loss: 0.29887, policy_loss: -52.18772, policy_entropy: 2.31330, alpha: 0.15150, time: 51.18599
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    80 ----
[CW] collect: return: 55.80224, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.29976, qf2_loss: 0.29814, policy_loss: -52.42817, policy_entropy: 2.05317, alpha: 0.14861, time: 50.93367
[CW] eval: return: 83.90356, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    81 ----
[CW] collect: return: 41.80650, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.30413, qf2_loss: 0.30087, policy_loss: -52.64144, policy_entropy: 1.92958, alpha: 0.14581, time: 50.93954
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    82 ----
[CW] collect: return: 34.51195, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.32071, qf2_loss: 0.31729, policy_loss: -52.89136, policy_entropy: 1.83321, alpha: 0.14307, time: 50.79226
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    83 ----
[CW] collect: return: 76.50118, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.34097, qf2_loss: 0.33811, policy_loss: -53.12695, policy_entropy: 1.75960, alpha: 0.14038, time: 50.99057
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    84 ----
[CW] collect: return: 75.39182, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.33341, qf2_loss: 0.33026, policy_loss: -53.35014, policy_entropy: 1.72685, alpha: 0.13773, time: 51.02041
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    85 ----
[CW] collect: return: 147.04777, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.31744, qf2_loss: 0.31360, policy_loss: -53.63491, policy_entropy: 1.60288, alpha: 0.13512, time: 52.08463
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    86 ----
[CW] collect: return: 75.82181, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.31718, qf2_loss: 0.31594, policy_loss: -53.90822, policy_entropy: 1.46215, alpha: 0.13258, time: 50.70112
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    87 ----
[CW] collect: return: 91.29770, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.34344, qf2_loss: 0.33809, policy_loss: -54.19512, policy_entropy: 1.28310, alpha: 0.13011, time: 50.76733
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    88 ----
[CW] collect: return: 94.19435, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.36448, qf2_loss: 0.35693, policy_loss: -54.43603, policy_entropy: 1.20124, alpha: 0.12771, time: 50.80033
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    89 ----
[CW] collect: return: 71.78357, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.34057, qf2_loss: 0.33555, policy_loss: -54.70261, policy_entropy: 1.12181, alpha: 0.12535, time: 51.10023
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    90 ----
[CW] collect: return: 118.61074, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.36262, qf2_loss: 0.35744, policy_loss: -54.95675, policy_entropy: 0.94083, alpha: 0.12304, time: 50.82598
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    91 ----
[CW] collect: return: 73.76457, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.39824, qf2_loss: 0.39409, policy_loss: -55.24885, policy_entropy: 0.88724, alpha: 0.12079, time: 50.65827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    92 ----
[CW] collect: return: 135.00130, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.41022, qf2_loss: 0.40361, policy_loss: -55.53696, policy_entropy: 0.86963, alpha: 0.11856, time: 52.36259
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    93 ----
[CW] collect: return: 121.63144, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.43602, qf2_loss: 0.43356, policy_loss: -55.80759, policy_entropy: 0.87767, alpha: 0.11636, time: 50.92404
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    94 ----
[CW] collect: return: 177.68020, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.41135, qf2_loss: 0.40518, policy_loss: -56.06637, policy_entropy: 0.81704, alpha: 0.11417, time: 51.00573
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    95 ----
[CW] collect: return: 86.44635, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.53635, qf2_loss: 0.53022, policy_loss: -56.23827, policy_entropy: 0.92296, alpha: 0.11200, time: 51.00004
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    96 ----
[CW] collect: return: 34.09384, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.40259, qf2_loss: 0.39439, policy_loss: -56.48619, policy_entropy: 0.81504, alpha: 0.10984, time: 50.88670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    97 ----
[CW] collect: return: 59.72758, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.41069, qf2_loss: 0.40911, policy_loss: -56.71128, policy_entropy: 0.84134, alpha: 0.10772, time: 50.94465
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    98 ----
[CW] collect: return: 73.88197, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.42631, qf2_loss: 0.41961, policy_loss: -56.90435, policy_entropy: 0.80770, alpha: 0.10563, time: 51.05125
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    99 ----
[CW] collect: return: 112.47526, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.51674, qf2_loss: 0.51707, policy_loss: -57.02392, policy_entropy: 0.88712, alpha: 0.10354, time: 51.05660
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   100 ----
[CW] collect: return: 129.05749, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.43571, qf2_loss: 0.43176, policy_loss: -57.25751, policy_entropy: 0.80803, alpha: 0.10149, time: 50.96235
[CW] eval: return: 88.88303, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   101 ----
[CW] collect: return: 41.88742, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.49293, qf2_loss: 0.49143, policy_loss: -57.36749, policy_entropy: 0.81292, alpha: 0.09946, time: 51.13674
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   102 ----
[CW] collect: return: 52.32830, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.42851, qf2_loss: 0.42630, policy_loss: -57.54003, policy_entropy: 0.74561, alpha: 0.09748, time: 50.93735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   103 ----
[CW] collect: return: 144.04278, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.43695, qf2_loss: 0.43332, policy_loss: -57.75536, policy_entropy: 0.66765, alpha: 0.09554, time: 50.86016
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   104 ----
[CW] collect: return: 67.58291, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.44821, qf2_loss: 0.44400, policy_loss: -57.77755, policy_entropy: 0.72365, alpha: 0.09363, time: 50.89965
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   105 ----
[CW] collect: return: 56.61490, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.44368, qf2_loss: 0.43888, policy_loss: -57.91863, policy_entropy: 0.72992, alpha: 0.09174, time: 50.90463
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   106 ----
[CW] collect: return: 138.03280, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.40955, qf2_loss: 0.40545, policy_loss: -58.07339, policy_entropy: 0.69742, alpha: 0.08987, time: 50.66046
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   107 ----
[CW] collect: return: 92.87884, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.41284, qf2_loss: 0.40700, policy_loss: -58.09899, policy_entropy: 0.68329, alpha: 0.08804, time: 50.78482
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   108 ----
[CW] collect: return: 55.72681, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.42020, qf2_loss: 0.41603, policy_loss: -58.29926, policy_entropy: 0.60204, alpha: 0.08625, time: 50.97562
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   109 ----
[CW] collect: return: 137.43524, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.42760, qf2_loss: 0.42623, policy_loss: -58.27797, policy_entropy: 0.67856, alpha: 0.08449, time: 51.07073
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   110 ----
[CW] collect: return: 53.53050, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.47230, qf2_loss: 0.46743, policy_loss: -58.43033, policy_entropy: 0.59542, alpha: 0.08275, time: 50.84763
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   111 ----
[CW] collect: return: 101.06976, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.42233, qf2_loss: 0.41437, policy_loss: -58.48214, policy_entropy: 0.56380, alpha: 0.08105, time: 50.70023
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   112 ----
[CW] collect: return: 77.63413, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.41721, qf2_loss: 0.41211, policy_loss: -58.64770, policy_entropy: 0.46974, alpha: 0.07940, time: 50.92499
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   113 ----
[CW] collect: return: 63.54319, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.46732, qf2_loss: 0.46535, policy_loss: -58.67335, policy_entropy: 0.49979, alpha: 0.07777, time: 51.11234
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   114 ----
[CW] collect: return: 57.18338, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.41145, qf2_loss: 0.40530, policy_loss: -58.70181, policy_entropy: 0.44569, alpha: 0.07618, time: 51.36206
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   115 ----
[CW] collect: return: 128.49078, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.41973, qf2_loss: 0.41531, policy_loss: -58.84425, policy_entropy: 0.26434, alpha: 0.07463, time: 50.99820
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   116 ----
[CW] collect: return: 48.02160, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.43829, qf2_loss: 0.43131, policy_loss: -58.85726, policy_entropy: 0.27993, alpha: 0.07313, time: 51.04566
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   117 ----
[CW] collect: return: 172.00717, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.41028, qf2_loss: 0.40508, policy_loss: -58.84818, policy_entropy: 0.28792, alpha: 0.07165, time: 51.08895
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   118 ----
[CW] collect: return: 98.77288, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.42507, qf2_loss: 0.42291, policy_loss: -59.01777, policy_entropy: 0.16283, alpha: 0.07020, time: 50.77580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   119 ----
[CW] collect: return: 150.78717, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.48629, qf2_loss: 0.48661, policy_loss: -59.26625, policy_entropy: -0.00790, alpha: 0.06880, time: 51.13831
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   120 ----
[CW] collect: return: 60.70101, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.46856, qf2_loss: 0.46636, policy_loss: -59.23415, policy_entropy: 0.04009, alpha: 0.06743, time: 51.02380
[CW] eval: return: 85.87548, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   121 ----
[CW] collect: return: 40.19302, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.42621, qf2_loss: 0.42233, policy_loss: -59.25918, policy_entropy: -0.04409, alpha: 0.06609, time: 51.12217
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   122 ----
[CW] collect: return: 60.73715, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.52145, qf2_loss: 0.51975, policy_loss: -59.34341, policy_entropy: -0.09554, alpha: 0.06477, time: 51.21710
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   123 ----
[CW] collect: return: 100.65831, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.47272, qf2_loss: 0.46984, policy_loss: -59.43149, policy_entropy: -0.16346, alpha: 0.06349, time: 51.10492
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   124 ----
[CW] collect: return: 115.35911, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.44666, qf2_loss: 0.44569, policy_loss: -59.52061, policy_entropy: -0.18980, alpha: 0.06223, time: 51.03697
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   125 ----
[CW] collect: return: 191.08869, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.47087, qf2_loss: 0.46493, policy_loss: -59.66186, policy_entropy: -0.38010, alpha: 0.06101, time: 51.21604
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   126 ----
[CW] collect: return: 108.63644, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.43307, qf2_loss: 0.42842, policy_loss: -59.75277, policy_entropy: -0.42781, alpha: 0.05983, time: 50.85937
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   127 ----
[CW] collect: return: 113.13351, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.49443, qf2_loss: 0.49283, policy_loss: -59.71818, policy_entropy: -0.43602, alpha: 0.05866, time: 51.28592
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   128 ----
[CW] collect: return: 176.07465, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.53640, qf2_loss: 0.53327, policy_loss: -59.97505, policy_entropy: -0.57235, alpha: 0.05753, time: 51.06518
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   129 ----
[CW] collect: return: 85.49201, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.54036, qf2_loss: 0.53792, policy_loss: -60.10271, policy_entropy: -0.69545, alpha: 0.05643, time: 51.55365
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   130 ----
[CW] collect: return: 270.90667, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.51960, qf2_loss: 0.51092, policy_loss: -60.21115, policy_entropy: -0.78216, alpha: 0.05536, time: 50.98309
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   131 ----
[CW] collect: return: 290.87347, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.58836, qf2_loss: 0.58592, policy_loss: -60.26988, policy_entropy: -0.77218, alpha: 0.05431, time: 50.95811
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   132 ----
[CW] collect: return: 187.47480, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.48394, qf2_loss: 0.48087, policy_loss: -60.48625, policy_entropy: -0.96019, alpha: 0.05329, time: 51.14086
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   133 ----
[CW] collect: return: 183.39018, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.50341, qf2_loss: 0.50000, policy_loss: -60.57766, policy_entropy: -0.99986, alpha: 0.05229, time: 51.64865
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   134 ----
[CW] collect: return: 111.93531, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.56646, qf2_loss: 0.55653, policy_loss: -60.81975, policy_entropy: -1.18338, alpha: 0.05132, time: 51.02222
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   135 ----
[CW] collect: return: 206.24511, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.60941, qf2_loss: 0.60387, policy_loss: -60.95957, policy_entropy: -1.22660, alpha: 0.05039, time: 51.08809
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   136 ----
[CW] collect: return: 167.62046, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.58662, qf2_loss: 0.57910, policy_loss: -61.15887, policy_entropy: -1.34317, alpha: 0.04947, time: 51.29661
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   137 ----
[CW] collect: return: 116.64253, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.52505, qf2_loss: 0.51801, policy_loss: -61.10904, policy_entropy: -1.43692, alpha: 0.04858, time: 51.01584
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   138 ----
[CW] collect: return: 158.56624, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.62406, qf2_loss: 0.62075, policy_loss: -61.27638, policy_entropy: -1.57022, alpha: 0.04771, time: 51.21572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   139 ----
[CW] collect: return: 119.18652, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.64615, qf2_loss: 0.63869, policy_loss: -61.25611, policy_entropy: -1.54355, alpha: 0.04686, time: 51.01416
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   140 ----
[CW] collect: return: 309.62855, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.66946, qf2_loss: 0.66725, policy_loss: -61.73544, policy_entropy: -1.80385, alpha: 0.04604, time: 52.18689
[CW] eval: return: 245.99654, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   141 ----
[CW] collect: return: 313.84370, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.61985, qf2_loss: 0.61712, policy_loss: -61.80264, policy_entropy: -1.84199, alpha: 0.04523, time: 51.18930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   142 ----
[CW] collect: return: 117.17194, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.63105, qf2_loss: 0.62610, policy_loss: -62.23932, policy_entropy: -2.09818, alpha: 0.04446, time: 51.09473
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   143 ----
[CW] collect: return: 328.33192, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.60948, qf2_loss: 0.60429, policy_loss: -62.24912, policy_entropy: -2.02719, alpha: 0.04372, time: 50.94989
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   144 ----
[CW] collect: return: 200.12090, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.63673, qf2_loss: 0.63499, policy_loss: -62.29514, policy_entropy: -2.04635, alpha: 0.04297, time: 50.84637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   145 ----
[CW] collect: return: 311.12603, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.61627, qf2_loss: 0.60935, policy_loss: -62.76550, policy_entropy: -2.28508, alpha: 0.04224, time: 50.76111
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   146 ----
[CW] collect: return: 84.50464, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.91544, qf2_loss: 0.91185, policy_loss: -62.55401, policy_entropy: -2.24136, alpha: 0.04153, time: 50.71026
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   147 ----
[CW] collect: return: 318.66769, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.80897, qf2_loss: 0.81794, policy_loss: -62.90502, policy_entropy: -2.42522, alpha: 0.04083, time: 51.04275
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   148 ----
[CW] collect: return: 182.17585, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.63657, qf2_loss: 0.63165, policy_loss: -62.98821, policy_entropy: -2.44902, alpha: 0.04016, time: 50.79531
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   149 ----
[CW] collect: return: 296.14312, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.63407, qf2_loss: 0.63211, policy_loss: -63.39535, policy_entropy: -2.59183, alpha: 0.03949, time: 50.60133
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   150 ----
[CW] collect: return: 295.91384, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.63832, qf2_loss: 0.63245, policy_loss: -63.53055, policy_entropy: -2.59916, alpha: 0.03885, time: 50.57623
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   151 ----
[CW] collect: return: 326.77882, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.79488, qf2_loss: 0.79515, policy_loss: -63.77880, policy_entropy: -2.86228, alpha: 0.03821, time: 50.89609
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   152 ----
[CW] collect: return: 360.97144, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.69181, qf2_loss: 0.68869, policy_loss: -63.84917, policy_entropy: -2.89811, alpha: 0.03762, time: 50.72158
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   153 ----
[CW] collect: return: 299.61549, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.81299, qf2_loss: 0.80737, policy_loss: -64.18661, policy_entropy: -3.00359, alpha: 0.03704, time: 50.79019
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   154 ----
[CW] collect: return: 304.60822, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.70285, qf2_loss: 0.69857, policy_loss: -64.48181, policy_entropy: -3.12195, alpha: 0.03647, time: 50.81616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   155 ----
[CW] collect: return: 371.41875, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 1.10448, qf2_loss: 1.10085, policy_loss: -64.97589, policy_entropy: -3.30696, alpha: 0.03593, time: 50.63350
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   156 ----
[CW] collect: return: 292.08928, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.76248, qf2_loss: 0.76119, policy_loss: -65.09804, policy_entropy: -3.37572, alpha: 0.03541, time: 50.93342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   157 ----
[CW] collect: return: 355.51067, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.74711, qf2_loss: 0.74907, policy_loss: -65.47726, policy_entropy: -3.49838, alpha: 0.03489, time: 50.81827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   158 ----
[CW] collect: return: 390.62933, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.73207, qf2_loss: 0.73597, policy_loss: -65.52710, policy_entropy: -3.54254, alpha: 0.03440, time: 51.20143
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   159 ----
[CW] collect: return: 379.53647, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.75635, qf2_loss: 0.75292, policy_loss: -65.59800, policy_entropy: -3.64886, alpha: 0.03392, time: 51.31586
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   160 ----
[CW] collect: return: 358.15642, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.78131, qf2_loss: 0.77667, policy_loss: -66.47052, policy_entropy: -4.00859, alpha: 0.03346, time: 50.98090
[CW] eval: return: 272.25794, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   161 ----
[CW] collect: return: 263.02637, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.79658, qf2_loss: 0.79656, policy_loss: -66.39283, policy_entropy: -3.99933, alpha: 0.03305, time: 51.18278
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   162 ----
[CW] collect: return: 367.02550, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.97845, qf2_loss: 0.98813, policy_loss: -67.06478, policy_entropy: -4.19138, alpha: 0.03266, time: 51.01975
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   163 ----
[CW] collect: return: 260.30437, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.74161, qf2_loss: 0.74358, policy_loss: -66.88878, policy_entropy: -4.09866, alpha: 0.03226, time: 51.05142
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   164 ----
[CW] collect: return: 263.03290, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 1.07031, qf2_loss: 1.07087, policy_loss: -67.43517, policy_entropy: -4.23050, alpha: 0.03185, time: 51.00532
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   165 ----
[CW] collect: return: 372.45601, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.77575, qf2_loss: 0.77473, policy_loss: -67.26848, policy_entropy: -4.16539, alpha: 0.03146, time: 51.30952
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   166 ----
[CW] collect: return: 109.88774, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.78039, qf2_loss: 0.78356, policy_loss: -67.48037, policy_entropy: -4.23328, alpha: 0.03106, time: 52.19466
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   167 ----
[CW] collect: return: 370.98174, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.83011, qf2_loss: 0.82606, policy_loss: -67.85504, policy_entropy: -4.43504, alpha: 0.03067, time: 50.95420
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   168 ----
[CW] collect: return: 83.06594, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.98536, qf2_loss: 0.98603, policy_loss: -68.12766, policy_entropy: -4.54367, alpha: 0.03033, time: 50.85466
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   169 ----
[CW] collect: return: 88.81080, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.89480, qf2_loss: 0.89097, policy_loss: -67.82328, policy_entropy: -4.53839, alpha: 0.02999, time: 51.02040
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   170 ----
[CW] collect: return: 282.80755, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 1.01450, qf2_loss: 1.02080, policy_loss: -68.71693, policy_entropy: -4.73227, alpha: 0.02965, time: 50.89595
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   171 ----
[CW] collect: return: 400.83006, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.86927, qf2_loss: 0.86403, policy_loss: -68.29753, policy_entropy: -4.67088, alpha: 0.02934, time: 50.93886
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   172 ----
[CW] collect: return: 376.19871, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.91813, qf2_loss: 0.92466, policy_loss: -68.94337, policy_entropy: -4.80894, alpha: 0.02902, time: 51.02931
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   173 ----
[CW] collect: return: 434.84544, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.86003, qf2_loss: 0.86357, policy_loss: -68.98585, policy_entropy: -4.71938, alpha: 0.02871, time: 50.93791
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   174 ----
[CW] collect: return: 383.43106, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 1.02168, qf2_loss: 1.01941, policy_loss: -69.62725, policy_entropy: -4.96433, alpha: 0.02841, time: 50.88093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   175 ----
[CW] collect: return: 426.88171, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.97029, qf2_loss: 0.97045, policy_loss: -69.84425, policy_entropy: -4.93456, alpha: 0.02812, time: 50.96826
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   176 ----
[CW] collect: return: 452.00937, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.94646, qf2_loss: 0.94920, policy_loss: -70.74924, policy_entropy: -5.10423, alpha: 0.02784, time: 50.82469
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   177 ----
[CW] collect: return: 447.81730, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 1.03076, qf2_loss: 1.02438, policy_loss: -71.06730, policy_entropy: -5.28833, alpha: 0.02763, time: 50.77260
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   178 ----
[CW] collect: return: 389.95799, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 1.13419, qf2_loss: 1.12484, policy_loss: -70.66417, policy_entropy: -5.25511, alpha: 0.02741, time: 50.80327
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   179 ----
[CW] collect: return: 248.89970, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 1.10463, qf2_loss: 1.09709, policy_loss: -71.16897, policy_entropy: -5.35991, alpha: 0.02721, time: 50.60976
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   180 ----
[CW] collect: return: 479.03057, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 1.12095, qf2_loss: 1.11124, policy_loss: -71.39068, policy_entropy: -5.50579, alpha: 0.02704, time: 50.68507
[CW] eval: return: 396.36379, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   181 ----
[CW] collect: return: 421.09002, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 1.14531, qf2_loss: 1.14358, policy_loss: -71.30701, policy_entropy: -5.46555, alpha: 0.02687, time: 51.34236
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   182 ----
[CW] collect: return: 428.35524, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.98687, qf2_loss: 0.97874, policy_loss: -72.05280, policy_entropy: -5.43724, alpha: 0.02669, time: 51.26552
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   183 ----
[CW] collect: return: 471.45404, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 1.01220, qf2_loss: 1.00254, policy_loss: -72.04329, policy_entropy: -5.52427, alpha: 0.02652, time: 51.18993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   184 ----
[CW] collect: return: 457.66454, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 1.11835, qf2_loss: 1.11987, policy_loss: -73.14353, policy_entropy: -5.59179, alpha: 0.02636, time: 51.58616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   185 ----
[CW] collect: return: 501.04787, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.99181, qf2_loss: 0.98913, policy_loss: -72.74136, policy_entropy: -5.44740, alpha: 0.02620, time: 51.29413
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   186 ----
[CW] collect: return: 468.98204, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 1.09678, qf2_loss: 1.09033, policy_loss: -73.74558, policy_entropy: -5.67849, alpha: 0.02603, time: 51.10835
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   187 ----
[CW] collect: return: 171.10297, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 1.10745, qf2_loss: 1.10790, policy_loss: -73.57251, policy_entropy: -5.77421, alpha: 0.02594, time: 51.10330
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   188 ----
[CW] collect: return: 486.89854, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 1.05911, qf2_loss: 1.06052, policy_loss: -74.06054, policy_entropy: -5.84310, alpha: 0.02584, time: 51.00210
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   189 ----
[CW] collect: return: 503.70026, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 1.32153, qf2_loss: 1.31570, policy_loss: -74.53979, policy_entropy: -5.94506, alpha: 0.02581, time: 51.48468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   190 ----
[CW] collect: return: 70.29716, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 1.63472, qf2_loss: 1.63273, policy_loss: -74.38621, policy_entropy: -5.91784, alpha: 0.02577, time: 50.98005
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   191 ----
[CW] collect: return: 409.54678, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 1.30258, qf2_loss: 1.30967, policy_loss: -75.27492, policy_entropy: -6.01671, alpha: 0.02574, time: 51.35003
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   192 ----
[CW] collect: return: 440.12825, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 1.44743, qf2_loss: 1.43854, policy_loss: -74.69350, policy_entropy: -6.03908, alpha: 0.02577, time: 51.20013
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   193 ----
[CW] collect: return: 61.64968, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 1.30974, qf2_loss: 1.30952, policy_loss: -75.14007, policy_entropy: -5.95650, alpha: 0.02577, time: 50.89658
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   194 ----
[CW] collect: return: 490.43847, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 1.36027, qf2_loss: 1.35619, policy_loss: -75.34334, policy_entropy: -6.03512, alpha: 0.02576, time: 50.82012
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   195 ----
[CW] collect: return: 53.09971, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 1.18399, qf2_loss: 1.20459, policy_loss: -75.48725, policy_entropy: -6.00445, alpha: 0.02577, time: 50.74257
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   196 ----
[CW] collect: return: 71.90915, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 1.17095, qf2_loss: 1.17633, policy_loss: -75.84329, policy_entropy: -6.12048, alpha: 0.02581, time: 51.01995
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   197 ----
[CW] collect: return: 534.11422, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 1.49024, qf2_loss: 1.48058, policy_loss: -76.20580, policy_entropy: -6.02885, alpha: 0.02587, time: 50.86592
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   198 ----
[CW] collect: return: 221.84379, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 1.23128, qf2_loss: 1.22914, policy_loss: -76.51456, policy_entropy: -5.95036, alpha: 0.02584, time: 50.84448
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   199 ----
[CW] collect: return: 409.48020, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 1.51633, qf2_loss: 1.52263, policy_loss: -77.57224, policy_entropy: -6.23209, alpha: 0.02588, time: 50.75068
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   200 ----
[CW] collect: return: 296.67224, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 3.06166, qf2_loss: 3.05227, policy_loss: -77.38060, policy_entropy: -6.14494, alpha: 0.02602, time: 50.99762
[CW] eval: return: 163.08352, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   201 ----
[CW] collect: return: 244.32188, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 1.44860, qf2_loss: 1.45452, policy_loss: -76.93237, policy_entropy: -5.95664, alpha: 0.02606, time: 51.44056
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   202 ----
[CW] collect: return: 148.01458, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 1.35786, qf2_loss: 1.35243, policy_loss: -76.65830, policy_entropy: -5.98190, alpha: 0.02604, time: 51.85975
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   203 ----
[CW] collect: return: 443.75582, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 1.60773, qf2_loss: 1.63243, policy_loss: -77.16842, policy_entropy: -6.11858, alpha: 0.02606, time: 51.09241
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   204 ----
[CW] collect: return: 217.80220, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 1.34657, qf2_loss: 1.36730, policy_loss: -77.09427, policy_entropy: -5.91934, alpha: 0.02606, time: 51.16609
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   205 ----
[CW] collect: return: 501.03302, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 1.99836, qf2_loss: 1.99732, policy_loss: -78.14939, policy_entropy: -6.19529, alpha: 0.02612, time: 50.99401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   206 ----
[CW] collect: return: 125.73197, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 1.46944, qf2_loss: 1.47865, policy_loss: -78.15988, policy_entropy: -6.17307, alpha: 0.02629, time: 50.85396
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   207 ----
[CW] collect: return: 201.04599, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 1.47252, qf2_loss: 1.47916, policy_loss: -78.19372, policy_entropy: -5.94432, alpha: 0.02631, time: 51.05301
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   208 ----
[CW] collect: return: 401.84686, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 1.67991, qf2_loss: 1.69767, policy_loss: -79.12110, policy_entropy: -6.31625, alpha: 0.02643, time: 50.88785
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   209 ----
[CW] collect: return: 510.75965, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 1.59340, qf2_loss: 1.61054, policy_loss: -78.98331, policy_entropy: -6.34799, alpha: 0.02665, time: 50.95216
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   210 ----
[CW] collect: return: 523.56466, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 2.05153, qf2_loss: 2.11052, policy_loss: -79.14010, policy_entropy: -6.31413, alpha: 0.02693, time: 51.11429
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   211 ----
[CW] collect: return: 162.02595, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 2.38710, qf2_loss: 2.33556, policy_loss: -79.47623, policy_entropy: -6.21081, alpha: 0.02719, time: 50.96735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   212 ----
[CW] collect: return: 514.96992, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 1.61817, qf2_loss: 1.63996, policy_loss: -79.80026, policy_entropy: -6.23655, alpha: 0.02745, time: 50.96816
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   213 ----
[CW] collect: return: 250.32817, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 1.85707, qf2_loss: 1.86267, policy_loss: -80.00035, policy_entropy: -6.23680, alpha: 0.02760, time: 50.78280
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   214 ----
[CW] collect: return: 551.29493, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 2.62716, qf2_loss: 2.69273, policy_loss: -80.74110, policy_entropy: -6.31165, alpha: 0.02790, time: 51.25666
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   215 ----
[CW] collect: return: 88.76913, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 2.08900, qf2_loss: 2.12037, policy_loss: -80.81857, policy_entropy: -6.32906, alpha: 0.02820, time: 50.91864
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   216 ----
[CW] collect: return: 493.81309, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 1.78081, qf2_loss: 1.80272, policy_loss: -79.93057, policy_entropy: -6.02315, alpha: 0.02836, time: 50.89701
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   217 ----
[CW] collect: return: 511.88385, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 1.90588, qf2_loss: 1.92991, policy_loss: -82.18846, policy_entropy: -6.29962, alpha: 0.02857, time: 51.06002
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   218 ----
[CW] collect: return: 486.32743, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 2.70629, qf2_loss: 2.73185, policy_loss: -81.78076, policy_entropy: -5.92778, alpha: 0.02869, time: 50.50856
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   219 ----
[CW] collect: return: 502.05159, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 3.31217, qf2_loss: 3.36227, policy_loss: -82.66722, policy_entropy: -6.07841, alpha: 0.02867, time: 50.97668
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   220 ----
[CW] collect: return: 505.24291, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 1.97690, qf2_loss: 1.98504, policy_loss: -82.37622, policy_entropy: -5.82539, alpha: 0.02872, time: 50.67550
[CW] eval: return: 467.13505, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   221 ----
[CW] collect: return: 497.99030, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 2.19392, qf2_loss: 2.21609, policy_loss: -83.34472, policy_entropy: -6.18988, alpha: 0.02862, time: 51.09326
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   222 ----
[CW] collect: return: 486.51497, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 2.04456, qf2_loss: 2.04811, policy_loss: -83.47990, policy_entropy: -6.13879, alpha: 0.02880, time: 50.60838
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   223 ----
[CW] collect: return: 481.73162, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 2.17721, qf2_loss: 2.18720, policy_loss: -83.44196, policy_entropy: -6.14130, alpha: 0.02896, time: 52.61562
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   224 ----
[CW] collect: return: 401.60286, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 2.13034, qf2_loss: 2.17101, policy_loss: -83.68993, policy_entropy: -5.98399, alpha: 0.02901, time: 50.72260
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   225 ----
[CW] collect: return: 496.85528, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 2.09062, qf2_loss: 2.13310, policy_loss: -84.84272, policy_entropy: -6.05173, alpha: 0.02907, time: 50.73127
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   226 ----
[CW] collect: return: 538.14115, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 2.54419, qf2_loss: 2.55776, policy_loss: -85.16055, policy_entropy: -6.11872, alpha: 0.02915, time: 50.88156
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   227 ----
[CW] collect: return: 328.09311, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 2.82761, qf2_loss: 2.89648, policy_loss: -85.86999, policy_entropy: -6.23185, alpha: 0.02927, time: 50.65568
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   228 ----
[CW] collect: return: 440.67565, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 2.53377, qf2_loss: 2.52874, policy_loss: -85.46157, policy_entropy: -6.11403, alpha: 0.02959, time: 50.80212
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   229 ----
[CW] collect: return: 503.78627, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 2.31457, qf2_loss: 2.35270, policy_loss: -85.96309, policy_entropy: -6.17828, alpha: 0.02975, time: 50.95138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   230 ----
[CW] collect: return: 403.45576, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 2.34108, qf2_loss: 2.37448, policy_loss: -86.91015, policy_entropy: -6.41613, alpha: 0.03002, time: 50.71799
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   231 ----
[CW] collect: return: 428.89137, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 3.20923, qf2_loss: 3.28239, policy_loss: -86.40659, policy_entropy: -6.25762, alpha: 0.03053, time: 51.18740
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   232 ----
[CW] collect: return: 513.31317, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 4.65883, qf2_loss: 4.71882, policy_loss: -87.29605, policy_entropy: -6.24802, alpha: 0.03090, time: 51.00651
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   233 ----
[CW] collect: return: 452.46508, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 3.86123, qf2_loss: 3.93632, policy_loss: -87.36264, policy_entropy: -6.19659, alpha: 0.03119, time: 50.73164
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   234 ----
[CW] collect: return: 520.14763, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 2.49992, qf2_loss: 2.55096, policy_loss: -87.61074, policy_entropy: -6.21415, alpha: 0.03135, time: 50.91729
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   235 ----
[CW] collect: return: 76.08706, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 2.43023, qf2_loss: 2.43858, policy_loss: -87.36359, policy_entropy: -6.10852, alpha: 0.03167, time: 50.73021
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   236 ----
[CW] collect: return: 383.40266, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 2.71020, qf2_loss: 2.71786, policy_loss: -88.75483, policy_entropy: -6.16182, alpha: 0.03188, time: 50.47596
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   237 ----
[CW] collect: return: 530.65907, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 2.29871, qf2_loss: 2.33341, policy_loss: -87.60188, policy_entropy: -5.86837, alpha: 0.03190, time: 50.49559
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   238 ----
[CW] collect: return: 477.45632, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 2.54472, qf2_loss: 2.56809, policy_loss: -89.00762, policy_entropy: -6.05208, alpha: 0.03188, time: 50.41313
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   239 ----
[CW] collect: return: 450.80417, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 3.95548, qf2_loss: 4.03799, policy_loss: -88.87878, policy_entropy: -5.98683, alpha: 0.03184, time: 50.73349
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   240 ----
[CW] collect: return: 432.68040, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 3.74387, qf2_loss: 3.74021, policy_loss: -89.84813, policy_entropy: -6.13507, alpha: 0.03189, time: 50.69208
[CW] eval: return: 488.47626, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   241 ----
[CW] collect: return: 509.52385, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 3.49145, qf2_loss: 3.58710, policy_loss: -90.19860, policy_entropy: -6.10837, alpha: 0.03209, time: 50.61035
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   242 ----
[CW] collect: return: 437.13025, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 3.23993, qf2_loss: 3.27856, policy_loss: -90.39044, policy_entropy: -6.17702, alpha: 0.03228, time: 50.67891
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   243 ----
[CW] collect: return: 482.29035, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 3.09551, qf2_loss: 3.14634, policy_loss: -90.82783, policy_entropy: -6.24856, alpha: 0.03256, time: 50.96057
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   244 ----
[CW] collect: return: 509.96901, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 2.83399, qf2_loss: 2.83773, policy_loss: -91.36058, policy_entropy: -6.36840, alpha: 0.03303, time: 50.56141
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   245 ----
[CW] collect: return: 424.04599, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 2.80843, qf2_loss: 2.85565, policy_loss: -91.29348, policy_entropy: -6.12045, alpha: 0.03346, time: 50.55140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   246 ----
[CW] collect: return: 506.58936, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 3.00835, qf2_loss: 3.06709, policy_loss: -92.32034, policy_entropy: -6.19870, alpha: 0.03363, time: 51.14129
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   247 ----
[CW] collect: return: 96.29524, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 3.15221, qf2_loss: 3.18479, policy_loss: -91.12069, policy_entropy: -6.26947, alpha: 0.03406, time: 50.47991
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   248 ----
[CW] collect: return: 448.02353, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 4.72923, qf2_loss: 4.79496, policy_loss: -92.20259, policy_entropy: -6.19281, alpha: 0.03439, time: 50.78836
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   249 ----
[CW] collect: return: 463.18191, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 2.70029, qf2_loss: 2.73374, policy_loss: -91.58228, policy_entropy: -6.05483, alpha: 0.03462, time: 51.26091
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   250 ----
[CW] collect: return: 454.96525, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 2.63933, qf2_loss: 2.64875, policy_loss: -93.15892, policy_entropy: -6.09890, alpha: 0.03468, time: 50.52548
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   251 ----
[CW] collect: return: 489.02521, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 3.07708, qf2_loss: 3.11162, policy_loss: -92.36000, policy_entropy: -5.90538, alpha: 0.03477, time: 50.72225
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   252 ----
[CW] collect: return: 53.78827, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 3.55764, qf2_loss: 3.63075, policy_loss: -93.68763, policy_entropy: -5.96573, alpha: 0.03464, time: 50.71856
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   253 ----
[CW] collect: return: 425.42530, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 4.10270, qf2_loss: 4.08418, policy_loss: -94.02348, policy_entropy: -6.02823, alpha: 0.03462, time: 50.69402
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   254 ----
[CW] collect: return: 444.82296, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 4.24852, qf2_loss: 4.28152, policy_loss: -93.35424, policy_entropy: -6.12689, alpha: 0.03473, time: 51.03348
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   255 ----
[CW] collect: return: 363.21765, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 3.23989, qf2_loss: 3.31294, policy_loss: -94.20233, policy_entropy: -6.07698, alpha: 0.03495, time: 51.01884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   256 ----
[CW] collect: return: 471.93663, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 3.31696, qf2_loss: 3.33369, policy_loss: -95.25475, policy_entropy: -6.16776, alpha: 0.03506, time: 50.97961
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   257 ----
[CW] collect: return: 462.09097, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 3.16563, qf2_loss: 3.21229, policy_loss: -94.71721, policy_entropy: -6.19343, alpha: 0.03543, time: 51.29306
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   258 ----
[CW] collect: return: 430.97430, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 3.13041, qf2_loss: 3.17780, policy_loss: -94.88513, policy_entropy: -6.08646, alpha: 0.03565, time: 50.99251
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   259 ----
[CW] collect: return: 491.44342, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 2.82377, qf2_loss: 2.84933, policy_loss: -95.58502, policy_entropy: -6.26991, alpha: 0.03595, time: 51.06138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   260 ----
[CW] collect: return: 86.87496, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 3.14503, qf2_loss: 3.18929, policy_loss: -94.34840, policy_entropy: -6.11449, alpha: 0.03641, time: 50.90529
[CW] eval: return: 472.55373, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   261 ----
[CW] collect: return: 455.32503, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 3.41435, qf2_loss: 3.44198, policy_loss: -95.23399, policy_entropy: -6.12672, alpha: 0.03662, time: 51.12556
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   262 ----
[CW] collect: return: 484.22032, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 2.86347, qf2_loss: 2.88128, policy_loss: -95.85943, policy_entropy: -5.88095, alpha: 0.03657, time: 51.25400
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   263 ----
[CW] collect: return: 487.80868, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 3.61958, qf2_loss: 3.64254, policy_loss: -95.44096, policy_entropy: -5.81861, alpha: 0.03639, time: 51.31146
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   264 ----
[CW] collect: return: 478.12457, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 5.00941, qf2_loss: 5.10024, policy_loss: -96.73047, policy_entropy: -6.08934, alpha: 0.03623, time: 51.05089
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   265 ----
[CW] collect: return: 463.44090, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 6.34343, qf2_loss: 6.37772, policy_loss: -96.61726, policy_entropy: -6.17629, alpha: 0.03642, time: 50.90288
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   266 ----
[CW] collect: return: 515.99064, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 3.73009, qf2_loss: 3.75369, policy_loss: -98.18594, policy_entropy: -6.01130, alpha: 0.03671, time: 51.02580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   267 ----
[CW] collect: return: 494.37223, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 2.90865, qf2_loss: 2.92316, policy_loss: -97.10215, policy_entropy: -6.13791, alpha: 0.03680, time: 51.03672
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   268 ----
[CW] collect: return: 475.33183, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 2.68563, qf2_loss: 2.66567, policy_loss: -97.92429, policy_entropy: -5.98442, alpha: 0.03695, time: 50.96780
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   269 ----
[CW] collect: return: 149.92762, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 2.80907, qf2_loss: 2.81235, policy_loss: -97.78451, policy_entropy: -6.07310, alpha: 0.03695, time: 50.80626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   270 ----
[CW] collect: return: 512.83661, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 3.15049, qf2_loss: 3.20836, policy_loss: -98.66015, policy_entropy: -6.17273, alpha: 0.03727, time: 50.76010
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   271 ----
[CW] collect: return: 480.70480, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 3.18041, qf2_loss: 3.17627, policy_loss: -99.08879, policy_entropy: -6.12889, alpha: 0.03753, time: 51.00945
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   272 ----
[CW] collect: return: 82.29595, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 3.33430, qf2_loss: 3.38271, policy_loss: -98.20907, policy_entropy: -5.84751, alpha: 0.03763, time: 51.45443
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   273 ----
[CW] collect: return: 500.46806, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 3.53746, qf2_loss: 3.56628, policy_loss: -99.07987, policy_entropy: -5.90805, alpha: 0.03732, time: 51.14800
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   274 ----
[CW] collect: return: 502.55776, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 2.88439, qf2_loss: 2.85819, policy_loss: -99.14412, policy_entropy: -5.91846, alpha: 0.03714, time: 50.82574
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   275 ----
[CW] collect: return: 421.11605, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 2.86961, qf2_loss: 2.89152, policy_loss: -98.94644, policy_entropy: -5.84773, alpha: 0.03692, time: 50.86823
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   276 ----
[CW] collect: return: 451.53758, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 2.70381, qf2_loss: 2.69145, policy_loss: -100.69603, policy_entropy: -5.91307, alpha: 0.03666, time: 50.75099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   277 ----
[CW] collect: return: 451.96657, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 3.30585, qf2_loss: 3.33807, policy_loss: -100.14867, policy_entropy: -6.07167, alpha: 0.03661, time: 50.83313
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   278 ----
[CW] collect: return: 540.47515, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 3.22551, qf2_loss: 3.23283, policy_loss: -100.96721, policy_entropy: -6.02632, alpha: 0.03674, time: 50.86286
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   279 ----
[CW] collect: return: 472.91744, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 3.69081, qf2_loss: 3.75928, policy_loss: -101.19857, policy_entropy: -6.11531, alpha: 0.03694, time: 51.11599
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   280 ----
[CW] collect: return: 537.87863, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 3.15156, qf2_loss: 3.17905, policy_loss: -101.18516, policy_entropy: -5.90856, alpha: 0.03688, time: 50.87590
[CW] eval: return: 501.40874, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   281 ----
[CW] collect: return: 507.33342, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 2.97738, qf2_loss: 2.95020, policy_loss: -100.65928, policy_entropy: -6.03539, alpha: 0.03680, time: 51.07699
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   282 ----
[CW] collect: return: 450.03535, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 3.36815, qf2_loss: 3.40780, policy_loss: -101.13701, policy_entropy: -6.13041, alpha: 0.03700, time: 50.80725
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   283 ----
[CW] collect: return: 442.57127, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 3.64629, qf2_loss: 3.63876, policy_loss: -101.13445, policy_entropy: -5.99306, alpha: 0.03709, time: 50.88085
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   284 ----
[CW] collect: return: 576.24486, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 3.69093, qf2_loss: 3.71404, policy_loss: -102.65736, policy_entropy: -6.17436, alpha: 0.03726, time: 51.19861
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   285 ----
[CW] collect: return: 397.46128, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 3.42232, qf2_loss: 3.39113, policy_loss: -103.27529, policy_entropy: -6.08096, alpha: 0.03753, time: 51.17816
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   286 ----
[CW] collect: return: 477.12538, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 3.45687, qf2_loss: 3.43290, policy_loss: -103.96214, policy_entropy: -6.08699, alpha: 0.03765, time: 50.96290
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   287 ----
[CW] collect: return: 492.24771, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 3.54747, qf2_loss: 3.55373, policy_loss: -103.68954, policy_entropy: -5.86605, alpha: 0.03764, time: 50.84674
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   288 ----
[CW] collect: return: 504.05649, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 2.92634, qf2_loss: 2.92771, policy_loss: -102.55573, policy_entropy: -5.84463, alpha: 0.03741, time: 50.82793
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   289 ----
[CW] collect: return: 468.94811, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 2.74929, qf2_loss: 2.75336, policy_loss: -103.77750, policy_entropy: -5.98289, alpha: 0.03719, time: 50.91616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   290 ----
[CW] collect: return: 209.68561, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 3.12753, qf2_loss: 3.13331, policy_loss: -104.12438, policy_entropy: -6.08101, alpha: 0.03729, time: 50.71280
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   291 ----
[CW] collect: return: 422.05792, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 3.22230, qf2_loss: 3.24506, policy_loss: -104.75313, policy_entropy: -5.94473, alpha: 0.03735, time: 51.10809
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   292 ----
[CW] collect: return: 509.05585, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 2.93189, qf2_loss: 2.95776, policy_loss: -105.16392, policy_entropy: -5.99092, alpha: 0.03718, time: 51.19435
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   293 ----
[CW] collect: return: 552.39592, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 2.89734, qf2_loss: 2.88096, policy_loss: -104.21151, policy_entropy: -5.91221, alpha: 0.03713, time: 51.20322
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   294 ----
[CW] collect: return: 535.70185, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 4.18430, qf2_loss: 4.21129, policy_loss: -104.16282, policy_entropy: -6.03232, alpha: 0.03710, time: 51.35509
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   295 ----
[CW] collect: return: 523.45515, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 11.11964, qf2_loss: 11.11036, policy_loss: -105.67199, policy_entropy: -6.56205, alpha: 0.03746, time: 51.13000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   296 ----
[CW] collect: return: 460.27875, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 4.94549, qf2_loss: 4.93101, policy_loss: -106.02487, policy_entropy: -6.27810, alpha: 0.03839, time: 50.74483
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   297 ----
[CW] collect: return: 563.78427, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 3.38252, qf2_loss: 3.35010, policy_loss: -105.97858, policy_entropy: -5.93643, alpha: 0.03851, time: 50.75379
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   298 ----
[CW] collect: return: 542.44540, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 2.97803, qf2_loss: 2.94759, policy_loss: -105.49492, policy_entropy: -5.91903, alpha: 0.03845, time: 50.81075
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   299 ----
[CW] collect: return: 420.42499, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 2.93186, qf2_loss: 2.90777, policy_loss: -106.15234, policy_entropy: -5.77816, alpha: 0.03819, time: 50.80388
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   300 ----
[CW] collect: return: 494.03123, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 3.35974, qf2_loss: 3.38614, policy_loss: -106.37100, policy_entropy: -5.78904, alpha: 0.03785, time: 51.04237
[CW] eval: return: 516.35753, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   301 ----
[CW] collect: return: 605.81127, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 3.81847, qf2_loss: 3.87230, policy_loss: -106.51090, policy_entropy: -5.95750, alpha: 0.03751, time: 51.27468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   302 ----
[CW] collect: return: 531.05084, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 3.91088, qf2_loss: 3.89433, policy_loss: -107.76180, policy_entropy: -6.07120, alpha: 0.03761, time: 51.28623
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   303 ----
[CW] collect: return: 547.98893, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 3.82265, qf2_loss: 3.88163, policy_loss: -107.35587, policy_entropy: -5.99605, alpha: 0.03761, time: 51.21858
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   304 ----
[CW] collect: return: 102.29457, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 3.25935, qf2_loss: 3.24319, policy_loss: -108.21135, policy_entropy: -6.08894, alpha: 0.03775, time: 50.88417
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   305 ----
[CW] collect: return: 526.30381, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 3.19301, qf2_loss: 3.16210, policy_loss: -108.61226, policy_entropy: -6.02337, alpha: 0.03785, time: 50.77560
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   306 ----
[CW] collect: return: 437.36908, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 3.32657, qf2_loss: 3.29262, policy_loss: -108.06034, policy_entropy: -5.85105, alpha: 0.03772, time: 51.01400
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   307 ----
[CW] collect: return: 246.90993, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 4.63049, qf2_loss: 4.70359, policy_loss: -109.41292, policy_entropy: -6.06300, alpha: 0.03761, time: 50.71959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   308 ----
[CW] collect: return: 482.99921, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 7.01000, qf2_loss: 6.87661, policy_loss: -108.46821, policy_entropy: -6.26550, alpha: 0.03786, time: 50.86519
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   309 ----
[CW] collect: return: 470.85900, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 5.41659, qf2_loss: 5.51488, policy_loss: -108.77897, policy_entropy: -6.10721, alpha: 0.03824, time: 51.17453
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   310 ----
[CW] collect: return: 571.69478, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 4.30144, qf2_loss: 4.32401, policy_loss: -109.52479, policy_entropy: -6.18497, alpha: 0.03854, time: 51.25966
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   311 ----
[CW] collect: return: 516.51381, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 6.53150, qf2_loss: 6.43103, policy_loss: -109.30026, policy_entropy: -6.04862, alpha: 0.03870, time: 51.04377
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   312 ----
[CW] collect: return: 198.70153, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 3.99307, qf2_loss: 3.94332, policy_loss: -110.22251, policy_entropy: -5.96727, alpha: 0.03878, time: 51.88921
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   313 ----
[CW] collect: return: 513.77711, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 3.32295, qf2_loss: 3.31826, policy_loss: -110.79193, policy_entropy: -5.93496, alpha: 0.03868, time: 51.29566
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   314 ----
[CW] collect: return: 384.98977, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 3.65749, qf2_loss: 3.68522, policy_loss: -109.96854, policy_entropy: -5.92722, alpha: 0.03850, time: 51.06684
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   315 ----
[CW] collect: return: 600.08264, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 3.75539, qf2_loss: 3.72830, policy_loss: -110.00698, policy_entropy: -5.89127, alpha: 0.03834, time: 50.86631
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   316 ----
[CW] collect: return: 496.61328, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 3.50839, qf2_loss: 3.55801, policy_loss: -110.88361, policy_entropy: -5.99502, alpha: 0.03825, time: 50.79319
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   317 ----
[CW] collect: return: 543.06774, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 4.71451, qf2_loss: 4.75762, policy_loss: -111.50921, policy_entropy: -5.89235, alpha: 0.03817, time: 53.17668
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   318 ----
[CW] collect: return: 469.93510, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 3.89705, qf2_loss: 3.86955, policy_loss: -112.41802, policy_entropy: -6.04030, alpha: 0.03805, time: 51.17596
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   319 ----
[CW] collect: return: 449.70993, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 3.91607, qf2_loss: 3.95867, policy_loss: -111.25396, policy_entropy: -5.96771, alpha: 0.03805, time: 50.66004
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   320 ----
[CW] collect: return: 511.51476, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 6.77164, qf2_loss: 6.76221, policy_loss: -110.90627, policy_entropy: -5.97715, alpha: 0.03803, time: 50.85775
[CW] eval: return: 514.47142, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   321 ----
[CW] collect: return: 545.82917, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 7.03288, qf2_loss: 7.04286, policy_loss: -112.07771, policy_entropy: -5.98669, alpha: 0.03802, time: 51.02961
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   322 ----
[CW] collect: return: 217.42416, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 4.53374, qf2_loss: 4.51185, policy_loss: -111.46297, policy_entropy: -5.99393, alpha: 0.03794, time: 51.34074
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   323 ----
[CW] collect: return: 460.46063, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 4.86292, qf2_loss: 4.88886, policy_loss: -112.95457, policy_entropy: -6.16824, alpha: 0.03819, time: 51.22113
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   324 ----
[CW] collect: return: 515.76950, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 4.18423, qf2_loss: 4.12325, policy_loss: -112.62909, policy_entropy: -6.12013, alpha: 0.03841, time: 51.04902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   325 ----
[CW] collect: return: 507.39834, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 4.59025, qf2_loss: 4.57991, policy_loss: -113.69903, policy_entropy: -6.28819, alpha: 0.03877, time: 50.98024
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   326 ----
[CW] collect: return: 524.59733, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 4.92156, qf2_loss: 4.90656, policy_loss: -113.69484, policy_entropy: -6.04661, alpha: 0.03906, time: 51.03635
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   327 ----
[CW] collect: return: 155.81148, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 3.96597, qf2_loss: 3.94397, policy_loss: -113.67699, policy_entropy: -5.93147, alpha: 0.03906, time: 51.18831
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   328 ----
[CW] collect: return: 480.27532, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 3.60861, qf2_loss: 3.64740, policy_loss: -114.48189, policy_entropy: -6.03635, alpha: 0.03903, time: 51.22823
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   329 ----
[CW] collect: return: 561.46930, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 6.49291, qf2_loss: 6.44268, policy_loss: -113.97649, policy_entropy: -6.13312, alpha: 0.03919, time: 50.69212
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   330 ----
[CW] collect: return: 489.34517, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 4.55248, qf2_loss: 4.57532, policy_loss: -113.75244, policy_entropy: -6.11441, alpha: 0.03943, time: 50.90914
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   331 ----
[CW] collect: return: 535.80815, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 4.18207, qf2_loss: 4.15450, policy_loss: -116.03326, policy_entropy: -6.10323, alpha: 0.03970, time: 51.06605
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   332 ----
[CW] collect: return: 506.24885, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 3.78817, qf2_loss: 3.78999, policy_loss: -115.27299, policy_entropy: -6.10064, alpha: 0.03990, time: 51.41391
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   333 ----
[CW] collect: return: 516.14786, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 3.62912, qf2_loss: 3.61120, policy_loss: -116.21887, policy_entropy: -6.08047, alpha: 0.04004, time: 51.26902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   334 ----
[CW] collect: return: 565.22537, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 5.16198, qf2_loss: 5.12339, policy_loss: -116.74507, policy_entropy: -6.05874, alpha: 0.04027, time: 51.27749
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   335 ----
[CW] collect: return: 469.46679, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 7.78393, qf2_loss: 7.68853, policy_loss: -115.89646, policy_entropy: -5.96835, alpha: 0.04033, time: 51.17140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   336 ----
[CW] collect: return: 441.63715, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 5.35317, qf2_loss: 5.36643, policy_loss: -115.44960, policy_entropy: -6.03509, alpha: 0.04030, time: 50.68191
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   337 ----
[CW] collect: return: 553.71748, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 6.59838, qf2_loss: 6.62424, policy_loss: -116.41922, policy_entropy: -5.93600, alpha: 0.04029, time: 50.87472
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   338 ----
[CW] collect: return: 459.96527, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 4.80390, qf2_loss: 4.77651, policy_loss: -117.14534, policy_entropy: -6.09398, alpha: 0.04026, time: 50.92388
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   339 ----
[CW] collect: return: 508.53258, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 3.89151, qf2_loss: 3.83569, policy_loss: -117.84718, policy_entropy: -5.93041, alpha: 0.04020, time: 50.53289
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   340 ----
[CW] collect: return: 522.10229, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 4.22068, qf2_loss: 4.22970, policy_loss: -118.01361, policy_entropy: -6.05496, alpha: 0.04030, time: 50.84057
[CW] eval: return: 436.16259, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   341 ----
[CW] collect: return: 401.59431, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 5.34912, qf2_loss: 5.33616, policy_loss: -118.67222, policy_entropy: -6.17525, alpha: 0.04046, time: 50.77695
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   342 ----
[CW] collect: return: 193.04777, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 6.48390, qf2_loss: 6.50716, policy_loss: -118.41079, policy_entropy: -6.15240, alpha: 0.04083, time: 50.68158
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   343 ----
[CW] collect: return: 505.80648, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 7.34746, qf2_loss: 7.31381, policy_loss: -118.45944, policy_entropy: -6.09668, alpha: 0.04108, time: 50.73046
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   344 ----
[CW] collect: return: 545.34193, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 5.34817, qf2_loss: 5.41213, policy_loss: -118.65178, policy_entropy: -6.10096, alpha: 0.04133, time: 50.96592
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   345 ----
[CW] collect: return: 523.40569, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 5.07623, qf2_loss: 5.07831, policy_loss: -118.37844, policy_entropy: -6.08468, alpha: 0.04143, time: 50.74462
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   346 ----
[CW] collect: return: 530.30717, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 4.33114, qf2_loss: 4.31599, policy_loss: -119.34456, policy_entropy: -6.26644, alpha: 0.04185, time: 52.09395
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   347 ----
[CW] collect: return: 528.52684, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 4.21405, qf2_loss: 4.24442, policy_loss: -119.03632, policy_entropy: -6.05460, alpha: 0.04234, time: 51.10270
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   348 ----
[CW] collect: return: 482.18845, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 4.39718, qf2_loss: 4.30130, policy_loss: -118.43389, policy_entropy: -5.74969, alpha: 0.04211, time: 51.01213
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   349 ----
[CW] collect: return: 456.15488, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 4.16172, qf2_loss: 4.13364, policy_loss: -118.63689, policy_entropy: -6.01071, alpha: 0.04175, time: 50.94834
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   350 ----
[CW] collect: return: 289.99461, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 4.53684, qf2_loss: 4.45448, policy_loss: -119.04403, policy_entropy: -5.76292, alpha: 0.04158, time: 50.94950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   351 ----
[CW] collect: return: 468.38822, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 4.15775, qf2_loss: 4.16230, policy_loss: -120.28509, policy_entropy: -6.26462, alpha: 0.04129, time: 51.04182
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   352 ----
[CW] collect: return: 513.50033, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 3.76683, qf2_loss: 3.74861, policy_loss: -119.59210, policy_entropy: -6.25492, alpha: 0.04204, time: 51.17207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   353 ----
[CW] collect: return: 528.99490, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 4.99623, qf2_loss: 4.99939, policy_loss: -120.87371, policy_entropy: -6.27849, alpha: 0.04265, time: 51.05236
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   354 ----
[CW] collect: return: 531.41058, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 4.74427, qf2_loss: 4.78081, policy_loss: -121.64417, policy_entropy: -6.25002, alpha: 0.04340, time: 50.87127
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   355 ----
[CW] collect: return: 518.24260, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 4.23112, qf2_loss: 4.21182, policy_loss: -120.20281, policy_entropy: -5.97756, alpha: 0.04372, time: 50.75816
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   356 ----
[CW] collect: return: 321.39846, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 6.16152, qf2_loss: 6.17014, policy_loss: -121.82499, policy_entropy: -6.04026, alpha: 0.04365, time: 51.30963
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   357 ----
[CW] collect: return: 487.73473, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 11.23574, qf2_loss: 11.21555, policy_loss: -121.12305, policy_entropy: -6.15581, alpha: 0.04395, time: 50.91628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   358 ----
[CW] collect: return: 462.62223, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 7.79072, qf2_loss: 7.71361, policy_loss: -121.52505, policy_entropy: -5.99015, alpha: 0.04409, time: 51.33683
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   359 ----
[CW] collect: return: 513.61872, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 5.13335, qf2_loss: 5.12177, policy_loss: -121.78900, policy_entropy: -5.96285, alpha: 0.04398, time: 51.24290
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   360 ----
[CW] collect: return: 547.48793, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 5.60642, qf2_loss: 5.58529, policy_loss: -121.94244, policy_entropy: -6.07313, alpha: 0.04410, time: 51.20593
[CW] eval: return: 467.22500, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   361 ----
[CW] collect: return: 563.76148, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 4.71177, qf2_loss: 4.62270, policy_loss: -121.58494, policy_entropy: -5.91744, alpha: 0.04409, time: 51.09680
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   362 ----
[CW] collect: return: 135.91057, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 5.67407, qf2_loss: 5.49038, policy_loss: -123.44302, policy_entropy: -5.97665, alpha: 0.04376, time: 51.13199
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   363 ----
[CW] collect: return: 612.51046, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 5.72862, qf2_loss: 5.78589, policy_loss: -123.59879, policy_entropy: -6.12738, alpha: 0.04397, time: 51.07187
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   364 ----
[CW] collect: return: 556.43582, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 5.83059, qf2_loss: 5.82185, policy_loss: -123.32270, policy_entropy: -5.98036, alpha: 0.04411, time: 51.01566
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   365 ----
[CW] collect: return: 523.62906, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 4.70551, qf2_loss: 4.73065, policy_loss: -123.64324, policy_entropy: -6.06776, alpha: 0.04428, time: 51.61406
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   366 ----
[CW] collect: return: 542.55960, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 4.12374, qf2_loss: 4.11064, policy_loss: -124.11031, policy_entropy: -6.16861, alpha: 0.04466, time: 51.32716
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   367 ----
[CW] collect: return: 559.41281, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 4.23854, qf2_loss: 4.17468, policy_loss: -123.32030, policy_entropy: -5.84046, alpha: 0.04471, time: 50.82634
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   368 ----
[CW] collect: return: 583.98843, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 4.54089, qf2_loss: 4.44353, policy_loss: -123.83006, policy_entropy: -5.74482, alpha: 0.04394, time: 50.96060
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   369 ----
[CW] collect: return: 569.38854, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 4.52719, qf2_loss: 4.50974, policy_loss: -122.51864, policy_entropy: -5.69691, alpha: 0.04337, time: 51.00006
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   370 ----
[CW] collect: return: 572.17704, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 4.02210, qf2_loss: 4.01138, policy_loss: -124.66213, policy_entropy: -5.99918, alpha: 0.04289, time: 51.03674
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   371 ----
[CW] collect: return: 572.01724, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 10.44555, qf2_loss: 10.39420, policy_loss: -125.61205, policy_entropy: -6.01992, alpha: 0.04300, time: 51.02399
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   372 ----
[CW] collect: return: 407.37227, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 6.31546, qf2_loss: 6.32049, policy_loss: -124.85977, policy_entropy: -5.94991, alpha: 0.04287, time: 51.01400
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   373 ----
[CW] collect: return: 498.83092, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 5.08659, qf2_loss: 5.11594, policy_loss: -124.96406, policy_entropy: -6.04205, alpha: 0.04288, time: 51.15185
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   374 ----
[CW] collect: return: 541.78852, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 5.10933, qf2_loss: 5.08737, policy_loss: -124.54994, policy_entropy: -5.91828, alpha: 0.04296, time: 50.95956
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   375 ----
[CW] collect: return: 535.42679, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 4.32134, qf2_loss: 4.27452, policy_loss: -125.78887, policy_entropy: -6.11514, alpha: 0.04289, time: 50.85944
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   376 ----
[CW] collect: return: 554.20707, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 5.08684, qf2_loss: 4.95705, policy_loss: -127.05911, policy_entropy: -6.01620, alpha: 0.04309, time: 51.07399
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   377 ----
[CW] collect: return: 456.93083, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 4.81515, qf2_loss: 4.80307, policy_loss: -126.54132, policy_entropy: -5.89482, alpha: 0.04299, time: 50.90188
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   378 ----
[CW] collect: return: 543.80265, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 6.21939, qf2_loss: 6.19387, policy_loss: -127.21886, policy_entropy: -6.01107, alpha: 0.04281, time: 51.05653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   379 ----
[CW] collect: return: 576.23518, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 5.73176, qf2_loss: 5.79357, policy_loss: -126.60045, policy_entropy: -5.89337, alpha: 0.04268, time: 51.01933
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   380 ----
[CW] collect: return: 561.82960, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 5.18020, qf2_loss: 5.03082, policy_loss: -128.54808, policy_entropy: -6.20146, alpha: 0.04279, time: 51.00970
[CW] eval: return: 458.98664, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   381 ----
[CW] collect: return: 534.85111, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 4.48486, qf2_loss: 4.40572, policy_loss: -128.49735, policy_entropy: -6.11428, alpha: 0.04320, time: 52.14365
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   382 ----
[CW] collect: return: 573.12588, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 5.35769, qf2_loss: 5.40797, policy_loss: -127.99257, policy_entropy: -5.93816, alpha: 0.04336, time: 51.07516
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   383 ----
[CW] collect: return: 455.52462, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 8.48340, qf2_loss: 8.54217, policy_loss: -128.03231, policy_entropy: -5.98155, alpha: 0.04313, time: 50.91100
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   384 ----
[CW] collect: return: 539.91133, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 8.71236, qf2_loss: 8.75203, policy_loss: -129.50337, policy_entropy: -6.16764, alpha: 0.04347, time: 50.85017
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   385 ----
[CW] collect: return: 520.67216, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 8.29267, qf2_loss: 8.29619, policy_loss: -128.35685, policy_entropy: -6.21850, alpha: 0.04380, time: 51.28242
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   386 ----
[CW] collect: return: 587.57084, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 5.21175, qf2_loss: 5.14572, policy_loss: -127.90168, policy_entropy: -6.23926, alpha: 0.04441, time: 51.15384
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   387 ----
[CW] collect: return: 47.77729, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 4.88798, qf2_loss: 4.83914, policy_loss: -128.24271, policy_entropy: -6.20906, alpha: 0.04500, time: 50.94828
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   388 ----
[CW] collect: return: 557.32983, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 5.59195, qf2_loss: 5.56091, policy_loss: -129.70072, policy_entropy: -6.12489, alpha: 0.04549, time: 50.93366
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   389 ----
[CW] collect: return: 39.40379, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 8.05607, qf2_loss: 7.98980, policy_loss: -130.73614, policy_entropy: -6.18056, alpha: 0.04598, time: 50.79718
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   390 ----
[CW] collect: return: 528.02569, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 5.28759, qf2_loss: 5.35520, policy_loss: -129.85571, policy_entropy: -5.94186, alpha: 0.04616, time: 50.86434
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   391 ----
[CW] collect: return: 566.62029, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 4.35618, qf2_loss: 4.30512, policy_loss: -130.43620, policy_entropy: -5.82650, alpha: 0.04579, time: 51.20453
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   392 ----
[CW] collect: return: 561.03416, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 4.50574, qf2_loss: 4.47232, policy_loss: -130.61654, policy_entropy: -5.89308, alpha: 0.04543, time: 51.33502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   393 ----
[CW] collect: return: 527.00074, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 4.30519, qf2_loss: 4.26545, policy_loss: -130.33775, policy_entropy: -5.82913, alpha: 0.04505, time: 51.56330
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   394 ----
[CW] collect: return: 532.18206, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 4.58670, qf2_loss: 4.51228, policy_loss: -131.25877, policy_entropy: -5.92575, alpha: 0.04467, time: 51.58116
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   395 ----
[CW] collect: return: 570.27736, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 4.82692, qf2_loss: 4.70214, policy_loss: -130.15356, policy_entropy: -5.87688, alpha: 0.04441, time: 51.38597
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   396 ----
[CW] collect: return: 563.76976, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 4.55013, qf2_loss: 4.52136, policy_loss: -130.49115, policy_entropy: -5.78672, alpha: 0.04405, time: 51.03155
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   397 ----
[CW] collect: return: 591.03547, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 5.77839, qf2_loss: 5.63324, policy_loss: -131.42047, policy_entropy: -6.00041, alpha: 0.04363, time: 51.21516
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   398 ----
[CW] collect: return: 558.72460, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 9.89614, qf2_loss: 9.89847, policy_loss: -132.86562, policy_entropy: -6.13304, alpha: 0.04387, time: 50.94253
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   399 ----
[CW] collect: return: 519.37177, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 9.80026, qf2_loss: 9.75627, policy_loss: -131.70918, policy_entropy: -6.14140, alpha: 0.04424, time: 51.13751
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   400 ----
[CW] collect: return: 181.65269, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 6.17036, qf2_loss: 6.11384, policy_loss: -131.24794, policy_entropy: -5.87449, alpha: 0.04442, time: 51.13082
[CW] eval: return: 466.51700, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   401 ----
[CW] collect: return: 542.47878, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 6.05311, qf2_loss: 5.91868, policy_loss: -131.56082, policy_entropy: -5.97821, alpha: 0.04418, time: 53.68383
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   402 ----
[CW] collect: return: 580.89520, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 5.19753, qf2_loss: 5.11937, policy_loss: -133.08471, policy_entropy: -5.96362, alpha: 0.04399, time: 50.83893
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   403 ----
[CW] collect: return: 584.07921, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 5.08773, qf2_loss: 4.97038, policy_loss: -132.61762, policy_entropy: -5.99149, alpha: 0.04388, time: 51.21822
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   404 ----
[CW] collect: return: 653.47568, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 5.23273, qf2_loss: 5.10835, policy_loss: -132.52059, policy_entropy: -5.88160, alpha: 0.04385, time: 50.78533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   405 ----
[CW] collect: return: 564.95914, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 5.37839, qf2_loss: 5.30342, policy_loss: -132.57962, policy_entropy: -5.87039, alpha: 0.04358, time: 50.91468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   406 ----
[CW] collect: return: 562.65225, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 4.80326, qf2_loss: 4.80189, policy_loss: -134.00585, policy_entropy: -6.03960, alpha: 0.04339, time: 50.93162
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   407 ----
[CW] collect: return: 376.69965, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 5.72856, qf2_loss: 5.71800, policy_loss: -132.71382, policy_entropy: -5.95299, alpha: 0.04337, time: 51.19199
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   408 ----
[CW] collect: return: 534.42721, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 7.45463, qf2_loss: 7.40174, policy_loss: -134.04158, policy_entropy: -5.96630, alpha: 0.04341, time: 51.10478
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   409 ----
[CW] collect: return: 530.75127, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 6.12176, qf2_loss: 5.99580, policy_loss: -135.04416, policy_entropy: -6.11567, alpha: 0.04339, time: 50.93770
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   410 ----
[CW] collect: return: 531.22167, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 4.77928, qf2_loss: 4.75957, policy_loss: -134.78928, policy_entropy: -6.02600, alpha: 0.04364, time: 51.10319
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   411 ----
[CW] collect: return: 592.35625, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 5.07781, qf2_loss: 5.14606, policy_loss: -135.61654, policy_entropy: -6.01447, alpha: 0.04369, time: 51.55889
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   412 ----
[CW] collect: return: 650.64060, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 4.99112, qf2_loss: 5.01565, policy_loss: -134.40473, policy_entropy: -5.92377, alpha: 0.04357, time: 51.41310
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   413 ----
[CW] collect: return: 533.64119, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 5.76925, qf2_loss: 5.77296, policy_loss: -136.93704, policy_entropy: -6.08803, alpha: 0.04354, time: 51.09125
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   414 ----
[CW] collect: return: 510.33730, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 5.80341, qf2_loss: 5.69889, policy_loss: -135.54260, policy_entropy: -5.93415, alpha: 0.04358, time: 51.23708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   415 ----
[CW] collect: return: 446.70799, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 5.80898, qf2_loss: 5.79376, policy_loss: -134.72893, policy_entropy: -5.86301, alpha: 0.04338, time: 51.06683
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   416 ----
[CW] collect: return: 583.51430, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 6.04294, qf2_loss: 6.02505, policy_loss: -135.32694, policy_entropy: -5.93956, alpha: 0.04315, time: 50.82897
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   417 ----
[CW] collect: return: 566.66469, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 7.35774, qf2_loss: 7.40766, policy_loss: -136.77996, policy_entropy: -6.09459, alpha: 0.04308, time: 51.01250
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   418 ----
[CW] collect: return: 590.14810, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 10.06368, qf2_loss: 10.25843, policy_loss: -136.07851, policy_entropy: -5.90474, alpha: 0.04325, time: 51.29338
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   419 ----
[CW] collect: return: 566.19996, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 6.93569, qf2_loss: 6.86053, policy_loss: -135.94517, policy_entropy: -6.11139, alpha: 0.04316, time: 51.04647
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   420 ----
[CW] collect: return: 516.03545, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 6.24972, qf2_loss: 6.22417, policy_loss: -136.27016, policy_entropy: -5.90478, alpha: 0.04327, time: 51.09619
[CW] eval: return: 557.46858, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   421 ----
[CW] collect: return: 615.53982, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 6.03387, qf2_loss: 5.97812, policy_loss: -137.81102, policy_entropy: -5.95808, alpha: 0.04298, time: 51.05078
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   422 ----
[CW] collect: return: 635.20905, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 5.43883, qf2_loss: 5.39043, policy_loss: -137.12893, policy_entropy: -6.20530, alpha: 0.04318, time: 51.94099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   423 ----
[CW] collect: return: 584.10858, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 5.24475, qf2_loss: 5.13568, policy_loss: -138.41101, policy_entropy: -6.14280, alpha: 0.04359, time: 51.42037
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   424 ----
[CW] collect: return: 559.32234, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 5.27600, qf2_loss: 5.25141, policy_loss: -138.85373, policy_entropy: -6.10944, alpha: 0.04387, time: 51.36384
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   425 ----
[CW] collect: return: 597.63555, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 5.14032, qf2_loss: 5.08691, policy_loss: -138.87577, policy_entropy: -6.02870, alpha: 0.04409, time: 51.30282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   426 ----
[CW] collect: return: 550.18807, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 8.98816, qf2_loss: 8.78099, policy_loss: -139.39921, policy_entropy: -6.12521, alpha: 0.04422, time: 51.50207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   427 ----
[CW] collect: return: 478.42946, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 7.23803, qf2_loss: 7.19909, policy_loss: -139.69566, policy_entropy: -6.14737, alpha: 0.04442, time: 51.28722
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   428 ----
[CW] collect: return: 630.98773, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 6.85479, qf2_loss: 6.88159, policy_loss: -139.14164, policy_entropy: -6.01533, alpha: 0.04483, time: 51.23463
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   429 ----
[CW] collect: return: 472.07144, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 11.04464, qf2_loss: 11.07553, policy_loss: -137.38995, policy_entropy: -6.02045, alpha: 0.04490, time: 51.58091
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   430 ----
[CW] collect: return: 531.35561, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 8.55494, qf2_loss: 8.54979, policy_loss: -139.64563, policy_entropy: -6.47739, alpha: 0.04533, time: 50.93693
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   431 ----
[CW] collect: return: 567.39282, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 6.37420, qf2_loss: 6.36213, policy_loss: -140.52054, policy_entropy: -6.19148, alpha: 0.04627, time: 51.01533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   432 ----
[CW] collect: return: 581.72306, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 6.16507, qf2_loss: 6.17684, policy_loss: -139.51238, policy_entropy: -6.07605, alpha: 0.04667, time: 51.01602
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   433 ----
[CW] collect: return: 528.02310, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 5.68291, qf2_loss: 5.65055, policy_loss: -140.73828, policy_entropy: -6.05262, alpha: 0.04678, time: 51.04584
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   434 ----
[CW] collect: return: 570.02313, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 5.83002, qf2_loss: 5.78234, policy_loss: -140.01636, policy_entropy: -6.03724, alpha: 0.04699, time: 50.99323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   435 ----
[CW] collect: return: 479.56635, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 4.99141, qf2_loss: 4.96092, policy_loss: -142.19131, policy_entropy: -6.08533, alpha: 0.04711, time: 51.36708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   436 ----
[CW] collect: return: 354.33021, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 5.51642, qf2_loss: 5.44748, policy_loss: -141.12935, policy_entropy: -5.99265, alpha: 0.04719, time: 50.91240
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   437 ----
[CW] collect: return: 504.05220, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 4.66339, qf2_loss: 4.66638, policy_loss: -140.40835, policy_entropy: -5.89034, alpha: 0.04716, time: 50.93384
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   438 ----
[CW] collect: return: 601.70468, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 5.46374, qf2_loss: 5.41999, policy_loss: -141.39992, policy_entropy: -5.94278, alpha: 0.04690, time: 51.05441
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   439 ----
[CW] collect: return: 581.76818, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 5.12951, qf2_loss: 5.12770, policy_loss: -141.57829, policy_entropy: -6.08006, alpha: 0.04681, time: 51.00263
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   440 ----
[CW] collect: return: 576.33473, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 4.87000, qf2_loss: 4.87356, policy_loss: -142.30010, policy_entropy: -6.05892, alpha: 0.04708, time: 51.03315
[CW] eval: return: 578.09672, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   441 ----
[CW] collect: return: 548.93472, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 6.00698, qf2_loss: 6.10481, policy_loss: -142.07726, policy_entropy: -5.86272, alpha: 0.04706, time: 50.91000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   442 ----
[CW] collect: return: 597.18129, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 6.77801, qf2_loss: 6.92781, policy_loss: -143.11996, policy_entropy: -5.95716, alpha: 0.04678, time: 50.80264
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   443 ----
[CW] collect: return: 609.64061, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 5.75530, qf2_loss: 5.61441, policy_loss: -144.06320, policy_entropy: -6.11895, alpha: 0.04679, time: 51.03259
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   444 ----
[CW] collect: return: 569.65956, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 6.03598, qf2_loss: 6.07199, policy_loss: -143.37534, policy_entropy: -6.01940, alpha: 0.04694, time: 50.88462
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   445 ----
[CW] collect: return: 499.38455, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 6.27034, qf2_loss: 6.30572, policy_loss: -142.94664, policy_entropy: -6.04119, alpha: 0.04704, time: 50.73200
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   446 ----
[CW] collect: return: 479.34984, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 6.40526, qf2_loss: 6.33854, policy_loss: -143.16225, policy_entropy: -6.10400, alpha: 0.04729, time: 51.03979
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   447 ----
[CW] collect: return: 552.59085, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 5.70235, qf2_loss: 5.66241, policy_loss: -142.67384, policy_entropy: -5.82826, alpha: 0.04728, time: 51.73502
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   448 ----
[CW] collect: return: 579.02778, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 5.13790, qf2_loss: 5.14708, policy_loss: -143.55119, policy_entropy: -6.07581, alpha: 0.04714, time: 51.60828
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   449 ----
[CW] collect: return: 643.92972, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 6.24824, qf2_loss: 6.33661, policy_loss: -144.20912, policy_entropy: -6.12460, alpha: 0.04727, time: 51.64673
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   450 ----
[CW] collect: return: 38.15976, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 13.53596, qf2_loss: 13.70587, policy_loss: -143.52591, policy_entropy: -6.18392, alpha: 0.04765, time: 51.55197
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   451 ----
[CW] collect: return: 542.34323, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 8.35455, qf2_loss: 8.39508, policy_loss: -145.31990, policy_entropy: -6.21343, alpha: 0.04826, time: 51.71599
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   452 ----
[CW] collect: return: 603.26992, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 6.76164, qf2_loss: 6.77631, policy_loss: -144.39546, policy_entropy: -6.10406, alpha: 0.04879, time: 51.33370
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   453 ----
[CW] collect: return: 609.43171, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 6.17777, qf2_loss: 6.19156, policy_loss: -145.36044, policy_entropy: -5.92487, alpha: 0.04880, time: 51.41542
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   454 ----
[CW] collect: return: 618.50711, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 5.77444, qf2_loss: 5.73365, policy_loss: -143.98746, policy_entropy: -5.93654, alpha: 0.04868, time: 51.29402
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   455 ----
[CW] collect: return: 598.44427, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 5.72103, qf2_loss: 5.74619, policy_loss: -145.17450, policy_entropy: -6.04257, alpha: 0.04852, time: 51.23170
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   456 ----
[CW] collect: return: 544.00977, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 5.61866, qf2_loss: 5.61052, policy_loss: -145.09934, policy_entropy: -5.83561, alpha: 0.04838, time: 51.20200
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   457 ----
[CW] collect: return: 600.99416, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 5.86971, qf2_loss: 5.78362, policy_loss: -147.14402, policy_entropy: -6.02100, alpha: 0.04821, time: 51.12987
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   458 ----
[CW] collect: return: 610.32721, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 5.06067, qf2_loss: 5.02869, policy_loss: -146.03107, policy_entropy: -6.10378, alpha: 0.04832, time: 51.01752
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   459 ----
[CW] collect: return: 563.71735, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 5.21493, qf2_loss: 5.21860, policy_loss: -145.12774, policy_entropy: -5.99975, alpha: 0.04857, time: 51.45282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   460 ----
[CW] collect: return: 483.69682, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 5.88758, qf2_loss: 5.83912, policy_loss: -146.60394, policy_entropy: -5.97457, alpha: 0.04839, time: 51.39435
[CW] eval: return: 596.97021, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   461 ----
[CW] collect: return: 627.95430, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 5.54207, qf2_loss: 5.58809, policy_loss: -147.31648, policy_entropy: -5.93037, alpha: 0.04833, time: 51.95245
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   462 ----
[CW] collect: return: 642.14143, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 5.47633, qf2_loss: 5.43526, policy_loss: -146.75909, policy_entropy: -5.88666, alpha: 0.04808, time: 51.49822
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   463 ----
[CW] collect: return: 657.01901, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 5.05071, qf2_loss: 5.10077, policy_loss: -146.61529, policy_entropy: -5.93590, alpha: 0.04784, time: 50.80365
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   464 ----
[CW] collect: return: 460.41450, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 5.18567, qf2_loss: 5.09536, policy_loss: -147.18975, policy_entropy: -5.99447, alpha: 0.04773, time: 50.88607
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   465 ----
[CW] collect: return: 641.74715, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 9.81645, qf2_loss: 9.79564, policy_loss: -148.44085, policy_entropy: -6.04707, alpha: 0.04765, time: 51.52606
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   466 ----
[CW] collect: return: 581.41731, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 20.99382, qf2_loss: 21.50022, policy_loss: -146.63060, policy_entropy: -6.40182, alpha: 0.04821, time: 51.71292
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   467 ----
[CW] collect: return: 618.65461, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 7.38021, qf2_loss: 7.41680, policy_loss: -147.48281, policy_entropy: -6.26433, alpha: 0.04938, time: 51.73255
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   468 ----
[CW] collect: return: 552.91877, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 6.02878, qf2_loss: 6.02685, policy_loss: -148.63656, policy_entropy: -6.01936, alpha: 0.04978, time: 51.55033
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   469 ----
[CW] collect: return: 560.62878, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 7.60659, qf2_loss: 7.61657, policy_loss: -147.92854, policy_entropy: -6.10011, alpha: 0.04993, time: 52.63894
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   470 ----
[CW] collect: return: 565.18654, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 4.68810, qf2_loss: 4.66136, policy_loss: -147.38383, policy_entropy: -5.84526, alpha: 0.04990, time: 51.80175
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   471 ----
[CW] collect: return: 592.36750, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 4.91460, qf2_loss: 4.88216, policy_loss: -147.10326, policy_entropy: -5.86609, alpha: 0.04944, time: 51.87038
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   472 ----
[CW] collect: return: 408.37541, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 5.03671, qf2_loss: 4.96328, policy_loss: -149.44524, policy_entropy: -5.96551, alpha: 0.04914, time: 51.68314
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   473 ----
[CW] collect: return: 599.79086, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 4.59829, qf2_loss: 4.58624, policy_loss: -148.59539, policy_entropy: -5.85973, alpha: 0.04904, time: 51.91059
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   474 ----
[CW] collect: return: 587.30875, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 4.91678, qf2_loss: 4.86371, policy_loss: -150.10455, policy_entropy: -5.93615, alpha: 0.04869, time: 51.84941
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   475 ----
[CW] collect: return: 112.58896, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 5.70807, qf2_loss: 5.67173, policy_loss: -148.97757, policy_entropy: -5.95364, alpha: 0.04856, time: 51.74205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   476 ----
[CW] collect: return: 575.37306, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 5.39445, qf2_loss: 5.38709, policy_loss: -149.76787, policy_entropy: -5.96521, alpha: 0.04841, time: 51.60279
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   477 ----
[CW] collect: return: 621.04724, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 4.77052, qf2_loss: 4.70072, policy_loss: -149.29489, policy_entropy: -5.80117, alpha: 0.04812, time: 52.05974
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   478 ----
[CW] collect: return: 620.93611, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 5.53675, qf2_loss: 5.57191, policy_loss: -149.52166, policy_entropy: -5.94617, alpha: 0.04773, time: 51.92297
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   479 ----
[CW] collect: return: 534.77932, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 5.00032, qf2_loss: 4.95124, policy_loss: -148.36008, policy_entropy: -5.87677, alpha: 0.04750, time: 51.84594
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   480 ----
[CW] collect: return: 508.34684, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 4.88300, qf2_loss: 4.89848, policy_loss: -150.71235, policy_entropy: -6.05085, alpha: 0.04741, time: 51.74041
[CW] eval: return: 518.92131, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   481 ----
[CW] collect: return: 502.52374, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 4.86377, qf2_loss: 4.89543, policy_loss: -151.20927, policy_entropy: -6.01909, alpha: 0.04759, time: 52.24194
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   482 ----
[CW] collect: return: 518.59133, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 5.15625, qf2_loss: 5.24442, policy_loss: -150.22109, policy_entropy: -5.96120, alpha: 0.04750, time: 51.59153
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   483 ----
[CW] collect: return: 599.49304, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 5.50285, qf2_loss: 5.57218, policy_loss: -151.08159, policy_entropy: -6.05870, alpha: 0.04750, time: 51.75959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   484 ----
[CW] collect: return: 111.05890, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 6.16670, qf2_loss: 5.95677, policy_loss: -151.68460, policy_entropy: -6.05570, alpha: 0.04752, time: 51.90151
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   485 ----
[CW] collect: return: 504.76123, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 5.08093, qf2_loss: 5.06518, policy_loss: -151.45986, policy_entropy: -5.99995, alpha: 0.04772, time: 51.95059
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   486 ----
[CW] collect: return: 489.35510, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 6.45987, qf2_loss: 6.42544, policy_loss: -150.58033, policy_entropy: -6.01093, alpha: 0.04768, time: 52.15025
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   487 ----
[CW] collect: return: 188.62681, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 6.12078, qf2_loss: 6.19590, policy_loss: -151.63182, policy_entropy: -5.99334, alpha: 0.04768, time: 51.71420
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   488 ----
[CW] collect: return: 623.74250, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 7.01416, qf2_loss: 7.08964, policy_loss: -151.32220, policy_entropy: -6.05979, alpha: 0.04774, time: 51.66102
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   489 ----
[CW] collect: return: 623.49754, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 5.48147, qf2_loss: 5.46180, policy_loss: -152.76802, policy_entropy: -6.13394, alpha: 0.04805, time: 52.54753
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   490 ----
[CW] collect: return: 625.04442, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 6.73708, qf2_loss: 6.79262, policy_loss: -152.25139, policy_entropy: -6.10947, alpha: 0.04833, time: 51.91138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   491 ----
[CW] collect: return: 612.09280, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 5.21097, qf2_loss: 5.06384, policy_loss: -152.35948, policy_entropy: -6.05167, alpha: 0.04862, time: 51.70682
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   492 ----
[CW] collect: return: 506.13711, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 5.02804, qf2_loss: 4.99061, policy_loss: -152.09568, policy_entropy: -5.82384, alpha: 0.04842, time: 51.95352
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   493 ----
[CW] collect: return: 551.71435, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 5.04512, qf2_loss: 4.97440, policy_loss: -154.16276, policy_entropy: -5.99548, alpha: 0.04815, time: 51.82634
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   494 ----
[CW] collect: return: 562.19695, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 5.10107, qf2_loss: 5.06954, policy_loss: -152.22544, policy_entropy: -5.87100, alpha: 0.04803, time: 51.62704
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   495 ----
[CW] collect: return: 582.57067, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 6.04113, qf2_loss: 6.12059, policy_loss: -153.51579, policy_entropy: -6.05312, alpha: 0.04793, time: 52.00611
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   496 ----
[CW] collect: return: 554.18199, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 6.00241, qf2_loss: 5.89310, policy_loss: -152.87499, policy_entropy: -6.06569, alpha: 0.04795, time: 51.76240
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   497 ----
[CW] collect: return: 587.45072, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 5.18119, qf2_loss: 5.14063, policy_loss: -152.72324, policy_entropy: -6.07389, alpha: 0.04823, time: 52.26260
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   498 ----
[CW] collect: return: 333.41184, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 5.70755, qf2_loss: 5.70989, policy_loss: -152.78615, policy_entropy: -5.94276, alpha: 0.04830, time: 51.89119
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   499 ----
[CW] collect: return: 416.37764, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 5.54135, qf2_loss: 5.46474, policy_loss: -153.78703, policy_entropy: -6.01771, alpha: 0.04820, time: 51.86743
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   500 ----
[CW] collect: return: 550.70184, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 5.83616, qf2_loss: 5.66377, policy_loss: -153.75867, policy_entropy: -6.03303, alpha: 0.04815, time: 52.16136
[CW] eval: return: 571.66525, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   501 ----
[CW] collect: return: 593.39948, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 5.14792, qf2_loss: 5.15494, policy_loss: -154.81849, policy_entropy: -5.96844, alpha: 0.04824, time: 52.13622
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   502 ----
[CW] collect: return: 663.09527, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 6.11181, qf2_loss: 6.04148, policy_loss: -154.77372, policy_entropy: -6.01680, alpha: 0.04825, time: 51.87608
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   503 ----
[CW] collect: return: 583.38857, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 5.30379, qf2_loss: 5.32711, policy_loss: -155.38744, policy_entropy: -5.96020, alpha: 0.04831, time: 51.97413
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   504 ----
[CW] collect: return: 591.48083, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 4.60068, qf2_loss: 4.65885, policy_loss: -155.37061, policy_entropy: -5.88870, alpha: 0.04808, time: 51.64865
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   505 ----
[CW] collect: return: 639.05235, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 4.99640, qf2_loss: 5.01425, policy_loss: -156.18173, policy_entropy: -5.93343, alpha: 0.04787, time: 51.93798
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   506 ----
[CW] collect: return: 613.96463, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 5.62966, qf2_loss: 5.57844, policy_loss: -157.08712, policy_entropy: -6.13764, alpha: 0.04788, time: 51.95279
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   507 ----
[CW] collect: return: 588.53922, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 6.67051, qf2_loss: 6.67612, policy_loss: -156.14615, policy_entropy: -6.00857, alpha: 0.04813, time: 51.86880
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   508 ----
[CW] collect: return: 601.43253, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 8.11972, qf2_loss: 8.16687, policy_loss: -156.55538, policy_entropy: -6.07247, alpha: 0.04825, time: 51.80531
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   509 ----
[CW] collect: return: 628.51033, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 6.60376, qf2_loss: 6.48644, policy_loss: -155.86865, policy_entropy: -6.04320, alpha: 0.04838, time: 51.93480
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   510 ----
[CW] collect: return: 615.72059, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 6.09838, qf2_loss: 6.01529, policy_loss: -156.15123, policy_entropy: -5.89972, alpha: 0.04844, time: 51.82720
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   511 ----
[CW] collect: return: 544.24285, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 5.74951, qf2_loss: 5.82525, policy_loss: -157.65866, policy_entropy: -6.14371, alpha: 0.04835, time: 51.90580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   512 ----
[CW] collect: return: 623.21434, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 4.94245, qf2_loss: 4.87082, policy_loss: -158.43508, policy_entropy: -6.06096, alpha: 0.04859, time: 51.79239
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   513 ----
[CW] collect: return: 618.94589, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 5.49918, qf2_loss: 5.41240, policy_loss: -156.52077, policy_entropy: -5.95516, alpha: 0.04861, time: 51.94720
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   514 ----
[CW] collect: return: 625.09319, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 5.92797, qf2_loss: 5.78730, policy_loss: -157.38214, policy_entropy: -6.02223, alpha: 0.04857, time: 51.83639
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   515 ----
[CW] collect: return: 612.51366, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 5.12445, qf2_loss: 5.15185, policy_loss: -157.28397, policy_entropy: -5.87915, alpha: 0.04846, time: 52.13039
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   516 ----
[CW] collect: return: 620.17128, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 6.16535, qf2_loss: 6.06085, policy_loss: -158.00472, policy_entropy: -6.02025, alpha: 0.04840, time: 52.00560
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   517 ----
[CW] collect: return: 595.06423, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 6.55367, qf2_loss: 6.52911, policy_loss: -157.71733, policy_entropy: -5.88496, alpha: 0.04816, time: 51.83024
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   518 ----
[CW] collect: return: 581.85994, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 5.48786, qf2_loss: 5.43653, policy_loss: -155.38648, policy_entropy: -5.94510, alpha: 0.04795, time: 51.85452
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   519 ----
[CW] collect: return: 634.57663, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 4.95886, qf2_loss: 4.94640, policy_loss: -158.69430, policy_entropy: -5.91779, alpha: 0.04788, time: 51.72776
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   520 ----
[CW] collect: return: 617.17252, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 6.62459, qf2_loss: 6.57069, policy_loss: -156.62736, policy_entropy: -5.85940, alpha: 0.04747, time: 52.02152
[CW] eval: return: 471.61653, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   521 ----
[CW] collect: return: 543.58201, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 5.91721, qf2_loss: 5.89735, policy_loss: -158.87734, policy_entropy: -6.08413, alpha: 0.04740, time: 52.04645
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   522 ----
[CW] collect: return: 576.98274, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 6.38350, qf2_loss: 6.34982, policy_loss: -159.75637, policy_entropy: -6.19702, alpha: 0.04781, time: 51.98772
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   523 ----
[CW] collect: return: 621.53597, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 7.69761, qf2_loss: 7.66790, policy_loss: -158.64096, policy_entropy: -6.02997, alpha: 0.04813, time: 51.87398
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   524 ----
[CW] collect: return: 630.61590, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 12.94847, qf2_loss: 13.14307, policy_loss: -159.10930, policy_entropy: -5.94596, alpha: 0.04807, time: 52.16766
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   525 ----
[CW] collect: return: 443.46199, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 11.38477, qf2_loss: 11.31720, policy_loss: -159.39135, policy_entropy: -6.01600, alpha: 0.04795, time: 51.75720
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   526 ----
[CW] collect: return: 626.19648, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 8.14094, qf2_loss: 8.17531, policy_loss: -158.99290, policy_entropy: -6.10276, alpha: 0.04810, time: 51.88893
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   527 ----
[CW] collect: return: 510.50622, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 5.17582, qf2_loss: 5.17582, policy_loss: -159.62216, policy_entropy: -6.10916, alpha: 0.04853, time: 52.11468
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   528 ----
[CW] collect: return: 578.85040, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 5.27594, qf2_loss: 5.25239, policy_loss: -160.57373, policy_entropy: -6.06724, alpha: 0.04885, time: 51.82871
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   529 ----
[CW] collect: return: 541.24536, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 4.96815, qf2_loss: 4.94076, policy_loss: -160.75325, policy_entropy: -5.92780, alpha: 0.04871, time: 51.89337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   530 ----
[CW] collect: return: 614.58523, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 7.95988, qf2_loss: 8.04051, policy_loss: -161.52284, policy_entropy: -6.00146, alpha: 0.04846, time: 53.32558
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   531 ----
[CW] collect: return: 605.87974, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 8.23557, qf2_loss: 8.12132, policy_loss: -161.73561, policy_entropy: -6.08416, alpha: 0.04870, time: 51.87724
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   532 ----
[CW] collect: return: 674.26569, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 6.89459, qf2_loss: 6.88774, policy_loss: -161.25472, policy_entropy: -6.16189, alpha: 0.04904, time: 51.64708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   533 ----
[CW] collect: return: 634.15381, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 7.09794, qf2_loss: 7.29186, policy_loss: -161.69542, policy_entropy: -6.29662, alpha: 0.04969, time: 52.79012
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   534 ----
[CW] collect: return: 565.00165, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 5.64130, qf2_loss: 5.56710, policy_loss: -161.19298, policy_entropy: -6.05473, alpha: 0.05038, time: 51.98336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   535 ----
[CW] collect: return: 477.93068, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 6.05399, qf2_loss: 6.01949, policy_loss: -161.10030, policy_entropy: -5.94664, alpha: 0.05038, time: 51.60222
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   536 ----
[CW] collect: return: 643.97554, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 5.40463, qf2_loss: 5.41089, policy_loss: -163.01196, policy_entropy: -6.06460, alpha: 0.05033, time: 51.69370
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   537 ----
[CW] collect: return: 629.72632, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 5.23075, qf2_loss: 5.19623, policy_loss: -161.69403, policy_entropy: -5.99913, alpha: 0.05048, time: 54.15822
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   538 ----
[CW] collect: return: 599.26065, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 5.34916, qf2_loss: 5.41260, policy_loss: -160.61840, policy_entropy: -5.95381, alpha: 0.05035, time: 51.92904
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   539 ----
[CW] collect: return: 630.26360, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 5.16838, qf2_loss: 5.06017, policy_loss: -163.48447, policy_entropy: -6.04760, alpha: 0.05042, time: 52.22441
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   540 ----
[CW] collect: return: 621.05707, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 4.97972, qf2_loss: 4.90067, policy_loss: -163.12314, policy_entropy: -5.94148, alpha: 0.05039, time: 52.04878
[CW] eval: return: 602.61715, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   541 ----
[CW] collect: return: 617.05381, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 5.14416, qf2_loss: 5.10466, policy_loss: -162.99758, policy_entropy: -5.88177, alpha: 0.05012, time: 51.93639
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   542 ----
[CW] collect: return: 533.19318, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 5.38478, qf2_loss: 5.37558, policy_loss: -162.11720, policy_entropy: -6.01522, alpha: 0.04988, time: 52.04405
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   543 ----
[CW] collect: return: 538.69665, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 7.38335, qf2_loss: 7.28202, policy_loss: -161.98830, policy_entropy: -5.99053, alpha: 0.04993, time: 52.06009
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   544 ----
[CW] collect: return: 598.67239, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 5.78313, qf2_loss: 5.74006, policy_loss: -162.73791, policy_entropy: -5.99446, alpha: 0.04997, time: 52.08298
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   545 ----
[CW] collect: return: 610.12993, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 5.48652, qf2_loss: 5.42225, policy_loss: -162.74886, policy_entropy: -6.01116, alpha: 0.04992, time: 51.93182
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   546 ----
[CW] collect: return: 594.87899, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 5.41787, qf2_loss: 5.40786, policy_loss: -164.11772, policy_entropy: -6.05941, alpha: 0.04996, time: 51.87510
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   547 ----
[CW] collect: return: 650.83160, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 5.43962, qf2_loss: 5.48314, policy_loss: -163.63406, policy_entropy: -6.01739, alpha: 0.05015, time: 51.92197
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   548 ----
[CW] collect: return: 624.55775, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 6.66976, qf2_loss: 6.50641, policy_loss: -165.52101, policy_entropy: -6.21026, alpha: 0.05042, time: 51.99992
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   549 ----
[CW] collect: return: 629.51794, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 9.46992, qf2_loss: 9.25428, policy_loss: -164.27444, policy_entropy: -6.12150, alpha: 0.05118, time: 52.05652
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   550 ----
[CW] collect: return: 677.23514, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 11.16620, qf2_loss: 11.19236, policy_loss: -164.01646, policy_entropy: -6.00878, alpha: 0.05112, time: 51.97986
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   551 ----
[CW] collect: return: 477.85371, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 8.21539, qf2_loss: 8.19690, policy_loss: -165.02105, policy_entropy: -6.09042, alpha: 0.05127, time: 51.95517
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   552 ----
[CW] collect: return: 478.61715, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 6.78676, qf2_loss: 6.77912, policy_loss: -164.54268, policy_entropy: -6.01681, alpha: 0.05155, time: 52.09952
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   553 ----
[CW] collect: return: 688.66918, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 5.69246, qf2_loss: 5.68712, policy_loss: -165.18129, policy_entropy: -6.00679, alpha: 0.05162, time: 52.28961
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   554 ----
[CW] collect: return: 588.66273, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 4.96619, qf2_loss: 5.00874, policy_loss: -165.62254, policy_entropy: -5.99033, alpha: 0.05158, time: 52.54589
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   555 ----
[CW] collect: return: 649.31685, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 5.57552, qf2_loss: 5.46517, policy_loss: -167.84648, policy_entropy: -6.11043, alpha: 0.05173, time: 51.88456
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   556 ----
[CW] collect: return: 666.81853, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 5.28751, qf2_loss: 5.29874, policy_loss: -166.18471, policy_entropy: -5.85925, alpha: 0.05172, time: 51.92090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   557 ----
[CW] collect: return: 651.82345, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 5.29494, qf2_loss: 5.20852, policy_loss: -167.56666, policy_entropy: -5.99811, alpha: 0.05140, time: 51.81515
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   558 ----
[CW] collect: return: 636.66996, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 6.04134, qf2_loss: 5.95115, policy_loss: -166.80957, policy_entropy: -5.91688, alpha: 0.05131, time: 51.85933
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   559 ----
[CW] collect: return: 615.52047, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 5.64421, qf2_loss: 5.61793, policy_loss: -167.19219, policy_entropy: -5.93662, alpha: 0.05118, time: 51.90836
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   560 ----
[CW] collect: return: 619.18060, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 5.46950, qf2_loss: 5.38070, policy_loss: -166.17521, policy_entropy: -5.80043, alpha: 0.05086, time: 52.32239
[CW] eval: return: 611.12176, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   561 ----
[CW] collect: return: 673.88698, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 5.22056, qf2_loss: 5.20869, policy_loss: -166.19723, policy_entropy: -5.83144, alpha: 0.05027, time: 52.21336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   562 ----
[CW] collect: return: 631.09146, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 5.69675, qf2_loss: 5.64349, policy_loss: -165.79043, policy_entropy: -5.94943, alpha: 0.04979, time: 51.76377
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   563 ----
[CW] collect: return: 620.07009, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 5.43871, qf2_loss: 5.44118, policy_loss: -167.95643, policy_entropy: -5.93026, alpha: 0.04972, time: 52.13185
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   564 ----
[CW] collect: return: 628.32566, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 5.06275, qf2_loss: 5.05005, policy_loss: -169.25155, policy_entropy: -6.03087, alpha: 0.04962, time: 51.88280
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   565 ----
[CW] collect: return: 564.85346, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 5.06236, qf2_loss: 5.06734, policy_loss: -168.02490, policy_entropy: -5.98209, alpha: 0.04963, time: 51.75998
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   566 ----
[CW] collect: return: 652.15724, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 5.70773, qf2_loss: 5.65421, policy_loss: -169.39763, policy_entropy: -6.08158, alpha: 0.04975, time: 51.67139
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   567 ----
[CW] collect: return: 660.39563, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 6.83025, qf2_loss: 6.91212, policy_loss: -169.26084, policy_entropy: -6.05517, alpha: 0.04996, time: 51.67679
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   568 ----
[CW] collect: return: 634.44165, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 5.94811, qf2_loss: 5.87255, policy_loss: -168.49129, policy_entropy: -5.98838, alpha: 0.05001, time: 51.94061
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   569 ----
[CW] collect: return: 639.68309, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 4.99564, qf2_loss: 5.04164, policy_loss: -169.95546, policy_entropy: -6.02365, alpha: 0.04998, time: 51.89892
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   570 ----
[CW] collect: return: 654.65975, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 4.94645, qf2_loss: 4.86229, policy_loss: -169.80831, policy_entropy: -5.94627, alpha: 0.04993, time: 51.98790
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   571 ----
[CW] collect: return: 648.98798, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 5.11444, qf2_loss: 5.08314, policy_loss: -170.28948, policy_entropy: -5.94856, alpha: 0.04974, time: 51.76884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   572 ----
[CW] collect: return: 663.48367, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 4.90730, qf2_loss: 4.90241, policy_loss: -168.02742, policy_entropy: -5.88964, alpha: 0.04954, time: 51.94473
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   573 ----
[CW] collect: return: 649.32122, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 4.96389, qf2_loss: 4.99591, policy_loss: -170.04869, policy_entropy: -5.99634, alpha: 0.04929, time: 51.95218
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   574 ----
[CW] collect: return: 662.18247, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 5.09294, qf2_loss: 5.07092, policy_loss: -170.22576, policy_entropy: -5.99417, alpha: 0.04931, time: 51.45783
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   575 ----
[CW] collect: return: 669.72529, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 11.37507, qf2_loss: 11.42902, policy_loss: -169.89279, policy_entropy: -6.12151, alpha: 0.04952, time: 51.61585
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   576 ----
[CW] collect: return: 535.39582, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 13.73988, qf2_loss: 13.55527, policy_loss: -172.40547, policy_entropy: -6.13858, alpha: 0.04992, time: 51.89520
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   577 ----
[CW] collect: return: 666.48461, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 10.74807, qf2_loss: 10.50655, policy_loss: -170.74161, policy_entropy: -6.02924, alpha: 0.05023, time: 51.93378
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   578 ----
[CW] collect: return: 624.36044, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 6.88009, qf2_loss: 6.94082, policy_loss: -170.96434, policy_entropy: -5.83868, alpha: 0.05003, time: 51.99209
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   579 ----
[CW] collect: return: 670.33809, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 5.57288, qf2_loss: 5.57046, policy_loss: -170.42200, policy_entropy: -5.96000, alpha: 0.04970, time: 51.87807
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   580 ----
[CW] collect: return: 630.41186, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 5.00072, qf2_loss: 4.97893, policy_loss: -172.88618, policy_entropy: -6.05289, alpha: 0.04974, time: 51.85586
[CW] eval: return: 599.88948, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   581 ----
[CW] collect: return: 695.08589, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 5.14527, qf2_loss: 5.08431, policy_loss: -172.69327, policy_entropy: -5.98914, alpha: 0.04978, time: 51.86059
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   582 ----
[CW] collect: return: 659.01923, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 4.97760, qf2_loss: 4.90605, policy_loss: -170.16783, policy_entropy: -5.91996, alpha: 0.04962, time: 51.97711
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   583 ----
[CW] collect: return: 668.20988, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 4.96000, qf2_loss: 4.90675, policy_loss: -171.80468, policy_entropy: -5.95004, alpha: 0.04943, time: 51.99886
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   584 ----
[CW] collect: return: 627.55640, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 4.76272, qf2_loss: 4.78333, policy_loss: -172.74462, policy_entropy: -5.93861, alpha: 0.04926, time: 51.96006
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   585 ----
[CW] collect: return: 708.46245, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 4.91001, qf2_loss: 4.88568, policy_loss: -173.07079, policy_entropy: -5.94003, alpha: 0.04915, time: 51.82642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   586 ----
[CW] collect: return: 641.66115, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 4.74793, qf2_loss: 4.74849, policy_loss: -172.94495, policy_entropy: -6.01966, alpha: 0.04903, time: 51.91585
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   587 ----
[CW] collect: return: 589.97535, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 5.37369, qf2_loss: 5.31434, policy_loss: -173.53309, policy_entropy: -6.10016, alpha: 0.04918, time: 51.91824
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   588 ----
[CW] collect: return: 676.91312, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 4.38188, qf2_loss: 4.33533, policy_loss: -173.43418, policy_entropy: -6.03865, alpha: 0.04949, time: 52.00401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   589 ----
[CW] collect: return: 582.22458, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 5.84856, qf2_loss: 5.76440, policy_loss: -173.62925, policy_entropy: -6.05326, alpha: 0.04962, time: 51.75145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   590 ----
[CW] collect: return: 665.58478, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 5.70310, qf2_loss: 5.63422, policy_loss: -173.50621, policy_entropy: -6.03608, alpha: 0.04969, time: 51.77561
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   591 ----
[CW] collect: return: 657.37033, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 5.04181, qf2_loss: 4.97329, policy_loss: -174.71982, policy_entropy: -6.12853, alpha: 0.04995, time: 51.80174
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   592 ----
[CW] collect: return: 704.63598, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 4.89350, qf2_loss: 4.81189, policy_loss: -173.46326, policy_entropy: -5.94430, alpha: 0.05003, time: 51.80993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   593 ----
[CW] collect: return: 685.30450, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 5.02885, qf2_loss: 4.96560, policy_loss: -175.04101, policy_entropy: -6.06927, alpha: 0.05011, time: 51.73657
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   594 ----
[CW] collect: return: 685.72603, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 5.99370, qf2_loss: 5.93809, policy_loss: -173.41100, policy_entropy: -6.01443, alpha: 0.05026, time: 51.81352
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   595 ----
[CW] collect: return: 657.96984, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 4.97865, qf2_loss: 4.96701, policy_loss: -174.81340, policy_entropy: -6.03818, alpha: 0.05034, time: 51.81636
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   596 ----
[CW] collect: return: 726.40623, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 6.27803, qf2_loss: 6.21224, policy_loss: -173.10100, policy_entropy: -5.98741, alpha: 0.05033, time: 51.98646
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   597 ----
[CW] collect: return: 653.25279, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 5.79773, qf2_loss: 5.72164, policy_loss: -175.75895, policy_entropy: -6.11758, alpha: 0.05041, time: 51.78065
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   598 ----
[CW] collect: return: 673.35740, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 9.52570, qf2_loss: 9.56397, policy_loss: -176.38096, policy_entropy: -5.97413, alpha: 0.05076, time: 52.07993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   599 ----
[CW] collect: return: 613.02616, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 19.79997, qf2_loss: 19.74034, policy_loss: -174.57119, policy_entropy: -6.15815, alpha: 0.05056, time: 51.76582
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   600 ----
[CW] collect: return: 601.07074, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 7.30705, qf2_loss: 7.30369, policy_loss: -174.19969, policy_entropy: -6.20604, alpha: 0.05141, time: 51.72766
[CW] eval: return: 581.34784, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   601 ----
[CW] collect: return: 601.88633, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 8.15390, qf2_loss: 8.13948, policy_loss: -176.13067, policy_entropy: -6.06329, alpha: 0.05186, time: 52.10729
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   602 ----
[CW] collect: return: 620.89583, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 6.39587, qf2_loss: 6.31647, policy_loss: -176.13636, policy_entropy: -6.00492, alpha: 0.05208, time: 51.95244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   603 ----
[CW] collect: return: 628.49054, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 5.56576, qf2_loss: 5.59684, policy_loss: -176.22787, policy_entropy: -5.87699, alpha: 0.05193, time: 51.85636
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   604 ----
[CW] collect: return: 686.50338, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 5.08314, qf2_loss: 5.11165, policy_loss: -176.81292, policy_entropy: -5.95662, alpha: 0.05156, time: 51.82090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   605 ----
[CW] collect: return: 680.79656, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 5.70003, qf2_loss: 5.60778, policy_loss: -176.60053, policy_entropy: -5.95648, alpha: 0.05130, time: 51.91440
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   606 ----
[CW] collect: return: 657.55335, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 5.59037, qf2_loss: 5.51583, policy_loss: -176.79795, policy_entropy: -6.05690, alpha: 0.05143, time: 52.02415
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   607 ----
[CW] collect: return: 623.82674, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 4.52259, qf2_loss: 4.48957, policy_loss: -177.71954, policy_entropy: -5.95233, alpha: 0.05142, time: 51.87014
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   608 ----
[CW] collect: return: 668.53285, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 4.93223, qf2_loss: 4.86590, policy_loss: -178.20594, policy_entropy: -5.97620, alpha: 0.05137, time: 53.72158
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   609 ----
[CW] collect: return: 661.88421, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 6.46192, qf2_loss: 6.43323, policy_loss: -177.55517, policy_entropy: -6.06723, alpha: 0.05146, time: 51.92855
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   610 ----
[CW] collect: return: 697.58781, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 8.54808, qf2_loss: 8.44525, policy_loss: -177.19762, policy_entropy: -6.06441, alpha: 0.05162, time: 52.07479
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   611 ----
[CW] collect: return: 608.74280, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 6.58538, qf2_loss: 6.60613, policy_loss: -176.75923, policy_entropy: -5.93665, alpha: 0.05168, time: 52.72928
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   612 ----
[CW] collect: return: 623.89587, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 5.31499, qf2_loss: 5.33733, policy_loss: -179.36949, policy_entropy: -6.08017, alpha: 0.05162, time: 51.64342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   613 ----
[CW] collect: return: 717.58744, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 5.16746, qf2_loss: 5.15067, policy_loss: -178.10120, policy_entropy: -5.93209, alpha: 0.05180, time: 51.86206
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   614 ----
[CW] collect: return: 647.37588, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 5.03836, qf2_loss: 4.96774, policy_loss: -178.45532, policy_entropy: -6.00499, alpha: 0.05157, time: 51.71911
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   615 ----
[CW] collect: return: 715.95104, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 4.81740, qf2_loss: 4.79816, policy_loss: -178.68967, policy_entropy: -6.03324, alpha: 0.05162, time: 51.74512
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   616 ----
[CW] collect: return: 667.39360, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 5.26710, qf2_loss: 5.21934, policy_loss: -181.04240, policy_entropy: -6.11491, alpha: 0.05184, time: 51.97710
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   617 ----
[CW] collect: return: 702.92435, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 5.34403, qf2_loss: 5.26634, policy_loss: -180.77271, policy_entropy: -5.98086, alpha: 0.05196, time: 51.90165
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   618 ----
[CW] collect: return: 690.41908, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 6.64404, qf2_loss: 6.64999, policy_loss: -180.44344, policy_entropy: -6.04841, alpha: 0.05197, time: 52.87158
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   619 ----
[CW] collect: return: 710.27098, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 5.34264, qf2_loss: 5.25902, policy_loss: -179.30416, policy_entropy: -5.95313, alpha: 0.05210, time: 51.81092
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   620 ----
[CW] collect: return: 711.90809, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 5.55491, qf2_loss: 5.55001, policy_loss: -179.39280, policy_entropy: -5.96971, alpha: 0.05192, time: 51.91876
[CW] eval: return: 675.79029, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   621 ----
[CW] collect: return: 688.95528, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 5.53311, qf2_loss: 5.36993, policy_loss: -179.01530, policy_entropy: -5.99954, alpha: 0.05184, time: 52.17246
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   622 ----
[CW] collect: return: 381.52486, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 5.40995, qf2_loss: 5.34973, policy_loss: -180.70696, policy_entropy: -6.03256, alpha: 0.05190, time: 51.80607
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   623 ----
[CW] collect: return: 647.66616, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 5.68776, qf2_loss: 5.75180, policy_loss: -180.56814, policy_entropy: -6.05873, alpha: 0.05200, time: 51.75680
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   624 ----
[CW] collect: return: 644.13567, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 5.88224, qf2_loss: 5.76239, policy_loss: -179.74014, policy_entropy: -6.09175, alpha: 0.05233, time: 51.81718
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   625 ----
[CW] collect: return: 695.09091, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 5.54149, qf2_loss: 5.51110, policy_loss: -181.50363, policy_entropy: -5.97085, alpha: 0.05250, time: 51.79135
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   626 ----
[CW] collect: return: 705.25766, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 6.27196, qf2_loss: 6.14900, policy_loss: -183.16179, policy_entropy: -6.07803, alpha: 0.05254, time: 51.79489
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   627 ----
[CW] collect: return: 692.94534, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 6.10072, qf2_loss: 5.95810, policy_loss: -181.66478, policy_entropy: -5.99696, alpha: 0.05259, time: 51.91037
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   628 ----
[CW] collect: return: 712.59823, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 5.67716, qf2_loss: 5.60710, policy_loss: -180.12334, policy_entropy: -5.91099, alpha: 0.05260, time: 52.01441
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   629 ----
[CW] collect: return: 695.40793, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 5.99242, qf2_loss: 5.97337, policy_loss: -182.86193, policy_entropy: -6.05334, alpha: 0.05245, time: 52.00901
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   630 ----
[CW] collect: return: 683.32005, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 4.92828, qf2_loss: 4.86400, policy_loss: -180.65218, policy_entropy: -5.81864, alpha: 0.05228, time: 52.03603
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   631 ----
[CW] collect: return: 671.64427, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 4.63243, qf2_loss: 4.60127, policy_loss: -183.43492, policy_entropy: -6.03276, alpha: 0.05193, time: 52.01513
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   632 ----
[CW] collect: return: 724.79023, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 5.22168, qf2_loss: 5.15584, policy_loss: -183.36225, policy_entropy: -6.02895, alpha: 0.05199, time: 51.94427
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   633 ----
[CW] collect: return: 691.76716, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 5.35223, qf2_loss: 5.31577, policy_loss: -183.48151, policy_entropy: -5.98815, alpha: 0.05206, time: 52.04445
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   634 ----
[CW] collect: return: 458.00720, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 5.16200, qf2_loss: 5.02300, policy_loss: -183.04626, policy_entropy: -6.07002, alpha: 0.05222, time: 51.95105
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   635 ----
[CW] collect: return: 693.03365, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 6.44533, qf2_loss: 6.33791, policy_loss: -182.05053, policy_entropy: -6.04852, alpha: 0.05235, time: 51.76076
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   636 ----
[CW] collect: return: 556.05520, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 4.40477, qf2_loss: 4.30131, policy_loss: -182.21426, policy_entropy: -6.04051, alpha: 0.05258, time: 51.73401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   637 ----
[CW] collect: return: 606.33616, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 5.40909, qf2_loss: 5.35322, policy_loss: -181.65282, policy_entropy: -5.94297, alpha: 0.05251, time: 51.76248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   638 ----
[CW] collect: return: 691.24990, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 5.24173, qf2_loss: 5.22305, policy_loss: -183.72362, policy_entropy: -5.98260, alpha: 0.05234, time: 51.89823
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   639 ----
[CW] collect: return: 635.78889, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 5.61803, qf2_loss: 5.49495, policy_loss: -182.76513, policy_entropy: -5.90816, alpha: 0.05229, time: 51.90955
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   640 ----
[CW] collect: return: 542.93976, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 7.78194, qf2_loss: 7.65908, policy_loss: -183.97835, policy_entropy: -6.00778, alpha: 0.05205, time: 51.86083
[CW] eval: return: 622.00492, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   641 ----
[CW] collect: return: 603.10923, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 6.47715, qf2_loss: 6.54997, policy_loss: -184.88599, policy_entropy: -6.18904, alpha: 0.05237, time: 52.14976
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   642 ----
[CW] collect: return: 647.21274, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 7.52472, qf2_loss: 7.43047, policy_loss: -184.81024, policy_entropy: -5.98743, alpha: 0.05272, time: 52.17843
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   643 ----
[CW] collect: return: 705.75870, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 8.36572, qf2_loss: 8.23043, policy_loss: -183.96183, policy_entropy: -5.87145, alpha: 0.05243, time: 51.68174
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   644 ----
[CW] collect: return: 647.78952, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 6.87136, qf2_loss: 6.76928, policy_loss: -184.63860, policy_entropy: -5.93209, alpha: 0.05220, time: 51.58263
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   645 ----
[CW] collect: return: 118.09474, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 6.21133, qf2_loss: 6.14712, policy_loss: -185.02982, policy_entropy: -6.02201, alpha: 0.05209, time: 51.58164
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   646 ----
[CW] collect: return: 694.37537, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 4.98713, qf2_loss: 4.89150, policy_loss: -185.25787, policy_entropy: -6.02011, alpha: 0.05209, time: 51.67740
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   647 ----
[CW] collect: return: 702.16222, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 6.04514, qf2_loss: 6.00413, policy_loss: -185.70853, policy_entropy: -5.95043, alpha: 0.05210, time: 51.71660
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   648 ----
[CW] collect: return: 654.24292, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 5.00150, qf2_loss: 4.96195, policy_loss: -186.42537, policy_entropy: -6.07257, alpha: 0.05198, time: 51.81414
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   649 ----
[CW] collect: return: 673.16831, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 6.07231, qf2_loss: 6.01498, policy_loss: -185.29316, policy_entropy: -5.90139, alpha: 0.05216, time: 51.90432
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   650 ----
[CW] collect: return: 650.66809, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 7.02971, qf2_loss: 6.97949, policy_loss: -185.98271, policy_entropy: -5.97122, alpha: 0.05181, time: 51.93991
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   651 ----
[CW] collect: return: 663.07483, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 5.41168, qf2_loss: 5.33571, policy_loss: -184.38368, policy_entropy: -5.94071, alpha: 0.05169, time: 51.66626
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   652 ----
[CW] collect: return: 639.36112, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 7.53148, qf2_loss: 7.34282, policy_loss: -185.92745, policy_entropy: -5.97403, alpha: 0.05159, time: 51.92423
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   653 ----
[CW] collect: return: 561.88339, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 8.89404, qf2_loss: 8.71420, policy_loss: -187.10108, policy_entropy: -6.18789, alpha: 0.05176, time: 51.88480
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   654 ----
[CW] collect: return: 697.67024, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 11.04097, qf2_loss: 11.03157, policy_loss: -185.85728, policy_entropy: -6.07533, alpha: 0.05230, time: 51.71217
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   655 ----
[CW] collect: return: 628.00535, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 9.86041, qf2_loss: 9.87546, policy_loss: -186.75494, policy_entropy: -6.02156, alpha: 0.05241, time: 51.60852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   656 ----
[CW] collect: return: 736.13408, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 9.58597, qf2_loss: 9.53881, policy_loss: -187.71191, policy_entropy: -6.10290, alpha: 0.05263, time: 51.56229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   657 ----
[CW] collect: return: 631.28621, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 6.58022, qf2_loss: 6.42748, policy_loss: -186.40391, policy_entropy: -5.94478, alpha: 0.05271, time: 51.77077
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   658 ----
[CW] collect: return: 671.28441, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 5.83000, qf2_loss: 5.75770, policy_loss: -187.61778, policy_entropy: -6.06996, alpha: 0.05258, time: 51.91466
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   659 ----
[CW] collect: return: 669.62835, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 5.51801, qf2_loss: 5.41543, policy_loss: -188.25088, policy_entropy: -6.00191, alpha: 0.05282, time: 51.96141
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   660 ----
[CW] collect: return: 688.88175, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 5.00763, qf2_loss: 4.95205, policy_loss: -188.92105, policy_entropy: -6.08018, alpha: 0.05296, time: 51.99441
[CW] eval: return: 663.78985, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   661 ----
[CW] collect: return: 663.70963, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 5.13518, qf2_loss: 5.03240, policy_loss: -188.77518, policy_entropy: -5.93006, alpha: 0.05313, time: 51.79616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   662 ----
[CW] collect: return: 707.13039, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 4.94071, qf2_loss: 4.94618, policy_loss: -188.51054, policy_entropy: -5.97378, alpha: 0.05285, time: 51.57059
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   663 ----
[CW] collect: return: 678.89665, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 5.07823, qf2_loss: 4.97475, policy_loss: -188.01631, policy_entropy: -6.00842, alpha: 0.05282, time: 51.43495
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   664 ----
[CW] collect: return: 700.73972, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 5.57634, qf2_loss: 5.46036, policy_loss: -187.64274, policy_entropy: -5.98464, alpha: 0.05290, time: 51.71650
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   665 ----
[CW] collect: return: 654.93666, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 5.34868, qf2_loss: 5.23046, policy_loss: -190.41688, policy_entropy: -6.06693, alpha: 0.05291, time: 51.82458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   666 ----
[CW] collect: return: 695.74412, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 5.40808, qf2_loss: 5.41634, policy_loss: -188.68750, policy_entropy: -5.92659, alpha: 0.05289, time: 51.47641
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   667 ----
[CW] collect: return: 589.74093, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 5.11854, qf2_loss: 5.04141, policy_loss: -188.47812, policy_entropy: -6.07380, alpha: 0.05286, time: 52.46698
[CW] ---------------------------

============================= JOB FEEDBACK =============================

NodeName=uc2n509
Job ID: 21913874
Array Job ID: 21913874_0
Cluster: uc2
User/Group: uprnr/stud
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 4
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-16:01:24 core-walltime
Job Wall-clock time: 10:00:21
Memory Utilized: 4.76 GB
Memory Efficiency: 8.12% of 58.59 GB
