{"collect/return": 159.96980337798595, "collect/steps": 1000.0, "collect/total_steps": 1006000.0, "train/qf1_loss": 0.06831507112830877, "train/qf2_loss": 0.06834761023521424, "train/policy_loss": -18.297650871276854, "train/policy_entropy": -6.038719625473022, "train/alpha": 0.00894941171631217, "train/time": 33.779317140579224, "eval/return": 158.15945556488586, "eval/steps": 1000.0, "_timestamp": 1678771331.224835, "_runtime": 37332.48131299019, "_step": 1000, "_wandb": {"runtime": 37332}}