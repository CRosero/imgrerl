{"collect/return": 255.43757976591587, "collect/steps": 1000.0, "collect/total_steps": 1003000.0, "train/qf1_loss": 65.77305978775024, "train/qf2_loss": 65.15986642837524, "train/policy_loss": -208.7522526550293, "train/policy_entropy": -5.777975082397461, "train/alpha": 0.06903500489890575, "train/time": 34.41079497337341, "eval/return": 277.9507738763972, "eval/steps": 1000.0, "_timestamp": 1678732496.6256793, "_runtime": 35908.3371193409, "_step": 997}