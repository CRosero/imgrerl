Hostname: uc2n904.localdomain
Found slurm config: SLURM
Seeting default slurm config
/home/kit/stud/uprnr/imgrerl/test_results/fixed_test-cs-non-k1m1-f-seed0/fixed_test-cs-non-k1m1-f-seed0/fixed_test-cs-non-k1m1-f-seed0__env.ecartpole-swingup/log/rep_00
[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
False
params: 
 {'env': {'env': 'cartpole-swingup'}} 

additionalVars: 
 {'seed': 0, 'agent': {'image_augmentation_K': 1, 'image_augmentation_M': 1, 'image_augmentation_type': <AugmentationType.NONE: 1>, 'image_augmentation_actor_critic_same_aug': False}}
conf_dict: 
 --------Config-------- 
seed: 0
cuda_id: 0
Subconfig: env
	env: cartpole-swingup
	obs_type: image
Subconfig: agent
	algo_name: sac
	encoder: lstm
	action_embedding_size: 32
	observ_embedding_size: 0
	reward_embedding_size: 32
	rnn_hidden_size: 512
	dqn_layers: [512, 512, 512]
	policy_layers: [512, 512, 512]
	lr: 0.0003
	gamma: 0.99
	tau: 0.005
	entropy_alpha: 1.0
	image_augmentation_type: AugmentationType.NONE
	image_augmentation_K: 1
	image_augmentation_M: 1
	image_augmentation_actor_critic_same_aug: False
Subconfig: rl
	sampled_seq_len: 64
	buffer_size: 1000000.0
	batch_size: 32
	rollouts_per_iter: 1
	num_updates_per_iter: 100
	num_init_rollouts: 5
Subconfig: eval
	interval: 20
	num_rollouts: 20
---------------------- 
 
Init feature extractor:  1 ;  32 ;  <function relu at 0x148ff6d367a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x148ff6d367a0>
Init feature extractor:  1 ;  164 ;  <function relu at 0x148ff6d367a0>
Critic: 
Critic_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (current_shortcut_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=164, bias=True)
  )
  (qf1): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
  (qf2): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
)
Init feature extractor:  1 ;  32 ;  <function relu at 0x148ff6d367a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x148ff6d367a0>
Actor: 
Actor_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (policy): TanhGaussianPolicy(
    (fc0): Linear(in_features=612, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
    (last_fc_log_std): Linear(in_features=512, out_features=1, bias=True)
  )
)
buffer RAM usage: 11.46 GB
Collecting train stats
[CW] ---- Iteration:     0 ----
[CW] collect: return: 123.27366, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 0.90217, qf2_loss: 0.90400, policy_loss: -2.30548, policy_entropy: 0.68251, alpha: 0.98504, time: 42.29783
[CW] eval: return: 105.24823, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     1 ----
[CW] collect: return: 61.17785, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.05410, qf2_loss: 0.05399, policy_loss: -2.63683, policy_entropy: 0.68193, alpha: 0.95626, time: 33.14049
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     2 ----
[CW] collect: return: 44.75693, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.04239, qf2_loss: 0.04263, policy_loss: -2.98553, policy_entropy: 0.68154, alpha: 0.92873, time: 33.05058
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     3 ----
[CW] collect: return: 107.63214, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.04168, qf2_loss: 0.04164, policy_loss: -3.45211, policy_entropy: 0.67986, alpha: 0.90235, time: 33.09525
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     4 ----
[CW] collect: return: 116.12814, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.04531, qf2_loss: 0.04478, policy_loss: -3.91464, policy_entropy: 0.67894, alpha: 0.87705, time: 33.25561
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     5 ----
[CW] collect: return: 58.21868, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.05002, qf2_loss: 0.04987, policy_loss: -4.31917, policy_entropy: 0.67647, alpha: 0.85278, time: 33.15514
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     6 ----
[CW] collect: return: 77.08710, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.06920, qf2_loss: 0.06871, policy_loss: -4.71151, policy_entropy: 0.67382, alpha: 0.82947, time: 33.28892
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     7 ----
[CW] collect: return: 94.70321, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.07986, qf2_loss: 0.07928, policy_loss: -5.24290, policy_entropy: 0.67129, alpha: 0.80707, time: 33.66591
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     8 ----
[CW] collect: return: 57.30794, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.09134, qf2_loss: 0.09127, policy_loss: -5.69468, policy_entropy: 0.66818, alpha: 0.78553, time: 33.46527
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     9 ----
[CW] collect: return: 80.23215, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.10077, qf2_loss: 0.10054, policy_loss: -6.14073, policy_entropy: 0.66285, alpha: 0.76480, time: 33.55368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    10 ----
[CW] collect: return: 225.06690, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.16185, qf2_loss: 0.16071, policy_loss: -6.84272, policy_entropy: 0.65590, alpha: 0.74487, time: 33.47733
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    11 ----
[CW] collect: return: 254.16363, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.18541, qf2_loss: 0.18425, policy_loss: -7.78425, policy_entropy: 0.64804, alpha: 0.72568, time: 33.46266
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    12 ----
[CW] collect: return: 68.04421, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.17033, qf2_loss: 0.17036, policy_loss: -8.40536, policy_entropy: 0.64038, alpha: 0.70721, time: 33.54405
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    13 ----
[CW] collect: return: 141.98564, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.26911, qf2_loss: 0.26805, policy_loss: -9.02556, policy_entropy: 0.63220, alpha: 0.68942, time: 33.52214
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    14 ----
[CW] collect: return: 159.43422, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.20773, qf2_loss: 0.20615, policy_loss: -9.79530, policy_entropy: 0.61504, alpha: 0.67228, time: 33.52716
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    15 ----
[CW] collect: return: 93.73666, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.20756, qf2_loss: 0.20678, policy_loss: -10.22616, policy_entropy: 0.60333, alpha: 0.65580, time: 33.62525
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    16 ----
[CW] collect: return: 83.99963, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.24267, qf2_loss: 0.24210, policy_loss: -10.82527, policy_entropy: 0.58483, alpha: 0.63993, time: 33.36547
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    17 ----
[CW] collect: return: 164.32287, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.38862, qf2_loss: 0.38896, policy_loss: -11.46235, policy_entropy: 0.56573, alpha: 0.62467, time: 33.31394
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    18 ----
[CW] collect: return: 168.41443, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.31247, qf2_loss: 0.31109, policy_loss: -12.23076, policy_entropy: 0.53114, alpha: 0.61003, time: 33.31883
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    19 ----
[CW] collect: return: 223.63611, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.41214, qf2_loss: 0.41063, policy_loss: -13.18567, policy_entropy: 0.49446, alpha: 0.59608, time: 32.89898
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    20 ----
[CW] collect: return: 234.03174, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.60862, qf2_loss: 0.60859, policy_loss: -14.07668, policy_entropy: 0.47020, alpha: 0.58268, time: 32.65232
[CW] eval: return: 190.92603, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    21 ----
[CW] collect: return: 239.73430, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.50755, qf2_loss: 0.50444, policy_loss: -15.16305, policy_entropy: 0.44204, alpha: 0.56979, time: 32.94681
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    22 ----
[CW] collect: return: 253.03618, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.50752, qf2_loss: 0.50979, policy_loss: -15.97758, policy_entropy: 0.41270, alpha: 0.55736, time: 33.48310
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    23 ----
[CW] collect: return: 260.67713, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.42237, qf2_loss: 0.42194, policy_loss: -16.66791, policy_entropy: 0.37731, alpha: 0.54544, time: 33.42735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    24 ----
[CW] collect: return: 209.39895, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.68739, qf2_loss: 0.68703, policy_loss: -17.85912, policy_entropy: 0.34095, alpha: 0.53401, time: 33.40318
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    25 ----
[CW] collect: return: 203.35095, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.54164, qf2_loss: 0.53692, policy_loss: -18.76955, policy_entropy: 0.33076, alpha: 0.52293, time: 33.49995
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    26 ----
[CW] collect: return: 226.63421, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.82082, qf2_loss: 0.81755, policy_loss: -19.85710, policy_entropy: 0.29517, alpha: 0.51219, time: 33.61195
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    27 ----
[CW] collect: return: 219.61470, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.66790, qf2_loss: 0.66106, policy_loss: -20.67687, policy_entropy: 0.26946, alpha: 0.50182, time: 33.81000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    28 ----
[CW] collect: return: 237.80695, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.64431, qf2_loss: 0.64330, policy_loss: -21.65918, policy_entropy: 0.24034, alpha: 0.49178, time: 33.64595
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    29 ----
[CW] collect: return: 338.74697, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.85451, qf2_loss: 0.84715, policy_loss: -22.65507, policy_entropy: 0.20997, alpha: 0.48209, time: 33.56389
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    30 ----
[CW] collect: return: 285.02023, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.88460, qf2_loss: 0.87474, policy_loss: -23.95717, policy_entropy: 0.17938, alpha: 0.47274, time: 33.26760
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    31 ----
[CW] collect: return: 253.85692, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.89351, qf2_loss: 0.88734, policy_loss: -25.02688, policy_entropy: 0.14162, alpha: 0.46372, time: 32.43617
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    32 ----
[CW] collect: return: 238.76805, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.81276, qf2_loss: 0.80185, policy_loss: -25.77899, policy_entropy: 0.09988, alpha: 0.45506, time: 32.97102
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    33 ----
[CW] collect: return: 206.52751, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 1.21016, qf2_loss: 1.20167, policy_loss: -27.08329, policy_entropy: 0.06710, alpha: 0.44673, time: 33.25390
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    34 ----
[CW] collect: return: 254.57042, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 1.29744, qf2_loss: 1.28399, policy_loss: -28.04286, policy_entropy: 0.02431, alpha: 0.43871, time: 33.32709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    35 ----
[CW] collect: return: 211.46322, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 1.10343, qf2_loss: 1.09032, policy_loss: -29.26473, policy_entropy: 0.00662, alpha: 0.43100, time: 33.66899
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    36 ----
[CW] collect: return: 248.12843, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.98311, qf2_loss: 0.96817, policy_loss: -29.97769, policy_entropy: -0.04173, alpha: 0.42350, time: 33.44503
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    37 ----
[CW] collect: return: 238.04998, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 1.08161, qf2_loss: 1.07787, policy_loss: -31.17059, policy_entropy: -0.06232, alpha: 0.41628, time: 33.37925
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    38 ----
[CW] collect: return: 307.45295, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 1.47521, qf2_loss: 1.47515, policy_loss: -32.44642, policy_entropy: -0.08380, alpha: 0.40924, time: 33.48582
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    39 ----
[CW] collect: return: 247.21131, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 1.32756, qf2_loss: 1.32382, policy_loss: -33.20329, policy_entropy: -0.11260, alpha: 0.40235, time: 33.50010
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    40 ----
[CW] collect: return: 272.68359, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 1.35931, qf2_loss: 1.33841, policy_loss: -34.37247, policy_entropy: -0.11967, alpha: 0.39564, time: 33.23862
[CW] eval: return: 249.10862, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    41 ----
[CW] collect: return: 239.84149, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 1.13093, qf2_loss: 1.11472, policy_loss: -35.30446, policy_entropy: -0.14821, alpha: 0.38901, time: 33.28148
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    42 ----
[CW] collect: return: 206.98240, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 1.34405, qf2_loss: 1.33951, policy_loss: -36.19484, policy_entropy: -0.18308, alpha: 0.38263, time: 33.59496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    43 ----
[CW] collect: return: 245.84927, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 1.15730, qf2_loss: 1.15331, policy_loss: -37.24410, policy_entropy: -0.19804, alpha: 0.37643, time: 33.83320
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    44 ----
[CW] collect: return: 152.48026, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 1.27241, qf2_loss: 1.25315, policy_loss: -38.11008, policy_entropy: -0.20671, alpha: 0.37029, time: 33.62498
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    45 ----
[CW] collect: return: 316.46778, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 1.32338, qf2_loss: 1.31021, policy_loss: -38.83488, policy_entropy: -0.21670, alpha: 0.36422, time: 33.73739
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    46 ----
[CW] collect: return: 273.45719, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 1.59644, qf2_loss: 1.60770, policy_loss: -40.07373, policy_entropy: -0.23370, alpha: 0.35822, time: 33.62075
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    47 ----
[CW] collect: return: 262.05880, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 1.33770, qf2_loss: 1.32845, policy_loss: -41.29972, policy_entropy: -0.24658, alpha: 0.35234, time: 33.70485
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    48 ----
[CW] collect: return: 218.96873, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 1.54640, qf2_loss: 1.53105, policy_loss: -42.11990, policy_entropy: -0.26105, alpha: 0.34656, time: 33.85722
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    49 ----
[CW] collect: return: 313.51121, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 1.30160, qf2_loss: 1.28631, policy_loss: -43.04857, policy_entropy: -0.28562, alpha: 0.34090, time: 33.52795
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    50 ----
[CW] collect: return: 202.48048, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 1.32702, qf2_loss: 1.31466, policy_loss: -44.32317, policy_entropy: -0.29936, alpha: 0.33538, time: 33.70652
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    51 ----
[CW] collect: return: 258.62268, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 1.34627, qf2_loss: 1.32717, policy_loss: -44.88918, policy_entropy: -0.31851, alpha: 0.32999, time: 33.46001
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    52 ----
[CW] collect: return: 277.84132, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 1.59318, qf2_loss: 1.58347, policy_loss: -45.99717, policy_entropy: -0.33418, alpha: 0.32472, time: 33.36142
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    53 ----
[CW] collect: return: 242.21150, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 1.38332, qf2_loss: 1.37784, policy_loss: -46.82308, policy_entropy: -0.32123, alpha: 0.31943, time: 33.59742
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    54 ----
[CW] collect: return: 259.66126, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 1.62722, qf2_loss: 1.61591, policy_loss: -47.88696, policy_entropy: -0.34140, alpha: 0.31416, time: 33.66145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    55 ----
[CW] collect: return: 194.75989, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 1.25357, qf2_loss: 1.24290, policy_loss: -48.81326, policy_entropy: -0.36068, alpha: 0.30904, time: 33.46959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    56 ----
[CW] collect: return: 232.55025, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 1.35690, qf2_loss: 1.34173, policy_loss: -50.04850, policy_entropy: -0.35716, alpha: 0.30397, time: 33.77644
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    57 ----
[CW] collect: return: 229.01408, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 1.62895, qf2_loss: 1.62747, policy_loss: -50.62553, policy_entropy: -0.38702, alpha: 0.29900, time: 33.51378
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    58 ----
[CW] collect: return: 258.71232, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 2.39684, qf2_loss: 2.38021, policy_loss: -51.52378, policy_entropy: -0.40169, alpha: 0.29413, time: 33.37003
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    59 ----
[CW] collect: return: 232.25007, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 1.32935, qf2_loss: 1.32091, policy_loss: -52.46007, policy_entropy: -0.42712, alpha: 0.28949, time: 33.42604
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    60 ----
[CW] collect: return: 291.01362, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 1.59653, qf2_loss: 1.58408, policy_loss: -53.46975, policy_entropy: -0.43450, alpha: 0.28493, time: 33.88113
[CW] eval: return: 248.43269, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    61 ----
[CW] collect: return: 266.53962, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 1.46368, qf2_loss: 1.45406, policy_loss: -54.14913, policy_entropy: -0.45752, alpha: 0.28047, time: 33.66584
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    62 ----
[CW] collect: return: 259.93215, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 1.42789, qf2_loss: 1.41961, policy_loss: -55.28525, policy_entropy: -0.48542, alpha: 0.27620, time: 33.63458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    63 ----
[CW] collect: return: 188.37792, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 1.59802, qf2_loss: 1.58595, policy_loss: -56.29992, policy_entropy: -0.50513, alpha: 0.27207, time: 33.62415
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    64 ----
[CW] collect: return: 260.38045, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 1.85974, qf2_loss: 1.85187, policy_loss: -57.40101, policy_entropy: -0.52938, alpha: 0.26807, time: 33.63268
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    65 ----
[CW] collect: return: 301.76845, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 1.60768, qf2_loss: 1.59688, policy_loss: -57.87146, policy_entropy: -0.55524, alpha: 0.26428, time: 33.58828
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    66 ----
[CW] collect: return: 208.22366, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 1.69413, qf2_loss: 1.67331, policy_loss: -58.82517, policy_entropy: -0.58990, alpha: 0.26072, time: 33.54703
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    67 ----
[CW] collect: return: 271.33808, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 1.64767, qf2_loss: 1.63842, policy_loss: -60.18898, policy_entropy: -0.58942, alpha: 0.25723, time: 33.66529
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    68 ----
[CW] collect: return: 207.81263, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 1.56154, qf2_loss: 1.54708, policy_loss: -60.69329, policy_entropy: -0.62186, alpha: 0.25381, time: 33.57151
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    69 ----
[CW] collect: return: 173.72958, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 2.14939, qf2_loss: 2.12493, policy_loss: -61.43100, policy_entropy: -0.64946, alpha: 0.25064, time: 33.26401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    70 ----
[CW] collect: return: 246.49434, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 1.68923, qf2_loss: 1.68331, policy_loss: -62.20361, policy_entropy: -0.66867, alpha: 0.24759, time: 33.07654
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    71 ----
[CW] collect: return: 263.45119, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 1.62451, qf2_loss: 1.61226, policy_loss: -62.99651, policy_entropy: -0.67373, alpha: 0.24461, time: 33.53087
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    72 ----
[CW] collect: return: 258.06549, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 1.67767, qf2_loss: 1.65458, policy_loss: -64.17390, policy_entropy: -0.69369, alpha: 0.24168, time: 33.36001
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    73 ----
[CW] collect: return: 210.04376, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 1.86755, qf2_loss: 1.86689, policy_loss: -64.81592, policy_entropy: -0.71379, alpha: 0.23896, time: 33.64452
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    74 ----
[CW] collect: return: 288.13364, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 1.99276, qf2_loss: 1.99392, policy_loss: -65.67736, policy_entropy: -0.74264, alpha: 0.23631, time: 33.48430
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    75 ----
[CW] collect: return: 250.61947, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 1.74643, qf2_loss: 1.72269, policy_loss: -66.74439, policy_entropy: -0.72912, alpha: 0.23375, time: 33.70186
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    76 ----
[CW] collect: return: 258.61591, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 1.41771, qf2_loss: 1.40661, policy_loss: -67.15492, policy_entropy: -0.76195, alpha: 0.23120, time: 33.54779
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    77 ----
[CW] collect: return: 255.84179, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 1.77659, qf2_loss: 1.77796, policy_loss: -68.45344, policy_entropy: -0.76866, alpha: 0.22879, time: 33.57227
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    78 ----
[CW] collect: return: 267.48134, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 1.66186, qf2_loss: 1.65850, policy_loss: -69.26088, policy_entropy: -0.79595, alpha: 0.22653, time: 33.61095
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    79 ----
[CW] collect: return: 218.94555, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 1.64059, qf2_loss: 1.62925, policy_loss: -70.29309, policy_entropy: -0.81345, alpha: 0.22443, time: 33.55767
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    80 ----
[CW] collect: return: 256.82677, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 1.60942, qf2_loss: 1.60397, policy_loss: -71.11147, policy_entropy: -0.80392, alpha: 0.22235, time: 33.66839
[CW] eval: return: 283.79114, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    81 ----
[CW] collect: return: 332.33059, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 2.01563, qf2_loss: 2.00509, policy_loss: -72.04318, policy_entropy: -0.82650, alpha: 0.22023, time: 33.53624
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    82 ----
[CW] collect: return: 324.19683, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 1.58464, qf2_loss: 1.58587, policy_loss: -73.20026, policy_entropy: -0.82049, alpha: 0.21821, time: 33.50179
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    83 ----
[CW] collect: return: 293.02819, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 1.96670, qf2_loss: 1.96314, policy_loss: -73.78020, policy_entropy: -0.84229, alpha: 0.21621, time: 33.43178
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    84 ----
[CW] collect: return: 279.16663, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 1.76776, qf2_loss: 1.77888, policy_loss: -74.35604, policy_entropy: -0.85212, alpha: 0.21436, time: 33.55580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    85 ----
[CW] collect: return: 260.08368, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 1.74895, qf2_loss: 1.73667, policy_loss: -75.67882, policy_entropy: -0.87352, alpha: 0.21264, time: 33.34797
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    86 ----
[CW] collect: return: 252.41478, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 1.62877, qf2_loss: 1.62047, policy_loss: -76.50586, policy_entropy: -0.86886, alpha: 0.21095, time: 33.44709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    87 ----
[CW] collect: return: 306.22647, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 1.53311, qf2_loss: 1.52090, policy_loss: -77.29200, policy_entropy: -0.87784, alpha: 0.20924, time: 33.34203
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    88 ----
[CW] collect: return: 247.64756, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 1.77845, qf2_loss: 1.76318, policy_loss: -77.80062, policy_entropy: -0.87413, alpha: 0.20759, time: 32.91767
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    89 ----
[CW] collect: return: 244.98571, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 1.87960, qf2_loss: 1.85129, policy_loss: -79.05173, policy_entropy: -0.87702, alpha: 0.20585, time: 33.49746
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    90 ----
[CW] collect: return: 278.66292, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 1.85817, qf2_loss: 1.84278, policy_loss: -79.88936, policy_entropy: -0.87554, alpha: 0.20406, time: 33.44970
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    91 ----
[CW] collect: return: 243.82340, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 1.66856, qf2_loss: 1.65003, policy_loss: -80.11123, policy_entropy: -0.91137, alpha: 0.20244, time: 33.36056
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    92 ----
[CW] collect: return: 224.12911, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 1.65204, qf2_loss: 1.63984, policy_loss: -81.33765, policy_entropy: -0.90029, alpha: 0.20100, time: 32.92323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    93 ----
[CW] collect: return: 411.56731, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 2.52269, qf2_loss: 2.51095, policy_loss: -82.09933, policy_entropy: -0.89231, alpha: 0.19939, time: 32.84022
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    94 ----
[CW] collect: return: 231.50790, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 1.57511, qf2_loss: 1.56426, policy_loss: -82.77965, policy_entropy: -0.91877, alpha: 0.19776, time: 33.36298
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    95 ----
[CW] collect: return: 361.97189, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 1.63128, qf2_loss: 1.62597, policy_loss: -83.81107, policy_entropy: -0.93388, alpha: 0.19663, time: 33.66066
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    96 ----
[CW] collect: return: 254.94422, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 1.68899, qf2_loss: 1.66569, policy_loss: -84.59901, policy_entropy: -0.93292, alpha: 0.19538, time: 33.61427
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    97 ----
[CW] collect: return: 235.60100, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 1.63331, qf2_loss: 1.60604, policy_loss: -85.14426, policy_entropy: -0.92903, alpha: 0.19410, time: 33.45644
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    98 ----
[CW] collect: return: 295.89665, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 1.46811, qf2_loss: 1.47747, policy_loss: -85.96173, policy_entropy: -0.94608, alpha: 0.19301, time: 33.62589
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    99 ----
[CW] collect: return: 316.24263, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 1.69850, qf2_loss: 1.67774, policy_loss: -87.20482, policy_entropy: -0.94773, alpha: 0.19206, time: 33.49545
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   100 ----
[CW] collect: return: 212.83366, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 1.60144, qf2_loss: 1.57401, policy_loss: -88.04614, policy_entropy: -0.95161, alpha: 0.19094, time: 33.40245
[CW] eval: return: 309.60972, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   101 ----
[CW] collect: return: 289.91268, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 1.83046, qf2_loss: 1.79364, policy_loss: -88.31660, policy_entropy: -0.94052, alpha: 0.18989, time: 33.30565
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   102 ----
[CW] collect: return: 296.76587, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 1.52932, qf2_loss: 1.50462, policy_loss: -89.34344, policy_entropy: -0.95200, alpha: 0.18868, time: 33.81392
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   103 ----
[CW] collect: return: 310.52326, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 1.70397, qf2_loss: 1.68416, policy_loss: -90.29327, policy_entropy: -0.96437, alpha: 0.18779, time: 33.56708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   104 ----
[CW] collect: return: 350.52240, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 1.93046, qf2_loss: 1.92529, policy_loss: -91.19511, policy_entropy: -0.95028, alpha: 0.18679, time: 33.52320
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   105 ----
[CW] collect: return: 314.57944, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 1.92466, qf2_loss: 1.91323, policy_loss: -92.18235, policy_entropy: -0.95794, alpha: 0.18569, time: 33.46640
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   106 ----
[CW] collect: return: 316.99266, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 2.11635, qf2_loss: 2.11058, policy_loss: -92.96690, policy_entropy: -0.95304, alpha: 0.18471, time: 33.44336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   107 ----
[CW] collect: return: 219.75071, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 1.77383, qf2_loss: 1.75029, policy_loss: -93.76789, policy_entropy: -0.96422, alpha: 0.18350, time: 33.63057
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   108 ----
[CW] collect: return: 242.91311, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 1.43114, qf2_loss: 1.41802, policy_loss: -94.15580, policy_entropy: -0.97753, alpha: 0.18279, time: 33.39940
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   109 ----
[CW] collect: return: 285.79231, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 1.43808, qf2_loss: 1.41924, policy_loss: -94.97702, policy_entropy: -0.98856, alpha: 0.18235, time: 33.09751
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   110 ----
[CW] collect: return: 281.84105, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 1.59786, qf2_loss: 1.58719, policy_loss: -95.71256, policy_entropy: -0.97648, alpha: 0.18195, time: 33.55124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   111 ----
[CW] collect: return: 316.64794, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 2.65299, qf2_loss: 2.65720, policy_loss: -96.79616, policy_entropy: -0.97555, alpha: 0.18109, time: 36.77132
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   112 ----
[CW] collect: return: 307.55971, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 1.64116, qf2_loss: 1.62338, policy_loss: -97.40525, policy_entropy: -0.97709, alpha: 0.18050, time: 33.30832
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   113 ----
[CW] collect: return: 341.35542, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 1.61072, qf2_loss: 1.59520, policy_loss: -98.19857, policy_entropy: -1.00787, alpha: 0.18013, time: 33.33567
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   114 ----
[CW] collect: return: 320.67002, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 1.54623, qf2_loss: 1.53729, policy_loss: -99.12509, policy_entropy: -1.01464, alpha: 0.18050, time: 33.56894
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   115 ----
[CW] collect: return: 256.38504, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 1.51148, qf2_loss: 1.50156, policy_loss: -99.21116, policy_entropy: -1.00728, alpha: 0.18088, time: 33.32305
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   116 ----
[CW] collect: return: 337.19839, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 1.48241, qf2_loss: 1.47113, policy_loss: -100.51895, policy_entropy: -1.02008, alpha: 0.18143, time: 33.64954
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   117 ----
[CW] collect: return: 311.55100, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 1.49589, qf2_loss: 1.49497, policy_loss: -100.74907, policy_entropy: -1.02584, alpha: 0.18222, time: 33.35900
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   118 ----
[CW] collect: return: 298.43244, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 2.31749, qf2_loss: 2.33520, policy_loss: -101.86937, policy_entropy: -1.01081, alpha: 0.18316, time: 33.53757
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   119 ----
[CW] collect: return: 289.71339, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 1.67133, qf2_loss: 1.65299, policy_loss: -102.37978, policy_entropy: -1.00900, alpha: 0.18337, time: 33.52697
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   120 ----
[CW] collect: return: 218.12460, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 1.65212, qf2_loss: 1.65089, policy_loss: -103.16553, policy_entropy: -1.00762, alpha: 0.18362, time: 33.46180
[CW] eval: return: 299.28035, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   121 ----
[CW] collect: return: 358.66270, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 2.39603, qf2_loss: 2.37273, policy_loss: -103.66883, policy_entropy: -0.99270, alpha: 0.18376, time: 33.46371
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   122 ----
[CW] collect: return: 350.15561, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 2.09549, qf2_loss: 2.10418, policy_loss: -104.89702, policy_entropy: -1.01662, alpha: 0.18379, time: 33.62671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   123 ----
[CW] collect: return: 313.47492, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 2.16541, qf2_loss: 2.16850, policy_loss: -105.29307, policy_entropy: -0.99046, alpha: 0.18406, time: 33.62788
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   124 ----
[CW] collect: return: 313.50707, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 1.57625, qf2_loss: 1.55902, policy_loss: -105.77433, policy_entropy: -1.01566, alpha: 0.18425, time: 33.51825
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   125 ----
[CW] collect: return: 390.77017, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 1.51679, qf2_loss: 1.50064, policy_loss: -106.52449, policy_entropy: -1.00909, alpha: 0.18484, time: 33.32167
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   126 ----
[CW] collect: return: 335.38274, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 1.57893, qf2_loss: 1.57621, policy_loss: -107.61517, policy_entropy: -1.01879, alpha: 0.18547, time: 33.22347
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   127 ----
[CW] collect: return: 286.64578, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 1.74614, qf2_loss: 1.73191, policy_loss: -108.14397, policy_entropy: -1.00199, alpha: 0.18616, time: 32.94586
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   128 ----
[CW] collect: return: 306.54286, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 2.09952, qf2_loss: 2.10074, policy_loss: -109.01805, policy_entropy: -1.00588, alpha: 0.18641, time: 33.28194
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   129 ----
[CW] collect: return: 300.86565, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 1.84915, qf2_loss: 1.85391, policy_loss: -109.85896, policy_entropy: -1.01131, alpha: 0.18698, time: 33.28554
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   130 ----
[CW] collect: return: 376.50000, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 1.68325, qf2_loss: 1.65333, policy_loss: -110.21839, policy_entropy: -1.01245, alpha: 0.18769, time: 33.60756
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   131 ----
[CW] collect: return: 269.41199, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 1.77311, qf2_loss: 1.76046, policy_loss: -110.81795, policy_entropy: -1.01415, alpha: 0.18852, time: 33.65774
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   132 ----
[CW] collect: return: 302.87824, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 2.43056, qf2_loss: 2.42883, policy_loss: -111.79678, policy_entropy: -0.99549, alpha: 0.18868, time: 33.33280
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   133 ----
[CW] collect: return: 332.54658, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 1.63900, qf2_loss: 1.63658, policy_loss: -112.35347, policy_entropy: -1.00952, alpha: 0.18920, time: 33.35852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   134 ----
[CW] collect: return: 326.12304, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 1.79492, qf2_loss: 1.81186, policy_loss: -112.76504, policy_entropy: -1.00994, alpha: 0.18936, time: 33.46638
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   135 ----
[CW] collect: return: 347.32512, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 1.70400, qf2_loss: 1.69872, policy_loss: -113.61277, policy_entropy: -1.01981, alpha: 0.19070, time: 33.78233
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   136 ----
[CW] collect: return: 335.36392, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 1.75828, qf2_loss: 1.76141, policy_loss: -114.70327, policy_entropy: -1.01213, alpha: 0.19161, time: 33.61965
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   137 ----
[CW] collect: return: 527.46505, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 1.96271, qf2_loss: 1.95105, policy_loss: -115.16594, policy_entropy: -1.00858, alpha: 0.19263, time: 33.34596
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   138 ----
[CW] collect: return: 220.57932, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 2.40565, qf2_loss: 2.41611, policy_loss: -115.64665, policy_entropy: -1.00047, alpha: 0.19307, time: 33.50766
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   139 ----
[CW] collect: return: 275.03953, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 3.46249, qf2_loss: 3.47923, policy_loss: -116.45641, policy_entropy: -0.98338, alpha: 0.19252, time: 33.38793
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   140 ----
[CW] collect: return: 405.19646, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 2.11585, qf2_loss: 2.11726, policy_loss: -117.30929, policy_entropy: -1.00017, alpha: 0.19194, time: 33.50227
[CW] eval: return: 323.09889, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   141 ----
[CW] collect: return: 302.09531, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 1.71782, qf2_loss: 1.69614, policy_loss: -117.91120, policy_entropy: -1.01079, alpha: 0.19224, time: 33.24749
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   142 ----
[CW] collect: return: 302.75175, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 1.79551, qf2_loss: 1.79575, policy_loss: -119.06552, policy_entropy: -1.01684, alpha: 0.19323, time: 33.10159
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   143 ----
[CW] collect: return: 347.95131, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 2.03599, qf2_loss: 2.03997, policy_loss: -119.16016, policy_entropy: -1.01218, alpha: 0.19458, time: 33.62526
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   144 ----
[CW] collect: return: 341.65133, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 2.00973, qf2_loss: 2.00991, policy_loss: -119.96380, policy_entropy: -1.00246, alpha: 0.19507, time: 33.36337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   145 ----
[CW] collect: return: 395.29122, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 2.80256, qf2_loss: 2.79994, policy_loss: -120.72227, policy_entropy: -0.99134, alpha: 0.19464, time: 33.46343
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   146 ----
[CW] collect: return: 277.12408, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 2.07709, qf2_loss: 2.07623, policy_loss: -121.11700, policy_entropy: -1.00597, alpha: 0.19473, time: 33.67664
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   147 ----
[CW] collect: return: 267.61873, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 1.92463, qf2_loss: 1.92698, policy_loss: -121.88279, policy_entropy: -0.99153, alpha: 0.19468, time: 33.58931
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   148 ----
[CW] collect: return: 302.20258, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 1.96228, qf2_loss: 1.96412, policy_loss: -122.92809, policy_entropy: -0.99664, alpha: 0.19438, time: 33.64380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   149 ----
[CW] collect: return: 199.16087, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 2.36022, qf2_loss: 2.38135, policy_loss: -123.07908, policy_entropy: -0.98921, alpha: 0.19346, time: 33.70214
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   150 ----
[CW] collect: return: 280.27036, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 2.13218, qf2_loss: 2.10990, policy_loss: -123.80070, policy_entropy: -1.00053, alpha: 0.19312, time: 33.71344
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   151 ----
[CW] collect: return: 248.66656, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 2.10362, qf2_loss: 2.08393, policy_loss: -124.39204, policy_entropy: -0.98914, alpha: 0.19304, time: 33.72842
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   152 ----
[CW] collect: return: 310.89625, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 2.43061, qf2_loss: 2.39994, policy_loss: -125.23483, policy_entropy: -0.98737, alpha: 0.19166, time: 33.87337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   153 ----
[CW] collect: return: 348.07800, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 2.06111, qf2_loss: 2.08151, policy_loss: -125.73862, policy_entropy: -1.00528, alpha: 0.19162, time: 33.61432
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   154 ----
[CW] collect: return: 267.92391, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 2.25283, qf2_loss: 2.23878, policy_loss: -126.22006, policy_entropy: -0.99809, alpha: 0.19127, time: 33.69883
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   155 ----
[CW] collect: return: 285.49273, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 2.17003, qf2_loss: 2.16362, policy_loss: -126.73332, policy_entropy: -0.99701, alpha: 0.19119, time: 33.77768
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   156 ----
[CW] collect: return: 294.46203, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 2.38740, qf2_loss: 2.37499, policy_loss: -127.49470, policy_entropy: -1.00294, alpha: 0.19116, time: 33.66132
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   157 ----
[CW] collect: return: 259.40622, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 2.76613, qf2_loss: 2.78371, policy_loss: -128.16221, policy_entropy: -0.99964, alpha: 0.19121, time: 33.40206
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   158 ----
[CW] collect: return: 282.93960, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 2.11646, qf2_loss: 2.10870, policy_loss: -128.53228, policy_entropy: -1.00852, alpha: 0.19166, time: 33.62283
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   159 ----
[CW] collect: return: 340.56857, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 2.07235, qf2_loss: 2.07127, policy_loss: -129.36264, policy_entropy: -1.01644, alpha: 0.19274, time: 33.70976
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   160 ----
[CW] collect: return: 217.91990, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 2.30364, qf2_loss: 2.27826, policy_loss: -129.93749, policy_entropy: -0.99936, alpha: 0.19362, time: 33.68124
[CW] eval: return: 312.42036, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   161 ----
[CW] collect: return: 248.32163, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 2.31254, qf2_loss: 2.30407, policy_loss: -130.25938, policy_entropy: -1.00553, alpha: 0.19379, time: 33.58864
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   162 ----
[CW] collect: return: 276.34762, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 2.39146, qf2_loss: 2.38428, policy_loss: -131.12557, policy_entropy: -1.00433, alpha: 0.19445, time: 33.25226
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   163 ----
[CW] collect: return: 231.98913, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 2.21179, qf2_loss: 2.20749, policy_loss: -131.96437, policy_entropy: -1.01024, alpha: 0.19447, time: 33.39112
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   164 ----
[CW] collect: return: 262.15307, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 2.63912, qf2_loss: 2.65590, policy_loss: -132.28820, policy_entropy: -1.00068, alpha: 0.19566, time: 33.47760
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   165 ----
[CW] collect: return: 289.13241, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 2.54060, qf2_loss: 2.52460, policy_loss: -132.67684, policy_entropy: -1.00383, alpha: 0.19563, time: 33.54333
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   166 ----
[CW] collect: return: 310.43341, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 2.43803, qf2_loss: 2.44524, policy_loss: -133.43803, policy_entropy: -0.99968, alpha: 0.19548, time: 33.88609
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   167 ----
[CW] collect: return: 357.27510, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 2.20886, qf2_loss: 2.18921, policy_loss: -134.35892, policy_entropy: -0.99277, alpha: 0.19560, time: 33.74338
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   168 ----
[CW] collect: return: 257.80167, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 2.09855, qf2_loss: 2.08209, policy_loss: -134.70655, policy_entropy: -1.00352, alpha: 0.19491, time: 33.61890
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   169 ----
[CW] collect: return: 416.41626, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 2.52467, qf2_loss: 2.52417, policy_loss: -135.42929, policy_entropy: -0.99790, alpha: 0.19519, time: 33.60540
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   170 ----
[CW] collect: return: 266.88128, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 2.52269, qf2_loss: 2.50323, policy_loss: -135.80202, policy_entropy: -1.00664, alpha: 0.19542, time: 33.55480
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   171 ----
[CW] collect: return: 272.27006, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 2.57037, qf2_loss: 2.55814, policy_loss: -136.37963, policy_entropy: -1.00596, alpha: 0.19615, time: 33.68964
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   172 ----
[CW] collect: return: 319.22536, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 2.29898, qf2_loss: 2.28114, policy_loss: -137.26321, policy_entropy: -1.02555, alpha: 0.19775, time: 33.67352
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   173 ----
[CW] collect: return: 453.09708, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 2.46367, qf2_loss: 2.47954, policy_loss: -138.05328, policy_entropy: -1.01700, alpha: 0.20013, time: 33.40317
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   174 ----
[CW] collect: return: 281.69818, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 2.78025, qf2_loss: 2.80857, policy_loss: -138.02960, policy_entropy: -1.00976, alpha: 0.20153, time: 33.53344
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   175 ----
[CW] collect: return: 263.55490, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 2.82245, qf2_loss: 2.82062, policy_loss: -138.99113, policy_entropy: -0.99026, alpha: 0.20167, time: 33.29446
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   176 ----
[CW] collect: return: 327.72782, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 2.13977, qf2_loss: 2.12533, policy_loss: -139.33457, policy_entropy: -1.00661, alpha: 0.20114, time: 33.11551
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   177 ----
[CW] collect: return: 380.09849, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 2.36191, qf2_loss: 2.37271, policy_loss: -139.81276, policy_entropy: -1.00737, alpha: 0.20177, time: 33.50603
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   178 ----
[CW] collect: return: 374.64734, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 2.66412, qf2_loss: 2.65926, policy_loss: -140.84278, policy_entropy: -1.01271, alpha: 0.20318, time: 33.68047
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   179 ----
[CW] collect: return: 240.56867, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 2.40919, qf2_loss: 2.39630, policy_loss: -141.06786, policy_entropy: -0.99119, alpha: 0.20296, time: 33.46594
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   180 ----
[CW] collect: return: 340.21039, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 2.36018, qf2_loss: 2.37050, policy_loss: -141.95448, policy_entropy: -1.01215, alpha: 0.20308, time: 33.50753
[CW] eval: return: 320.54329, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   181 ----
[CW] collect: return: 372.88813, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 5.32259, qf2_loss: 5.39701, policy_loss: -142.81666, policy_entropy: -1.00268, alpha: 0.20438, time: 33.35517
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   182 ----
[CW] collect: return: 282.61465, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 2.41222, qf2_loss: 2.39928, policy_loss: -143.07777, policy_entropy: -1.00441, alpha: 0.20444, time: 33.60104
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   183 ----
[CW] collect: return: 397.84375, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 2.11933, qf2_loss: 2.09749, policy_loss: -143.76682, policy_entropy: -1.01634, alpha: 0.20537, time: 33.36769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   184 ----
[CW] collect: return: 214.96467, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 2.26254, qf2_loss: 2.28732, policy_loss: -143.77930, policy_entropy: -1.02252, alpha: 0.20771, time: 33.51355
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   185 ----
[CW] collect: return: 359.25998, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 2.61460, qf2_loss: 2.60913, policy_loss: -144.49967, policy_entropy: -1.00948, alpha: 0.20929, time: 33.71438
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   186 ----
[CW] collect: return: 317.27205, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 2.39682, qf2_loss: 2.39282, policy_loss: -145.35227, policy_entropy: -1.00825, alpha: 0.21042, time: 33.41995
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   187 ----
[CW] collect: return: 296.93604, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 2.48779, qf2_loss: 2.51848, policy_loss: -146.15310, policy_entropy: -1.02723, alpha: 0.21212, time: 33.44540
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   188 ----
[CW] collect: return: 364.24318, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 2.66805, qf2_loss: 2.62686, policy_loss: -146.61830, policy_entropy: -1.00863, alpha: 0.21436, time: 33.48229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   189 ----
[CW] collect: return: 273.28981, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 2.77903, qf2_loss: 2.77559, policy_loss: -147.28559, policy_entropy: -1.01237, alpha: 0.21607, time: 33.20108
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   190 ----
[CW] collect: return: 439.26149, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 2.71852, qf2_loss: 2.70694, policy_loss: -147.51009, policy_entropy: -1.02051, alpha: 0.21770, time: 33.19012
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   191 ----
[CW] collect: return: 316.65126, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 2.76075, qf2_loss: 2.75369, policy_loss: -147.77294, policy_entropy: -1.01471, alpha: 0.21941, time: 33.60819
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   192 ----
[CW] collect: return: 271.37312, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 2.73534, qf2_loss: 2.74968, policy_loss: -148.53742, policy_entropy: -1.02222, alpha: 0.22152, time: 33.44320
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   193 ----
[CW] collect: return: 331.85003, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 3.96717, qf2_loss: 3.96142, policy_loss: -149.37199, policy_entropy: -1.01242, alpha: 0.22353, time: 33.43426
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   194 ----
[CW] collect: return: 184.35611, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 2.51738, qf2_loss: 2.51261, policy_loss: -149.36275, policy_entropy: -1.00015, alpha: 0.22478, time: 33.54984
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   195 ----
[CW] collect: return: 344.20304, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 2.46568, qf2_loss: 2.43967, policy_loss: -149.90483, policy_entropy: -1.00117, alpha: 0.22521, time: 33.58519
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   196 ----
[CW] collect: return: 401.43720, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 2.99190, qf2_loss: 2.98507, policy_loss: -150.72513, policy_entropy: -1.01174, alpha: 0.22575, time: 33.76145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   197 ----
[CW] collect: return: 282.74238, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 3.16511, qf2_loss: 3.15911, policy_loss: -152.10305, policy_entropy: -1.02078, alpha: 0.22765, time: 33.67145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   198 ----
[CW] collect: return: 267.34230, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 2.64056, qf2_loss: 2.62597, policy_loss: -151.75525, policy_entropy: -1.01902, alpha: 0.23055, time: 33.42622
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   199 ----
[CW] collect: return: 273.36214, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 2.77829, qf2_loss: 2.76778, policy_loss: -152.03222, policy_entropy: -1.01869, alpha: 0.23306, time: 33.29775
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   200 ----
[CW] collect: return: 230.25402, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 3.10673, qf2_loss: 3.10713, policy_loss: -152.62814, policy_entropy: -1.01254, alpha: 0.23494, time: 33.46651
[CW] eval: return: 295.73143, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   201 ----
[CW] collect: return: 171.02277, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 2.77318, qf2_loss: 2.75275, policy_loss: -153.18309, policy_entropy: -1.00847, alpha: 0.23609, time: 33.30868
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   202 ----
[CW] collect: return: 170.40406, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 2.82974, qf2_loss: 2.83952, policy_loss: -153.18778, policy_entropy: -1.02224, alpha: 0.23831, time: 33.55309
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   203 ----
[CW] collect: return: 252.96165, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 3.16216, qf2_loss: 3.20467, policy_loss: -154.60217, policy_entropy: -1.01001, alpha: 0.24116, time: 32.46852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   204 ----
[CW] collect: return: 403.99678, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 3.07154, qf2_loss: 3.06435, policy_loss: -155.04291, policy_entropy: -1.00420, alpha: 0.24179, time: 33.16738
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   205 ----
[CW] collect: return: 294.92244, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 2.37265, qf2_loss: 2.34175, policy_loss: -155.33891, policy_entropy: -1.01089, alpha: 0.24271, time: 33.78748
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   206 ----
[CW] collect: return: 291.26232, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 3.04468, qf2_loss: 3.04031, policy_loss: -156.06170, policy_entropy: -1.00520, alpha: 0.24382, time: 33.42954
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   207 ----
[CW] collect: return: 215.73770, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 3.45563, qf2_loss: 3.48768, policy_loss: -156.78499, policy_entropy: -1.01282, alpha: 0.24474, time: 33.28946
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   208 ----
[CW] collect: return: 340.09595, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 2.76868, qf2_loss: 2.78211, policy_loss: -157.24568, policy_entropy: -0.98862, alpha: 0.24514, time: 33.69993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   209 ----
[CW] collect: return: 302.28443, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 2.78004, qf2_loss: 2.78835, policy_loss: -157.65328, policy_entropy: -1.01782, alpha: 0.24602, time: 33.36666
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   210 ----
[CW] collect: return: 282.59646, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 2.93093, qf2_loss: 2.95612, policy_loss: -157.30212, policy_entropy: -1.01998, alpha: 0.24913, time: 33.43653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   211 ----
[CW] collect: return: 262.98106, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 3.15817, qf2_loss: 3.18022, policy_loss: -158.38921, policy_entropy: -1.00036, alpha: 0.25020, time: 33.61012
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   212 ----
[CW] collect: return: 376.09316, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 2.86939, qf2_loss: 2.89318, policy_loss: -158.75230, policy_entropy: -1.00289, alpha: 0.25071, time: 33.20712
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   213 ----
[CW] collect: return: 415.79201, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 4.62662, qf2_loss: 4.63662, policy_loss: -158.88472, policy_entropy: -1.00233, alpha: 0.25043, time: 33.41720
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   214 ----
[CW] collect: return: 255.15535, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 3.15651, qf2_loss: 3.12689, policy_loss: -159.56223, policy_entropy: -0.99751, alpha: 0.25085, time: 33.50178
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   215 ----
[CW] collect: return: 355.85177, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 2.72074, qf2_loss: 2.71767, policy_loss: -160.49909, policy_entropy: -1.01240, alpha: 0.25173, time: 33.57751
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   216 ----
[CW] collect: return: 277.84133, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 2.73822, qf2_loss: 2.73984, policy_loss: -160.76464, policy_entropy: -0.99408, alpha: 0.25219, time: 34.41130
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   217 ----
[CW] collect: return: 249.90459, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 2.51076, qf2_loss: 2.50982, policy_loss: -161.13395, policy_entropy: -1.01284, alpha: 0.25296, time: 40.44733
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   218 ----
[CW] collect: return: 248.60776, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 2.87031, qf2_loss: 2.86045, policy_loss: -161.77892, policy_entropy: -1.00775, alpha: 0.25378, time: 34.04342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   219 ----
[CW] collect: return: 273.24415, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 3.25075, qf2_loss: 3.27422, policy_loss: -161.77816, policy_entropy: -1.01762, alpha: 0.25551, time: 33.61624
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   220 ----
[CW] collect: return: 379.75906, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 3.12453, qf2_loss: 3.12921, policy_loss: -162.47602, policy_entropy: -0.99420, alpha: 0.25680, time: 33.69555
[CW] eval: return: 345.65098, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   221 ----
[CW] collect: return: 302.07112, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 2.87211, qf2_loss: 2.84070, policy_loss: -162.68236, policy_entropy: -1.00207, alpha: 0.25677, time: 33.38177
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   222 ----
[CW] collect: return: 300.06361, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 3.12860, qf2_loss: 3.16506, policy_loss: -163.21649, policy_entropy: -0.99728, alpha: 0.25649, time: 36.88116
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   223 ----
[CW] collect: return: 422.25911, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 3.41938, qf2_loss: 3.43548, policy_loss: -163.74374, policy_entropy: -0.99693, alpha: 0.25658, time: 34.32697
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   224 ----
[CW] collect: return: 331.85549, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 3.05413, qf2_loss: 3.03585, policy_loss: -164.44046, policy_entropy: -1.00062, alpha: 0.25630, time: 33.67459
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   225 ----
[CW] collect: return: 284.78292, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 2.94731, qf2_loss: 2.93421, policy_loss: -164.87472, policy_entropy: -1.01001, alpha: 0.25624, time: 33.89633
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   226 ----
[CW] collect: return: 345.62863, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 3.74713, qf2_loss: 3.75453, policy_loss: -165.90721, policy_entropy: -1.00335, alpha: 0.25777, time: 33.84508
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   227 ----
[CW] collect: return: 242.24447, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 2.96043, qf2_loss: 2.94086, policy_loss: -165.93187, policy_entropy: -1.01210, alpha: 0.25853, time: 33.86013
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   228 ----
[CW] collect: return: 380.68746, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 3.62703, qf2_loss: 3.60477, policy_loss: -166.55821, policy_entropy: -1.00528, alpha: 0.26013, time: 34.04381
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   229 ----
[CW] collect: return: 388.15476, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 3.21403, qf2_loss: 3.24063, policy_loss: -166.41278, policy_entropy: -0.99997, alpha: 0.26122, time: 33.72992
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   230 ----
[CW] collect: return: 270.28463, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 2.89452, qf2_loss: 2.89029, policy_loss: -166.78315, policy_entropy: -1.00465, alpha: 0.26061, time: 33.55369
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   231 ----
[CW] collect: return: 360.29795, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 4.24655, qf2_loss: 4.25766, policy_loss: -168.40939, policy_entropy: -1.00056, alpha: 0.26132, time: 33.86469
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   232 ----
[CW] collect: return: 303.84811, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 3.28682, qf2_loss: 3.28157, policy_loss: -168.22679, policy_entropy: -0.99750, alpha: 0.26115, time: 33.86308
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   233 ----
[CW] collect: return: 355.21604, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 3.19836, qf2_loss: 3.19423, policy_loss: -168.22870, policy_entropy: -0.99297, alpha: 0.26024, time: 33.86138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   234 ----
[CW] collect: return: 256.17396, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 2.73032, qf2_loss: 2.74128, policy_loss: -168.54665, policy_entropy: -1.00444, alpha: 0.26098, time: 33.44869
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   235 ----
[CW] collect: return: 293.47833, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 3.06789, qf2_loss: 3.05945, policy_loss: -169.63910, policy_entropy: -1.00778, alpha: 0.26124, time: 33.61594
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   236 ----
[CW] collect: return: 367.84295, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 3.23518, qf2_loss: 3.23583, policy_loss: -169.57451, policy_entropy: -1.00369, alpha: 0.26218, time: 33.74469
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   237 ----
[CW] collect: return: 285.57070, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 4.53975, qf2_loss: 4.58197, policy_loss: -169.24498, policy_entropy: -1.00244, alpha: 0.26231, time: 34.04830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   238 ----
[CW] collect: return: 326.99334, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 2.99883, qf2_loss: 3.02090, policy_loss: -170.90819, policy_entropy: -0.99425, alpha: 0.26207, time: 34.19876
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   239 ----
[CW] collect: return: 406.39673, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 3.18061, qf2_loss: 3.17866, policy_loss: -170.35922, policy_entropy: -1.00680, alpha: 0.26241, time: 33.73859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   240 ----
[CW] collect: return: 374.84101, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 4.07164, qf2_loss: 4.08553, policy_loss: -171.76041, policy_entropy: -0.99946, alpha: 0.26318, time: 33.50361
[CW] eval: return: 327.91146, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   241 ----
[CW] collect: return: 237.29136, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 3.05608, qf2_loss: 3.07885, policy_loss: -172.15455, policy_entropy: -0.99335, alpha: 0.26258, time: 33.69746
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   242 ----
[CW] collect: return: 343.71648, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 2.85352, qf2_loss: 2.82950, policy_loss: -172.88363, policy_entropy: -1.01101, alpha: 0.26228, time: 33.69140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   243 ----
[CW] collect: return: 274.75089, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 2.75798, qf2_loss: 2.76470, policy_loss: -172.46209, policy_entropy: -1.00339, alpha: 0.26441, time: 33.48380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   244 ----
[CW] collect: return: 366.12447, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 3.28338, qf2_loss: 3.28259, policy_loss: -172.97847, policy_entropy: -0.99066, alpha: 0.26398, time: 33.73710
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   245 ----
[CW] collect: return: 264.21506, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 3.06846, qf2_loss: 3.09038, policy_loss: -173.90294, policy_entropy: -0.98686, alpha: 0.26161, time: 34.09335
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   246 ----
[CW] collect: return: 267.20792, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 3.92723, qf2_loss: 3.92441, policy_loss: -174.02906, policy_entropy: -0.99026, alpha: 0.26082, time: 33.89793
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   247 ----
[CW] collect: return: 301.53508, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 3.56785, qf2_loss: 3.58591, policy_loss: -174.24873, policy_entropy: -1.00047, alpha: 0.25964, time: 33.45907
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   248 ----
[CW] collect: return: 267.77329, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 3.57870, qf2_loss: 3.60406, policy_loss: -174.85367, policy_entropy: -1.00009, alpha: 0.25916, time: 33.52772
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   249 ----
[CW] collect: return: 284.41755, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 3.46140, qf2_loss: 3.47532, policy_loss: -175.04659, policy_entropy: -0.97995, alpha: 0.25793, time: 33.56300
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   250 ----
[CW] collect: return: 378.42308, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 3.79018, qf2_loss: 3.77966, policy_loss: -175.82346, policy_entropy: -1.00346, alpha: 0.25692, time: 33.95533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   251 ----
[CW] collect: return: 349.84206, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 3.35039, qf2_loss: 3.33610, policy_loss: -175.30008, policy_entropy: -1.00835, alpha: 0.25760, time: 33.90691
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   252 ----
[CW] collect: return: 298.68778, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 3.60443, qf2_loss: 3.60730, policy_loss: -175.82545, policy_entropy: -0.98958, alpha: 0.25738, time: 33.96699
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   253 ----
[CW] collect: return: 323.00915, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 5.01991, qf2_loss: 5.10073, policy_loss: -175.77517, policy_entropy: -0.98963, alpha: 0.25595, time: 34.15880
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   254 ----
[CW] collect: return: 338.03371, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 3.56979, qf2_loss: 3.52246, policy_loss: -176.60910, policy_entropy: -1.00350, alpha: 0.25548, time: 33.75319
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   255 ----
[CW] collect: return: 269.69213, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 2.78874, qf2_loss: 2.81855, policy_loss: -177.58898, policy_entropy: -1.01596, alpha: 0.25634, time: 33.58695
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   256 ----
[CW] collect: return: 352.38837, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 3.00910, qf2_loss: 3.00970, policy_loss: -177.78068, policy_entropy: -0.99766, alpha: 0.25728, time: 33.80333
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   257 ----
[CW] collect: return: 265.61243, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 3.69028, qf2_loss: 3.67304, policy_loss: -178.19245, policy_entropy: -0.99589, alpha: 0.25750, time: 33.89695
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   258 ----
[CW] collect: return: 399.90476, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 3.43576, qf2_loss: 3.45207, policy_loss: -178.23559, policy_entropy: -0.99418, alpha: 0.25651, time: 33.85643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   259 ----
[CW] collect: return: 305.74769, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 3.54234, qf2_loss: 3.54721, policy_loss: -178.72516, policy_entropy: -0.99312, alpha: 0.25516, time: 33.81117
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   260 ----
[CW] collect: return: 372.59184, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 3.68449, qf2_loss: 3.72245, policy_loss: -179.50356, policy_entropy: -0.99525, alpha: 0.25506, time: 33.66817
[CW] eval: return: 333.48783, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   261 ----
[CW] collect: return: 294.77809, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 4.16516, qf2_loss: 4.16253, policy_loss: -179.38222, policy_entropy: -0.99597, alpha: 0.25447, time: 33.91391
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   262 ----
[CW] collect: return: 298.78393, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 4.62327, qf2_loss: 4.69056, policy_loss: -179.86597, policy_entropy: -0.99566, alpha: 0.25352, time: 33.52803
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   263 ----
[CW] collect: return: 366.14012, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 3.44679, qf2_loss: 3.44952, policy_loss: -180.50847, policy_entropy: -0.99059, alpha: 0.25267, time: 33.48856
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   264 ----
[CW] collect: return: 364.99349, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 3.59043, qf2_loss: 3.55507, policy_loss: -180.71938, policy_entropy: -1.00000, alpha: 0.25187, time: 33.89895
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   265 ----
[CW] collect: return: 412.11529, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 3.55159, qf2_loss: 3.60037, policy_loss: -180.83432, policy_entropy: -1.00548, alpha: 0.25157, time: 33.90727
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   266 ----
[CW] collect: return: 349.49182, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 3.30461, qf2_loss: 3.31791, policy_loss: -181.20226, policy_entropy: -1.00719, alpha: 0.25316, time: 33.88701
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   267 ----
[CW] collect: return: 354.30617, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 3.13553, qf2_loss: 3.12414, policy_loss: -181.51394, policy_entropy: -1.00388, alpha: 0.25395, time: 33.69903
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   268 ----
[CW] collect: return: 453.46255, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 3.77343, qf2_loss: 3.81708, policy_loss: -182.62399, policy_entropy: -1.00183, alpha: 0.25452, time: 33.82460
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   269 ----
[CW] collect: return: 395.91844, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 4.98103, qf2_loss: 5.05678, policy_loss: -182.97739, policy_entropy: -0.99205, alpha: 0.25403, time: 34.21290
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   270 ----
[CW] collect: return: 448.42867, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 4.05787, qf2_loss: 4.06520, policy_loss: -182.49413, policy_entropy: -0.99573, alpha: 0.25355, time: 33.68209
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   271 ----
[CW] collect: return: 291.91128, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 3.85599, qf2_loss: 3.90711, policy_loss: -183.45960, policy_entropy: -0.99471, alpha: 0.25226, time: 33.72993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   272 ----
[CW] collect: return: 251.15734, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 3.48187, qf2_loss: 3.48137, policy_loss: -183.88126, policy_entropy: -0.99561, alpha: 0.25195, time: 33.50395
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   273 ----
[CW] collect: return: 348.23350, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 3.99520, qf2_loss: 4.02213, policy_loss: -183.76043, policy_entropy: -1.00212, alpha: 0.25126, time: 33.82206
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   274 ----
[CW] collect: return: 433.82712, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 4.32001, qf2_loss: 4.31936, policy_loss: -184.90515, policy_entropy: -1.00978, alpha: 0.25210, time: 33.60010
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   275 ----
[CW] collect: return: 447.88431, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 4.17666, qf2_loss: 4.19984, policy_loss: -184.67256, policy_entropy: -1.01078, alpha: 0.25357, time: 33.75488
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   276 ----
[CW] collect: return: 328.20768, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 4.97628, qf2_loss: 4.98598, policy_loss: -185.38654, policy_entropy: -1.02474, alpha: 0.25623, time: 33.83919
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   277 ----
[CW] collect: return: 243.73551, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 3.72720, qf2_loss: 3.70607, policy_loss: -185.35175, policy_entropy: -1.01728, alpha: 0.25884, time: 33.61922
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   278 ----
[CW] collect: return: 513.44376, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 4.08454, qf2_loss: 4.08298, policy_loss: -186.13660, policy_entropy: -1.01451, alpha: 0.26174, time: 33.82391
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   279 ----
[CW] collect: return: 327.66757, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 3.89092, qf2_loss: 3.92226, policy_loss: -186.55350, policy_entropy: -1.01621, alpha: 0.26390, time: 33.71999
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   280 ----
[CW] collect: return: 345.51531, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 5.72440, qf2_loss: 5.66988, policy_loss: -186.80709, policy_entropy: -0.99432, alpha: 0.26470, time: 33.75470
[CW] eval: return: 340.52934, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   281 ----
[CW] collect: return: 303.58756, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 5.70519, qf2_loss: 5.75628, policy_loss: -187.38184, policy_entropy: -1.01340, alpha: 0.26527, time: 34.14702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   282 ----
[CW] collect: return: 284.82581, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 4.16693, qf2_loss: 4.10747, policy_loss: -186.99971, policy_entropy: -0.99426, alpha: 0.26559, time: 33.93054
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   283 ----
[CW] collect: return: 312.01550, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 3.46288, qf2_loss: 3.47166, policy_loss: -187.63367, policy_entropy: -1.00816, alpha: 0.26522, time: 33.57138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   284 ----
[CW] collect: return: 223.10456, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 3.80094, qf2_loss: 3.83321, policy_loss: -188.37838, policy_entropy: -1.00315, alpha: 0.26670, time: 33.88642
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   285 ----
[CW] collect: return: 280.29848, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 3.58631, qf2_loss: 3.59940, policy_loss: -188.72995, policy_entropy: -1.00420, alpha: 0.26706, time: 33.75580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   286 ----
[CW] collect: return: 327.44813, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 4.18510, qf2_loss: 4.21968, policy_loss: -188.29984, policy_entropy: -1.01188, alpha: 0.26813, time: 34.13978
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   287 ----
[CW] collect: return: 230.00580, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 4.62697, qf2_loss: 4.67491, policy_loss: -189.11138, policy_entropy: -1.01155, alpha: 0.27023, time: 33.80106
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   288 ----
[CW] collect: return: 314.33203, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 4.15867, qf2_loss: 4.23826, policy_loss: -189.54494, policy_entropy: -0.99842, alpha: 0.27095, time: 33.80372
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   289 ----
[CW] collect: return: 360.58192, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 4.03156, qf2_loss: 4.06875, policy_loss: -189.79724, policy_entropy: -1.01605, alpha: 0.27209, time: 33.66027
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   290 ----
[CW] collect: return: 333.60844, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 3.83370, qf2_loss: 3.83958, policy_loss: -190.96590, policy_entropy: -1.00853, alpha: 0.27417, time: 33.89263
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   291 ----
[CW] collect: return: 274.98343, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 3.42383, qf2_loss: 3.40975, policy_loss: -190.04485, policy_entropy: -1.00354, alpha: 0.27568, time: 33.74311
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   292 ----
[CW] collect: return: 273.33939, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 4.21231, qf2_loss: 4.25584, policy_loss: -191.05380, policy_entropy: -1.00038, alpha: 0.27543, time: 33.72236
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   293 ----
[CW] collect: return: 308.61036, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 4.28429, qf2_loss: 4.31069, policy_loss: -190.60718, policy_entropy: -1.00406, alpha: 0.27548, time: 33.93613
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   294 ----
[CW] collect: return: 339.05276, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 4.45257, qf2_loss: 4.42720, policy_loss: -191.30542, policy_entropy: -1.00338, alpha: 0.27631, time: 33.82386
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   295 ----
[CW] collect: return: 383.85090, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 4.77179, qf2_loss: 4.79377, policy_loss: -191.88351, policy_entropy: -0.99726, alpha: 0.27631, time: 33.78869
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   296 ----
[CW] collect: return: 295.22646, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 4.72793, qf2_loss: 4.76861, policy_loss: -192.28646, policy_entropy: -0.98712, alpha: 0.27514, time: 33.79930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   297 ----
[CW] collect: return: 293.02561, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 5.06528, qf2_loss: 5.05333, policy_loss: -191.92589, policy_entropy: -0.99226, alpha: 0.27313, time: 33.68849
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   298 ----
[CW] collect: return: 347.71936, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 4.37914, qf2_loss: 4.40158, policy_loss: -192.98009, policy_entropy: -0.99457, alpha: 0.27229, time: 33.82537
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   299 ----
[CW] collect: return: 282.30306, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 3.98382, qf2_loss: 4.00531, policy_loss: -193.22107, policy_entropy: -0.99943, alpha: 0.27138, time: 33.73665
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   300 ----
[CW] collect: return: 316.51916, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 4.36878, qf2_loss: 4.36450, policy_loss: -193.97766, policy_entropy: -0.99717, alpha: 0.27200, time: 33.73748
[CW] eval: return: 334.86055, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   301 ----
[CW] collect: return: 387.74159, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 4.22781, qf2_loss: 4.27758, policy_loss: -193.15912, policy_entropy: -1.00632, alpha: 0.27221, time: 33.83122
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   302 ----
[CW] collect: return: 320.18080, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 4.30292, qf2_loss: 4.32336, policy_loss: -194.06768, policy_entropy: -0.99556, alpha: 0.27244, time: 33.88732
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   303 ----
[CW] collect: return: 312.87171, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 4.48625, qf2_loss: 4.48839, policy_loss: -194.48415, policy_entropy: -1.00828, alpha: 0.27227, time: 33.97663
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   304 ----
[CW] collect: return: 264.98284, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 3.83855, qf2_loss: 3.87826, policy_loss: -194.93998, policy_entropy: -1.01277, alpha: 0.27377, time: 33.92647
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   305 ----
[CW] collect: return: 345.15973, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 3.80488, qf2_loss: 3.81753, policy_loss: -195.06626, policy_entropy: -1.01532, alpha: 0.27602, time: 33.89020
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   306 ----
[CW] collect: return: 298.00616, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 3.98965, qf2_loss: 4.00043, policy_loss: -194.84921, policy_entropy: -1.01660, alpha: 0.27815, time: 33.88059
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   307 ----
[CW] collect: return: 230.05503, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 3.57828, qf2_loss: 3.56855, policy_loss: -195.75809, policy_entropy: -1.00651, alpha: 0.28005, time: 33.82175
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   308 ----
[CW] collect: return: 343.94584, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 4.09736, qf2_loss: 4.11687, policy_loss: -195.62653, policy_entropy: -1.01584, alpha: 0.28204, time: 33.82086
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   309 ----
[CW] collect: return: 303.31330, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 5.96809, qf2_loss: 5.93286, policy_loss: -196.36063, policy_entropy: -1.00031, alpha: 0.28308, time: 33.66758
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   310 ----
[CW] collect: return: 353.98963, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 5.68500, qf2_loss: 5.68517, policy_loss: -196.33055, policy_entropy: -0.99520, alpha: 0.28315, time: 33.66601
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   311 ----
[CW] collect: return: 348.46471, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 5.30595, qf2_loss: 5.37748, policy_loss: -196.76217, policy_entropy: -1.01217, alpha: 0.28390, time: 33.98607
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   312 ----
[CW] collect: return: 331.29879, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 4.14910, qf2_loss: 4.17346, policy_loss: -197.48593, policy_entropy: -1.00896, alpha: 0.28513, time: 33.73125
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   313 ----
[CW] collect: return: 318.64503, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 4.47708, qf2_loss: 4.50644, policy_loss: -197.33223, policy_entropy: -0.99898, alpha: 0.28624, time: 33.54959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   314 ----
[CW] collect: return: 315.17442, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 3.56918, qf2_loss: 3.57517, policy_loss: -197.48895, policy_entropy: -1.01265, alpha: 0.28651, time: 33.73546
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   315 ----
[CW] collect: return: 390.95547, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 3.89211, qf2_loss: 3.93234, policy_loss: -197.43310, policy_entropy: -1.00473, alpha: 0.28857, time: 33.97597
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   316 ----
[CW] collect: return: 327.65549, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 4.86766, qf2_loss: 4.88200, policy_loss: -197.94764, policy_entropy: -1.00392, alpha: 0.28916, time: 33.86250
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   317 ----
[CW] collect: return: 357.82047, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 4.51184, qf2_loss: 4.53911, policy_loss: -198.08197, policy_entropy: -1.00405, alpha: 0.28944, time: 33.62152
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   318 ----
[CW] collect: return: 309.15918, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 4.58329, qf2_loss: 4.61678, policy_loss: -198.87395, policy_entropy: -1.00264, alpha: 0.29004, time: 33.84839
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   319 ----
[CW] collect: return: 352.06074, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 3.85433, qf2_loss: 3.91810, policy_loss: -198.89215, policy_entropy: -1.00443, alpha: 0.29055, time: 33.86127
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   320 ----
[CW] collect: return: 305.97171, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 4.12350, qf2_loss: 4.13368, policy_loss: -199.02352, policy_entropy: -1.00006, alpha: 0.29099, time: 33.82615
[CW] eval: return: 343.61569, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   321 ----
[CW] collect: return: 225.15369, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 4.08827, qf2_loss: 4.06546, policy_loss: -199.59804, policy_entropy: -0.99836, alpha: 0.29091, time: 34.01427
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   322 ----
[CW] collect: return: 286.29095, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 5.56260, qf2_loss: 5.58534, policy_loss: -200.18177, policy_entropy: -0.98853, alpha: 0.28967, time: 33.76356
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   323 ----
[CW] collect: return: 319.84015, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 4.96276, qf2_loss: 4.98081, policy_loss: -200.38283, policy_entropy: -0.98627, alpha: 0.28811, time: 33.84881
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   324 ----
[CW] collect: return: 407.39618, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 5.90967, qf2_loss: 5.88467, policy_loss: -200.10453, policy_entropy: -0.99347, alpha: 0.28609, time: 33.72243
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   325 ----
[CW] collect: return: 358.25978, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 4.93488, qf2_loss: 4.93942, policy_loss: -201.07012, policy_entropy: -0.99561, alpha: 0.28562, time: 33.66657
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   326 ----
[CW] collect: return: 300.70595, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 5.20050, qf2_loss: 5.24587, policy_loss: -201.47432, policy_entropy: -0.99837, alpha: 0.28443, time: 33.68490
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   327 ----
[CW] collect: return: 341.09600, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 4.98649, qf2_loss: 5.03021, policy_loss: -202.02232, policy_entropy: -0.99676, alpha: 0.28440, time: 38.93800
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   328 ----
[CW] collect: return: 257.93063, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 4.15562, qf2_loss: 4.16782, policy_loss: -201.27852, policy_entropy: -1.00669, alpha: 0.28468, time: 33.55417
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   329 ----
[CW] collect: return: 292.69339, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 4.25270, qf2_loss: 4.26129, policy_loss: -201.91571, policy_entropy: -1.01645, alpha: 0.28653, time: 33.85897
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   330 ----
[CW] collect: return: 301.38368, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 5.27413, qf2_loss: 5.31444, policy_loss: -202.79034, policy_entropy: -1.01253, alpha: 0.28845, time: 33.77466
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   331 ----
[CW] collect: return: 283.96779, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 4.92808, qf2_loss: 4.91938, policy_loss: -202.31562, policy_entropy: -0.98401, alpha: 0.28891, time: 33.94423
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   332 ----
[CW] collect: return: 360.12228, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 3.96352, qf2_loss: 3.96151, policy_loss: -202.32078, policy_entropy: -1.00134, alpha: 0.28695, time: 33.75270
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   333 ----
[CW] collect: return: 320.69668, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 4.45545, qf2_loss: 4.50479, policy_loss: -203.02356, policy_entropy: -0.99270, alpha: 0.28740, time: 37.12095
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   334 ----
[CW] collect: return: 367.93956, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 5.03730, qf2_loss: 5.07993, policy_loss: -202.68000, policy_entropy: -0.99092, alpha: 0.28581, time: 33.82160
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   335 ----
[CW] collect: return: 378.62284, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 5.49161, qf2_loss: 5.54986, policy_loss: -203.46404, policy_entropy: -1.01263, alpha: 0.28562, time: 33.83135
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   336 ----
[CW] collect: return: 306.71190, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 4.57341, qf2_loss: 4.62530, policy_loss: -203.96875, policy_entropy: -1.00113, alpha: 0.28651, time: 33.90505
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   337 ----
[CW] collect: return: 271.77865, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 4.17294, qf2_loss: 4.17175, policy_loss: -204.64088, policy_entropy: -0.99055, alpha: 0.28633, time: 33.65915
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   338 ----
[CW] collect: return: 350.65824, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 3.91083, qf2_loss: 3.93486, policy_loss: -203.90483, policy_entropy: -0.99299, alpha: 0.28508, time: 33.66593
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   339 ----
[CW] collect: return: 294.47387, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 4.71046, qf2_loss: 4.73657, policy_loss: -204.40904, policy_entropy: -1.00082, alpha: 0.28411, time: 33.87889
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   340 ----
[CW] collect: return: 353.67366, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 4.68349, qf2_loss: 4.72099, policy_loss: -204.64237, policy_entropy: -0.99643, alpha: 0.28421, time: 33.79480
[CW] eval: return: 346.25770, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   341 ----
[CW] collect: return: 438.15013, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 4.24159, qf2_loss: 4.28713, policy_loss: -204.76503, policy_entropy: -1.01262, alpha: 0.28494, time: 33.40429
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   342 ----
[CW] collect: return: 281.12393, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 5.34053, qf2_loss: 5.30089, policy_loss: -205.12186, policy_entropy: -0.99574, alpha: 0.28569, time: 33.26232
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   343 ----
[CW] collect: return: 227.57619, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 5.39431, qf2_loss: 5.41179, policy_loss: -205.72307, policy_entropy: -0.99247, alpha: 0.28519, time: 33.57192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   344 ----
[CW] collect: return: 387.84294, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 4.75366, qf2_loss: 4.77507, policy_loss: -206.09141, policy_entropy: -1.00591, alpha: 0.28447, time: 33.74842
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   345 ----
[CW] collect: return: 338.92310, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 4.44930, qf2_loss: 4.45157, policy_loss: -206.34757, policy_entropy: -0.98729, alpha: 0.28367, time: 33.69557
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   346 ----
[CW] collect: return: 305.25785, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 4.61014, qf2_loss: 4.62812, policy_loss: -205.46330, policy_entropy: -0.99099, alpha: 0.28254, time: 33.87990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   347 ----
[CW] collect: return: 403.10483, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 4.55353, qf2_loss: 4.55842, policy_loss: -206.87825, policy_entropy: -0.98342, alpha: 0.28078, time: 33.70222
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   348 ----
[CW] collect: return: 265.94133, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 4.17877, qf2_loss: 4.22732, policy_loss: -206.72359, policy_entropy: -0.99807, alpha: 0.27891, time: 33.60737
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   349 ----
[CW] collect: return: 307.70969, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 5.26264, qf2_loss: 5.24864, policy_loss: -207.14060, policy_entropy: -0.98948, alpha: 0.27781, time: 33.65396
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   350 ----
[CW] collect: return: 433.21102, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 4.86124, qf2_loss: 4.86721, policy_loss: -207.36194, policy_entropy: -0.98339, alpha: 0.27626, time: 33.86262
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   351 ----
[CW] collect: return: 310.47364, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 5.47745, qf2_loss: 5.45658, policy_loss: -207.88164, policy_entropy: -0.99887, alpha: 0.27521, time: 33.55370
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   352 ----
[CW] collect: return: 324.43265, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 4.83134, qf2_loss: 4.79623, policy_loss: -208.28994, policy_entropy: -0.99096, alpha: 0.27430, time: 33.56754
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   353 ----
[CW] collect: return: 321.58159, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 4.92737, qf2_loss: 4.91903, policy_loss: -207.74448, policy_entropy: -1.00240, alpha: 0.27357, time: 33.39015
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   354 ----
[CW] collect: return: 356.76233, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 4.27220, qf2_loss: 4.30380, policy_loss: -208.15684, policy_entropy: -1.00147, alpha: 0.27430, time: 33.67672
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   355 ----
[CW] collect: return: 333.57962, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 5.25405, qf2_loss: 5.28857, policy_loss: -209.03389, policy_entropy: -1.00139, alpha: 0.27415, time: 33.65231
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   356 ----
[CW] collect: return: 370.03522, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 4.59239, qf2_loss: 4.61986, policy_loss: -209.38009, policy_entropy: -0.99055, alpha: 0.27353, time: 33.72164
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   357 ----
[CW] collect: return: 311.94183, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 4.78290, qf2_loss: 4.82333, policy_loss: -208.63538, policy_entropy: -0.99600, alpha: 0.27269, time: 33.95876
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   358 ----
[CW] collect: return: 322.43664, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 4.75060, qf2_loss: 4.77367, policy_loss: -209.30500, policy_entropy: -0.98743, alpha: 0.27119, time: 33.60245
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   359 ----
[CW] collect: return: 300.48594, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 4.67878, qf2_loss: 4.66266, policy_loss: -210.02288, policy_entropy: -1.01131, alpha: 0.27090, time: 33.77974
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   360 ----
[CW] collect: return: 314.32283, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 4.91239, qf2_loss: 4.91236, policy_loss: -209.95183, policy_entropy: -0.99278, alpha: 0.27116, time: 34.08458
[CW] eval: return: 342.39308, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   361 ----
[CW] collect: return: 285.90514, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 4.75396, qf2_loss: 4.70973, policy_loss: -210.73693, policy_entropy: -0.98446, alpha: 0.26973, time: 33.52465
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   362 ----
[CW] collect: return: 385.75422, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 5.24952, qf2_loss: 5.23202, policy_loss: -209.87120, policy_entropy: -0.98108, alpha: 0.26756, time: 33.55571
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   363 ----
[CW] collect: return: 278.97139, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 4.61035, qf2_loss: 4.61086, policy_loss: -210.57042, policy_entropy: -0.99086, alpha: 0.26576, time: 33.61752
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   364 ----
[CW] collect: return: 370.31105, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 5.00987, qf2_loss: 5.05239, policy_loss: -210.44499, policy_entropy: -1.00113, alpha: 0.26466, time: 33.43821
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   365 ----
[CW] collect: return: 287.04950, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 4.78107, qf2_loss: 4.79041, policy_loss: -211.46426, policy_entropy: -0.99579, alpha: 0.26379, time: 33.77477
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   366 ----
[CW] collect: return: 362.15494, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 4.75424, qf2_loss: 4.75439, policy_loss: -211.01120, policy_entropy: -0.99960, alpha: 0.26402, time: 33.45598
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   367 ----
[CW] collect: return: 315.43682, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 4.94622, qf2_loss: 4.95985, policy_loss: -211.63695, policy_entropy: -1.00680, alpha: 0.26466, time: 33.71741
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   368 ----
[CW] collect: return: 266.53486, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 6.39346, qf2_loss: 6.36263, policy_loss: -211.59861, policy_entropy: -0.99252, alpha: 0.26391, time: 33.59180
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   369 ----
[CW] collect: return: 356.31748, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 6.12291, qf2_loss: 6.14288, policy_loss: -212.42894, policy_entropy: -1.00112, alpha: 0.26390, time: 33.74160
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   370 ----
[CW] collect: return: 310.42170, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 4.45048, qf2_loss: 4.45599, policy_loss: -212.33735, policy_entropy: -1.00065, alpha: 0.26386, time: 33.55827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   371 ----
[CW] collect: return: 316.23727, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 5.11364, qf2_loss: 5.16428, policy_loss: -212.29640, policy_entropy: -0.99501, alpha: 0.26401, time: 33.66740
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   372 ----
[CW] collect: return: 327.92455, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 5.57733, qf2_loss: 5.61686, policy_loss: -211.79426, policy_entropy: -0.99344, alpha: 0.26322, time: 33.83571
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   373 ----
[CW] collect: return: 316.09328, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 5.67518, qf2_loss: 5.72945, policy_loss: -211.98885, policy_entropy: -1.00629, alpha: 0.26292, time: 33.52979
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   374 ----
[CW] collect: return: 336.85181, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 5.19155, qf2_loss: 5.19446, policy_loss: -213.20769, policy_entropy: -0.99443, alpha: 0.26317, time: 33.57703
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   375 ----
[CW] collect: return: 323.33251, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 4.58081, qf2_loss: 4.59685, policy_loss: -213.46455, policy_entropy: -1.00514, alpha: 0.26273, time: 33.59951
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   376 ----
[CW] collect: return: 320.13146, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 4.18849, qf2_loss: 4.17385, policy_loss: -213.06455, policy_entropy: -0.99342, alpha: 0.26319, time: 33.73125
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   377 ----
[CW] collect: return: 241.97672, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 4.68171, qf2_loss: 4.70008, policy_loss: -213.43078, policy_entropy: -1.00024, alpha: 0.26236, time: 33.21592
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   378 ----
[CW] collect: return: 442.62699, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 6.25143, qf2_loss: 6.28659, policy_loss: -213.73573, policy_entropy: -1.00242, alpha: 0.26280, time: 33.36256
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   379 ----
[CW] collect: return: 314.96923, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 5.50940, qf2_loss: 5.53306, policy_loss: -213.67553, policy_entropy: -0.98508, alpha: 0.26147, time: 33.55247
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   380 ----
[CW] collect: return: 325.76182, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 5.30533, qf2_loss: 5.30695, policy_loss: -213.65942, policy_entropy: -0.99628, alpha: 0.25978, time: 32.91290
[CW] eval: return: 344.68287, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   381 ----
[CW] collect: return: 242.68943, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 4.97723, qf2_loss: 5.04556, policy_loss: -214.33384, policy_entropy: -1.01053, alpha: 0.26039, time: 33.33345
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   382 ----
[CW] collect: return: 340.99178, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 5.28868, qf2_loss: 5.28121, policy_loss: -214.51696, policy_entropy: -1.00540, alpha: 0.26152, time: 33.46350
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   383 ----
[CW] collect: return: 305.56460, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 5.05672, qf2_loss: 5.02936, policy_loss: -214.69481, policy_entropy: -1.00348, alpha: 0.26265, time: 33.56014
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   384 ----
[CW] collect: return: 298.62997, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 6.74040, qf2_loss: 6.79952, policy_loss: -215.07042, policy_entropy: -0.99768, alpha: 0.26242, time: 33.48362
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   385 ----
[CW] collect: return: 445.08184, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 5.27939, qf2_loss: 5.25662, policy_loss: -215.01745, policy_entropy: -0.99828, alpha: 0.26217, time: 33.55412
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   386 ----
[CW] collect: return: 352.17606, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 5.03787, qf2_loss: 4.99775, policy_loss: -214.92420, policy_entropy: -1.00492, alpha: 0.26267, time: 35.67797
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   387 ----
[CW] collect: return: 281.27727, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 5.32978, qf2_loss: 5.31556, policy_loss: -216.01296, policy_entropy: -1.00104, alpha: 0.26251, time: 33.53643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   388 ----
[CW] collect: return: 334.66523, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 4.95753, qf2_loss: 4.95619, policy_loss: -215.80968, policy_entropy: -1.00937, alpha: 0.26375, time: 33.64635
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   389 ----
[CW] collect: return: 368.91968, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 5.52713, qf2_loss: 5.55396, policy_loss: -216.14943, policy_entropy: -1.00191, alpha: 0.26491, time: 33.47222
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   390 ----
[CW] collect: return: 448.98907, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 5.93027, qf2_loss: 5.88971, policy_loss: -215.75920, policy_entropy: -1.00331, alpha: 0.26503, time: 33.59827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   391 ----
[CW] collect: return: 384.52271, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 5.64651, qf2_loss: 5.74534, policy_loss: -216.65692, policy_entropy: -0.99812, alpha: 0.26528, time: 33.64618
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   392 ----
[CW] collect: return: 292.72495, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 6.04825, qf2_loss: 6.07533, policy_loss: -216.53837, policy_entropy: -0.98717, alpha: 0.26411, time: 33.47368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   393 ----
[CW] collect: return: 342.39731, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 4.97618, qf2_loss: 4.94850, policy_loss: -216.63508, policy_entropy: -0.99828, alpha: 0.26297, time: 33.71541
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   394 ----
[CW] collect: return: 336.41440, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 5.05741, qf2_loss: 5.04981, policy_loss: -217.73928, policy_entropy: -1.01565, alpha: 0.26392, time: 33.58856
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   395 ----
[CW] collect: return: 362.40115, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 5.43638, qf2_loss: 5.40636, policy_loss: -217.34855, policy_entropy: -0.99247, alpha: 0.26485, time: 33.71739
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   396 ----
[CW] collect: return: 295.36960, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 5.46138, qf2_loss: 5.52726, policy_loss: -217.70079, policy_entropy: -0.99666, alpha: 0.26400, time: 33.66379
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   397 ----
[CW] collect: return: 338.86032, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 5.29389, qf2_loss: 5.31877, policy_loss: -217.64928, policy_entropy: -1.00531, alpha: 0.26324, time: 33.40296
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   398 ----
[CW] collect: return: 370.63809, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 6.42333, qf2_loss: 6.45118, policy_loss: -217.17065, policy_entropy: -0.99180, alpha: 0.26333, time: 33.78507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   399 ----
[CW] collect: return: 432.26598, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 6.82721, qf2_loss: 6.86553, policy_loss: -218.64330, policy_entropy: -0.99490, alpha: 0.26213, time: 33.68097
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   400 ----
[CW] collect: return: 283.60681, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 6.01864, qf2_loss: 6.05364, policy_loss: -218.41046, policy_entropy: -1.00165, alpha: 0.26264, time: 33.22472
[CW] eval: return: 330.88981, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   401 ----
[CW] collect: return: 263.09921, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 5.51626, qf2_loss: 5.54426, policy_loss: -217.89451, policy_entropy: -0.99448, alpha: 0.26202, time: 33.62845
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   402 ----
[CW] collect: return: 378.25931, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 6.55742, qf2_loss: 6.57698, policy_loss: -217.69855, policy_entropy: -1.00400, alpha: 0.26152, time: 33.33054
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   403 ----
[CW] collect: return: 254.80759, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 6.17518, qf2_loss: 6.17033, policy_loss: -218.68806, policy_entropy: -1.00205, alpha: 0.26174, time: 33.58480
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   404 ----
[CW] collect: return: 346.65880, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 5.37485, qf2_loss: 5.33404, policy_loss: -219.72438, policy_entropy: -1.01476, alpha: 0.26365, time: 33.39140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   405 ----
[CW] collect: return: 316.05960, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 5.49062, qf2_loss: 5.45515, policy_loss: -219.25976, policy_entropy: -0.99985, alpha: 0.26451, time: 33.16413
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   406 ----
[CW] collect: return: 308.59263, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 5.11988, qf2_loss: 5.15484, policy_loss: -219.15696, policy_entropy: -1.00196, alpha: 0.26497, time: 33.39347
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   407 ----
[CW] collect: return: 341.20263, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 5.16843, qf2_loss: 5.21108, policy_loss: -219.04910, policy_entropy: -1.00002, alpha: 0.26509, time: 33.58166
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   408 ----
[CW] collect: return: 342.02621, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 7.39306, qf2_loss: 7.37046, policy_loss: -219.05631, policy_entropy: -0.99530, alpha: 0.26532, time: 33.64309
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   409 ----
[CW] collect: return: 398.03343, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 5.95871, qf2_loss: 5.94229, policy_loss: -220.55638, policy_entropy: -0.98133, alpha: 0.26281, time: 33.72245
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   410 ----
[CW] collect: return: 316.71303, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 5.56209, qf2_loss: 5.56483, policy_loss: -220.16449, policy_entropy: -1.00087, alpha: 0.26131, time: 33.23125
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   411 ----
[CW] collect: return: 370.47721, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 5.45102, qf2_loss: 5.47071, policy_loss: -220.28032, policy_entropy: -1.00983, alpha: 0.26213, time: 33.44086
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   412 ----
[CW] collect: return: 341.20919, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 5.14714, qf2_loss: 5.11502, policy_loss: -219.74531, policy_entropy: -1.00371, alpha: 0.26368, time: 33.65475
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   413 ----
[CW] collect: return: 329.22687, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 5.89457, qf2_loss: 5.92918, policy_loss: -221.01144, policy_entropy: -1.00813, alpha: 0.26422, time: 33.53150
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   414 ----
[CW] collect: return: 328.36113, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 5.75096, qf2_loss: 5.73243, policy_loss: -220.44820, policy_entropy: -0.99496, alpha: 0.26445, time: 33.61621
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   415 ----
[CW] collect: return: 310.78683, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 5.44959, qf2_loss: 5.40612, policy_loss: -220.53476, policy_entropy: -0.99720, alpha: 0.26379, time: 33.55034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   416 ----
[CW] collect: return: 423.23791, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 7.45846, qf2_loss: 7.49431, policy_loss: -220.21813, policy_entropy: -1.00389, alpha: 0.26406, time: 33.54897
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   417 ----
[CW] collect: return: 302.09017, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 5.46101, qf2_loss: 5.47118, policy_loss: -220.95764, policy_entropy: -0.99708, alpha: 0.26335, time: 33.85157
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   418 ----
[CW] collect: return: 293.00405, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 5.50799, qf2_loss: 5.50706, policy_loss: -221.55364, policy_entropy: -1.00391, alpha: 0.26403, time: 33.63118
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   419 ----
[CW] collect: return: 269.80903, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 6.73428, qf2_loss: 6.74377, policy_loss: -221.41897, policy_entropy: -0.99490, alpha: 0.26452, time: 33.58820
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   420 ----
[CW] collect: return: 296.73262, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 6.90859, qf2_loss: 6.91015, policy_loss: -220.31866, policy_entropy: -0.99627, alpha: 0.26371, time: 33.23824
[CW] eval: return: 329.79383, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   421 ----
[CW] collect: return: 283.61356, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 5.85394, qf2_loss: 5.87884, policy_loss: -221.46146, policy_entropy: -0.99480, alpha: 0.26257, time: 33.68922
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   422 ----
[CW] collect: return: 386.98908, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 6.20659, qf2_loss: 6.27520, policy_loss: -222.57144, policy_entropy: -1.00841, alpha: 0.26301, time: 33.55574
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   423 ----
[CW] collect: return: 331.33257, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 5.61426, qf2_loss: 5.62042, policy_loss: -222.10575, policy_entropy: -1.00981, alpha: 0.26428, time: 33.48523
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   424 ----
[CW] collect: return: 375.57768, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 5.25788, qf2_loss: 5.24153, policy_loss: -222.50332, policy_entropy: -1.00768, alpha: 0.26593, time: 33.38490
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   425 ----
[CW] collect: return: 387.21979, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 5.72414, qf2_loss: 5.66751, policy_loss: -222.18592, policy_entropy: -1.00728, alpha: 0.26685, time: 33.66142
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   426 ----
[CW] collect: return: 360.38503, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 6.09080, qf2_loss: 6.11681, policy_loss: -221.51454, policy_entropy: -0.99449, alpha: 0.26657, time: 33.39038
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   427 ----
[CW] collect: return: 290.09255, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 6.05359, qf2_loss: 6.03989, policy_loss: -222.69856, policy_entropy: -1.00576, alpha: 0.26715, time: 33.55633
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   428 ----
[CW] collect: return: 349.92615, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 5.69485, qf2_loss: 5.74359, policy_loss: -222.47655, policy_entropy: -1.00729, alpha: 0.26847, time: 33.51577
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   429 ----
[CW] collect: return: 366.98270, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 5.60533, qf2_loss: 5.60668, policy_loss: -223.42086, policy_entropy: -1.00599, alpha: 0.26907, time: 33.47679
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   430 ----
[CW] collect: return: 281.86043, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 5.18368, qf2_loss: 5.20107, policy_loss: -223.03459, policy_entropy: -1.00942, alpha: 0.27032, time: 33.42470
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   431 ----
[CW] collect: return: 338.22594, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 6.79223, qf2_loss: 6.90615, policy_loss: -223.50215, policy_entropy: -0.99091, alpha: 0.27062, time: 33.59522
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   432 ----
[CW] collect: return: 260.77811, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 6.11121, qf2_loss: 6.15898, policy_loss: -223.58729, policy_entropy: -0.98936, alpha: 0.26884, time: 33.63172
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   433 ----
[CW] collect: return: 328.82435, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 6.59921, qf2_loss: 6.55658, policy_loss: -223.98775, policy_entropy: -0.99369, alpha: 0.26744, time: 33.69845
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   434 ----
[CW] collect: return: 434.26982, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 5.96852, qf2_loss: 5.98826, policy_loss: -224.20824, policy_entropy: -0.99862, alpha: 0.26726, time: 33.59176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   435 ----
[CW] collect: return: 351.43169, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 5.93325, qf2_loss: 5.93126, policy_loss: -224.43139, policy_entropy: -1.00610, alpha: 0.26747, time: 33.51686
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   436 ----
[CW] collect: return: 355.74647, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 6.34109, qf2_loss: 6.36345, policy_loss: -224.46566, policy_entropy: -1.00984, alpha: 0.26858, time: 33.71689
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   437 ----
[CW] collect: return: 303.26277, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 8.20228, qf2_loss: 8.25723, policy_loss: -224.52318, policy_entropy: -0.99577, alpha: 0.26919, time: 33.53274
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   438 ----
[CW] collect: return: 353.79340, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 6.45227, qf2_loss: 6.49218, policy_loss: -224.66477, policy_entropy: -0.99860, alpha: 0.26859, time: 33.71424
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   439 ----
[CW] collect: return: 316.80158, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 6.18510, qf2_loss: 6.25827, policy_loss: -224.52412, policy_entropy: -0.99332, alpha: 0.26782, time: 40.85971
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   440 ----
[CW] collect: return: 267.68501, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 6.97166, qf2_loss: 6.98014, policy_loss: -225.11499, policy_entropy: -1.00282, alpha: 0.26802, time: 33.52689
[CW] eval: return: 335.22964, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   441 ----
[CW] collect: return: 348.40004, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 6.07504, qf2_loss: 6.10883, policy_loss: -225.99838, policy_entropy: -1.00413, alpha: 0.26837, time: 33.70348
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   442 ----
[CW] collect: return: 407.44098, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 5.40153, qf2_loss: 5.46084, policy_loss: -225.34273, policy_entropy: -1.00835, alpha: 0.26895, time: 33.46166
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   443 ----
[CW] collect: return: 425.29525, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 9.85551, qf2_loss: 9.72882, policy_loss: -224.96106, policy_entropy: -0.99306, alpha: 0.26919, time: 33.13181
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   444 ----
[CW] collect: return: 274.83168, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 10.56805, qf2_loss: 10.66437, policy_loss: -224.95953, policy_entropy: -0.99294, alpha: 0.26805, time: 33.45700
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   445 ----
[CW] collect: return: 353.77029, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 7.06133, qf2_loss: 7.14107, policy_loss: -226.29866, policy_entropy: -1.01243, alpha: 0.26825, time: 33.72753
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   446 ----
[CW] collect: return: 398.77357, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 6.27028, qf2_loss: 6.39454, policy_loss: -226.25196, policy_entropy: -1.00381, alpha: 0.27052, time: 33.81287
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   447 ----
[CW] collect: return: 325.94333, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 6.96669, qf2_loss: 7.02451, policy_loss: -226.00264, policy_entropy: -1.00059, alpha: 0.27022, time: 33.50839
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   448 ----
[CW] collect: return: 263.93836, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 6.32942, qf2_loss: 6.33209, policy_loss: -225.91072, policy_entropy: -1.00180, alpha: 0.27002, time: 33.68428
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   449 ----
[CW] collect: return: 347.74171, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 5.77248, qf2_loss: 5.80793, policy_loss: -226.47564, policy_entropy: -1.01355, alpha: 0.27169, time: 33.66503
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   450 ----
[CW] collect: return: 314.33837, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 5.89529, qf2_loss: 5.94638, policy_loss: -227.17072, policy_entropy: -1.00328, alpha: 0.27259, time: 33.34617
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   451 ----
[CW] collect: return: 308.62095, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 9.21342, qf2_loss: 9.22200, policy_loss: -225.59587, policy_entropy: -0.99214, alpha: 0.27270, time: 33.31690
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   452 ----
[CW] collect: return: 257.99951, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 8.73507, qf2_loss: 8.78415, policy_loss: -225.95332, policy_entropy: -0.98704, alpha: 0.27099, time: 33.67188
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   453 ----
[CW] collect: return: 387.45027, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 7.08292, qf2_loss: 7.06546, policy_loss: -227.24793, policy_entropy: -0.98491, alpha: 0.26898, time: 33.48402
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   454 ----
[CW] collect: return: 381.02486, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 6.27497, qf2_loss: 6.27755, policy_loss: -226.36614, policy_entropy: -0.98993, alpha: 0.26725, time: 33.21231
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   455 ----
[CW] collect: return: 284.35059, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 6.53340, qf2_loss: 6.47851, policy_loss: -226.98294, policy_entropy: -1.00968, alpha: 0.26680, time: 33.70025
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   456 ----
[CW] collect: return: 312.73002, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 6.56613, qf2_loss: 6.61303, policy_loss: -227.67290, policy_entropy: -0.99082, alpha: 0.26692, time: 33.30744
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   457 ----
[CW] collect: return: 376.39199, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 7.04769, qf2_loss: 7.07217, policy_loss: -227.08391, policy_entropy: -0.99854, alpha: 0.26626, time: 33.34792
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   458 ----
[CW] collect: return: 295.66946, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 7.51654, qf2_loss: 7.50575, policy_loss: -228.10988, policy_entropy: -1.00967, alpha: 0.26619, time: 33.25953
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   459 ----
[CW] collect: return: 341.71196, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 6.64255, qf2_loss: 6.69550, policy_loss: -228.06088, policy_entropy: -1.00418, alpha: 0.26745, time: 33.45643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   460 ----
[CW] collect: return: 433.16784, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 8.16583, qf2_loss: 8.17775, policy_loss: -228.26859, policy_entropy: -1.00688, alpha: 0.26812, time: 33.37054
[CW] eval: return: 342.46408, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   461 ----
[CW] collect: return: 307.94356, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 6.75070, qf2_loss: 6.83956, policy_loss: -227.55560, policy_entropy: -1.01305, alpha: 0.26955, time: 33.54585
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   462 ----
[CW] collect: return: 358.64590, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 6.57909, qf2_loss: 6.64597, policy_loss: -227.57681, policy_entropy: -0.99492, alpha: 0.27105, time: 33.65951
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   463 ----
[CW] collect: return: 384.19557, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 6.64744, qf2_loss: 6.68712, policy_loss: -227.86757, policy_entropy: -0.99899, alpha: 0.27073, time: 33.57338
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   464 ----
[CW] collect: return: 404.45619, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 6.89808, qf2_loss: 6.86713, policy_loss: -228.13112, policy_entropy: -0.99138, alpha: 0.26971, time: 33.11947
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   465 ----
[CW] collect: return: 356.01854, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 7.36984, qf2_loss: 7.38867, policy_loss: -227.84988, policy_entropy: -0.99626, alpha: 0.26832, time: 33.76170
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   466 ----
[CW] collect: return: 283.38432, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 7.12272, qf2_loss: 7.07467, policy_loss: -228.67974, policy_entropy: -1.00026, alpha: 0.26827, time: 33.27237
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   467 ----
[CW] collect: return: 261.03779, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 8.63733, qf2_loss: 8.67266, policy_loss: -229.10882, policy_entropy: -1.00481, alpha: 0.26840, time: 33.29231
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   468 ----
[CW] collect: return: 377.20803, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 7.88676, qf2_loss: 7.88462, policy_loss: -229.82188, policy_entropy: -1.00773, alpha: 0.26940, time: 33.48623
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   469 ----
[CW] collect: return: 307.59112, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 7.19800, qf2_loss: 7.24490, policy_loss: -229.54424, policy_entropy: -0.99605, alpha: 0.27049, time: 33.63911
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   470 ----
[CW] collect: return: 349.25126, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 8.16176, qf2_loss: 8.20556, policy_loss: -229.58863, policy_entropy: -0.99552, alpha: 0.26958, time: 33.45016
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   471 ----
[CW] collect: return: 283.34554, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 10.14603, qf2_loss: 10.10866, policy_loss: -229.44699, policy_entropy: -1.01349, alpha: 0.27037, time: 33.54481
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   472 ----
[CW] collect: return: 268.59586, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 7.99341, qf2_loss: 8.07946, policy_loss: -229.07553, policy_entropy: -0.99171, alpha: 0.27021, time: 33.45047
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   473 ----
[CW] collect: return: 341.84069, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 9.30155, qf2_loss: 9.38506, policy_loss: -229.50266, policy_entropy: -0.99865, alpha: 0.26926, time: 33.66049
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   474 ----
[CW] collect: return: 368.10942, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 11.17153, qf2_loss: 11.11649, policy_loss: -229.50573, policy_entropy: -1.00330, alpha: 0.27042, time: 33.87330
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   475 ----
[CW] collect: return: 297.08573, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 10.13969, qf2_loss: 10.21355, policy_loss: -229.56939, policy_entropy: -0.99351, alpha: 0.26971, time: 33.54964
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   476 ----
[CW] collect: return: 361.56467, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 7.48230, qf2_loss: 7.45837, policy_loss: -228.90745, policy_entropy: -0.99944, alpha: 0.26944, time: 33.44356
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   477 ----
[CW] collect: return: 327.79175, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 6.41373, qf2_loss: 6.43684, policy_loss: -229.72666, policy_entropy: -0.99933, alpha: 0.26920, time: 33.60025
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   478 ----
[CW] collect: return: 431.17816, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 6.44820, qf2_loss: 6.57277, policy_loss: -231.15355, policy_entropy: -1.02028, alpha: 0.27018, time: 33.38140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   479 ----
[CW] collect: return: 375.81388, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 6.29344, qf2_loss: 6.30336, policy_loss: -229.86315, policy_entropy: -0.99722, alpha: 0.27250, time: 33.45774
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   480 ----
[CW] collect: return: 366.91863, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 5.95673, qf2_loss: 5.95612, policy_loss: -231.12651, policy_entropy: -1.00869, alpha: 0.27271, time: 33.61184
[CW] eval: return: 350.57226, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   481 ----
[CW] collect: return: 333.71881, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 6.11586, qf2_loss: 6.10845, policy_loss: -230.58903, policy_entropy: -1.00868, alpha: 0.27345, time: 33.52210
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   482 ----
[CW] collect: return: 279.48089, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 5.75045, qf2_loss: 5.65706, policy_loss: -230.09916, policy_entropy: -1.00402, alpha: 0.27517, time: 33.57578
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   483 ----
[CW] collect: return: 280.94097, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 5.73613, qf2_loss: 5.72414, policy_loss: -230.45854, policy_entropy: -1.00315, alpha: 0.27570, time: 33.48246
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   484 ----
[CW] collect: return: 315.63659, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 5.80905, qf2_loss: 5.77665, policy_loss: -230.99515, policy_entropy: -1.00015, alpha: 0.27576, time: 33.79055
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   485 ----
[CW] collect: return: 288.58471, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 5.99019, qf2_loss: 5.97574, policy_loss: -230.78750, policy_entropy: -1.00903, alpha: 0.27662, time: 33.27476
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   486 ----
[CW] collect: return: 359.51211, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 6.30687, qf2_loss: 6.32216, policy_loss: -230.76214, policy_entropy: -1.00044, alpha: 0.27667, time: 33.22154
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   487 ----
[CW] collect: return: 382.79927, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 6.96108, qf2_loss: 6.96266, policy_loss: -231.34184, policy_entropy: -1.00765, alpha: 0.27804, time: 33.47699
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   488 ----
[CW] collect: return: 373.39980, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 6.62905, qf2_loss: 6.72353, policy_loss: -231.18455, policy_entropy: -0.99014, alpha: 0.27831, time: 33.54018
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   489 ----
[CW] collect: return: 341.39473, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 6.27110, qf2_loss: 6.35565, policy_loss: -231.08115, policy_entropy: -0.99035, alpha: 0.27654, time: 33.42870
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   490 ----
[CW] collect: return: 389.31436, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 5.90280, qf2_loss: 5.85384, policy_loss: -231.16287, policy_entropy: -0.99530, alpha: 0.27640, time: 32.97068
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   491 ----
[CW] collect: return: 503.18833, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 6.61441, qf2_loss: 6.62082, policy_loss: -231.84134, policy_entropy: -1.00466, alpha: 0.27507, time: 33.32536
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   492 ----
[CW] collect: return: 293.65310, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 7.97065, qf2_loss: 8.01461, policy_loss: -232.07224, policy_entropy: -1.00634, alpha: 0.27623, time: 33.40045
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   493 ----
[CW] collect: return: 308.93914, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 5.98560, qf2_loss: 5.99072, policy_loss: -231.70136, policy_entropy: -1.00215, alpha: 0.27691, time: 33.36524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   494 ----
[CW] collect: return: 336.37347, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 5.74917, qf2_loss: 5.74615, policy_loss: -231.95868, policy_entropy: -0.99079, alpha: 0.27610, time: 33.43383
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   495 ----
[CW] collect: return: 317.07220, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 5.70898, qf2_loss: 5.66947, policy_loss: -231.36946, policy_entropy: -1.00808, alpha: 0.27619, time: 33.59029
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   496 ----
[CW] collect: return: 433.88479, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 5.87247, qf2_loss: 5.89185, policy_loss: -232.88858, policy_entropy: -0.99489, alpha: 0.27577, time: 33.39413
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   497 ----
[CW] collect: return: 355.34452, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 6.31345, qf2_loss: 6.31807, policy_loss: -232.35306, policy_entropy: -0.99722, alpha: 0.27556, time: 33.48570
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   498 ----
[CW] collect: return: 393.55939, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 7.80114, qf2_loss: 7.82310, policy_loss: -232.06520, policy_entropy: -0.98918, alpha: 0.27449, time: 33.51030
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   499 ----
[CW] collect: return: 290.44845, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 8.02405, qf2_loss: 7.97983, policy_loss: -232.86204, policy_entropy: -0.99440, alpha: 0.27301, time: 38.06028
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   500 ----
[CW] collect: return: 348.07370, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 6.57504, qf2_loss: 6.51298, policy_loss: -233.34971, policy_entropy: -0.99949, alpha: 0.27219, time: 33.28591
[CW] eval: return: 324.99462, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   501 ----
[CW] collect: return: 389.68647, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 6.81308, qf2_loss: 6.77341, policy_loss: -231.81995, policy_entropy: -1.00133, alpha: 0.27257, time: 33.52630
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   502 ----
[CW] collect: return: 359.47938, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 6.55799, qf2_loss: 6.56601, policy_loss: -232.09094, policy_entropy: -0.99112, alpha: 0.27210, time: 33.44235
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   503 ----
[CW] collect: return: 403.44961, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 6.66643, qf2_loss: 6.66158, policy_loss: -233.22365, policy_entropy: -0.99830, alpha: 0.27099, time: 33.61387
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   504 ----
[CW] collect: return: 385.86643, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 6.37930, qf2_loss: 6.36255, policy_loss: -233.78316, policy_entropy: -0.99847, alpha: 0.27030, time: 33.24803
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   505 ----
[CW] collect: return: 324.53981, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 7.45414, qf2_loss: 7.38145, policy_loss: -233.30148, policy_entropy: -1.00522, alpha: 0.27117, time: 33.67001
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   506 ----
[CW] collect: return: 228.96615, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 7.48680, qf2_loss: 7.50686, policy_loss: -233.50410, policy_entropy: -0.99865, alpha: 0.27193, time: 33.79572
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   507 ----
[CW] collect: return: 248.05848, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 7.13960, qf2_loss: 7.15430, policy_loss: -233.78034, policy_entropy: -0.99702, alpha: 0.27163, time: 33.75625
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   508 ----
[CW] collect: return: 244.66394, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 6.54505, qf2_loss: 6.58033, policy_loss: -233.66885, policy_entropy: -0.99681, alpha: 0.27077, time: 33.64653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   509 ----
[CW] collect: return: 263.31477, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 7.14893, qf2_loss: 7.21937, policy_loss: -233.42467, policy_entropy: -1.01605, alpha: 0.27103, time: 33.29633
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   510 ----
[CW] collect: return: 349.30336, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 6.22897, qf2_loss: 6.20386, policy_loss: -233.87268, policy_entropy: -0.99378, alpha: 0.27250, time: 33.63963
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   511 ----
[CW] collect: return: 249.24956, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 6.02507, qf2_loss: 6.04325, policy_loss: -234.15149, policy_entropy: -1.00301, alpha: 0.27205, time: 33.21536
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   512 ----
[CW] collect: return: 403.23580, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 6.09224, qf2_loss: 6.16402, policy_loss: -234.56384, policy_entropy: -1.00221, alpha: 0.27247, time: 33.62312
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   513 ----
[CW] collect: return: 298.16985, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 6.76355, qf2_loss: 6.72884, policy_loss: -234.96039, policy_entropy: -0.99670, alpha: 0.27228, time: 33.17203
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   514 ----
[CW] collect: return: 309.95781, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 6.32876, qf2_loss: 6.33963, policy_loss: -233.95188, policy_entropy: -0.99962, alpha: 0.27251, time: 33.60254
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   515 ----
[CW] collect: return: 316.09167, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 5.97117, qf2_loss: 5.99251, policy_loss: -234.05418, policy_entropy: -1.00644, alpha: 0.27273, time: 33.58282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   516 ----
[CW] collect: return: 310.25988, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 6.88761, qf2_loss: 6.90878, policy_loss: -235.18772, policy_entropy: -1.00528, alpha: 0.27383, time: 33.53466
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   517 ----
[CW] collect: return: 361.62305, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 6.09421, qf2_loss: 6.11692, policy_loss: -234.35452, policy_entropy: -1.00399, alpha: 0.27488, time: 33.34634
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   518 ----
[CW] collect: return: 375.90780, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 6.15782, qf2_loss: 6.14751, policy_loss: -235.56186, policy_entropy: -0.99894, alpha: 0.27506, time: 33.56709
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   519 ----
[CW] collect: return: 356.98159, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 7.60276, qf2_loss: 7.58959, policy_loss: -235.20751, policy_entropy: -0.99512, alpha: 0.27447, time: 33.34822
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   520 ----
[CW] collect: return: 296.57863, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 7.15182, qf2_loss: 7.10646, policy_loss: -234.44166, policy_entropy: -0.98637, alpha: 0.27286, time: 33.52296
[CW] eval: return: 345.44607, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   521 ----
[CW] collect: return: 370.14278, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 6.01494, qf2_loss: 6.05721, policy_loss: -234.90218, policy_entropy: -0.99189, alpha: 0.27140, time: 33.53498
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   522 ----
[CW] collect: return: 284.84254, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 5.67846, qf2_loss: 5.68488, policy_loss: -235.21763, policy_entropy: -0.99409, alpha: 0.26986, time: 33.51781
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   523 ----
[CW] collect: return: 477.40713, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 7.12436, qf2_loss: 7.02728, policy_loss: -235.90834, policy_entropy: -1.00063, alpha: 0.26967, time: 33.58008
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   524 ----
[CW] collect: return: 328.41726, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 6.73336, qf2_loss: 6.71514, policy_loss: -235.09662, policy_entropy: -0.98825, alpha: 0.26825, time: 33.46850
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   525 ----
[CW] collect: return: 460.48562, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 7.25403, qf2_loss: 7.20919, policy_loss: -235.60748, policy_entropy: -1.00284, alpha: 0.26766, time: 33.42607
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   526 ----
[CW] collect: return: 276.27928, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 8.54630, qf2_loss: 8.60309, policy_loss: -235.66951, policy_entropy: -1.00300, alpha: 0.26769, time: 33.15158
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   527 ----
[CW] collect: return: 271.45515, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 6.56228, qf2_loss: 6.55770, policy_loss: -236.23514, policy_entropy: -0.99408, alpha: 0.26760, time: 33.59243
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   528 ----
[CW] collect: return: 415.68697, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 6.89704, qf2_loss: 6.81127, policy_loss: -236.12013, policy_entropy: -0.99277, alpha: 0.26720, time: 33.54789
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   529 ----
[CW] collect: return: 366.11669, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 7.29422, qf2_loss: 7.27994, policy_loss: -234.93045, policy_entropy: -1.00232, alpha: 0.26651, time: 33.48202
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   530 ----
[CW] collect: return: 374.02877, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 6.35015, qf2_loss: 6.48116, policy_loss: -236.70329, policy_entropy: -1.00909, alpha: 0.26724, time: 33.28266
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   531 ----
[CW] collect: return: 367.24318, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 5.85054, qf2_loss: 5.76500, policy_loss: -236.44567, policy_entropy: -1.00779, alpha: 0.26940, time: 33.27927
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   532 ----
[CW] collect: return: 374.59455, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 5.54948, qf2_loss: 5.57628, policy_loss: -235.91537, policy_entropy: -1.00178, alpha: 0.26974, time: 33.51509
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   533 ----
[CW] collect: return: 341.44144, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 6.29040, qf2_loss: 6.34177, policy_loss: -236.22960, policy_entropy: -0.99760, alpha: 0.26952, time: 33.51146
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   534 ----
[CW] collect: return: 379.09338, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 6.21049, qf2_loss: 6.27177, policy_loss: -235.83858, policy_entropy: -1.00246, alpha: 0.26940, time: 33.56142
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   535 ----
[CW] collect: return: 301.08195, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 5.73199, qf2_loss: 5.69161, policy_loss: -236.09819, policy_entropy: -1.00067, alpha: 0.26948, time: 32.99075
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   536 ----
[CW] collect: return: 377.26620, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 5.91706, qf2_loss: 5.88781, policy_loss: -236.72278, policy_entropy: -1.00378, alpha: 0.26989, time: 33.57907
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   537 ----
[CW] collect: return: 258.69934, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 6.29009, qf2_loss: 6.23951, policy_loss: -236.20093, policy_entropy: -1.00702, alpha: 0.27098, time: 33.64948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   538 ----
[CW] collect: return: 312.14655, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 6.02866, qf2_loss: 6.06984, policy_loss: -236.17165, policy_entropy: -0.99784, alpha: 0.27143, time: 33.16844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   539 ----
[CW] collect: return: 365.56307, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 5.92846, qf2_loss: 5.96093, policy_loss: -237.27863, policy_entropy: -0.99812, alpha: 0.27121, time: 33.19954
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   540 ----
[CW] collect: return: 367.65086, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 6.22798, qf2_loss: 6.18155, policy_loss: -237.11677, policy_entropy: -1.00091, alpha: 0.27163, time: 33.21001
[CW] eval: return: 327.33240, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   541 ----
[CW] collect: return: 369.23811, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 8.11271, qf2_loss: 8.15554, policy_loss: -237.12689, policy_entropy: -0.98818, alpha: 0.26996, time: 33.62926
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   542 ----
[CW] collect: return: 247.17866, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 6.71172, qf2_loss: 6.71885, policy_loss: -237.94706, policy_entropy: -1.00182, alpha: 0.26948, time: 32.98007
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   543 ----
[CW] collect: return: 289.44627, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 6.25585, qf2_loss: 6.26491, policy_loss: -237.23232, policy_entropy: -0.99127, alpha: 0.26895, time: 33.66401
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   544 ----
[CW] collect: return: 348.58138, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 6.03217, qf2_loss: 5.99382, policy_loss: -237.32247, policy_entropy: -0.99623, alpha: 0.26779, time: 33.54596
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   545 ----
[CW] collect: return: 358.27983, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 5.67031, qf2_loss: 5.59828, policy_loss: -236.88987, policy_entropy: -1.00593, alpha: 0.26845, time: 33.49957
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   546 ----
[CW] collect: return: 326.63828, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 6.00163, qf2_loss: 6.04524, policy_loss: -237.32515, policy_entropy: -1.00212, alpha: 0.26830, time: 33.74850
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   547 ----
[CW] collect: return: 280.40501, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 7.24866, qf2_loss: 7.27287, policy_loss: -237.90523, policy_entropy: -0.98572, alpha: 0.26741, time: 33.50786
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   548 ----
[CW] collect: return: 266.50738, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 7.60324, qf2_loss: 7.67150, policy_loss: -237.68344, policy_entropy: -0.99736, alpha: 0.26633, time: 33.67355
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   549 ----
[CW] collect: return: 347.77755, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 7.17279, qf2_loss: 7.11811, policy_loss: -238.16833, policy_entropy: -1.01116, alpha: 0.26685, time: 33.56473
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   550 ----
[CW] collect: return: 346.62518, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 5.78066, qf2_loss: 5.70897, policy_loss: -238.60680, policy_entropy: -1.01662, alpha: 0.26846, time: 33.62679
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   551 ----
[CW] collect: return: 430.42849, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 6.55861, qf2_loss: 6.52783, policy_loss: -237.69047, policy_entropy: -0.99447, alpha: 0.27046, time: 36.40204
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   552 ----
[CW] collect: return: 293.35379, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 6.24730, qf2_loss: 6.25016, policy_loss: -237.90658, policy_entropy: -1.00214, alpha: 0.26978, time: 33.21411
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   553 ----
[CW] collect: return: 357.73038, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 6.57678, qf2_loss: 6.56722, policy_loss: -237.33579, policy_entropy: -0.99483, alpha: 0.27022, time: 33.75982
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   554 ----
[CW] collect: return: 299.20775, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 6.76956, qf2_loss: 6.78707, policy_loss: -237.69733, policy_entropy: -0.99564, alpha: 0.26867, time: 33.60521
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   555 ----
[CW] collect: return: 356.28024, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 6.83267, qf2_loss: 6.77927, policy_loss: -237.87594, policy_entropy: -0.98786, alpha: 0.26760, time: 33.66181
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   556 ----
[CW] collect: return: 355.62271, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 6.64825, qf2_loss: 6.66645, policy_loss: -237.99228, policy_entropy: -1.00115, alpha: 0.26629, time: 33.05911
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   557 ----
[CW] collect: return: 345.28041, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 6.29783, qf2_loss: 6.34628, policy_loss: -238.67899, policy_entropy: -1.01588, alpha: 0.26771, time: 33.26175
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   558 ----
[CW] collect: return: 262.27580, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 5.73891, qf2_loss: 5.73125, policy_loss: -237.06010, policy_entropy: -0.99294, alpha: 0.26846, time: 33.55054
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   559 ----
[CW] collect: return: 421.00739, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 5.83987, qf2_loss: 5.84041, policy_loss: -238.50657, policy_entropy: -1.00333, alpha: 0.26850, time: 33.36977
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   560 ----
[CW] collect: return: 330.20233, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 5.94048, qf2_loss: 5.96630, policy_loss: -239.02078, policy_entropy: -1.01254, alpha: 0.26956, time: 33.61106
[CW] eval: return: 366.14812, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   561 ----
[CW] collect: return: 296.32977, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 6.62625, qf2_loss: 6.66124, policy_loss: -238.57194, policy_entropy: -0.99730, alpha: 0.27028, time: 33.28844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   562 ----
[CW] collect: return: 335.85210, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 6.68839, qf2_loss: 6.71911, policy_loss: -238.86294, policy_entropy: -1.00421, alpha: 0.27042, time: 33.54477
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   563 ----
[CW] collect: return: 364.49929, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 6.95763, qf2_loss: 6.95952, policy_loss: -237.83526, policy_entropy: -0.99394, alpha: 0.26979, time: 33.43436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   564 ----
[CW] collect: return: 279.62397, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 5.45637, qf2_loss: 5.48353, policy_loss: -239.24143, policy_entropy: -1.00888, alpha: 0.27044, time: 33.54605
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   565 ----
[CW] collect: return: 384.90058, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 5.33693, qf2_loss: 5.32067, policy_loss: -237.46176, policy_entropy: -1.00105, alpha: 0.27189, time: 33.61647
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   566 ----
[CW] collect: return: 376.08770, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 6.71877, qf2_loss: 6.77819, policy_loss: -238.35820, policy_entropy: -0.99722, alpha: 0.27114, time: 36.59022
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   567 ----
[CW] collect: return: 324.87836, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 6.21134, qf2_loss: 6.16625, policy_loss: -239.02464, policy_entropy: -0.99016, alpha: 0.27034, time: 33.42139
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   568 ----
[CW] collect: return: 353.92992, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 6.31744, qf2_loss: 6.29976, policy_loss: -239.33679, policy_entropy: -1.00669, alpha: 0.26985, time: 33.63887
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   569 ----
[CW] collect: return: 320.56463, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 5.95196, qf2_loss: 5.99568, policy_loss: -239.58827, policy_entropy: -0.99782, alpha: 0.27026, time: 33.56901
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   570 ----
[CW] collect: return: 374.80369, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 7.15998, qf2_loss: 7.20303, policy_loss: -239.36465, policy_entropy: -0.99334, alpha: 0.26948, time: 33.40639
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   571 ----
[CW] collect: return: 305.47878, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 6.27205, qf2_loss: 6.31690, policy_loss: -239.48914, policy_entropy: -1.00355, alpha: 0.26943, time: 33.27544
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   572 ----
[CW] collect: return: 352.72462, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 7.26166, qf2_loss: 7.21768, policy_loss: -238.77553, policy_entropy: -0.99714, alpha: 0.26944, time: 33.69221
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   573 ----
[CW] collect: return: 222.14161, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 6.23298, qf2_loss: 6.19267, policy_loss: -239.47312, policy_entropy: -1.00097, alpha: 0.26945, time: 33.37061
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   574 ----
[CW] collect: return: 336.44877, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 6.36036, qf2_loss: 6.38604, policy_loss: -239.44951, policy_entropy: -0.99712, alpha: 0.26906, time: 33.77077
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   575 ----
[CW] collect: return: 370.01878, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 6.27720, qf2_loss: 6.31867, policy_loss: -240.08954, policy_entropy: -0.98889, alpha: 0.26789, time: 33.19540
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   576 ----
[CW] collect: return: 307.97083, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 5.87067, qf2_loss: 5.87095, policy_loss: -239.40508, policy_entropy: -0.99987, alpha: 0.26756, time: 33.70927
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   577 ----
[CW] collect: return: 375.26376, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 7.20028, qf2_loss: 7.18041, policy_loss: -240.31094, policy_entropy: -0.99810, alpha: 0.26710, time: 33.45362
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   578 ----
[CW] collect: return: 336.44638, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 6.19638, qf2_loss: 6.24261, policy_loss: -239.74281, policy_entropy: -0.99547, alpha: 0.26682, time: 33.26997
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   579 ----
[CW] collect: return: 317.71616, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 5.77329, qf2_loss: 5.80936, policy_loss: -239.85009, policy_entropy: -0.99822, alpha: 0.26606, time: 33.53791
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   580 ----
[CW] collect: return: 330.22272, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 5.93392, qf2_loss: 5.89433, policy_loss: -239.76390, policy_entropy: -1.00302, alpha: 0.26602, time: 33.46452
[CW] eval: return: 326.87684, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   581 ----
[CW] collect: return: 279.44404, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 6.00172, qf2_loss: 6.03886, policy_loss: -239.55191, policy_entropy: -1.00323, alpha: 0.26580, time: 33.58820
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   582 ----
[CW] collect: return: 233.43117, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 6.01988, qf2_loss: 6.03277, policy_loss: -240.00691, policy_entropy: -0.99833, alpha: 0.26674, time: 33.50844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   583 ----
[CW] collect: return: 340.16923, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 7.27968, qf2_loss: 7.29232, policy_loss: -239.85119, policy_entropy: -1.00213, alpha: 0.26621, time: 33.78209
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   584 ----
[CW] collect: return: 372.05710, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 6.84111, qf2_loss: 6.85509, policy_loss: -240.11609, policy_entropy: -1.00411, alpha: 0.26714, time: 33.50740
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   585 ----
[CW] collect: return: 346.19443, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 8.37499, qf2_loss: 8.29961, policy_loss: -240.95802, policy_entropy: -0.99151, alpha: 0.26712, time: 33.74437
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   586 ----
[CW] collect: return: 380.44449, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 6.73775, qf2_loss: 6.72486, policy_loss: -240.63449, policy_entropy: -0.99065, alpha: 0.26553, time: 33.59288
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   587 ----
[CW] collect: return: 361.18805, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 5.96241, qf2_loss: 5.90479, policy_loss: -241.18386, policy_entropy: -1.00551, alpha: 0.26540, time: 33.74705
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   588 ----
[CW] collect: return: 336.82615, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 6.83185, qf2_loss: 6.87285, policy_loss: -240.63955, policy_entropy: -0.99753, alpha: 0.26509, time: 33.44384
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   589 ----
[CW] collect: return: 358.40531, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 7.06214, qf2_loss: 7.05640, policy_loss: -241.07863, policy_entropy: -1.00069, alpha: 0.26511, time: 33.69713
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   590 ----
[CW] collect: return: 403.34870, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 6.43188, qf2_loss: 6.45618, policy_loss: -240.35035, policy_entropy: -0.99092, alpha: 0.26465, time: 33.25423
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   591 ----
[CW] collect: return: 334.01933, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 6.27730, qf2_loss: 6.29247, policy_loss: -239.98918, policy_entropy: -1.00275, alpha: 0.26439, time: 33.55670
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   592 ----
[CW] collect: return: 308.27456, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 6.74810, qf2_loss: 6.78525, policy_loss: -240.75657, policy_entropy: -0.99781, alpha: 0.26386, time: 33.55543
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   593 ----
[CW] collect: return: 402.07454, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 6.29298, qf2_loss: 6.29699, policy_loss: -241.24042, policy_entropy: -0.99260, alpha: 0.26349, time: 33.48255
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   594 ----
[CW] collect: return: 412.87232, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 6.13478, qf2_loss: 6.13687, policy_loss: -241.57698, policy_entropy: -0.99951, alpha: 0.26240, time: 33.88115
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   595 ----
[CW] collect: return: 376.25234, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 6.59002, qf2_loss: 6.64512, policy_loss: -241.47318, policy_entropy: -1.00483, alpha: 0.26354, time: 33.63436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   596 ----
[CW] collect: return: 282.43185, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 6.57231, qf2_loss: 6.56661, policy_loss: -240.51958, policy_entropy: -0.99077, alpha: 0.26222, time: 33.49960
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   597 ----
[CW] collect: return: 255.09955, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 6.36209, qf2_loss: 6.36720, policy_loss: -240.65806, policy_entropy: -0.99005, alpha: 0.26113, time: 33.57853
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   598 ----
[CW] collect: return: 384.14227, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 5.97802, qf2_loss: 5.96029, policy_loss: -241.81764, policy_entropy: -1.00347, alpha: 0.26010, time: 33.41205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   599 ----
[CW] collect: return: 281.53200, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 6.01403, qf2_loss: 6.01964, policy_loss: -241.10610, policy_entropy: -0.99231, alpha: 0.26046, time: 33.79743
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   600 ----
[CW] collect: return: 386.47109, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 6.70837, qf2_loss: 6.74339, policy_loss: -240.92179, policy_entropy: -0.98773, alpha: 0.25889, time: 33.36130
[CW] eval: return: 357.39585, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   601 ----
[CW] collect: return: 271.73000, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 6.00869, qf2_loss: 5.98600, policy_loss: -241.51205, policy_entropy: -1.00393, alpha: 0.25823, time: 33.72705
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   602 ----
[CW] collect: return: 292.59999, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 6.33125, qf2_loss: 6.29590, policy_loss: -241.38306, policy_entropy: -0.98608, alpha: 0.25745, time: 33.49555
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   603 ----
[CW] collect: return: 311.44828, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 6.65740, qf2_loss: 6.61315, policy_loss: -242.14983, policy_entropy: -1.01716, alpha: 0.25739, time: 33.63061
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   604 ----
[CW] collect: return: 319.65399, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 6.65057, qf2_loss: 6.71305, policy_loss: -240.96492, policy_entropy: -0.99520, alpha: 0.25866, time: 33.40331
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   605 ----
[CW] collect: return: 344.38825, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 7.25848, qf2_loss: 7.22559, policy_loss: -241.11494, policy_entropy: -1.00358, alpha: 0.25829, time: 33.69238
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   606 ----
[CW] collect: return: 292.63685, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 6.69621, qf2_loss: 6.71689, policy_loss: -241.32151, policy_entropy: -0.99841, alpha: 0.25845, time: 33.47865
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   607 ----
[CW] collect: return: 279.40594, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 6.55905, qf2_loss: 6.55380, policy_loss: -241.59228, policy_entropy: -0.98821, alpha: 0.25739, time: 33.51426
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   608 ----
[CW] collect: return: 264.64400, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 6.39136, qf2_loss: 6.33212, policy_loss: -241.18175, policy_entropy: -0.99393, alpha: 0.25631, time: 33.59922
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   609 ----
[CW] collect: return: 373.00353, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 7.07647, qf2_loss: 7.10219, policy_loss: -241.95151, policy_entropy: -0.99241, alpha: 0.25455, time: 33.68881
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   610 ----
[CW] collect: return: 286.18878, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 6.74156, qf2_loss: 6.79267, policy_loss: -241.87743, policy_entropy: -0.99843, alpha: 0.25460, time: 33.41026
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   611 ----
[CW] collect: return: 352.67025, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 5.98109, qf2_loss: 5.99943, policy_loss: -242.31149, policy_entropy: -0.99647, alpha: 0.25325, time: 33.78145
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   612 ----
[CW] collect: return: 372.47333, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 6.39587, qf2_loss: 6.37215, policy_loss: -242.38107, policy_entropy: -1.00255, alpha: 0.25394, time: 32.92613
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   613 ----
[CW] collect: return: 319.49146, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 7.83000, qf2_loss: 7.82257, policy_loss: -242.04396, policy_entropy: -0.99414, alpha: 0.25330, time: 33.58246
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   614 ----
[CW] collect: return: 299.41121, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 6.55510, qf2_loss: 6.56466, policy_loss: -242.54886, policy_entropy: -1.00611, alpha: 0.25391, time: 33.49958
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   615 ----
[CW] collect: return: 387.09290, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 6.15369, qf2_loss: 6.14009, policy_loss: -242.31791, policy_entropy: -1.00227, alpha: 0.25422, time: 33.63929
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   616 ----
[CW] collect: return: 332.55061, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 6.20364, qf2_loss: 6.16948, policy_loss: -242.17683, policy_entropy: -0.98938, alpha: 0.25415, time: 33.47237
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   617 ----
[CW] collect: return: 299.53163, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 6.18879, qf2_loss: 6.22624, policy_loss: -242.39112, policy_entropy: -0.99127, alpha: 0.25171, time: 33.41886
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   618 ----
[CW] collect: return: 387.16322, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 6.22248, qf2_loss: 6.18213, policy_loss: -243.06896, policy_entropy: -1.00920, alpha: 0.25236, time: 33.54752
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   619 ----
[CW] collect: return: 396.56638, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 8.04930, qf2_loss: 8.10769, policy_loss: -242.82634, policy_entropy: -0.99843, alpha: 0.25277, time: 33.14534
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   620 ----
[CW] collect: return: 258.40010, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 8.68253, qf2_loss: 8.73013, policy_loss: -243.09822, policy_entropy: -0.99531, alpha: 0.25208, time: 33.35706
[CW] eval: return: 328.46663, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   621 ----
[CW] collect: return: 268.79649, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 7.21293, qf2_loss: 7.25954, policy_loss: -243.16223, policy_entropy: -0.99398, alpha: 0.25138, time: 33.59967
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   622 ----
[CW] collect: return: 333.73084, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 6.33966, qf2_loss: 6.32519, policy_loss: -242.70872, policy_entropy: -1.00248, alpha: 0.25110, time: 33.36186
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   623 ----
[CW] collect: return: 392.15969, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 5.94945, qf2_loss: 5.92291, policy_loss: -243.08725, policy_entropy: -0.99728, alpha: 0.25160, time: 33.55970
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   624 ----
[CW] collect: return: 352.63060, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 6.25762, qf2_loss: 6.24937, policy_loss: -242.85679, policy_entropy: -0.99395, alpha: 0.25067, time: 33.27690
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   625 ----
[CW] collect: return: 360.91636, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 6.60739, qf2_loss: 6.56926, policy_loss: -242.05185, policy_entropy: -0.99741, alpha: 0.24981, time: 33.72791
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   626 ----
[CW] collect: return: 364.14622, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 6.12862, qf2_loss: 6.13840, policy_loss: -242.89439, policy_entropy: -0.99381, alpha: 0.24940, time: 33.48512
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   627 ----
[CW] collect: return: 463.19847, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 7.15399, qf2_loss: 7.19507, policy_loss: -243.29449, policy_entropy: -1.00907, alpha: 0.24878, time: 33.37797
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   628 ----
[CW] collect: return: 313.87420, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 7.11160, qf2_loss: 7.17187, policy_loss: -243.49109, policy_entropy: -0.99577, alpha: 0.24969, time: 33.23053
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   629 ----
[CW] collect: return: 380.63049, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 6.19752, qf2_loss: 6.21877, policy_loss: -242.86548, policy_entropy: -1.01187, alpha: 0.24951, time: 33.62328
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   630 ----
[CW] collect: return: 342.09082, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 6.10502, qf2_loss: 6.05938, policy_loss: -242.98460, policy_entropy: -0.99620, alpha: 0.25100, time: 33.46383
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   631 ----
[CW] collect: return: 370.52696, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 8.15923, qf2_loss: 8.17936, policy_loss: -243.56849, policy_entropy: -0.99873, alpha: 0.25074, time: 33.59273
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   632 ----
[CW] collect: return: 331.99375, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 6.68471, qf2_loss: 6.70780, policy_loss: -243.17558, policy_entropy: -0.99777, alpha: 0.25021, time: 33.19679
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   633 ----
[CW] collect: return: 420.43259, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 6.89017, qf2_loss: 6.88109, policy_loss: -243.42976, policy_entropy: -1.00259, alpha: 0.25009, time: 33.47323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   634 ----
[CW] collect: return: 285.39759, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 6.51004, qf2_loss: 6.48249, policy_loss: -243.27614, policy_entropy: -0.99458, alpha: 0.25007, time: 33.54344
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   635 ----
[CW] collect: return: 365.53817, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 6.22160, qf2_loss: 6.27264, policy_loss: -243.18918, policy_entropy: -0.99864, alpha: 0.24946, time: 33.71989
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   636 ----
[CW] collect: return: 238.29370, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 6.73800, qf2_loss: 6.75263, policy_loss: -243.49150, policy_entropy: -0.98364, alpha: 0.24847, time: 33.33630
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   637 ----
[CW] collect: return: 364.49027, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 6.74184, qf2_loss: 6.74860, policy_loss: -243.45614, policy_entropy: -1.00091, alpha: 0.24752, time: 33.25259
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   638 ----
[CW] collect: return: 299.41778, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 7.45110, qf2_loss: 7.44584, policy_loss: -242.88342, policy_entropy: -0.99712, alpha: 0.24681, time: 33.06054
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   639 ----
[CW] collect: return: 365.65186, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 7.25736, qf2_loss: 7.30534, policy_loss: -243.14515, policy_entropy: -0.99366, alpha: 0.24631, time: 33.25391
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   640 ----
[CW] collect: return: 412.63706, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 7.80639, qf2_loss: 7.77565, policy_loss: -243.82079, policy_entropy: -0.99930, alpha: 0.24527, time: 33.16139
[CW] eval: return: 325.94353, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   641 ----
[CW] collect: return: 260.84803, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 7.03185, qf2_loss: 7.08455, policy_loss: -243.52108, policy_entropy: -1.01752, alpha: 0.24631, time: 33.44223
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   642 ----
[CW] collect: return: 398.10391, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 7.24961, qf2_loss: 7.27528, policy_loss: -244.02370, policy_entropy: -1.00397, alpha: 0.24893, time: 33.39438
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   643 ----
[CW] collect: return: 329.93890, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 6.64944, qf2_loss: 6.67717, policy_loss: -244.02906, policy_entropy: -0.99797, alpha: 0.24858, time: 33.74909
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   644 ----
[CW] collect: return: 342.37243, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 6.42088, qf2_loss: 6.44321, policy_loss: -243.82681, policy_entropy: -0.99889, alpha: 0.24778, time: 33.37700
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   645 ----
[CW] collect: return: 326.24400, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 6.49350, qf2_loss: 6.44713, policy_loss: -245.01121, policy_entropy: -1.01049, alpha: 0.24916, time: 33.68046
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   646 ----
[CW] collect: return: 376.76716, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 7.05218, qf2_loss: 6.97557, policy_loss: -243.76827, policy_entropy: -0.99302, alpha: 0.24971, time: 33.60192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   647 ----
[CW] collect: return: 401.09909, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 8.73764, qf2_loss: 8.71632, policy_loss: -244.52788, policy_entropy: -0.99195, alpha: 0.24811, time: 33.88158
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   648 ----
[CW] collect: return: 375.17192, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 7.47920, qf2_loss: 7.48127, policy_loss: -243.76123, policy_entropy: -0.98593, alpha: 0.24616, time: 33.27617
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   649 ----
[CW] collect: return: 373.99865, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 7.01091, qf2_loss: 7.04175, policy_loss: -244.90842, policy_entropy: -1.00753, alpha: 0.24648, time: 33.38688
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   650 ----
[CW] collect: return: 369.84847, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 7.08099, qf2_loss: 6.99967, policy_loss: -243.96826, policy_entropy: -0.99995, alpha: 0.24644, time: 33.38436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   651 ----
[CW] collect: return: 323.57714, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 6.34015, qf2_loss: 6.36360, policy_loss: -244.93424, policy_entropy: -0.99973, alpha: 0.24665, time: 33.63474
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   652 ----
[CW] collect: return: 329.10813, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 5.99697, qf2_loss: 6.03564, policy_loss: -243.45642, policy_entropy: -1.00421, alpha: 0.24674, time: 33.45281
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   653 ----
[CW] collect: return: 325.96875, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 6.81431, qf2_loss: 6.80294, policy_loss: -244.66938, policy_entropy: -1.00145, alpha: 0.24723, time: 33.59534
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   654 ----
[CW] collect: return: 390.66252, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 6.91392, qf2_loss: 6.90075, policy_loss: -244.50607, policy_entropy: -0.99974, alpha: 0.24747, time: 33.43886
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   655 ----
[CW] collect: return: 242.05854, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 7.77801, qf2_loss: 7.78643, policy_loss: -244.44559, policy_entropy: -1.00035, alpha: 0.24756, time: 33.59773
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   656 ----
[CW] collect: return: 393.46237, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 7.55464, qf2_loss: 7.57768, policy_loss: -244.77014, policy_entropy: -0.99492, alpha: 0.24735, time: 33.60553
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   657 ----
[CW] collect: return: 302.82764, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 6.68339, qf2_loss: 6.70200, policy_loss: -243.97282, policy_entropy: -0.99655, alpha: 0.24640, time: 33.90316
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   658 ----
[CW] collect: return: 352.58405, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 7.24185, qf2_loss: 7.30906, policy_loss: -244.45133, policy_entropy: -0.99515, alpha: 0.24584, time: 33.59848
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   659 ----
[CW] collect: return: 372.44269, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 7.24702, qf2_loss: 7.32175, policy_loss: -244.20821, policy_entropy: -0.99324, alpha: 0.24472, time: 33.70770
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   660 ----
[CW] collect: return: 318.55567, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 6.70424, qf2_loss: 6.74927, policy_loss: -244.76191, policy_entropy: -0.99630, alpha: 0.24450, time: 33.57800
[CW] eval: return: 350.96147, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   661 ----
[CW] collect: return: 353.76421, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 6.71477, qf2_loss: 6.70242, policy_loss: -244.23897, policy_entropy: -0.99875, alpha: 0.24387, time: 33.90433
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   662 ----
[CW] collect: return: 310.17362, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 6.54828, qf2_loss: 6.55015, policy_loss: -244.39529, policy_entropy: -0.99747, alpha: 0.24346, time: 33.33652
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   663 ----
[CW] collect: return: 423.69168, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 6.55537, qf2_loss: 6.50710, policy_loss: -244.64501, policy_entropy: -0.99721, alpha: 0.24323, time: 35.83396
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   664 ----
[CW] collect: return: 336.65852, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 6.66591, qf2_loss: 6.66182, policy_loss: -245.08405, policy_entropy: -1.00728, alpha: 0.24265, time: 33.67052
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   665 ----
[CW] collect: return: 279.00870, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 6.92350, qf2_loss: 7.03366, policy_loss: -244.32143, policy_entropy: -1.01396, alpha: 0.24538, time: 33.73721
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   666 ----
[CW] collect: return: 370.29187, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 6.73692, qf2_loss: 6.75807, policy_loss: -243.91648, policy_entropy: -0.98264, alpha: 0.24442, time: 33.65252
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   667 ----
[CW] collect: return: 291.71146, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 7.46123, qf2_loss: 7.44458, policy_loss: -244.27510, policy_entropy: -1.00128, alpha: 0.24350, time: 33.54975
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   668 ----
[CW] collect: return: 274.13592, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 7.02407, qf2_loss: 7.00883, policy_loss: -245.33177, policy_entropy: -1.00319, alpha: 0.24391, time: 33.54053
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   669 ----
[CW] collect: return: 366.07075, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 7.41780, qf2_loss: 7.41367, policy_loss: -244.68398, policy_entropy: -0.99209, alpha: 0.24393, time: 34.19483
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   670 ----
[CW] collect: return: 292.02898, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 7.96841, qf2_loss: 8.00812, policy_loss: -245.03927, policy_entropy: -1.00010, alpha: 0.24300, time: 33.53133
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   671 ----
[CW] collect: return: 305.59092, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 8.82449, qf2_loss: 8.90292, policy_loss: -245.05473, policy_entropy: -1.00470, alpha: 0.24362, time: 33.71998
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   672 ----
[CW] collect: return: 343.39117, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 6.86567, qf2_loss: 6.82754, policy_loss: -245.36731, policy_entropy: -1.00538, alpha: 0.24410, time: 33.51868
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   673 ----
[CW] collect: return: 290.75567, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 6.66754, qf2_loss: 6.69463, policy_loss: -244.96167, policy_entropy: -0.99259, alpha: 0.24439, time: 33.65831
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   674 ----
[CW] collect: return: 368.47117, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 7.09350, qf2_loss: 7.11888, policy_loss: -245.02192, policy_entropy: -0.99905, alpha: 0.24342, time: 33.50518
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   675 ----
[CW] collect: return: 354.27865, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 6.69332, qf2_loss: 6.70166, policy_loss: -245.38680, policy_entropy: -1.00090, alpha: 0.24317, time: 33.65322
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   676 ----
[CW] collect: return: 268.23686, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 6.76733, qf2_loss: 6.74117, policy_loss: -246.54730, policy_entropy: -1.01966, alpha: 0.24460, time: 33.47042
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   677 ----
[CW] collect: return: 363.26962, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 6.61269, qf2_loss: 6.64276, policy_loss: -246.32904, policy_entropy: -1.00709, alpha: 0.24664, time: 33.41799
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   678 ----
[CW] collect: return: 343.85723, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 6.50238, qf2_loss: 6.48018, policy_loss: -244.96502, policy_entropy: -0.98722, alpha: 0.24626, time: 36.48976
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   679 ----
[CW] collect: return: 390.60011, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 7.32124, qf2_loss: 7.30711, policy_loss: -245.90355, policy_entropy: -0.99908, alpha: 0.24502, time: 33.58724
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   680 ----
[CW] collect: return: 385.23245, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 9.17381, qf2_loss: 9.19776, policy_loss: -246.33925, policy_entropy: -1.00295, alpha: 0.24528, time: 33.45240
[CW] eval: return: 340.87263, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   681 ----
[CW] collect: return: 285.09227, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 7.36208, qf2_loss: 7.41016, policy_loss: -245.53908, policy_entropy: -0.99142, alpha: 0.24509, time: 33.68616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   682 ----
[CW] collect: return: 431.48018, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 6.49380, qf2_loss: 6.42093, policy_loss: -245.25806, policy_entropy: -0.99382, alpha: 0.24406, time: 33.44111
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   683 ----
[CW] collect: return: 427.12027, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 6.53524, qf2_loss: 6.49300, policy_loss: -245.82914, policy_entropy: -0.99393, alpha: 0.24290, time: 33.57920
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   684 ----
[CW] collect: return: 309.46450, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 6.71823, qf2_loss: 6.67748, policy_loss: -245.61109, policy_entropy: -1.00471, alpha: 0.24296, time: 33.58444
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   685 ----
[CW] collect: return: 368.42147, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 6.92923, qf2_loss: 6.95676, policy_loss: -246.67988, policy_entropy: -1.00992, alpha: 0.24352, time: 33.42460
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   686 ----
[CW] collect: return: 413.24165, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 6.67645, qf2_loss: 6.65654, policy_loss: -245.65306, policy_entropy: -1.00177, alpha: 0.24467, time: 33.47725
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   687 ----
[CW] collect: return: 343.15414, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 6.70083, qf2_loss: 6.64433, policy_loss: -246.42580, policy_entropy: -0.99887, alpha: 0.24477, time: 33.43271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   688 ----
[CW] collect: return: 392.10340, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 6.86234, qf2_loss: 6.83628, policy_loss: -246.34892, policy_entropy: -0.99850, alpha: 0.24441, time: 33.18507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   689 ----
[CW] collect: return: 313.09690, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 7.09538, qf2_loss: 7.11400, policy_loss: -245.99688, policy_entropy: -1.00484, alpha: 0.24482, time: 33.28545
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   690 ----
[CW] collect: return: 362.81782, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 8.72469, qf2_loss: 8.73447, policy_loss: -245.78211, policy_entropy: -0.99821, alpha: 0.24514, time: 33.51244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   691 ----
[CW] collect: return: 280.47067, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 7.46386, qf2_loss: 7.42938, policy_loss: -246.42167, policy_entropy: -0.99887, alpha: 0.24463, time: 33.63712
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   692 ----
[CW] collect: return: 407.46318, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 7.36947, qf2_loss: 7.46525, policy_loss: -246.66549, policy_entropy: -1.00074, alpha: 0.24497, time: 33.53780
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   693 ----
[CW] collect: return: 363.76474, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 6.42859, qf2_loss: 6.44204, policy_loss: -245.76557, policy_entropy: -0.99685, alpha: 0.24480, time: 33.60016
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   694 ----
[CW] collect: return: 346.90551, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 7.29502, qf2_loss: 7.33626, policy_loss: -246.61349, policy_entropy: -0.99724, alpha: 0.24465, time: 33.42558
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   695 ----
[CW] collect: return: 351.89511, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 8.16078, qf2_loss: 8.23346, policy_loss: -247.07574, policy_entropy: -0.99454, alpha: 0.24412, time: 33.63321
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   696 ----
[CW] collect: return: 321.00051, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 7.91143, qf2_loss: 7.92417, policy_loss: -246.66200, policy_entropy: -1.00575, alpha: 0.24363, time: 33.36057
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   697 ----
[CW] collect: return: 256.62987, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 6.90115, qf2_loss: 6.93879, policy_loss: -245.80903, policy_entropy: -0.99992, alpha: 0.24459, time: 32.99465
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   698 ----
[CW] collect: return: 302.91408, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 7.33968, qf2_loss: 7.28408, policy_loss: -246.10647, policy_entropy: -0.98957, alpha: 0.24326, time: 33.22177
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   699 ----
[CW] collect: return: 300.56581, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 8.79556, qf2_loss: 8.77027, policy_loss: -245.64750, policy_entropy: -0.99557, alpha: 0.24235, time: 33.63749
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   700 ----
[CW] collect: return: 310.13370, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 6.97365, qf2_loss: 6.93622, policy_loss: -246.45237, policy_entropy: -1.00186, alpha: 0.24200, time: 33.52971
[CW] eval: return: 352.79804, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   701 ----
[CW] collect: return: 353.10206, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 7.52605, qf2_loss: 7.42724, policy_loss: -247.27427, policy_entropy: -1.01653, alpha: 0.24300, time: 33.44740
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   702 ----
[CW] collect: return: 278.09955, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 7.11469, qf2_loss: 7.09133, policy_loss: -246.44749, policy_entropy: -1.00200, alpha: 0.24422, time: 33.60000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   703 ----
[CW] collect: return: 443.92781, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 7.53300, qf2_loss: 7.53266, policy_loss: -247.54506, policy_entropy: -1.00622, alpha: 0.24522, time: 34.22398
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   704 ----
[CW] collect: return: 368.21282, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 7.07989, qf2_loss: 7.09261, policy_loss: -246.92697, policy_entropy: -0.99571, alpha: 0.24584, time: 33.48094
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   705 ----
[CW] collect: return: 348.66098, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 6.72251, qf2_loss: 6.70442, policy_loss: -246.39641, policy_entropy: -0.99500, alpha: 0.24509, time: 33.73367
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   706 ----
[CW] collect: return: 246.90301, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 6.95433, qf2_loss: 7.04356, policy_loss: -247.70389, policy_entropy: -1.00478, alpha: 0.24445, time: 33.65235
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   707 ----
[CW] collect: return: 273.65895, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 7.04686, qf2_loss: 6.96046, policy_loss: -246.11758, policy_entropy: -0.99553, alpha: 0.24477, time: 33.56629
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   708 ----
[CW] collect: return: 361.73844, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 7.16875, qf2_loss: 7.16546, policy_loss: -245.72111, policy_entropy: -1.00339, alpha: 0.24481, time: 33.26409
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   709 ----
[CW] collect: return: 440.99828, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 6.95082, qf2_loss: 6.99914, policy_loss: -247.66403, policy_entropy: -1.00033, alpha: 0.24510, time: 33.57801
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   710 ----
[CW] collect: return: 460.54530, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 6.93411, qf2_loss: 6.91398, policy_loss: -246.65589, policy_entropy: -0.99544, alpha: 0.24499, time: 33.58093
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   711 ----
[CW] collect: return: 403.96596, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 7.35586, qf2_loss: 7.29799, policy_loss: -248.13975, policy_entropy: -1.00912, alpha: 0.24499, time: 33.03308
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   712 ----
[CW] collect: return: 334.04788, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 6.94511, qf2_loss: 6.97258, policy_loss: -247.09019, policy_entropy: -0.99813, alpha: 0.24521, time: 33.13912
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   713 ----
[CW] collect: return: 295.21456, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 8.20140, qf2_loss: 8.29218, policy_loss: -247.28333, policy_entropy: -0.98799, alpha: 0.24446, time: 33.23766
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   714 ----
[CW] collect: return: 409.14806, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 8.52207, qf2_loss: 8.53213, policy_loss: -247.82768, policy_entropy: -0.99043, alpha: 0.24299, time: 33.32699
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   715 ----
[CW] collect: return: 320.69609, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 9.29256, qf2_loss: 9.28608, policy_loss: -248.65113, policy_entropy: -1.00121, alpha: 0.24247, time: 33.70433
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   716 ----
[CW] collect: return: 358.24267, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 7.35625, qf2_loss: 7.28838, policy_loss: -248.03402, policy_entropy: -0.99694, alpha: 0.24202, time: 33.62169
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   717 ----
[CW] collect: return: 313.17610, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 7.25417, qf2_loss: 7.24975, policy_loss: -247.96119, policy_entropy: -0.98845, alpha: 0.24086, time: 33.88165
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   718 ----
[CW] collect: return: 344.21695, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 7.05172, qf2_loss: 7.04278, policy_loss: -246.73389, policy_entropy: -0.99312, alpha: 0.23931, time: 33.72345
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   719 ----
[CW] collect: return: 313.74178, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 6.64996, qf2_loss: 6.62469, policy_loss: -247.44768, policy_entropy: -1.00261, alpha: 0.23889, time: 33.53126
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   720 ----
[CW] collect: return: 396.13737, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 6.93318, qf2_loss: 6.86830, policy_loss: -247.30119, policy_entropy: -0.99267, alpha: 0.23930, time: 33.34827
[CW] eval: return: 325.95374, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   721 ----
[CW] collect: return: 352.61121, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 7.00186, qf2_loss: 7.03493, policy_loss: -247.67107, policy_entropy: -0.99990, alpha: 0.23912, time: 33.48834
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   722 ----
[CW] collect: return: 360.50848, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 7.08571, qf2_loss: 7.14183, policy_loss: -247.56484, policy_entropy: -1.01318, alpha: 0.23892, time: 33.43436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   723 ----
[CW] collect: return: 288.45015, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 7.63894, qf2_loss: 7.73259, policy_loss: -247.88167, policy_entropy: -0.99883, alpha: 0.24007, time: 33.48972
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   724 ----
[CW] collect: return: 369.31224, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 7.18933, qf2_loss: 7.22556, policy_loss: -247.69664, policy_entropy: -0.99815, alpha: 0.24016, time: 33.61707
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   725 ----
[CW] collect: return: 407.25653, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 7.22570, qf2_loss: 7.21784, policy_loss: -247.29588, policy_entropy: -1.00722, alpha: 0.24061, time: 35.12638
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   726 ----
[CW] collect: return: 371.40529, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 7.89655, qf2_loss: 7.95827, policy_loss: -247.69736, policy_entropy: -1.00031, alpha: 0.24041, time: 33.56219
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   727 ----
[CW] collect: return: 376.72419, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 6.93962, qf2_loss: 6.94986, policy_loss: -248.10852, policy_entropy: -0.99929, alpha: 0.24081, time: 33.71901
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   728 ----
[CW] collect: return: 358.44377, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 7.20937, qf2_loss: 7.23384, policy_loss: -247.64348, policy_entropy: -1.00945, alpha: 0.24155, time: 33.38272
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   729 ----
[CW] collect: return: 341.25615, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 6.90510, qf2_loss: 6.93404, policy_loss: -247.93516, policy_entropy: -0.98473, alpha: 0.24193, time: 33.47021
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   730 ----
[CW] collect: return: 269.79855, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 7.01687, qf2_loss: 6.95775, policy_loss: -247.47630, policy_entropy: -1.00773, alpha: 0.24105, time: 33.20557
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   731 ----
[CW] collect: return: 351.34423, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 8.16505, qf2_loss: 8.15954, policy_loss: -248.05346, policy_entropy: -1.00788, alpha: 0.24147, time: 33.72181
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   732 ----
[CW] collect: return: 350.68107, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 8.00535, qf2_loss: 8.03978, policy_loss: -248.56357, policy_entropy: -1.00045, alpha: 0.24203, time: 33.54343
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   733 ----
[CW] collect: return: 439.03742, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 6.50556, qf2_loss: 6.53233, policy_loss: -248.35466, policy_entropy: -1.01050, alpha: 0.24322, time: 33.57993
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   734 ----
[CW] collect: return: 391.63656, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 7.06936, qf2_loss: 7.07633, policy_loss: -247.29132, policy_entropy: -0.99250, alpha: 0.24365, time: 33.47426
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   735 ----
[CW] collect: return: 384.00287, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 6.77981, qf2_loss: 6.78891, policy_loss: -248.96640, policy_entropy: -1.00749, alpha: 0.24331, time: 32.91300
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   736 ----
[CW] collect: return: 356.01194, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 6.63137, qf2_loss: 6.67369, policy_loss: -248.57819, policy_entropy: -0.99386, alpha: 0.24299, time: 34.32259
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   737 ----
[CW] collect: return: 284.13396, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 7.39229, qf2_loss: 7.40653, policy_loss: -248.37906, policy_entropy: -0.99604, alpha: 0.24248, time: 34.35207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   738 ----
[CW] collect: return: 335.20913, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 7.78827, qf2_loss: 7.83042, policy_loss: -248.19725, policy_entropy: -1.00574, alpha: 0.24287, time: 33.72690
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   739 ----
[CW] collect: return: 325.09641, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 8.84272, qf2_loss: 8.91827, policy_loss: -248.55838, policy_entropy: -1.01047, alpha: 0.24387, time: 33.38853
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   740 ----
[CW] collect: return: 443.86214, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 7.80457, qf2_loss: 7.82085, policy_loss: -248.86430, policy_entropy: -0.99709, alpha: 0.24455, time: 33.36845
[CW] eval: return: 344.91437, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   741 ----
[CW] collect: return: 382.83385, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 6.94807, qf2_loss: 6.99680, policy_loss: -248.72453, policy_entropy: -1.00473, alpha: 0.24472, time: 33.57440
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   742 ----
[CW] collect: return: 287.50850, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 6.39784, qf2_loss: 6.40611, policy_loss: -248.49392, policy_entropy: -1.00687, alpha: 0.24513, time: 33.18566
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   743 ----
[CW] collect: return: 374.49266, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 7.19383, qf2_loss: 7.21007, policy_loss: -249.07838, policy_entropy: -0.99094, alpha: 0.24545, time: 33.68346
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   744 ----
[CW] collect: return: 346.43108, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 7.43037, qf2_loss: 7.44858, policy_loss: -248.26280, policy_entropy: -1.00517, alpha: 0.24482, time: 33.59597
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   745 ----
[CW] collect: return: 329.96401, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 8.16400, qf2_loss: 8.20538, policy_loss: -248.42657, policy_entropy: -1.00995, alpha: 0.24568, time: 33.75220
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   746 ----
[CW] collect: return: 360.73991, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 8.97684, qf2_loss: 8.93472, policy_loss: -248.69403, policy_entropy: -1.00173, alpha: 0.24716, time: 33.58041
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   747 ----
[CW] collect: return: 401.55017, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 7.27991, qf2_loss: 7.38092, policy_loss: -248.33323, policy_entropy: -0.99253, alpha: 0.24707, time: 33.59344
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   748 ----
[CW] collect: return: 359.31958, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 6.90702, qf2_loss: 6.91621, policy_loss: -248.90240, policy_entropy: -1.01419, alpha: 0.24716, time: 33.59007
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   749 ----
[CW] collect: return: 316.84147, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 7.38546, qf2_loss: 7.45264, policy_loss: -249.49564, policy_entropy: -1.00435, alpha: 0.24873, time: 32.88166
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   750 ----
[CW] collect: return: 239.37002, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 7.17262, qf2_loss: 7.17310, policy_loss: -249.06170, policy_entropy: -1.00944, alpha: 0.24988, time: 33.08270
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   751 ----
[CW] collect: return: 337.65625, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 8.72286, qf2_loss: 8.73652, policy_loss: -249.08603, policy_entropy: -1.00003, alpha: 0.25009, time: 33.16905
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   752 ----
[CW] collect: return: 310.66440, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 8.56213, qf2_loss: 8.57581, policy_loss: -248.47086, policy_entropy: -0.99385, alpha: 0.25006, time: 33.45450
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   753 ----
[CW] collect: return: 272.65450, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 6.96105, qf2_loss: 7.00148, policy_loss: -249.52182, policy_entropy: -1.00787, alpha: 0.24986, time: 33.23151
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   754 ----
[CW] collect: return: 364.07649, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 6.62205, qf2_loss: 6.59845, policy_loss: -248.89077, policy_entropy: -0.99513, alpha: 0.25036, time: 33.02665
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   755 ----
[CW] collect: return: 318.04716, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 7.22990, qf2_loss: 7.29552, policy_loss: -248.97205, policy_entropy: -0.99610, alpha: 0.24958, time: 33.33534
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   756 ----
[CW] collect: return: 399.20725, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 6.83259, qf2_loss: 6.80579, policy_loss: -249.09945, policy_entropy: -0.99832, alpha: 0.24982, time: 33.50930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   757 ----
[CW] collect: return: 390.30608, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 7.57301, qf2_loss: 7.67053, policy_loss: -248.27982, policy_entropy: -1.00108, alpha: 0.24929, time: 33.50191
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   758 ----
[CW] collect: return: 348.10056, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 7.81217, qf2_loss: 7.79328, policy_loss: -249.53308, policy_entropy: -1.00562, alpha: 0.24944, time: 33.39536
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   759 ----
[CW] collect: return: 302.99596, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 7.47376, qf2_loss: 7.36690, policy_loss: -248.75381, policy_entropy: -0.99094, alpha: 0.24930, time: 33.49999
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   760 ----
[CW] collect: return: 406.83627, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 6.60625, qf2_loss: 6.66261, policy_loss: -248.74889, policy_entropy: -1.00440, alpha: 0.24877, time: 33.36868
[CW] eval: return: 330.22448, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   761 ----
[CW] collect: return: 365.09074, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 7.04325, qf2_loss: 7.07098, policy_loss: -249.43694, policy_entropy: -1.00172, alpha: 0.24905, time: 33.53698
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   762 ----
[CW] collect: return: 274.10236, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 8.18294, qf2_loss: 8.20869, policy_loss: -249.02517, policy_entropy: -1.00066, alpha: 0.24914, time: 33.10205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   763 ----
[CW] collect: return: 332.71825, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 7.80181, qf2_loss: 7.84460, policy_loss: -249.53603, policy_entropy: -0.99132, alpha: 0.24958, time: 33.39030
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   764 ----
[CW] collect: return: 301.45891, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 7.41564, qf2_loss: 7.41245, policy_loss: -249.61030, policy_entropy: -1.00558, alpha: 0.24874, time: 33.54561
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   765 ----
[CW] collect: return: 383.49959, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 7.33417, qf2_loss: 7.40873, policy_loss: -249.41230, policy_entropy: -0.99912, alpha: 0.24949, time: 33.27970
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   766 ----
[CW] collect: return: 278.49308, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 7.01732, qf2_loss: 7.08622, policy_loss: -249.70118, policy_entropy: -1.00128, alpha: 0.24911, time: 33.39901
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   767 ----
[CW] collect: return: 294.87373, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 6.67819, qf2_loss: 6.75348, policy_loss: -249.38981, policy_entropy: -0.98471, alpha: 0.24821, time: 33.55496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   768 ----
[CW] collect: return: 335.54843, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 7.54357, qf2_loss: 7.53797, policy_loss: -249.49253, policy_entropy: -1.00296, alpha: 0.24732, time: 33.58485
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   769 ----
[CW] collect: return: 402.63084, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 8.67437, qf2_loss: 8.68257, policy_loss: -248.88176, policy_entropy: -1.00172, alpha: 0.24727, time: 33.79672
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   770 ----
[CW] collect: return: 291.91552, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 7.97679, qf2_loss: 8.01182, policy_loss: -250.22652, policy_entropy: -1.00262, alpha: 0.24781, time: 33.54776
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   771 ----
[CW] collect: return: 288.21821, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 7.21187, qf2_loss: 7.19783, policy_loss: -249.40622, policy_entropy: -1.00252, alpha: 0.24854, time: 33.64660
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   772 ----
[CW] collect: return: 391.29488, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 6.67983, qf2_loss: 6.70853, policy_loss: -249.26718, policy_entropy: -1.00044, alpha: 0.24807, time: 33.39892
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   773 ----
[CW] collect: return: 336.59312, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 6.90175, qf2_loss: 6.90251, policy_loss: -249.04948, policy_entropy: -0.99695, alpha: 0.24852, time: 33.66281
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   774 ----
[CW] collect: return: 433.38841, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 7.00162, qf2_loss: 7.08189, policy_loss: -248.80347, policy_entropy: -0.99602, alpha: 0.24781, time: 33.27795
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   775 ----
[CW] collect: return: 300.18034, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 6.94960, qf2_loss: 6.94191, policy_loss: -250.26782, policy_entropy: -0.99564, alpha: 0.24741, time: 34.43361
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   776 ----
[CW] collect: return: 374.66380, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 6.88442, qf2_loss: 6.83563, policy_loss: -249.07321, policy_entropy: -1.01966, alpha: 0.24783, time: 33.44185
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   777 ----
[CW] collect: return: 432.77126, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 6.80180, qf2_loss: 6.86888, policy_loss: -248.94716, policy_entropy: -0.99693, alpha: 0.24893, time: 33.36545
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   778 ----
[CW] collect: return: 318.80857, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 7.00156, qf2_loss: 7.03673, policy_loss: -249.09895, policy_entropy: -0.99215, alpha: 0.24886, time: 33.39381
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   779 ----
[CW] collect: return: 312.67355, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 7.58836, qf2_loss: 7.59575, policy_loss: -249.48377, policy_entropy: -1.00118, alpha: 0.24744, time: 33.41017
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   780 ----
[CW] collect: return: 358.81488, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 7.48767, qf2_loss: 7.49005, policy_loss: -249.33000, policy_entropy: -0.99716, alpha: 0.24821, time: 33.41207
[CW] eval: return: 348.42932, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   781 ----
[CW] collect: return: 370.24968, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 7.56825, qf2_loss: 7.63005, policy_loss: -249.89351, policy_entropy: -1.00415, alpha: 0.24830, time: 35.77380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   782 ----
[CW] collect: return: 371.80769, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 9.68039, qf2_loss: 9.71943, policy_loss: -249.72130, policy_entropy: -1.00201, alpha: 0.24848, time: 33.51814
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   783 ----
[CW] collect: return: 267.49341, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 8.12353, qf2_loss: 8.19009, policy_loss: -248.84115, policy_entropy: -0.99555, alpha: 0.24799, time: 33.61342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   784 ----
[CW] collect: return: 331.14206, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 10.64508, qf2_loss: 10.64443, policy_loss: -250.56649, policy_entropy: -0.99997, alpha: 0.24836, time: 33.45535
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   785 ----
[CW] collect: return: 324.27181, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 7.37439, qf2_loss: 7.38440, policy_loss: -249.46569, policy_entropy: -0.98390, alpha: 0.24731, time: 33.50794
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   786 ----
[CW] collect: return: 283.40729, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 6.41506, qf2_loss: 6.43298, policy_loss: -248.85078, policy_entropy: -0.99156, alpha: 0.24516, time: 33.32869
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   787 ----
[CW] collect: return: 313.59116, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 6.50387, qf2_loss: 6.46890, policy_loss: -250.49305, policy_entropy: -1.01001, alpha: 0.24504, time: 33.52071
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   788 ----
[CW] collect: return: 337.34233, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 6.19506, qf2_loss: 6.21884, policy_loss: -249.04184, policy_entropy: -1.00157, alpha: 0.24586, time: 33.32081
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   789 ----
[CW] collect: return: 360.76190, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 6.65709, qf2_loss: 6.64021, policy_loss: -249.93591, policy_entropy: -1.00435, alpha: 0.24592, time: 33.42959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   790 ----
[CW] collect: return: 303.17060, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 7.23162, qf2_loss: 7.34285, policy_loss: -249.04301, policy_entropy: -0.98914, alpha: 0.24644, time: 36.85085
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   791 ----
[CW] collect: return: 321.10340, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 7.75834, qf2_loss: 7.79969, policy_loss: -249.49196, policy_entropy: -0.99771, alpha: 0.24511, time: 33.56827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   792 ----
[CW] collect: return: 339.01891, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 7.92516, qf2_loss: 7.89802, policy_loss: -249.76368, policy_entropy: -1.01262, alpha: 0.24561, time: 33.26177
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   793 ----
[CW] collect: return: 403.03153, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 7.35724, qf2_loss: 7.42122, policy_loss: -250.05398, policy_entropy: -1.00460, alpha: 0.24678, time: 33.54146
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   794 ----
[CW] collect: return: 419.18483, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 6.83012, qf2_loss: 6.84970, policy_loss: -249.82712, policy_entropy: -1.00251, alpha: 0.24745, time: 35.35337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   795 ----
[CW] collect: return: 348.46584, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 7.24711, qf2_loss: 7.29584, policy_loss: -250.25934, policy_entropy: -1.00877, alpha: 0.24878, time: 33.46477
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   796 ----
[CW] collect: return: 316.52919, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 7.57661, qf2_loss: 7.65773, policy_loss: -249.93956, policy_entropy: -0.99951, alpha: 0.24929, time: 33.46673
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   797 ----
[CW] collect: return: 329.97267, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 7.14773, qf2_loss: 7.17478, policy_loss: -249.61734, policy_entropy: -0.98944, alpha: 0.24879, time: 33.37161
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   798 ----
[CW] collect: return: 370.17806, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 7.17068, qf2_loss: 7.12087, policy_loss: -250.24300, policy_entropy: -1.00253, alpha: 0.24786, time: 33.68333
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   799 ----
[CW] collect: return: 345.99863, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 6.93636, qf2_loss: 7.01114, policy_loss: -249.71395, policy_entropy: -1.00662, alpha: 0.24841, time: 33.57712
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   800 ----
[CW] collect: return: 424.91869, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 7.70840, qf2_loss: 7.71070, policy_loss: -250.96277, policy_entropy: -1.00102, alpha: 0.24894, time: 33.23056
[CW] eval: return: 364.99092, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   801 ----
[CW] collect: return: 379.85996, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 7.41703, qf2_loss: 7.40266, policy_loss: -249.48891, policy_entropy: -1.00517, alpha: 0.24894, time: 33.42667
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   802 ----
[CW] collect: return: 324.91293, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 7.67682, qf2_loss: 7.65146, policy_loss: -250.55611, policy_entropy: -1.00111, alpha: 0.24958, time: 33.52368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   803 ----
[CW] collect: return: 371.46142, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 7.49538, qf2_loss: 7.42845, policy_loss: -249.59239, policy_entropy: -1.00411, alpha: 0.25059, time: 33.52099
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   804 ----
[CW] collect: return: 408.83641, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 7.61447, qf2_loss: 7.55249, policy_loss: -250.75617, policy_entropy: -1.00315, alpha: 0.25095, time: 33.47219
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   805 ----
[CW] collect: return: 361.09604, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 7.72459, qf2_loss: 7.67338, policy_loss: -250.34511, policy_entropy: -1.00444, alpha: 0.25159, time: 33.13467
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   806 ----
[CW] collect: return: 268.82228, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 7.42305, qf2_loss: 7.43142, policy_loss: -251.20731, policy_entropy: -1.00702, alpha: 0.25220, time: 33.29632
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   807 ----
[CW] collect: return: 264.64933, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 7.35390, qf2_loss: 7.38729, policy_loss: -250.88325, policy_entropy: -0.99517, alpha: 0.25265, time: 33.22596
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   808 ----
[CW] collect: return: 363.87337, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 7.59605, qf2_loss: 7.57520, policy_loss: -251.19805, policy_entropy: -1.01105, alpha: 0.25259, time: 33.23965
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   809 ----
[CW] collect: return: 345.62047, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 7.94690, qf2_loss: 7.94600, policy_loss: -249.97150, policy_entropy: -0.99164, alpha: 0.25311, time: 33.34910
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   810 ----
[CW] collect: return: 333.85494, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 7.69274, qf2_loss: 7.69836, policy_loss: -250.59086, policy_entropy: -1.00046, alpha: 0.25277, time: 33.61712
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   811 ----
[CW] collect: return: 298.31406, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 7.74275, qf2_loss: 7.73522, policy_loss: -250.27580, policy_entropy: -1.00029, alpha: 0.25284, time: 33.61554
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   812 ----
[CW] collect: return: 320.64955, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 7.06640, qf2_loss: 7.05807, policy_loss: -249.88770, policy_entropy: -0.99518, alpha: 0.25237, time: 33.56763
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   813 ----
[CW] collect: return: 302.39855, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 7.09159, qf2_loss: 7.15898, policy_loss: -250.81883, policy_entropy: -1.00829, alpha: 0.25279, time: 33.68521
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   814 ----
[CW] collect: return: 401.17348, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 7.83080, qf2_loss: 7.84937, policy_loss: -251.13420, policy_entropy: -0.99627, alpha: 0.25272, time: 33.72874
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   815 ----
[CW] collect: return: 343.74429, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 9.00953, qf2_loss: 9.01029, policy_loss: -250.03509, policy_entropy: -0.99984, alpha: 0.25263, time: 33.58820
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   816 ----
[CW] collect: return: 419.03273, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 8.61893, qf2_loss: 8.60958, policy_loss: -250.42997, policy_entropy: -0.97859, alpha: 0.25122, time: 33.63017
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   817 ----
[CW] collect: return: 368.15294, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 7.15935, qf2_loss: 7.12043, policy_loss: -250.47359, policy_entropy: -0.99294, alpha: 0.24888, time: 33.51823
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   818 ----
[CW] collect: return: 320.78832, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 6.52405, qf2_loss: 6.55149, policy_loss: -251.03787, policy_entropy: -1.00075, alpha: 0.24837, time: 33.57121
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   819 ----
[CW] collect: return: 325.96526, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 6.73837, qf2_loss: 6.74621, policy_loss: -250.05580, policy_entropy: -0.98525, alpha: 0.24724, time: 33.68817
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   820 ----
[CW] collect: return: 315.50855, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 7.03529, qf2_loss: 7.07189, policy_loss: -250.59953, policy_entropy: -1.01143, alpha: 0.24665, time: 33.44641
[CW] eval: return: 363.71382, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   821 ----
[CW] collect: return: 358.70505, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 7.23840, qf2_loss: 7.31958, policy_loss: -251.04148, policy_entropy: -1.00271, alpha: 0.24839, time: 33.39253
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   822 ----
[CW] collect: return: 427.77487, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 6.83088, qf2_loss: 6.83051, policy_loss: -250.29604, policy_entropy: -0.99731, alpha: 0.24819, time: 33.49871
[CW] ---------------------------

============================= JOB FEEDBACK =============================

NodeName=uc2n904
Job ID: 21893752
Array Job ID: 21893752_0
Cluster: uc2
User/Group: uprnr/stud
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 4
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-08:01:32 core-walltime
Job Wall-clock time: 08:00:23
Memory Utilized: 4.95 GB
Memory Efficiency: 8.44% of 58.59 GB
