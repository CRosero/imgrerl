{"collect/return": 427.77486749144737, "collect/steps": 1000.0, "collect/total_steps": 828000.0, "train/qf1_loss": 6.830881290435791, "train/qf2_loss": 6.8305065011978146, "train/policy_loss": -250.29603713989258, "train/policy_entropy": -0.9973055481910705, "train/alpha": 0.24819257959723473, "train/time": 33.49870729446411, "eval/return": 363.7138159324706, "eval/steps": 1000.0, "_timestamp": 1678399629.4991372, "_runtime": 28790.78807926178, "_step": 822}