{"collect/return": 582.1641139611602, "collect/steps": 1000.0, "collect/total_steps": 669000.0, "train/qf1_loss": 6.836822090148925, "train/qf2_loss": 6.812067594528198, "train/policy_loss": -136.534818649292, "train/policy_entropy": -5.985612359046936, "train/alpha": 0.03286743327975273, "train/time": 51.4144983291626, "eval/return": 479.68520637143047, "eval/steps": 1000.0, "_timestamp": 1678739753.9227364, "_runtime": 35949.596549510956, "_step": 663}