{"collect/return": 845.576883627451, "collect/steps": 1000.0, "collect/total_steps": 637000.0, "train/qf1_loss": 53.45383697509766, "train/qf2_loss": 53.2920902633667, "train/policy_loss": -606.1466491699218, "train/policy_entropy": -1.0072420156002044, "train/alpha": 0.4237824445962906, "train/time": 43.9729962348938, "eval/return": 836.6963184151075, "eval/steps": 1000.0, "_timestamp": 1678385775.0159862, "_runtime": 28672.776218175888, "_step": 631}