{"collect/return": 591.8298962997505, "collect/steps": 1000.0, "collect/total_steps": 681000.0, "train/qf1_loss": 13.024582042694092, "train/qf2_loss": 13.050318059921265, "train/policy_loss": -153.419931640625, "train/policy_entropy": -6.208094353675842, "train/alpha": 0.04886506795883179, "train/time": 50.66736626625061, "eval/return": 566.4422845482037, "eval/steps": 1000.0, "_timestamp": 1678737779.0539439, "_runtime": 35891.11612391472, "_step": 675}