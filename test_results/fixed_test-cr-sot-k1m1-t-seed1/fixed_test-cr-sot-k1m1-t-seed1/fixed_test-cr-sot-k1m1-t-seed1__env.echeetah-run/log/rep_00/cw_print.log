[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 18.21158, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.63091, qf2_loss: 1.61255, policy_loss: -7.80178, policy_entropy: 4.09876, alpha: 0.98504, time: 55.12324
[CW] eval: return: 14.31161, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 19.19426, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.09473, qf2_loss: 0.09446, policy_loss: -8.53149, policy_entropy: 4.10034, alpha: 0.95626, time: 48.79101
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 4.37048, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.08727, qf2_loss: 0.08686, policy_loss: -9.21721, policy_entropy: 4.09993, alpha: 0.92871, time: 48.73975
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 14.12504, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.07807, qf2_loss: 0.07780, policy_loss: -10.15977, policy_entropy: 4.10004, alpha: 0.90231, time: 48.79534
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 10.68169, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06947, qf2_loss: 0.06920, policy_loss: -11.24344, policy_entropy: 4.10163, alpha: 0.87699, time: 49.13729
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 16.91474, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06678, qf2_loss: 0.06559, policy_loss: -12.38898, policy_entropy: 4.10029, alpha: 0.85267, time: 49.10939
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 5.20705, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.07573, qf2_loss: 0.07338, policy_loss: -13.57296, policy_entropy: 4.10209, alpha: 0.82931, time: 49.16395
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 14.90531, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.08729, qf2_loss: 0.08671, policy_loss: -14.78751, policy_entropy: 4.10062, alpha: 0.80683, time: 49.17866
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 25.56478, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.09090, qf2_loss: 0.09245, policy_loss: -16.01436, policy_entropy: 4.10023, alpha: 0.78520, time: 49.09794
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 17.65607, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.08818, qf2_loss: 0.09016, policy_loss: -17.22492, policy_entropy: 4.10037, alpha: 0.76436, time: 48.89383
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 10.04898, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.09450, qf2_loss: 0.09691, policy_loss: -18.41825, policy_entropy: 4.09868, alpha: 0.74427, time: 48.97223
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 4.58160, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.09095, qf2_loss: 0.09328, policy_loss: -19.59391, policy_entropy: 4.10009, alpha: 0.72489, time: 49.41308
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 27.05157, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.09706, qf2_loss: 0.09968, policy_loss: -20.74658, policy_entropy: 4.10108, alpha: 0.70618, time: 49.43266
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 8.57626, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.09047, qf2_loss: 0.09271, policy_loss: -21.86285, policy_entropy: 4.10032, alpha: 0.68810, time: 50.18328
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 12.64836, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.09763, qf2_loss: 0.10012, policy_loss: -22.95950, policy_entropy: 4.10050, alpha: 0.67063, time: 50.15389
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 8.78484, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.12070, qf2_loss: 0.12429, policy_loss: -24.01405, policy_entropy: 4.09802, alpha: 0.65373, time: 50.28452
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 13.66904, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.08374, qf2_loss: 0.08530, policy_loss: -25.05777, policy_entropy: 4.09780, alpha: 0.63738, time: 50.14263
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 7.95744, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.15285, qf2_loss: 0.15783, policy_loss: -26.07081, policy_entropy: 4.09791, alpha: 0.62154, time: 49.47806
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 5.17444, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.09031, qf2_loss: 0.09239, policy_loss: -27.04241, policy_entropy: 4.09711, alpha: 0.60620, time: 49.41321
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 6.02922, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.15675, qf2_loss: 0.16167, policy_loss: -27.97970, policy_entropy: 4.09600, alpha: 0.59134, time: 50.08347
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 7.91685, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.09317, qf2_loss: 0.09479, policy_loss: -28.91232, policy_entropy: 4.09569, alpha: 0.57692, time: 50.16039
[CW] eval: return: 13.66325, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 15.72998, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.11842, qf2_loss: 0.12112, policy_loss: -29.81938, policy_entropy: 4.09397, alpha: 0.56294, time: 49.95938
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 12.07048, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.13251, qf2_loss: 0.13576, policy_loss: -30.68498, policy_entropy: 4.09397, alpha: 0.54937, time: 49.77729
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 15.53250, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.12795, qf2_loss: 0.13067, policy_loss: -31.53628, policy_entropy: 4.09354, alpha: 0.53619, time: 49.96788
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 13.68762, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.13129, qf2_loss: 0.13418, policy_loss: -32.35645, policy_entropy: 4.09215, alpha: 0.52339, time: 50.29046
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 11.02134, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.13975, qf2_loss: 0.14285, policy_loss: -33.16020, policy_entropy: 4.09110, alpha: 0.51095, time: 50.36591
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 7.16263, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.18641, qf2_loss: 0.19152, policy_loss: -33.91785, policy_entropy: 4.09023, alpha: 0.49886, time: 50.28980
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 16.41646, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.12903, qf2_loss: 0.13139, policy_loss: -34.65932, policy_entropy: 4.09024, alpha: 0.48711, time: 50.36130
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 17.31789, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.13715, qf2_loss: 0.13976, policy_loss: -35.39330, policy_entropy: 4.08806, alpha: 0.47568, time: 50.38027
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 12.87174, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.16636, qf2_loss: 0.17011, policy_loss: -36.08787, policy_entropy: 4.08935, alpha: 0.46455, time: 50.17550
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 30.36330, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.12951, qf2_loss: 0.13161, policy_loss: -36.78920, policy_entropy: 4.08732, alpha: 0.45373, time: 50.19814
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 10.93618, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.16556, qf2_loss: 0.16917, policy_loss: -37.45577, policy_entropy: 4.08400, alpha: 0.44319, time: 50.15429
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 11.39088, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.16413, qf2_loss: 0.16780, policy_loss: -38.08771, policy_entropy: 4.08346, alpha: 0.43293, time: 50.03217
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 9.42696, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.15551, qf2_loss: 0.15837, policy_loss: -38.68858, policy_entropy: 4.08349, alpha: 0.42293, time: 50.16830
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 17.11160, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.18955, qf2_loss: 0.19379, policy_loss: -39.27952, policy_entropy: 4.08024, alpha: 0.41320, time: 49.89725
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 9.02574, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.13109, qf2_loss: 0.13324, policy_loss: -39.85437, policy_entropy: 4.07801, alpha: 0.40371, time: 50.42080
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 8.55928, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.21344, qf2_loss: 0.21786, policy_loss: -40.39760, policy_entropy: 4.08231, alpha: 0.39446, time: 50.81341
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 18.36260, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.10076, qf2_loss: 0.10108, policy_loss: -40.93054, policy_entropy: 4.08338, alpha: 0.38544, time: 50.77901
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 13.78382, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.15709, qf2_loss: 0.15984, policy_loss: -41.45233, policy_entropy: 4.08228, alpha: 0.37665, time: 50.92908
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 26.63165, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.16301, qf2_loss: 0.16571, policy_loss: -41.95491, policy_entropy: 4.07992, alpha: 0.36808, time: 50.37904
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 20.08067, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.13506, qf2_loss: 0.13652, policy_loss: -42.43849, policy_entropy: 4.07798, alpha: 0.35971, time: 50.34617
[CW] eval: return: 17.49905, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 19.11216, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.17190, qf2_loss: 0.17477, policy_loss: -42.90923, policy_entropy: 4.07586, alpha: 0.35156, time: 50.27857
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 19.63433, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.16718, qf2_loss: 0.16936, policy_loss: -43.35833, policy_entropy: 4.07457, alpha: 0.34360, time: 50.29305
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 13.31231, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.15016, qf2_loss: 0.15145, policy_loss: -43.80187, policy_entropy: 4.07294, alpha: 0.33583, time: 50.27307
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 13.48043, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.19021, qf2_loss: 0.19303, policy_loss: -44.21072, policy_entropy: 4.07037, alpha: 0.32826, time: 50.19902
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 13.77861, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.13388, qf2_loss: 0.13430, policy_loss: -44.60180, policy_entropy: 4.06996, alpha: 0.32086, time: 50.38729
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 12.41024, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.25295, qf2_loss: 0.25606, policy_loss: -44.98436, policy_entropy: 4.07477, alpha: 0.31364, time: 50.41939
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 23.29722, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.11330, qf2_loss: 0.11434, policy_loss: -45.35391, policy_entropy: 4.07396, alpha: 0.30659, time: 50.61551
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 18.48082, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.12669, qf2_loss: 0.12743, policy_loss: -45.72211, policy_entropy: 4.06819, alpha: 0.29970, time: 50.63246
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 22.92364, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.14550, qf2_loss: 0.14652, policy_loss: -46.06347, policy_entropy: 4.06338, alpha: 0.29298, time: 50.71848
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 22.14784, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.18779, qf2_loss: 0.19026, policy_loss: -46.41029, policy_entropy: 4.06148, alpha: 0.28642, time: 50.63920
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 14.29199, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.16058, qf2_loss: 0.16148, policy_loss: -46.72783, policy_entropy: 4.05814, alpha: 0.28002, time: 51.00476
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 22.21787, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.15390, qf2_loss: 0.15496, policy_loss: -47.03848, policy_entropy: 4.05521, alpha: 0.27376, time: 50.63179
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 30.04911, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.16390, qf2_loss: 0.16498, policy_loss: -47.33826, policy_entropy: 4.05000, alpha: 0.26765, time: 50.77452
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 10.96330, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.17042, qf2_loss: 0.17160, policy_loss: -47.62624, policy_entropy: 4.04492, alpha: 0.26168, time: 50.62780
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 25.99225, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.17471, qf2_loss: 0.17549, policy_loss: -47.89255, policy_entropy: 4.04323, alpha: 0.25585, time: 51.95722
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 25.23256, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.15520, qf2_loss: 0.15587, policy_loss: -48.16452, policy_entropy: 4.03600, alpha: 0.25016, time: 50.67948
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 14.98814, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.17067, qf2_loss: 0.17182, policy_loss: -48.40823, policy_entropy: 4.03241, alpha: 0.24459, time: 50.58904
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 13.47298, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.18509, qf2_loss: 0.18596, policy_loss: -48.63304, policy_entropy: 4.02866, alpha: 0.23916, time: 50.84261
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 15.27688, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.19351, qf2_loss: 0.19540, policy_loss: -48.85935, policy_entropy: 4.02283, alpha: 0.23385, time: 50.88006
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 14.40696, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.17742, qf2_loss: 0.17895, policy_loss: -49.06881, policy_entropy: 4.01882, alpha: 0.22866, time: 50.51188
[CW] eval: return: 26.18661, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 23.71912, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.16928, qf2_loss: 0.17016, policy_loss: -49.27922, policy_entropy: 4.01116, alpha: 0.22359, time: 50.75673
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 38.31997, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.15567, qf2_loss: 0.15625, policy_loss: -49.48521, policy_entropy: 3.99785, alpha: 0.21864, time: 50.56308
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 22.12470, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.21959, qf2_loss: 0.22150, policy_loss: -49.67655, policy_entropy: 3.98897, alpha: 0.21380, time: 50.54161
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 23.79212, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.18512, qf2_loss: 0.18635, policy_loss: -49.85066, policy_entropy: 3.98180, alpha: 0.20908, time: 50.60942
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 46.91457, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.18631, qf2_loss: 0.18756, policy_loss: -50.03668, policy_entropy: 3.96553, alpha: 0.20446, time: 50.65506
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 30.51485, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.22380, qf2_loss: 0.22555, policy_loss: -50.20886, policy_entropy: 3.95160, alpha: 0.19995, time: 50.40788
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 37.84086, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.20352, qf2_loss: 0.20514, policy_loss: -50.36946, policy_entropy: 3.94127, alpha: 0.19555, time: 50.49432
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 52.94833, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.21111, qf2_loss: 0.21295, policy_loss: -50.52932, policy_entropy: 3.91240, alpha: 0.19124, time: 52.18482
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 16.92008, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.21613, qf2_loss: 0.21848, policy_loss: -50.65218, policy_entropy: 3.90284, alpha: 0.18704, time: 50.37002
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 52.94621, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.20730, qf2_loss: 0.20866, policy_loss: -50.81542, policy_entropy: 3.85382, alpha: 0.18294, time: 50.40494
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 41.74125, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.24078, qf2_loss: 0.24444, policy_loss: -50.95302, policy_entropy: 3.80191, alpha: 0.17895, time: 50.52456
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 38.73134, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.27044, qf2_loss: 0.27357, policy_loss: -51.06422, policy_entropy: 3.78707, alpha: 0.17505, time: 50.25109
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 78.30187, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.22812, qf2_loss: 0.23221, policy_loss: -51.21866, policy_entropy: 3.69249, alpha: 0.17126, time: 50.55268
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 32.33111, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.22559, qf2_loss: 0.22893, policy_loss: -51.35263, policy_entropy: 3.63388, alpha: 0.16756, time: 50.41173
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 74.98674, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.25544, qf2_loss: 0.26018, policy_loss: -51.45791, policy_entropy: 3.56448, alpha: 0.16397, time: 50.45518
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 107.32327, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.32588, qf2_loss: 0.33039, policy_loss: -51.62659, policy_entropy: 3.45224, alpha: 0.16046, time: 50.55396
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 65.27011, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.26210, qf2_loss: 0.26693, policy_loss: -51.78011, policy_entropy: 3.22499, alpha: 0.15709, time: 50.47199
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 87.79524, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.31021, qf2_loss: 0.31421, policy_loss: -51.93206, policy_entropy: 3.05375, alpha: 0.15384, time: 50.36801
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 43.92646, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.33609, qf2_loss: 0.33871, policy_loss: -52.06230, policy_entropy: 2.85450, alpha: 0.15071, time: 50.36854
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 103.15736, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.33399, qf2_loss: 0.33587, policy_loss: -52.26479, policy_entropy: 2.61253, alpha: 0.14768, time: 52.09550
[CW] eval: return: 66.10982, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 43.68173, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.35038, qf2_loss: 0.35419, policy_loss: -52.41566, policy_entropy: 2.28539, alpha: 0.14480, time: 50.55428
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 106.32325, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.42487, qf2_loss: 0.42936, policy_loss: -52.65079, policy_entropy: 2.06535, alpha: 0.14204, time: 50.73595
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 45.14682, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.34596, qf2_loss: 0.34989, policy_loss: -52.85347, policy_entropy: 1.80533, alpha: 0.13938, time: 50.50944
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 73.05648, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.36725, qf2_loss: 0.37103, policy_loss: -53.12468, policy_entropy: 1.61885, alpha: 0.13682, time: 50.36710
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 93.06904, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.42992, qf2_loss: 0.43346, policy_loss: -53.39861, policy_entropy: 1.50559, alpha: 0.13431, time: 50.34476
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 49.23166, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.37359, qf2_loss: 0.37633, policy_loss: -53.63070, policy_entropy: 1.38398, alpha: 0.13187, time: 50.28645
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 60.55406, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.40466, qf2_loss: 0.40830, policy_loss: -53.79955, policy_entropy: 1.28845, alpha: 0.12947, time: 50.30141
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 129.91285, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.44077, qf2_loss: 0.44386, policy_loss: -54.02065, policy_entropy: 1.25326, alpha: 0.12711, time: 50.39535
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 53.35839, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.40352, qf2_loss: 0.40637, policy_loss: -54.24935, policy_entropy: 1.16907, alpha: 0.12477, time: 50.25418
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 93.00097, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.45133, qf2_loss: 0.45315, policy_loss: -54.48379, policy_entropy: 1.13941, alpha: 0.12248, time: 50.33812
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 98.90290, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.50758, qf2_loss: 0.51154, policy_loss: -54.70573, policy_entropy: 0.97707, alpha: 0.12022, time: 50.29642
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 55.92273, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.43850, qf2_loss: 0.44054, policy_loss: -54.93204, policy_entropy: 0.71074, alpha: 0.11804, time: 50.46729
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 40.76227, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.39233, qf2_loss: 0.39552, policy_loss: -55.09731, policy_entropy: 0.65558, alpha: 0.11593, time: 50.50627
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 61.80891, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.49707, qf2_loss: 0.50229, policy_loss: -55.33041, policy_entropy: 0.52379, alpha: 0.11386, time: 50.51517
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 59.72806, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.43526, qf2_loss: 0.43940, policy_loss: -55.58419, policy_entropy: 0.35152, alpha: 0.11183, time: 50.70466
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 26.20906, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.39966, qf2_loss: 0.40425, policy_loss: -55.77839, policy_entropy: 0.25637, alpha: 0.10987, time: 50.68382
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 68.98308, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.45931, qf2_loss: 0.46454, policy_loss: -56.03922, policy_entropy: 0.17697, alpha: 0.10793, time: 50.77713
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 87.73593, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.45484, qf2_loss: 0.45929, policy_loss: -56.26784, policy_entropy: 0.01432, alpha: 0.10603, time: 50.52567
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 63.45370, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.44116, qf2_loss: 0.44732, policy_loss: -56.48249, policy_entropy: -0.09750, alpha: 0.10418, time: 50.50978
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 116.85751, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.43573, qf2_loss: 0.44081, policy_loss: -56.71412, policy_entropy: -0.12440, alpha: 0.10236, time: 50.63413
[CW] eval: return: 92.43114, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 58.48352, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.43987, qf2_loss: 0.44565, policy_loss: -56.95924, policy_entropy: -0.15231, alpha: 0.10056, time: 50.55048
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 66.72918, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.45591, qf2_loss: 0.46125, policy_loss: -57.20593, policy_entropy: -0.23708, alpha: 0.09877, time: 50.60280
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 107.96898, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.46215, qf2_loss: 0.46809, policy_loss: -57.33832, policy_entropy: -0.20406, alpha: 0.09700, time: 50.26944
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 123.46041, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.43963, qf2_loss: 0.44266, policy_loss: -57.55204, policy_entropy: -0.19481, alpha: 0.09524, time: 50.16244
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 57.26844, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.43051, qf2_loss: 0.43444, policy_loss: -57.82336, policy_entropy: -0.29303, alpha: 0.09350, time: 50.64849
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 134.39675, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.52852, qf2_loss: 0.53596, policy_loss: -58.00184, policy_entropy: -0.21408, alpha: 0.09177, time: 50.21893
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 74.74040, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.47048, qf2_loss: 0.47195, policy_loss: -58.24461, policy_entropy: -0.28023, alpha: 0.09005, time: 50.60831
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 128.83487, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.45590, qf2_loss: 0.45964, policy_loss: -58.41784, policy_entropy: -0.32401, alpha: 0.08836, time: 50.51960
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 86.82674, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.55084, qf2_loss: 0.55985, policy_loss: -58.63157, policy_entropy: -0.32271, alpha: 0.08669, time: 50.60194
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 54.88303, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.49260, qf2_loss: 0.49665, policy_loss: -58.82108, policy_entropy: -0.33936, alpha: 0.08504, time: 50.46776
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 63.14611, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.50649, qf2_loss: 0.51065, policy_loss: -58.92359, policy_entropy: -0.29065, alpha: 0.08340, time: 50.46024
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 51.08113, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.41272, qf2_loss: 0.41583, policy_loss: -59.02560, policy_entropy: -0.26364, alpha: 0.08177, time: 50.23615
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 67.94549, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.38888, qf2_loss: 0.39351, policy_loss: -59.17496, policy_entropy: -0.22000, alpha: 0.08013, time: 50.20236
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 141.74311, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.43929, qf2_loss: 0.44233, policy_loss: -59.46209, policy_entropy: -0.27385, alpha: 0.07853, time: 50.34454
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 65.70131, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.44057, qf2_loss: 0.44064, policy_loss: -59.55005, policy_entropy: -0.19412, alpha: 0.07695, time: 50.44876
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 46.73893, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.50847, qf2_loss: 0.50957, policy_loss: -59.53486, policy_entropy: -0.12848, alpha: 0.07537, time: 50.26152
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 54.36109, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.42799, qf2_loss: 0.42975, policy_loss: -59.66584, policy_entropy: -0.15284, alpha: 0.07380, time: 50.42490
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 85.93423, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.44438, qf2_loss: 0.44439, policy_loss: -59.79528, policy_entropy: -0.20837, alpha: 0.07227, time: 50.11544
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 89.92676, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.56551, qf2_loss: 0.56555, policy_loss: -59.82590, policy_entropy: -0.21875, alpha: 0.07078, time: 50.39983
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 94.97276, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.47343, qf2_loss: 0.47422, policy_loss: -60.00798, policy_entropy: -0.31896, alpha: 0.06932, time: 51.84693
[CW] eval: return: 69.99165, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 50.83468, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.39641, qf2_loss: 0.39750, policy_loss: -60.08929, policy_entropy: -0.45117, alpha: 0.06791, time: 50.54305
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 71.61882, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.38972, qf2_loss: 0.38965, policy_loss: -60.09597, policy_entropy: -0.42604, alpha: 0.06653, time: 50.66489
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 47.30154, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.39167, qf2_loss: 0.38970, policy_loss: -59.99962, policy_entropy: -0.69547, alpha: 0.06520, time: 50.11426
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 66.91629, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.40661, qf2_loss: 0.40662, policy_loss: -60.06088, policy_entropy: -0.66423, alpha: 0.06393, time: 51.14541
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 34.69598, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.49991, qf2_loss: 0.49692, policy_loss: -60.06988, policy_entropy: -0.64571, alpha: 0.06266, time: 50.26408
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 31.08067, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.43459, qf2_loss: 0.43261, policy_loss: -60.12050, policy_entropy: -0.69678, alpha: 0.06141, time: 50.21041
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 94.40045, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.43469, qf2_loss: 0.43305, policy_loss: -60.19531, policy_entropy: -0.68133, alpha: 0.06017, time: 50.67997
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 90.34149, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.61791, qf2_loss: 0.62133, policy_loss: -60.25875, policy_entropy: -0.80072, alpha: 0.05897, time: 50.34585
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 105.71119, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.41389, qf2_loss: 0.40794, policy_loss: -60.31137, policy_entropy: -0.89519, alpha: 0.05781, time: 50.30293
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 89.96875, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.49689, qf2_loss: 0.49238, policy_loss: -60.36529, policy_entropy: -0.78707, alpha: 0.05665, time: 50.77374
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 38.57878, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.45392, qf2_loss: 0.44973, policy_loss: -60.30351, policy_entropy: -0.83660, alpha: 0.05551, time: 50.73682
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 60.46776, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.45159, qf2_loss: 0.44759, policy_loss: -60.34599, policy_entropy: -1.07285, alpha: 0.05441, time: 51.23110
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 50.43112, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.48639, qf2_loss: 0.48395, policy_loss: -60.13326, policy_entropy: -1.07026, alpha: 0.05336, time: 50.30086
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 48.96513, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.45655, qf2_loss: 0.45276, policy_loss: -60.39869, policy_entropy: -1.17989, alpha: 0.05231, time: 50.33893
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 39.44655, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.49921, qf2_loss: 0.49483, policy_loss: -60.47789, policy_entropy: -1.13618, alpha: 0.05130, time: 50.35048
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 48.19644, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.61654, qf2_loss: 0.61214, policy_loss: -60.24251, policy_entropy: -1.03837, alpha: 0.05028, time: 50.28920
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 94.81317, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.60863, qf2_loss: 0.60580, policy_loss: -60.28681, policy_entropy: -1.02208, alpha: 0.04925, time: 50.35552
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 72.03012, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.47962, qf2_loss: 0.47405, policy_loss: -60.34931, policy_entropy: -1.33397, alpha: 0.04828, time: 50.73450
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 120.17398, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.54756, qf2_loss: 0.54036, policy_loss: -60.35543, policy_entropy: -1.41903, alpha: 0.04734, time: 50.36473
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 43.14994, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.56888, qf2_loss: 0.56411, policy_loss: -60.38357, policy_entropy: -1.50844, alpha: 0.04645, time: 50.43281
[CW] eval: return: 84.85341, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 51.47594, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.54588, qf2_loss: 0.53772, policy_loss: -60.40932, policy_entropy: -1.53706, alpha: 0.04557, time: 50.80148
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 101.79392, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.61066, qf2_loss: 0.60780, policy_loss: -60.48727, policy_entropy: -1.73186, alpha: 0.04471, time: 50.38675
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 49.70397, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.52360, qf2_loss: 0.51783, policy_loss: -60.75288, policy_entropy: -1.72989, alpha: 0.04389, time: 50.39180
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 89.70500, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.73075, qf2_loss: 0.72607, policy_loss: -60.74936, policy_entropy: -1.88824, alpha: 0.04308, time: 50.79797
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 77.33986, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.52493, qf2_loss: 0.51738, policy_loss: -60.63233, policy_entropy: -1.75699, alpha: 0.04229, time: 50.62530
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 129.07297, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.51058, qf2_loss: 0.50341, policy_loss: -60.75356, policy_entropy: -1.75314, alpha: 0.04148, time: 50.84157
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 112.30546, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.65460, qf2_loss: 0.64580, policy_loss: -60.76900, policy_entropy: -1.81336, alpha: 0.04069, time: 50.38410
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 200.20312, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.65944, qf2_loss: 0.65509, policy_loss: -60.66750, policy_entropy: -1.72157, alpha: 0.03991, time: 50.49599
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 170.93912, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.65036, qf2_loss: 0.64461, policy_loss: -60.97801, policy_entropy: -1.89465, alpha: 0.03913, time: 50.77025
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 80.05005, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.54981, qf2_loss: 0.54241, policy_loss: -61.00662, policy_entropy: -1.94429, alpha: 0.03838, time: 50.69941
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 253.70878, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.59815, qf2_loss: 0.59205, policy_loss: -61.11190, policy_entropy: -2.07785, alpha: 0.03765, time: 50.85941
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 154.57073, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.58482, qf2_loss: 0.57623, policy_loss: -61.07081, policy_entropy: -2.18474, alpha: 0.03697, time: 50.57279
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 135.76826, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.60644, qf2_loss: 0.59879, policy_loss: -61.56124, policy_entropy: -2.32674, alpha: 0.03630, time: 52.00799
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 150.26525, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.87892, qf2_loss: 0.87913, policy_loss: -61.53538, policy_entropy: -2.46203, alpha: 0.03565, time: 50.75109
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 84.40802, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.56924, qf2_loss: 0.56056, policy_loss: -61.58374, policy_entropy: -2.52852, alpha: 0.03503, time: 50.68458
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 129.52123, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.69911, qf2_loss: 0.69563, policy_loss: -61.96479, policy_entropy: -2.77956, alpha: 0.03444, time: 50.68585
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 197.61551, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.64110, qf2_loss: 0.63393, policy_loss: -61.69180, policy_entropy: -2.69283, alpha: 0.03387, time: 50.66074
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 39.94486, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.60341, qf2_loss: 0.59614, policy_loss: -61.86740, policy_entropy: -2.82447, alpha: 0.03329, time: 50.72947
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 192.30349, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.62614, qf2_loss: 0.62077, policy_loss: -61.93015, policy_entropy: -2.95013, alpha: 0.03274, time: 50.56667
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 240.09055, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.70962, qf2_loss: 0.70172, policy_loss: -62.25935, policy_entropy: -3.18908, alpha: 0.03222, time: 50.59244
[CW] eval: return: 187.09227, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 192.04007, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.62035, qf2_loss: 0.61329, policy_loss: -62.23089, policy_entropy: -3.32988, alpha: 0.03173, time: 50.81685
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 250.33809, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.60577, qf2_loss: 0.60121, policy_loss: -62.31185, policy_entropy: -3.54096, alpha: 0.03128, time: 50.65685
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 238.72070, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.63631, qf2_loss: 0.62899, policy_loss: -62.43021, policy_entropy: -3.27030, alpha: 0.03081, time: 50.33067
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 149.65541, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.60382, qf2_loss: 0.59676, policy_loss: -62.71629, policy_entropy: -3.37447, alpha: 0.03032, time: 50.19857
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 38.20709, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.66243, qf2_loss: 0.65861, policy_loss: -62.37472, policy_entropy: -3.32105, alpha: 0.02984, time: 50.24675
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 115.49372, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.85196, qf2_loss: 0.84710, policy_loss: -62.66259, policy_entropy: -3.57679, alpha: 0.02936, time: 50.44079
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 223.03001, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.69638, qf2_loss: 0.68959, policy_loss: -62.76952, policy_entropy: -4.10326, alpha: 0.02896, time: 50.82119
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 222.81693, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.69003, qf2_loss: 0.68381, policy_loss: -63.04867, policy_entropy: -4.31092, alpha: 0.02863, time: 50.88235
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 241.73632, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.69379, qf2_loss: 0.68863, policy_loss: -63.03103, policy_entropy: -4.26491, alpha: 0.02830, time: 50.32213
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 163.70839, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.91591, qf2_loss: 0.90949, policy_loss: -63.31078, policy_entropy: -4.18806, alpha: 0.02797, time: 50.38700
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 212.90165, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.73730, qf2_loss: 0.73016, policy_loss: -63.63669, policy_entropy: -4.11089, alpha: 0.02760, time: 50.72902
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 203.19357, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.78189, qf2_loss: 0.77029, policy_loss: -63.63300, policy_entropy: -4.03495, alpha: 0.02721, time: 50.76444
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 153.88781, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.75070, qf2_loss: 0.74130, policy_loss: -63.47682, policy_entropy: -4.06063, alpha: 0.02683, time: 50.13424
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 204.51152, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.86656, qf2_loss: 0.85919, policy_loss: -63.71843, policy_entropy: -4.12756, alpha: 0.02643, time: 50.43696
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 148.32589, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.74033, qf2_loss: 0.73148, policy_loss: -63.74890, policy_entropy: -4.26598, alpha: 0.02606, time: 50.22842
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 264.21021, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.72136, qf2_loss: 0.71445, policy_loss: -63.88825, policy_entropy: -4.02232, alpha: 0.02568, time: 50.21383
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 253.83075, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.96198, qf2_loss: 0.95232, policy_loss: -63.93476, policy_entropy: -3.71651, alpha: 0.02524, time: 50.21903
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 261.06115, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.90263, qf2_loss: 0.90060, policy_loss: -64.16168, policy_entropy: -4.00966, alpha: 0.02479, time: 50.29733
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 212.75353, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.79386, qf2_loss: 0.78534, policy_loss: -64.19199, policy_entropy: -4.27740, alpha: 0.02441, time: 50.39975
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 163.73611, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.79956, qf2_loss: 0.79083, policy_loss: -64.40092, policy_entropy: -4.45141, alpha: 0.02405, time: 50.23785
[CW] eval: return: 195.28634, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 256.99004, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.83288, qf2_loss: 0.82481, policy_loss: -64.71795, policy_entropy: -4.62698, alpha: 0.02375, time: 50.66035
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 232.76896, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.97129, qf2_loss: 0.96159, policy_loss: -64.67023, policy_entropy: -4.62090, alpha: 0.02345, time: 50.42285
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 136.80612, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.96804, qf2_loss: 0.95886, policy_loss: -64.63278, policy_entropy: -4.91557, alpha: 0.02318, time: 50.08338
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 292.18851, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.97825, qf2_loss: 0.96800, policy_loss: -64.91039, policy_entropy: -4.83765, alpha: 0.02293, time: 50.21093
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 36.53263, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.87589, qf2_loss: 0.86340, policy_loss: -64.96742, policy_entropy: -4.82733, alpha: 0.02268, time: 50.28256
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 176.13284, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.92114, qf2_loss: 0.91603, policy_loss: -64.94491, policy_entropy: -4.75505, alpha: 0.02238, time: 50.48082
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 240.40967, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 1.12070, qf2_loss: 1.10715, policy_loss: -65.13599, policy_entropy: -5.20265, alpha: 0.02214, time: 50.51369
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 184.30245, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 1.04327, qf2_loss: 1.03473, policy_loss: -65.69687, policy_entropy: -5.34598, alpha: 0.02197, time: 50.49276
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 308.29930, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 1.18733, qf2_loss: 1.18232, policy_loss: -65.45866, policy_entropy: -5.50138, alpha: 0.02180, time: 50.09310
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 316.23296, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 1.01833, qf2_loss: 1.01429, policy_loss: -65.48328, policy_entropy: -5.95848, alpha: 0.02175, time: 49.96421
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 292.06144, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 1.01967, qf2_loss: 1.01006, policy_loss: -66.06536, policy_entropy: -6.03687, alpha: 0.02174, time: 53.88111
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 158.82530, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.99745, qf2_loss: 0.99069, policy_loss: -65.82496, policy_entropy: -6.19904, alpha: 0.02178, time: 49.89024
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 26.64349, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 1.14483, qf2_loss: 1.14140, policy_loss: -66.43630, policy_entropy: -5.86718, alpha: 0.02180, time: 49.86748
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 317.18748, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 1.33094, qf2_loss: 1.30932, policy_loss: -66.50643, policy_entropy: -5.59798, alpha: 0.02171, time: 50.26453
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 278.18610, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 1.28417, qf2_loss: 1.27644, policy_loss: -66.66979, policy_entropy: -5.68293, alpha: 0.02159, time: 50.44949
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 320.28778, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 1.15205, qf2_loss: 1.14954, policy_loss: -66.70298, policy_entropy: -5.85361, alpha: 0.02153, time: 50.07521
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 276.68258, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 1.07650, qf2_loss: 1.07845, policy_loss: -66.93751, policy_entropy: -5.70971, alpha: 0.02145, time: 51.04496
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 292.65223, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 1.11191, qf2_loss: 1.09998, policy_loss: -67.09841, policy_entropy: -5.95722, alpha: 0.02138, time: 54.36698
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 298.04536, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 1.22892, qf2_loss: 1.22593, policy_loss: -67.54109, policy_entropy: -6.03075, alpha: 0.02137, time: 50.81986
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 290.50320, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 1.13719, qf2_loss: 1.13370, policy_loss: -67.60923, policy_entropy: -5.98299, alpha: 0.02138, time: 50.29774
[CW] eval: return: 209.86704, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 337.40053, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 1.40018, qf2_loss: 1.39880, policy_loss: -67.66229, policy_entropy: -5.96718, alpha: 0.02138, time: 50.56882
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 323.49351, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 1.40654, qf2_loss: 1.40128, policy_loss: -68.26272, policy_entropy: -5.92532, alpha: 0.02136, time: 50.40704
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 83.14989, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 1.12539, qf2_loss: 1.12128, policy_loss: -68.18424, policy_entropy: -6.03439, alpha: 0.02132, time: 50.14805
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 162.74507, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 1.25241, qf2_loss: 1.24195, policy_loss: -68.19112, policy_entropy: -5.90214, alpha: 0.02132, time: 50.17980
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 312.39150, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 1.25835, qf2_loss: 1.25254, policy_loss: -68.18476, policy_entropy: -5.93290, alpha: 0.02130, time: 50.76213
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 217.45752, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 1.35529, qf2_loss: 1.34971, policy_loss: -68.75059, policy_entropy: -5.79135, alpha: 0.02124, time: 50.89856
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 253.15128, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 1.30942, qf2_loss: 1.29675, policy_loss: -68.64281, policy_entropy: -5.70691, alpha: 0.02108, time: 50.90294
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 174.61998, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 1.52926, qf2_loss: 1.53067, policy_loss: -69.14527, policy_entropy: -5.85406, alpha: 0.02097, time: 50.74133
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 311.30355, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 1.30279, qf2_loss: 1.29682, policy_loss: -69.03060, policy_entropy: -5.84324, alpha: 0.02087, time: 50.49471
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 184.69047, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 1.38523, qf2_loss: 1.37897, policy_loss: -69.38761, policy_entropy: -6.14376, alpha: 0.02086, time: 50.75852
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 322.32708, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 1.75567, qf2_loss: 1.74032, policy_loss: -69.31482, policy_entropy: -6.18562, alpha: 0.02096, time: 50.73979
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 87.02651, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 1.45885, qf2_loss: 1.46033, policy_loss: -69.39293, policy_entropy: -6.24090, alpha: 0.02110, time: 50.77874
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 350.72347, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 1.35942, qf2_loss: 1.35843, policy_loss: -70.06737, policy_entropy: -6.03538, alpha: 0.02119, time: 50.38236
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 302.87501, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 1.48013, qf2_loss: 1.47005, policy_loss: -69.70343, policy_entropy: -6.17141, alpha: 0.02125, time: 50.55363
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 310.39840, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 1.51938, qf2_loss: 1.50818, policy_loss: -70.30183, policy_entropy: -6.21255, alpha: 0.02141, time: 50.73292
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 222.03029, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 1.49368, qf2_loss: 1.48707, policy_loss: -70.22423, policy_entropy: -6.11395, alpha: 0.02152, time: 50.47328
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 150.84970, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 1.68383, qf2_loss: 1.67603, policy_loss: -70.31504, policy_entropy: -6.10524, alpha: 0.02161, time: 50.71244
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 323.88121, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 1.59080, qf2_loss: 1.58077, policy_loss: -70.99974, policy_entropy: -6.09375, alpha: 0.02173, time: 50.60766
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 318.77079, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 1.49971, qf2_loss: 1.49680, policy_loss: -70.92594, policy_entropy: -6.07268, alpha: 0.02178, time: 50.72445
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 206.86130, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 1.68800, qf2_loss: 1.66909, policy_loss: -71.34228, policy_entropy: -5.85960, alpha: 0.02173, time: 50.59995
[CW] eval: return: 228.25692, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 336.62028, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 1.66725, qf2_loss: 1.66265, policy_loss: -71.16279, policy_entropy: -5.90532, alpha: 0.02165, time: 50.61500
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 210.47309, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 1.98816, qf2_loss: 1.98770, policy_loss: -71.93351, policy_entropy: -6.07880, alpha: 0.02160, time: 50.54944
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 331.83661, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 1.69983, qf2_loss: 1.69207, policy_loss: -71.47661, policy_entropy: -5.88870, alpha: 0.02164, time: 50.82056
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 318.40981, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 1.46860, qf2_loss: 1.46195, policy_loss: -71.82239, policy_entropy: -5.84673, alpha: 0.02148, time: 50.39512
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 280.23421, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 1.68560, qf2_loss: 1.67146, policy_loss: -72.27916, policy_entropy: -5.89427, alpha: 0.02139, time: 50.49479
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 326.95346, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 1.85532, qf2_loss: 1.85462, policy_loss: -72.68484, policy_entropy: -6.02561, alpha: 0.02135, time: 50.56549
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 181.82959, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 1.55993, qf2_loss: 1.56548, policy_loss: -72.75992, policy_entropy: -5.89754, alpha: 0.02131, time: 50.50905
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 363.46969, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 1.78971, qf2_loss: 1.76703, policy_loss: -72.46438, policy_entropy: -5.73909, alpha: 0.02113, time: 50.74732
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 305.35570, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 2.35196, qf2_loss: 2.34259, policy_loss: -72.93393, policy_entropy: -6.13080, alpha: 0.02099, time: 50.70331
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 412.11816, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 2.06567, qf2_loss: 2.07787, policy_loss: -72.96187, policy_entropy: -6.25740, alpha: 0.02122, time: 50.82308
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 231.47068, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 1.67296, qf2_loss: 1.66370, policy_loss: -73.00903, policy_entropy: -6.23159, alpha: 0.02148, time: 50.36468
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 198.21020, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 1.60647, qf2_loss: 1.58538, policy_loss: -73.25350, policy_entropy: -6.09693, alpha: 0.02171, time: 50.39961
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 365.00908, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 1.58060, qf2_loss: 1.58884, policy_loss: -73.36483, policy_entropy: -5.61691, alpha: 0.02158, time: 50.26482
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 282.60690, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 1.74389, qf2_loss: 1.73487, policy_loss: -73.92349, policy_entropy: -5.73897, alpha: 0.02117, time: 50.29052
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 119.95573, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 1.62343, qf2_loss: 1.62752, policy_loss: -74.11601, policy_entropy: -5.90266, alpha: 0.02097, time: 50.47466
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 127.22114, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 1.81038, qf2_loss: 1.80905, policy_loss: -73.87537, policy_entropy: -5.72094, alpha: 0.02080, time: 50.32640
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 312.87475, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 1.78171, qf2_loss: 1.78064, policy_loss: -73.96896, policy_entropy: -5.83472, alpha: 0.02051, time: 50.29590
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 314.01878, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 1.77886, qf2_loss: 1.77697, policy_loss: -74.45206, policy_entropy: -6.09090, alpha: 0.02046, time: 50.42564
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 89.11259, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 3.30794, qf2_loss: 3.31199, policy_loss: -73.67483, policy_entropy: -5.93797, alpha: 0.02053, time: 50.52883
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 261.21452, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 2.24181, qf2_loss: 2.25170, policy_loss: -74.03465, policy_entropy: -5.98265, alpha: 0.02044, time: 50.88716
[CW] eval: return: 242.09631, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 290.50772, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 1.62348, qf2_loss: 1.62088, policy_loss: -74.43657, policy_entropy: -6.10314, alpha: 0.02047, time: 50.80977
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 134.35635, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 1.56999, qf2_loss: 1.57089, policy_loss: -74.96742, policy_entropy: -6.14982, alpha: 0.02064, time: 50.70670
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 164.68669, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 1.85077, qf2_loss: 1.85236, policy_loss: -74.79997, policy_entropy: -5.99303, alpha: 0.02068, time: 50.56584
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 303.12187, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 1.87223, qf2_loss: 1.85459, policy_loss: -75.13944, policy_entropy: -5.97509, alpha: 0.02077, time: 50.45255
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 280.00798, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 2.46170, qf2_loss: 2.47217, policy_loss: -74.89298, policy_entropy: -5.95344, alpha: 0.02062, time: 50.63152
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 310.78034, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 2.33488, qf2_loss: 2.32909, policy_loss: -75.19714, policy_entropy: -6.03876, alpha: 0.02064, time: 50.46506
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 311.36279, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 2.04834, qf2_loss: 2.05330, policy_loss: -75.28167, policy_entropy: -6.01814, alpha: 0.02068, time: 50.34128
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 42.61096, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 2.41885, qf2_loss: 2.42681, policy_loss: -75.88019, policy_entropy: -5.96568, alpha: 0.02065, time: 50.78799
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 144.70777, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 1.92434, qf2_loss: 1.93914, policy_loss: -75.26889, policy_entropy: -5.95585, alpha: 0.02061, time: 50.38213
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 305.20191, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 2.02997, qf2_loss: 2.02094, policy_loss: -75.19715, policy_entropy: -5.98061, alpha: 0.02059, time: 50.51732
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 303.84753, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 1.89883, qf2_loss: 1.91534, policy_loss: -75.50219, policy_entropy: -6.02983, alpha: 0.02060, time: 51.80196
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 296.30299, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 2.14594, qf2_loss: 2.14907, policy_loss: -76.21235, policy_entropy: -6.02499, alpha: 0.02059, time: 49.96233
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 357.87888, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 2.29242, qf2_loss: 2.29276, policy_loss: -76.75106, policy_entropy: -6.14225, alpha: 0.02071, time: 50.10022
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 247.74014, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 2.92893, qf2_loss: 2.94245, policy_loss: -75.78542, policy_entropy: -6.11792, alpha: 0.02092, time: 50.06543
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 156.09164, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 2.45068, qf2_loss: 2.44469, policy_loss: -75.95730, policy_entropy: -5.74350, alpha: 0.02088, time: 50.17026
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 314.48614, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 2.02982, qf2_loss: 2.02935, policy_loss: -76.57850, policy_entropy: -5.96095, alpha: 0.02059, time: 49.98698
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 386.72646, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 2.07181, qf2_loss: 2.10253, policy_loss: -76.36071, policy_entropy: -6.23373, alpha: 0.02070, time: 50.49213
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 141.63533, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 1.90619, qf2_loss: 1.89503, policy_loss: -76.55772, policy_entropy: -6.33572, alpha: 0.02105, time: 51.41728
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 350.34612, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 1.87526, qf2_loss: 1.87207, policy_loss: -76.95657, policy_entropy: -6.11576, alpha: 0.02139, time: 49.88539
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 332.45011, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 1.89427, qf2_loss: 1.90249, policy_loss: -76.78988, policy_entropy: -5.85351, alpha: 0.02135, time: 50.13413
[CW] eval: return: 262.35327, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 215.62653, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 2.57319, qf2_loss: 2.60172, policy_loss: -76.70053, policy_entropy: -5.82700, alpha: 0.02120, time: 51.59176
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 336.89220, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 2.17172, qf2_loss: 2.16702, policy_loss: -77.36956, policy_entropy: -6.35675, alpha: 0.02121, time: 50.41315
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 347.52208, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 2.47531, qf2_loss: 2.48937, policy_loss: -77.26363, policy_entropy: -6.19841, alpha: 0.02161, time: 50.37654
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 201.26086, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 4.69566, qf2_loss: 4.70095, policy_loss: -77.19753, policy_entropy: -6.11483, alpha: 0.02177, time: 50.11336
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 332.59250, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 4.16259, qf2_loss: 4.15723, policy_loss: -77.25445, policy_entropy: -6.64670, alpha: 0.02225, time: 50.59857
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 32.68754, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 2.54568, qf2_loss: 2.56361, policy_loss: -77.38773, policy_entropy: -6.47986, alpha: 0.02300, time: 50.28197
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 337.67633, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 2.54516, qf2_loss: 2.56376, policy_loss: -77.49144, policy_entropy: -5.62246, alpha: 0.02305, time: 50.86505
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 126.39111, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 2.70317, qf2_loss: 2.70802, policy_loss: -77.82857, policy_entropy: -6.11797, alpha: 0.02278, time: 50.34355
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 305.36737, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 2.29179, qf2_loss: 2.30238, policy_loss: -77.83432, policy_entropy: -6.30560, alpha: 0.02312, time: 50.62103
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 313.57773, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 1.96188, qf2_loss: 1.95936, policy_loss: -78.83832, policy_entropy: -5.99382, alpha: 0.02339, time: 50.29955
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 336.38962, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 2.21009, qf2_loss: 2.21931, policy_loss: -78.16402, policy_entropy: -5.80638, alpha: 0.02323, time: 50.53075
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 362.11901, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 2.50102, qf2_loss: 2.51487, policy_loss: -77.94360, policy_entropy: -5.73210, alpha: 0.02290, time: 50.29727
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 192.64159, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 2.17404, qf2_loss: 2.18312, policy_loss: -79.02693, policy_entropy: -6.15685, alpha: 0.02282, time: 50.30606
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 334.03194, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 2.92074, qf2_loss: 2.92277, policy_loss: -79.16775, policy_entropy: -6.15909, alpha: 0.02301, time: 50.46783
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 211.42825, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 2.25349, qf2_loss: 2.26257, policy_loss: -79.56007, policy_entropy: -6.26746, alpha: 0.02331, time: 50.72333
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 338.80306, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 2.01520, qf2_loss: 2.03266, policy_loss: -79.03689, policy_entropy: -5.91843, alpha: 0.02345, time: 50.52555
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 254.78226, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 2.35995, qf2_loss: 2.37151, policy_loss: -79.63993, policy_entropy: -6.02917, alpha: 0.02343, time: 50.23975
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 288.59767, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 3.08209, qf2_loss: 3.08658, policy_loss: -79.85340, policy_entropy: -6.03118, alpha: 0.02346, time: 50.48184
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 325.30207, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 2.55435, qf2_loss: 2.55599, policy_loss: -79.60626, policy_entropy: -5.97350, alpha: 0.02350, time: 50.34467
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 80.88596, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 2.62859, qf2_loss: 2.62311, policy_loss: -79.62221, policy_entropy: -5.98693, alpha: 0.02347, time: 50.35563
[CW] eval: return: 265.87511, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 345.62598, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 2.74494, qf2_loss: 2.71904, policy_loss: -79.39562, policy_entropy: -5.89426, alpha: 0.02338, time: 50.59710
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 278.51825, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 2.27934, qf2_loss: 2.28693, policy_loss: -79.97321, policy_entropy: -6.23859, alpha: 0.02345, time: 50.29159
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 360.23540, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 2.59003, qf2_loss: 2.58368, policy_loss: -79.54815, policy_entropy: -6.00692, alpha: 0.02368, time: 50.43494
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 322.66853, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 3.76789, qf2_loss: 3.77575, policy_loss: -80.48671, policy_entropy: -5.79143, alpha: 0.02350, time: 50.64647
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 218.46237, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 2.71478, qf2_loss: 2.70845, policy_loss: -80.04990, policy_entropy: -5.99541, alpha: 0.02331, time: 50.49014
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 196.37506, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 2.50641, qf2_loss: 2.51315, policy_loss: -79.97909, policy_entropy: -6.05713, alpha: 0.02331, time: 50.75196
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 22.57933, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 3.48144, qf2_loss: 3.48856, policy_loss: -79.78299, policy_entropy: -6.49829, alpha: 0.02364, time: 50.72238
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 251.93904, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 2.62815, qf2_loss: 2.61153, policy_loss: -80.44328, policy_entropy: -6.15979, alpha: 0.02434, time: 50.67647
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 238.65486, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 2.31748, qf2_loss: 2.31626, policy_loss: -80.36299, policy_entropy: -5.96436, alpha: 0.02442, time: 50.64124
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 262.34726, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 2.88994, qf2_loss: 2.88208, policy_loss: -81.22649, policy_entropy: -6.19577, alpha: 0.02454, time: 50.35326
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 245.30079, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 2.62213, qf2_loss: 2.61248, policy_loss: -80.36334, policy_entropy: -5.87695, alpha: 0.02464, time: 50.50446
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 349.37748, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 3.47996, qf2_loss: 3.47209, policy_loss: -80.71362, policy_entropy: -5.77685, alpha: 0.02430, time: 51.58500
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 223.72178, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 2.39452, qf2_loss: 2.41105, policy_loss: -81.83871, policy_entropy: -5.98987, alpha: 0.02413, time: 50.25558
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 322.40511, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 3.73173, qf2_loss: 3.70256, policy_loss: -81.15220, policy_entropy: -6.09608, alpha: 0.02417, time: 50.70461
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 236.51006, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 2.84307, qf2_loss: 2.79101, policy_loss: -81.11710, policy_entropy: -6.07080, alpha: 0.02437, time: 50.30001
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 316.71631, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 2.80934, qf2_loss: 2.78578, policy_loss: -82.49512, policy_entropy: -6.31332, alpha: 0.02462, time: 50.61972
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 350.66513, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 2.48113, qf2_loss: 2.49072, policy_loss: -81.87660, policy_entropy: -6.00724, alpha: 0.02497, time: 50.29777
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 187.70779, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 2.70705, qf2_loss: 2.67842, policy_loss: -82.17082, policy_entropy: -6.01568, alpha: 0.02494, time: 50.26802
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 210.77188, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 2.58541, qf2_loss: 2.56911, policy_loss: -82.00992, policy_entropy: -6.02917, alpha: 0.02502, time: 50.41931
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 341.05742, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 2.82039, qf2_loss: 2.77400, policy_loss: -82.22775, policy_entropy: -5.96302, alpha: 0.02498, time: 50.21123
[CW] eval: return: 253.02009, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 160.20350, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 3.02637, qf2_loss: 2.98808, policy_loss: -82.43803, policy_entropy: -5.98375, alpha: 0.02495, time: 50.35083
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 86.56335, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 3.20849, qf2_loss: 3.19182, policy_loss: -81.84157, policy_entropy: -5.85289, alpha: 0.02483, time: 50.26711
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 99.89942, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 3.43323, qf2_loss: 3.42559, policy_loss: -81.96595, policy_entropy: -5.83642, alpha: 0.02462, time: 50.34488
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 347.26911, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 7.54639, qf2_loss: 7.50996, policy_loss: -82.77111, policy_entropy: -5.95316, alpha: 0.02434, time: 50.38696
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 42.09664, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 4.09186, qf2_loss: 4.08817, policy_loss: -81.79098, policy_entropy: -5.61060, alpha: 0.02400, time: 50.84371
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 380.39865, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 3.21136, qf2_loss: 3.14331, policy_loss: -82.37915, policy_entropy: -5.90338, alpha: 0.02355, time: 51.35002
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 365.99386, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 3.35398, qf2_loss: 3.29956, policy_loss: -82.71667, policy_entropy: -6.07947, alpha: 0.02346, time: 50.46578
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 295.37147, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 2.67761, qf2_loss: 2.69140, policy_loss: -82.54825, policy_entropy: -6.12279, alpha: 0.02363, time: 50.50262
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 292.03419, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 2.60162, qf2_loss: 2.57048, policy_loss: -83.40338, policy_entropy: -6.05478, alpha: 0.02380, time: 50.73621
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 363.20713, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 2.46453, qf2_loss: 2.42559, policy_loss: -83.86731, policy_entropy: -6.18890, alpha: 0.02401, time: 50.74109
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 172.29538, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 2.43851, qf2_loss: 2.40839, policy_loss: -83.55427, policy_entropy: -5.94123, alpha: 0.02415, time: 50.62618
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 399.48131, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 2.67360, qf2_loss: 2.65411, policy_loss: -83.04553, policy_entropy: -5.91969, alpha: 0.02406, time: 50.65944
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 371.36406, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 3.05108, qf2_loss: 3.02634, policy_loss: -83.83532, policy_entropy: -6.01977, alpha: 0.02399, time: 50.42081
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 367.95087, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 2.60973, qf2_loss: 2.59722, policy_loss: -83.39541, policy_entropy: -6.00456, alpha: 0.02392, time: 50.86153
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 327.69994, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 3.06116, qf2_loss: 3.01808, policy_loss: -83.51919, policy_entropy: -6.10470, alpha: 0.02404, time: 52.50929
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 381.51615, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 2.89606, qf2_loss: 2.86249, policy_loss: -83.71527, policy_entropy: -6.01347, alpha: 0.02420, time: 50.54252
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 392.37343, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 3.20295, qf2_loss: 3.17885, policy_loss: -83.98952, policy_entropy: -5.99048, alpha: 0.02425, time: 50.47062
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 361.06935, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 2.81188, qf2_loss: 2.77338, policy_loss: -84.99887, policy_entropy: -6.03680, alpha: 0.02418, time: 50.33098
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 413.00290, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 3.86630, qf2_loss: 3.87508, policy_loss: -84.74108, policy_entropy: -5.98374, alpha: 0.02422, time: 50.94045
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 353.62365, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 5.67195, qf2_loss: 5.63763, policy_loss: -84.70035, policy_entropy: -6.17813, alpha: 0.02430, time: 50.76164
[CW] eval: return: 344.93895, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 360.67320, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 6.29012, qf2_loss: 6.30623, policy_loss: -84.59049, policy_entropy: -6.15533, alpha: 0.02458, time: 50.46310
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 414.32751, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 3.26484, qf2_loss: 3.21034, policy_loss: -85.30849, policy_entropy: -6.23991, alpha: 0.02497, time: 50.42371
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 379.02676, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 2.85074, qf2_loss: 2.85633, policy_loss: -86.26948, policy_entropy: -6.15864, alpha: 0.02534, time: 50.44717
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 386.67199, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 2.69873, qf2_loss: 2.66997, policy_loss: -85.60753, policy_entropy: -5.91075, alpha: 0.02542, time: 50.40053
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 386.13347, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 2.80845, qf2_loss: 2.78440, policy_loss: -85.93726, policy_entropy: -6.08384, alpha: 0.02533, time: 50.41570
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 410.70835, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 2.59536, qf2_loss: 2.58725, policy_loss: -84.83764, policy_entropy: -6.18137, alpha: 0.02568, time: 50.21930
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 397.33494, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 3.25285, qf2_loss: 3.23226, policy_loss: -85.75639, policy_entropy: -5.99256, alpha: 0.02580, time: 50.43623
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 388.99044, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 3.29623, qf2_loss: 3.27250, policy_loss: -85.95022, policy_entropy: -5.96596, alpha: 0.02580, time: 50.39846
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 382.80052, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 2.80909, qf2_loss: 2.76181, policy_loss: -87.39321, policy_entropy: -6.23361, alpha: 0.02593, time: 50.35480
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 212.61007, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 3.19022, qf2_loss: 3.15892, policy_loss: -87.13339, policy_entropy: -6.18104, alpha: 0.02630, time: 50.55327
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 429.81302, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 3.08577, qf2_loss: 3.05532, policy_loss: -88.06364, policy_entropy: -6.38009, alpha: 0.02674, time: 50.71915
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 399.07488, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 2.80645, qf2_loss: 2.80039, policy_loss: -86.31316, policy_entropy: -6.33902, alpha: 0.02755, time: 51.23590
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 405.59663, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 3.60547, qf2_loss: 3.54995, policy_loss: -87.26877, policy_entropy: -6.31832, alpha: 0.02825, time: 51.19562
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 420.18945, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 4.06445, qf2_loss: 4.00198, policy_loss: -86.49682, policy_entropy: -6.16178, alpha: 0.02870, time: 51.03361
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 438.86124, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 4.51659, qf2_loss: 4.53349, policy_loss: -87.08852, policy_entropy: -6.15315, alpha: 0.02899, time: 51.28460
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 450.49754, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 3.08574, qf2_loss: 3.03704, policy_loss: -87.55910, policy_entropy: -6.18583, alpha: 0.02939, time: 51.11404
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 411.21668, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 2.60106, qf2_loss: 2.57185, policy_loss: -88.26996, policy_entropy: -6.06462, alpha: 0.02980, time: 50.80469
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 201.38379, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 3.00668, qf2_loss: 2.98551, policy_loss: -87.22378, policy_entropy: -6.06333, alpha: 0.02989, time: 50.88185
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 40.82387, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 4.37262, qf2_loss: 4.30672, policy_loss: -87.93256, policy_entropy: -6.20562, alpha: 0.03020, time: 50.66419
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 357.38284, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 3.74675, qf2_loss: 3.75520, policy_loss: -88.72664, policy_entropy: -5.98204, alpha: 0.03041, time: 50.69567
[CW] eval: return: 288.26693, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 414.64650, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 3.36984, qf2_loss: 3.36528, policy_loss: -88.72130, policy_entropy: -5.99428, alpha: 0.03051, time: 50.79228
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 398.85620, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 2.80428, qf2_loss: 2.76832, policy_loss: -88.87289, policy_entropy: -6.04688, alpha: 0.03038, time: 50.66735
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 183.46268, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 3.07462, qf2_loss: 3.05456, policy_loss: -88.91906, policy_entropy: -6.18847, alpha: 0.03076, time: 50.72022
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 116.49119, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 3.06128, qf2_loss: 3.04363, policy_loss: -89.55501, policy_entropy: -5.96526, alpha: 0.03090, time: 50.87862
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 390.16418, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 2.77422, qf2_loss: 2.75359, policy_loss: -89.15576, policy_entropy: -5.82890, alpha: 0.03076, time: 50.68825
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 424.39734, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 3.35686, qf2_loss: 3.35057, policy_loss: -88.76630, policy_entropy: -5.51981, alpha: 0.03005, time: 50.49459
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 450.78541, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 5.66906, qf2_loss: 5.61695, policy_loss: -89.61783, policy_entropy: -5.84860, alpha: 0.02924, time: 50.32434
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 366.50996, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 5.98855, qf2_loss: 5.95599, policy_loss: -89.86467, policy_entropy: -6.19324, alpha: 0.02928, time: 50.39970
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 296.35644, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 7.27935, qf2_loss: 7.28686, policy_loss: -89.39905, policy_entropy: -6.04518, alpha: 0.02951, time: 50.38190
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 217.03606, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 3.38499, qf2_loss: 3.38804, policy_loss: -89.29289, policy_entropy: -5.85347, alpha: 0.02937, time: 50.53149
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 349.46840, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 3.11984, qf2_loss: 3.08858, policy_loss: -90.32943, policy_entropy: -6.15275, alpha: 0.02942, time: 50.74924
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 315.55577, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 3.21077, qf2_loss: 3.20430, policy_loss: -90.50161, policy_entropy: -5.93629, alpha: 0.02963, time: 50.43037
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 202.14128, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 3.51309, qf2_loss: 3.45138, policy_loss: -89.92456, policy_entropy: -6.09239, alpha: 0.02957, time: 50.34320
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 320.19072, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 3.28115, qf2_loss: 3.26302, policy_loss: -90.79183, policy_entropy: -6.05356, alpha: 0.02966, time: 50.63914
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 139.97167, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 4.19384, qf2_loss: 4.15555, policy_loss: -90.92395, policy_entropy: -6.14380, alpha: 0.02988, time: 50.90988
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 329.50430, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 3.27185, qf2_loss: 3.24626, policy_loss: -91.61242, policy_entropy: -6.29376, alpha: 0.03032, time: 50.38804
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 361.91268, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 3.82440, qf2_loss: 3.80271, policy_loss: -90.04358, policy_entropy: -6.18584, alpha: 0.03082, time: 50.72263
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 360.77227, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 3.46193, qf2_loss: 3.48429, policy_loss: -91.54693, policy_entropy: -6.20172, alpha: 0.03116, time: 50.52239
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 24.08340, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 3.14473, qf2_loss: 3.12711, policy_loss: -91.15382, policy_entropy: -6.14417, alpha: 0.03165, time: 50.72852
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 111.93533, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 3.85965, qf2_loss: 3.84057, policy_loss: -91.02313, policy_entropy: -5.96301, alpha: 0.03180, time: 50.21926
[CW] eval: return: 348.06910, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 409.57130, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 3.56611, qf2_loss: 3.58665, policy_loss: -89.88472, policy_entropy: -6.03860, alpha: 0.03174, time: 50.26125
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 295.02666, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 3.74992, qf2_loss: 3.71504, policy_loss: -92.72319, policy_entropy: -6.08714, alpha: 0.03197, time: 50.13511
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 172.71269, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 3.19037, qf2_loss: 3.18929, policy_loss: -91.32625, policy_entropy: -5.94785, alpha: 0.03193, time: 50.41242
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 379.85314, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 3.32552, qf2_loss: 3.33811, policy_loss: -92.06150, policy_entropy: -6.06036, alpha: 0.03203, time: 50.24218
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 292.52401, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 3.01623, qf2_loss: 2.97924, policy_loss: -92.76698, policy_entropy: -6.07192, alpha: 0.03206, time: 50.31560
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 363.52943, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 3.98800, qf2_loss: 4.01429, policy_loss: -92.29383, policy_entropy: -6.00283, alpha: 0.03226, time: 50.27581
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 295.65916, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 6.54541, qf2_loss: 6.51017, policy_loss: -92.96127, policy_entropy: -6.24038, alpha: 0.03250, time: 50.59799
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 304.30211, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 3.85676, qf2_loss: 3.85751, policy_loss: -92.27918, policy_entropy: -6.07485, alpha: 0.03290, time: 50.35120
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 410.00492, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 3.01573, qf2_loss: 3.04135, policy_loss: -92.16842, policy_entropy: -6.03442, alpha: 0.03312, time: 50.27060
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 320.12217, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 3.31176, qf2_loss: 3.32878, policy_loss: -92.79333, policy_entropy: -6.11118, alpha: 0.03332, time: 50.41227
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 404.38527, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 3.25469, qf2_loss: 3.27327, policy_loss: -93.20297, policy_entropy: -6.02932, alpha: 0.03350, time: 50.21514
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 428.99125, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 3.30529, qf2_loss: 3.29527, policy_loss: -93.16098, policy_entropy: -6.10813, alpha: 0.03354, time: 50.42546
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 401.04199, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 3.05883, qf2_loss: 3.03166, policy_loss: -93.71994, policy_entropy: -6.08127, alpha: 0.03399, time: 50.31504
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 409.75864, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 3.84690, qf2_loss: 3.84759, policy_loss: -93.83101, policy_entropy: -5.89199, alpha: 0.03392, time: 50.19000
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 423.12271, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 3.43458, qf2_loss: 3.44854, policy_loss: -94.34030, policy_entropy: -6.09069, alpha: 0.03387, time: 50.23661
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 432.77886, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 3.10167, qf2_loss: 3.12117, policy_loss: -94.28351, policy_entropy: -6.07748, alpha: 0.03415, time: 50.48986
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 459.63545, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 3.74975, qf2_loss: 3.75059, policy_loss: -93.55729, policy_entropy: -5.86365, alpha: 0.03408, time: 50.55126
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 454.17501, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 3.51492, qf2_loss: 3.49479, policy_loss: -94.43717, policy_entropy: -5.94532, alpha: 0.03381, time: 50.37080
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 416.07371, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 3.58961, qf2_loss: 3.62351, policy_loss: -94.53940, policy_entropy: -6.06328, alpha: 0.03393, time: 52.05377
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 184.65343, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 3.47021, qf2_loss: 3.47893, policy_loss: -95.77849, policy_entropy: -5.88129, alpha: 0.03386, time: 50.52349
[CW] eval: return: 441.30112, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 420.25961, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 3.38491, qf2_loss: 3.39179, policy_loss: -95.08882, policy_entropy: -5.69653, alpha: 0.03309, time: 50.38137
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 475.38356, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 3.00868, qf2_loss: 3.02207, policy_loss: -95.46486, policy_entropy: -5.94638, alpha: 0.03260, time: 50.30641
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 464.93863, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 3.18421, qf2_loss: 3.19409, policy_loss: -94.35961, policy_entropy: -6.08697, alpha: 0.03275, time: 50.32858
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 412.94561, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 4.84859, qf2_loss: 4.82319, policy_loss: -95.69788, policy_entropy: -6.03197, alpha: 0.03283, time: 50.29748
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 439.08345, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 3.59789, qf2_loss: 3.59264, policy_loss: -95.32084, policy_entropy: -5.49902, alpha: 0.03248, time: 50.48391
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 421.10447, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 3.35851, qf2_loss: 3.38496, policy_loss: -96.24078, policy_entropy: -5.85471, alpha: 0.03154, time: 50.43195
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 449.63174, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 3.88084, qf2_loss: 3.84233, policy_loss: -95.83473, policy_entropy: -5.98120, alpha: 0.03141, time: 49.97272
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 462.96810, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 3.87494, qf2_loss: 3.88756, policy_loss: -96.54500, policy_entropy: -6.12132, alpha: 0.03157, time: 50.03386
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 500.90225, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 3.78498, qf2_loss: 3.83484, policy_loss: -96.54563, policy_entropy: -6.16644, alpha: 0.03182, time: 52.65131
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 474.16253, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 4.38158, qf2_loss: 4.38651, policy_loss: -97.79064, policy_entropy: -6.37296, alpha: 0.03234, time: 50.16353
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 477.75194, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 4.04026, qf2_loss: 4.09262, policy_loss: -97.02538, policy_entropy: -5.86305, alpha: 0.03289, time: 50.28978
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 215.40094, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 3.13937, qf2_loss: 3.14778, policy_loss: -97.59293, policy_entropy: -5.84082, alpha: 0.03244, time: 50.15208
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 185.62937, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 3.30229, qf2_loss: 3.29964, policy_loss: -97.05933, policy_entropy: -5.90556, alpha: 0.03206, time: 50.35645
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 516.32059, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 2.90702, qf2_loss: 2.90588, policy_loss: -98.08968, policy_entropy: -5.87342, alpha: 0.03193, time: 50.39943
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 486.33918, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 4.67494, qf2_loss: 4.71037, policy_loss: -98.35882, policy_entropy: -5.85558, alpha: 0.03156, time: 50.51336
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 462.83781, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 5.58689, qf2_loss: 5.54566, policy_loss: -98.66976, policy_entropy: -6.13907, alpha: 0.03158, time: 50.63475
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 452.10690, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 6.09969, qf2_loss: 6.12718, policy_loss: -97.88996, policy_entropy: -6.21421, alpha: 0.03190, time: 50.09189
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 499.56458, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 3.72265, qf2_loss: 3.75526, policy_loss: -97.91513, policy_entropy: -6.09952, alpha: 0.03220, time: 50.61984
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 479.39031, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 3.33498, qf2_loss: 3.35039, policy_loss: -99.92826, policy_entropy: -6.09548, alpha: 0.03244, time: 50.89371
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 488.56772, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 3.54272, qf2_loss: 3.51029, policy_loss: -98.76849, policy_entropy: -6.13099, alpha: 0.03278, time: 50.70074
[CW] eval: return: 475.63006, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 475.70129, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 2.98330, qf2_loss: 2.98789, policy_loss: -100.54214, policy_entropy: -6.12350, alpha: 0.03300, time: 50.98183
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 483.37893, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 3.07091, qf2_loss: 3.10044, policy_loss: -101.09998, policy_entropy: -6.12046, alpha: 0.03332, time: 50.68647
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 298.58924, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 3.51601, qf2_loss: 3.54784, policy_loss: -100.19610, policy_entropy: -6.08103, alpha: 0.03347, time: 50.65369
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 386.20698, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 3.72342, qf2_loss: 3.73864, policy_loss: -99.08159, policy_entropy: -5.98893, alpha: 0.03358, time: 50.67375
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 400.63502, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 3.70984, qf2_loss: 3.69727, policy_loss: -100.74686, policy_entropy: -6.01871, alpha: 0.03361, time: 50.84622
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 346.98964, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 4.02117, qf2_loss: 4.01624, policy_loss: -100.87777, policy_entropy: -6.05775, alpha: 0.03370, time: 50.53627
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 418.69329, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 4.25820, qf2_loss: 4.23627, policy_loss: -100.81184, policy_entropy: -6.14043, alpha: 0.03389, time: 50.44914
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 222.20575, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 4.48337, qf2_loss: 4.50933, policy_loss: -100.35050, policy_entropy: -6.07786, alpha: 0.03421, time: 50.61111
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 543.53966, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 4.71222, qf2_loss: 4.74066, policy_loss: -100.73120, policy_entropy: -5.91014, alpha: 0.03408, time: 50.50700
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 494.19038, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 3.88472, qf2_loss: 3.93713, policy_loss: -101.52794, policy_entropy: -6.08071, alpha: 0.03416, time: 50.85534
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 495.58939, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 7.19454, qf2_loss: 7.17045, policy_loss: -101.43556, policy_entropy: -6.15846, alpha: 0.03446, time: 51.64683
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 438.69824, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 6.19431, qf2_loss: 6.25945, policy_loss: -101.65258, policy_entropy: -6.00178, alpha: 0.03471, time: 50.49733
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 444.32167, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 3.63387, qf2_loss: 3.62015, policy_loss: -102.70838, policy_entropy: -6.10869, alpha: 0.03476, time: 50.36328
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 460.91252, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 3.14544, qf2_loss: 3.16714, policy_loss: -102.05427, policy_entropy: -5.94952, alpha: 0.03484, time: 50.39860
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 498.86908, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 3.24188, qf2_loss: 3.21716, policy_loss: -102.18326, policy_entropy: -6.00003, alpha: 0.03472, time: 50.71994
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 372.99002, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 3.26344, qf2_loss: 3.25008, policy_loss: -102.77604, policy_entropy: -5.93583, alpha: 0.03481, time: 50.33818
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 449.73857, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 3.55998, qf2_loss: 3.53642, policy_loss: -102.33298, policy_entropy: -5.93966, alpha: 0.03463, time: 51.26136
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 241.64357, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 3.54599, qf2_loss: 3.51670, policy_loss: -103.15715, policy_entropy: -5.98314, alpha: 0.03448, time: 51.26677
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 462.12256, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 4.68363, qf2_loss: 4.70581, policy_loss: -103.90787, policy_entropy: -5.99938, alpha: 0.03441, time: 50.59653
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 502.91361, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 7.11270, qf2_loss: 7.17886, policy_loss: -103.11291, policy_entropy: -6.20399, alpha: 0.03467, time: 50.80233
[CW] eval: return: 422.25630, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 384.59727, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 4.63770, qf2_loss: 4.65286, policy_loss: -104.82010, policy_entropy: -6.19828, alpha: 0.03500, time: 50.78408
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 486.52882, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 3.44053, qf2_loss: 3.44373, policy_loss: -104.14256, policy_entropy: -6.10954, alpha: 0.03564, time: 50.78889
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 440.09664, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 3.37061, qf2_loss: 3.36755, policy_loss: -104.64199, policy_entropy: -6.00120, alpha: 0.03566, time: 50.70672
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 471.68232, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 3.38527, qf2_loss: 3.40110, policy_loss: -104.82265, policy_entropy: -6.00023, alpha: 0.03568, time: 50.53818
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 141.83287, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 3.46164, qf2_loss: 3.47490, policy_loss: -103.70140, policy_entropy: -5.70569, alpha: 0.03539, time: 50.78004
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 503.35885, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 3.44291, qf2_loss: 3.45469, policy_loss: -106.30686, policy_entropy: -5.95205, alpha: 0.03476, time: 50.71626
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 118.10199, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 3.67836, qf2_loss: 3.69838, policy_loss: -104.48546, policy_entropy: -5.85551, alpha: 0.03472, time: 50.44853
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 473.86619, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 4.29934, qf2_loss: 4.25286, policy_loss: -105.26631, policy_entropy: -5.81728, alpha: 0.03426, time: 50.61013
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 479.97202, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 6.97443, qf2_loss: 7.00675, policy_loss: -105.19012, policy_entropy: -6.15345, alpha: 0.03427, time: 50.93956
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 509.79832, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 6.57881, qf2_loss: 6.55352, policy_loss: -106.01533, policy_entropy: -6.21302, alpha: 0.03459, time: 50.66275
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 498.79283, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 3.42851, qf2_loss: 3.43835, policy_loss: -106.73362, policy_entropy: -6.05474, alpha: 0.03496, time: 50.52278
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 503.50102, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 3.74565, qf2_loss: 3.73401, policy_loss: -106.58940, policy_entropy: -5.95229, alpha: 0.03502, time: 50.85542
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 510.66496, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 3.64805, qf2_loss: 3.65049, policy_loss: -106.12219, policy_entropy: -5.81895, alpha: 0.03476, time: 50.69435
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 36.28833, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 4.16991, qf2_loss: 4.18402, policy_loss: -107.00366, policy_entropy: -6.00143, alpha: 0.03455, time: 50.61575
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 493.36615, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 4.28173, qf2_loss: 4.31169, policy_loss: -106.72127, policy_entropy: -6.11644, alpha: 0.03466, time: 50.46563
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 498.97575, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 6.33061, qf2_loss: 6.35181, policy_loss: -106.54064, policy_entropy: -5.88692, alpha: 0.03466, time: 50.86871
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 492.00483, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 4.59012, qf2_loss: 4.55212, policy_loss: -106.35684, policy_entropy: -5.94654, alpha: 0.03438, time: 50.25564
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 499.25407, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 9.81380, qf2_loss: 9.75275, policy_loss: -106.97871, policy_entropy: -6.04141, alpha: 0.03445, time: 50.43674
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 455.86591, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 8.47317, qf2_loss: 8.52409, policy_loss: -107.36369, policy_entropy: -6.26852, alpha: 0.03476, time: 50.31355
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 219.02322, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 6.15425, qf2_loss: 6.18383, policy_loss: -106.38053, policy_entropy: -6.20603, alpha: 0.03525, time: 50.47143
[CW] eval: return: 458.21822, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 474.70239, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 3.84023, qf2_loss: 3.84276, policy_loss: -108.63220, policy_entropy: -6.11073, alpha: 0.03567, time: 50.57704
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 489.93644, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 4.84393, qf2_loss: 4.84756, policy_loss: -107.88810, policy_entropy: -5.99425, alpha: 0.03569, time: 50.50667
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 493.63366, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 3.65866, qf2_loss: 3.63527, policy_loss: -109.37478, policy_entropy: -6.05292, alpha: 0.03583, time: 50.86304
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 492.92213, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 3.52022, qf2_loss: 3.51807, policy_loss: -108.98377, policy_entropy: -5.88992, alpha: 0.03566, time: 50.76680
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 515.53390, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 3.93170, qf2_loss: 3.91670, policy_loss: -107.70351, policy_entropy: -5.92879, alpha: 0.03548, time: 50.98026
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 336.98228, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 4.13945, qf2_loss: 4.13317, policy_loss: -108.58496, policy_entropy: -6.01311, alpha: 0.03549, time: 50.89099
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 529.36367, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 3.78969, qf2_loss: 3.76286, policy_loss: -110.61287, policy_entropy: -6.07030, alpha: 0.03556, time: 50.84401
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 557.18798, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 4.36000, qf2_loss: 4.36023, policy_loss: -108.88042, policy_entropy: -5.88927, alpha: 0.03558, time: 50.55352
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 509.88257, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 7.40134, qf2_loss: 7.42779, policy_loss: -109.55146, policy_entropy: -6.04177, alpha: 0.03543, time: 50.54498
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 491.38011, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 5.94623, qf2_loss: 5.85894, policy_loss: -110.74090, policy_entropy: -6.18863, alpha: 0.03566, time: 50.82756
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 507.53654, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 4.16520, qf2_loss: 4.17550, policy_loss: -109.27096, policy_entropy: -6.03862, alpha: 0.03602, time: 52.01638
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 464.28676, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 3.87275, qf2_loss: 3.85286, policy_loss: -110.62009, policy_entropy: -6.12626, alpha: 0.03615, time: 50.36143
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 520.65230, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 4.12751, qf2_loss: 4.13742, policy_loss: -110.39818, policy_entropy: -6.04963, alpha: 0.03626, time: 52.33748
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 513.51478, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 6.89477, qf2_loss: 6.90054, policy_loss: -110.73043, policy_entropy: -6.17319, alpha: 0.03653, time: 50.56941
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 473.74271, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 18.22050, qf2_loss: 18.33558, policy_loss: -111.50460, policy_entropy: -6.51284, alpha: 0.03715, time: 50.67317
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 501.94854, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 10.00255, qf2_loss: 9.98343, policy_loss: -111.87915, policy_entropy: -6.42057, alpha: 0.03842, time: 50.78956
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 491.91818, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 5.95907, qf2_loss: 5.95673, policy_loss: -111.58131, policy_entropy: -6.19443, alpha: 0.03900, time: 50.68032
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 534.93245, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 4.07092, qf2_loss: 4.08552, policy_loss: -110.69514, policy_entropy: -6.09115, alpha: 0.03935, time: 51.55558
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 534.45553, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 4.01931, qf2_loss: 4.00990, policy_loss: -111.19052, policy_entropy: -5.91605, alpha: 0.03947, time: 50.74024
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 282.49244, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 4.17983, qf2_loss: 4.21345, policy_loss: -112.25960, policy_entropy: -5.99364, alpha: 0.03926, time: 50.59198
[CW] eval: return: 493.71615, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 526.68181, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 4.21080, qf2_loss: 4.25249, policy_loss: -112.11526, policy_entropy: -5.94374, alpha: 0.03937, time: 50.69546
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 497.55112, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 4.25157, qf2_loss: 4.27909, policy_loss: -113.77630, policy_entropy: -6.04619, alpha: 0.03923, time: 50.27929
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 473.41343, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 3.95293, qf2_loss: 3.92069, policy_loss: -112.72569, policy_entropy: -5.91746, alpha: 0.03912, time: 50.70607
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 502.65564, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 4.48968, qf2_loss: 4.47983, policy_loss: -112.82568, policy_entropy: -5.95435, alpha: 0.03907, time: 50.67544
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 527.92336, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 3.95361, qf2_loss: 3.93845, policy_loss: -112.38823, policy_entropy: -5.89656, alpha: 0.03891, time: 50.86161
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 230.86169, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 4.38937, qf2_loss: 4.36040, policy_loss: -115.29856, policy_entropy: -6.09265, alpha: 0.03888, time: 50.73595
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 496.05957, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 3.97812, qf2_loss: 3.95231, policy_loss: -113.62862, policy_entropy: -6.09428, alpha: 0.03904, time: 50.61045
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 550.83588, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 4.05378, qf2_loss: 4.03177, policy_loss: -114.31411, policy_entropy: -6.04382, alpha: 0.03920, time: 50.62602
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 539.78558, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 4.42877, qf2_loss: 4.35494, policy_loss: -114.36141, policy_entropy: -6.05958, alpha: 0.03929, time: 50.55398
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 529.96017, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 4.28556, qf2_loss: 4.28546, policy_loss: -114.23746, policy_entropy: -6.03367, alpha: 0.03943, time: 50.34723
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 528.20870, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 4.82052, qf2_loss: 4.81957, policy_loss: -114.72027, policy_entropy: -5.97219, alpha: 0.03942, time: 50.55231
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 529.63096, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 5.27617, qf2_loss: 5.34991, policy_loss: -113.77211, policy_entropy: -5.78497, alpha: 0.03918, time: 50.93577
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 530.25015, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 5.26687, qf2_loss: 5.24578, policy_loss: -114.42846, policy_entropy: -6.01584, alpha: 0.03895, time: 50.58169
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 133.41650, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 5.89384, qf2_loss: 5.87724, policy_loss: -115.89016, policy_entropy: -6.10181, alpha: 0.03914, time: 50.65411
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 498.22724, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 5.10654, qf2_loss: 5.10848, policy_loss: -114.37904, policy_entropy: -5.94997, alpha: 0.03920, time: 50.76381
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 494.20213, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 4.84258, qf2_loss: 4.87817, policy_loss: -115.67053, policy_entropy: -5.98880, alpha: 0.03912, time: 50.41710
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 493.20767, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 4.78650, qf2_loss: 4.86193, policy_loss: -115.56550, policy_entropy: -5.97418, alpha: 0.03902, time: 50.69040
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 545.13195, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 8.49014, qf2_loss: 8.55025, policy_loss: -115.48656, policy_entropy: -5.99363, alpha: 0.03897, time: 50.59598
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 518.39486, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 14.20572, qf2_loss: 14.19105, policy_loss: -116.61300, policy_entropy: -6.22659, alpha: 0.03918, time: 50.82880
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 455.80801, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 7.07245, qf2_loss: 7.00110, policy_loss: -116.02523, policy_entropy: -6.19105, alpha: 0.03963, time: 50.69870
[CW] eval: return: 488.05863, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 512.09549, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 4.98265, qf2_loss: 4.99997, policy_loss: -116.40601, policy_entropy: -6.15383, alpha: 0.04000, time: 50.58194
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 516.04933, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 6.92292, qf2_loss: 7.07463, policy_loss: -116.98477, policy_entropy: -6.16416, alpha: 0.04039, time: 50.86039
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 505.85948, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 5.06196, qf2_loss: 5.05905, policy_loss: -117.14157, policy_entropy: -5.94009, alpha: 0.04053, time: 50.63291
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 98.71782, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 5.02977, qf2_loss: 5.01578, policy_loss: -117.37007, policy_entropy: -5.99817, alpha: 0.04040, time: 50.58460
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 524.85764, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 4.57174, qf2_loss: 4.51799, policy_loss: -117.38479, policy_entropy: -5.86892, alpha: 0.04030, time: 50.52377
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 497.82254, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 4.39236, qf2_loss: 4.39895, policy_loss: -117.45216, policy_entropy: -5.80476, alpha: 0.04000, time: 50.56592
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 131.77664, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 5.56523, qf2_loss: 5.59321, policy_loss: -118.45621, policy_entropy: -5.97041, alpha: 0.03970, time: 50.78947
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 103.13094, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 28.18186, qf2_loss: 28.16651, policy_loss: -116.90602, policy_entropy: -6.00822, alpha: 0.03967, time: 52.11471
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 478.49991, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 14.55278, qf2_loss: 14.76098, policy_loss: -117.46522, policy_entropy: -6.50451, alpha: 0.04019, time: 51.22654
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 482.04795, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 8.71203, qf2_loss: 8.78360, policy_loss: -118.38198, policy_entropy: -6.36916, alpha: 0.04110, time: 51.35153
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 504.74666, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 5.47393, qf2_loss: 5.43116, policy_loss: -118.46687, policy_entropy: -6.19833, alpha: 0.04172, time: 50.93380
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 524.71617, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 4.90175, qf2_loss: 4.88054, policy_loss: -119.82067, policy_entropy: -6.15163, alpha: 0.04214, time: 51.27491
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 543.04848, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 5.12875, qf2_loss: 5.08366, policy_loss: -119.28923, policy_entropy: -6.02935, alpha: 0.04230, time: 51.49770
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 518.92949, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 4.56314, qf2_loss: 4.56122, policy_loss: -119.09740, policy_entropy: -6.01188, alpha: 0.04231, time: 50.69419
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 511.11376, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 4.84755, qf2_loss: 4.83463, policy_loss: -119.11179, policy_entropy: -6.00349, alpha: 0.04239, time: 50.92521
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 470.01614, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 5.43689, qf2_loss: 5.37218, policy_loss: -120.04155, policy_entropy: -6.01945, alpha: 0.04232, time: 50.70761
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 527.58254, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 6.54615, qf2_loss: 6.62080, policy_loss: -119.29188, policy_entropy: -5.87483, alpha: 0.04228, time: 50.80452
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 516.59159, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 5.39685, qf2_loss: 5.47562, policy_loss: -120.52972, policy_entropy: -5.95322, alpha: 0.04208, time: 50.83568
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 485.27671, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 4.90538, qf2_loss: 4.92410, policy_loss: -120.78227, policy_entropy: -5.85410, alpha: 0.04194, time: 50.55532
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 502.39901, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 5.46585, qf2_loss: 5.45309, policy_loss: -120.32810, policy_entropy: -5.95693, alpha: 0.04165, time: 50.80507
[CW] eval: return: 493.97320, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 478.13498, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 4.98497, qf2_loss: 4.96447, policy_loss: -121.32596, policy_entropy: -5.94528, alpha: 0.04152, time: 50.74203
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 538.95200, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 7.70889, qf2_loss: 7.81369, policy_loss: -120.92181, policy_entropy: -5.97355, alpha: 0.04145, time: 51.05897
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 530.75583, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 6.00866, qf2_loss: 6.10634, policy_loss: -121.12935, policy_entropy: -5.95404, alpha: 0.04139, time: 50.74792
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 538.83278, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 5.78029, qf2_loss: 5.66220, policy_loss: -121.45326, policy_entropy: -6.05861, alpha: 0.04126, time: 50.79334
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 529.54673, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 9.28994, qf2_loss: 9.24815, policy_loss: -122.35470, policy_entropy: -6.16787, alpha: 0.04162, time: 51.05466
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 173.59617, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 10.14054, qf2_loss: 10.19075, policy_loss: -120.69021, policy_entropy: -5.95819, alpha: 0.04172, time: 50.81844
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 562.21286, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 5.77976, qf2_loss: 5.78928, policy_loss: -123.13900, policy_entropy: -6.12235, alpha: 0.04188, time: 51.03956
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 545.72228, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 5.46636, qf2_loss: 5.44624, policy_loss: -122.83018, policy_entropy: -5.99970, alpha: 0.04193, time: 51.19859
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 573.52437, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 5.34476, qf2_loss: 5.34979, policy_loss: -122.84637, policy_entropy: -5.98881, alpha: 0.04196, time: 50.76960
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 430.24846, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 5.76000, qf2_loss: 5.78952, policy_loss: -123.10144, policy_entropy: -6.05008, alpha: 0.04204, time: 51.03024
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 495.89319, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 5.29823, qf2_loss: 5.30347, policy_loss: -121.52371, policy_entropy: -5.88568, alpha: 0.04196, time: 50.98381
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 557.01934, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 5.92574, qf2_loss: 5.88187, policy_loss: -123.13015, policy_entropy: -6.04617, alpha: 0.04176, time: 51.03617
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 539.46034, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 5.18290, qf2_loss: 5.14440, policy_loss: -122.32841, policy_entropy: -5.87376, alpha: 0.04180, time: 51.21376
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 520.22151, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 5.67260, qf2_loss: 5.67561, policy_loss: -123.42141, policy_entropy: -5.96655, alpha: 0.04161, time: 50.95840
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 537.36901, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 8.57303, qf2_loss: 8.61891, policy_loss: -124.28160, policy_entropy: -5.85889, alpha: 0.04141, time: 50.81583
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 494.24684, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 5.98615, qf2_loss: 5.95107, policy_loss: -123.92399, policy_entropy: -5.97755, alpha: 0.04116, time: 50.97030
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 560.93181, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 7.42075, qf2_loss: 7.43590, policy_loss: -123.96558, policy_entropy: -5.92514, alpha: 0.04118, time: 50.94764
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 435.06402, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 18.89348, qf2_loss: 19.04586, policy_loss: -124.70539, policy_entropy: -5.91339, alpha: 0.04086, time: 50.77368
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 421.31084, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 11.98297, qf2_loss: 12.01623, policy_loss: -126.01657, policy_entropy: -6.36123, alpha: 0.04112, time: 50.81692
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 400.56457, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 7.13861, qf2_loss: 7.14419, policy_loss: -126.68020, policy_entropy: -6.19721, alpha: 0.04180, time: 50.79540
[CW] eval: return: 451.59647, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 550.31328, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 6.67297, qf2_loss: 6.70342, policy_loss: -125.31585, policy_entropy: -6.06853, alpha: 0.04210, time: 50.99675
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 493.78547, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 5.68149, qf2_loss: 5.69469, policy_loss: -125.03960, policy_entropy: -6.05082, alpha: 0.04227, time: 51.07844
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 473.55391, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 5.31810, qf2_loss: 5.25456, policy_loss: -124.94650, policy_entropy: -6.17168, alpha: 0.04247, time: 50.70058
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 457.12162, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 6.76753, qf2_loss: 6.76507, policy_loss: -127.39469, policy_entropy: -6.11344, alpha: 0.04288, time: 50.72309
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 522.59464, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 6.29784, qf2_loss: 6.18118, policy_loss: -124.68249, policy_entropy: -5.78522, alpha: 0.04279, time: 52.92282
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 542.00821, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 6.76308, qf2_loss: 6.66522, policy_loss: -126.44082, policy_entropy: -6.04798, alpha: 0.04254, time: 50.78875
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 500.88161, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 5.65592, qf2_loss: 5.64573, policy_loss: -128.76037, policy_entropy: -6.15357, alpha: 0.04267, time: 52.33120
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 491.84499, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 6.26646, qf2_loss: 6.11285, policy_loss: -127.67030, policy_entropy: -6.23752, alpha: 0.04317, time: 50.82341
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 531.97889, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 6.36284, qf2_loss: 6.25918, policy_loss: -128.87741, policy_entropy: -6.27540, alpha: 0.04387, time: 50.63459
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 553.50272, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 7.94494, qf2_loss: 7.87917, policy_loss: -126.99493, policy_entropy: -6.02820, alpha: 0.04426, time: 50.63180
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 440.32979, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 7.77520, qf2_loss: 7.71208, policy_loss: -127.48693, policy_entropy: -6.03212, alpha: 0.04437, time: 50.95714
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 531.12435, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 7.09260, qf2_loss: 6.98974, policy_loss: -126.68484, policy_entropy: -6.10919, alpha: 0.04451, time: 50.58221
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 426.68695, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 6.40201, qf2_loss: 6.30021, policy_loss: -128.83753, policy_entropy: -6.18940, alpha: 0.04482, time: 50.56220
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 477.40498, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 5.97980, qf2_loss: 5.90814, policy_loss: -129.32650, policy_entropy: -6.21919, alpha: 0.04530, time: 51.45688
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 553.49938, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 6.10537, qf2_loss: 5.98594, policy_loss: -128.33477, policy_entropy: -6.18801, alpha: 0.04581, time: 50.57830
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 491.55958, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 7.84383, qf2_loss: 7.93727, policy_loss: -128.76127, policy_entropy: -6.16629, alpha: 0.04636, time: 50.44245
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 490.66726, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 11.11150, qf2_loss: 11.12224, policy_loss: -130.29312, policy_entropy: -6.00157, alpha: 0.04652, time: 50.59914
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 406.51043, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 9.94320, qf2_loss: 9.83067, policy_loss: -131.12254, policy_entropy: -6.12733, alpha: 0.04677, time: 50.59201
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 452.16776, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 10.31921, qf2_loss: 10.17980, policy_loss: -130.09107, policy_entropy: -6.08401, alpha: 0.04693, time: 50.56196
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 513.12678, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 8.14382, qf2_loss: 8.09292, policy_loss: -129.53800, policy_entropy: -6.07960, alpha: 0.04719, time: 50.77209
[CW] eval: return: 334.75119, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 431.95039, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 15.42936, qf2_loss: 15.55719, policy_loss: -131.06552, policy_entropy: -6.07128, alpha: 0.04736, time: 50.82839
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 269.63377, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 11.35992, qf2_loss: 11.41097, policy_loss: -130.40144, policy_entropy: -6.32969, alpha: 0.04779, time: 50.89267
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 353.87953, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 15.84361, qf2_loss: 15.67571, policy_loss: -132.51936, policy_entropy: -6.04409, alpha: 0.04842, time: 50.99627
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 497.54517, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 8.43011, qf2_loss: 8.49688, policy_loss: -130.68619, policy_entropy: -5.79614, alpha: 0.04828, time: 50.92485
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 24.77321, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 12.33682, qf2_loss: 12.28160, policy_loss: -130.58816, policy_entropy: -5.93610, alpha: 0.04788, time: 50.60910
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 25.18090, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 8.22621, qf2_loss: 8.21398, policy_loss: -131.26716, policy_entropy: -5.97278, alpha: 0.04772, time: 50.62205
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 401.90112, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 7.77746, qf2_loss: 7.66974, policy_loss: -132.17489, policy_entropy: -5.79376, alpha: 0.04738, time: 50.70859
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 421.65562, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 6.41315, qf2_loss: 6.32442, policy_loss: -132.09232, policy_entropy: -5.79031, alpha: 0.04689, time: 50.54879
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 335.90505, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 7.29881, qf2_loss: 7.24790, policy_loss: -132.86947, policy_entropy: -5.94348, alpha: 0.04655, time: 50.79158
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 438.16270, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 7.51597, qf2_loss: 7.43543, policy_loss: -132.59249, policy_entropy: -5.89224, alpha: 0.04624, time: 50.59426
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 228.47808, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 7.29707, qf2_loss: 7.29373, policy_loss: -132.11844, policy_entropy: -5.87893, alpha: 0.04599, time: 50.94377
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 110.26353, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 7.85886, qf2_loss: 7.88345, policy_loss: -132.14589, policy_entropy: -5.91761, alpha: 0.04566, time: 50.93325
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 435.36889, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 7.51714, qf2_loss: 7.52041, policy_loss: -133.45153, policy_entropy: -6.15777, alpha: 0.04587, time: 50.88415
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 16.20852, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 7.63281, qf2_loss: 7.62845, policy_loss: -132.80079, policy_entropy: -5.89756, alpha: 0.04596, time: 50.51151
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 173.12257, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 6.95407, qf2_loss: 6.90491, policy_loss: -132.30376, policy_entropy: -5.81959, alpha: 0.04555, time: 50.58998
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 542.26894, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 6.50515, qf2_loss: 6.41383, policy_loss: -133.30889, policy_entropy: -5.98140, alpha: 0.04524, time: 50.81354
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 539.23525, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 7.66338, qf2_loss: 7.65814, policy_loss: -133.31559, policy_entropy: -6.05955, alpha: 0.04529, time: 50.62061
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 275.20459, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 17.88870, qf2_loss: 17.83161, policy_loss: -133.34340, policy_entropy: -5.97794, alpha: 0.04532, time: 50.70173
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 48.72141, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 14.54996, qf2_loss: 14.74315, policy_loss: -133.89353, policy_entropy: -5.82582, alpha: 0.04507, time: 50.80920
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 293.84155, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 10.31617, qf2_loss: 10.37575, policy_loss: -134.47479, policy_entropy: -5.93497, alpha: 0.04482, time: 50.64863
[CW] eval: return: 297.11398, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 32.98175, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 8.28174, qf2_loss: 8.23809, policy_loss: -133.49674, policy_entropy: -5.80986, alpha: 0.04449, time: 50.67385
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 519.28948, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 6.71926, qf2_loss: 6.56541, policy_loss: -133.05484, policy_entropy: -5.84404, alpha: 0.04399, time: 50.84899
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 454.19918, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 6.28675, qf2_loss: 6.28461, policy_loss: -133.05891, policy_entropy: -6.03900, alpha: 0.04383, time: 50.49740
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 482.44065, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 6.35273, qf2_loss: 6.29748, policy_loss: -134.13102, policy_entropy: -5.97034, alpha: 0.04390, time: 50.91017
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 302.21680, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 6.63574, qf2_loss: 6.54066, policy_loss: -135.04772, policy_entropy: -6.01026, alpha: 0.04393, time: 50.97693
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 431.48678, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 6.21992, qf2_loss: 6.10948, policy_loss: -134.11004, policy_entropy: -5.93473, alpha: 0.04381, time: 50.54179
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 420.00249, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 6.15616, qf2_loss: 6.16807, policy_loss: -134.97471, policy_entropy: -5.96620, alpha: 0.04364, time: 50.59694
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 505.63384, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 5.53319, qf2_loss: 5.46197, policy_loss: -135.96933, policy_entropy: -6.04412, alpha: 0.04367, time: 50.73792
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 242.11486, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 5.59278, qf2_loss: 5.52146, policy_loss: -136.31296, policy_entropy: -6.14590, alpha: 0.04386, time: 50.54697
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 430.55468, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 5.61035, qf2_loss: 5.43356, policy_loss: -136.11695, policy_entropy: -5.95377, alpha: 0.04409, time: 50.77869
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 522.08087, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 5.60513, qf2_loss: 5.54111, policy_loss: -134.71369, policy_entropy: -5.88665, alpha: 0.04395, time: 50.90146
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 371.09218, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 6.58759, qf2_loss: 6.52721, policy_loss: -134.84695, policy_entropy: -5.69439, alpha: 0.04341, time: 50.71549
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 467.34377, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 5.75002, qf2_loss: 5.69215, policy_loss: -135.96049, policy_entropy: -5.72533, alpha: 0.04266, time: 50.72265
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 511.02795, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 5.83419, qf2_loss: 5.78924, policy_loss: -136.25803, policy_entropy: -5.85633, alpha: 0.04213, time: 50.66001
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 552.77811, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 6.43341, qf2_loss: 6.41784, policy_loss: -135.98455, policy_entropy: -5.81589, alpha: 0.04178, time: 50.66376
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 514.29199, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 7.09719, qf2_loss: 7.05729, policy_loss: -137.08410, policy_entropy: -5.82937, alpha: 0.04128, time: 50.83540
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 421.43127, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 9.00199, qf2_loss: 8.90349, policy_loss: -136.66516, policy_entropy: -6.13691, alpha: 0.04124, time: 50.74148
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 243.76979, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 6.19533, qf2_loss: 6.08405, policy_loss: -136.50194, policy_entropy: -5.81151, alpha: 0.04132, time: 51.60034
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 465.22031, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 5.62190, qf2_loss: 5.53247, policy_loss: -135.58296, policy_entropy: -5.69377, alpha: 0.04067, time: 50.70864
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 297.65663, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 5.63001, qf2_loss: 5.50111, policy_loss: -136.72022, policy_entropy: -5.82105, alpha: 0.04021, time: 50.57929
[CW] eval: return: 335.43149, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 551.43603, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 6.88263, qf2_loss: 6.71842, policy_loss: -138.33472, policy_entropy: -6.04068, alpha: 0.03994, time: 50.60038
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 237.03343, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 5.62142, qf2_loss: 5.72996, policy_loss: -137.77194, policy_entropy: -5.87652, alpha: 0.03996, time: 50.61480
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 257.79539, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 8.35958, qf2_loss: 8.33372, policy_loss: -137.83671, policy_entropy: -5.86260, alpha: 0.03964, time: 50.86114
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 29.62650, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 5.91832, qf2_loss: 5.87634, policy_loss: -137.61607, policy_entropy: -5.86508, alpha: 0.03940, time: 50.45578
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 537.11063, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 5.70697, qf2_loss: 5.68970, policy_loss: -135.93284, policy_entropy: -5.62902, alpha: 0.03895, time: 50.63020
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 31.57277, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 4.98303, qf2_loss: 4.90584, policy_loss: -135.60164, policy_entropy: -5.70601, alpha: 0.03822, time: 50.93312
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 534.80050, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 6.56284, qf2_loss: 6.56843, policy_loss: -138.57600, policy_entropy: -5.80364, alpha: 0.03776, time: 50.63575
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 454.72192, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 5.72625, qf2_loss: 5.76254, policy_loss: -137.00685, policy_entropy: -5.96923, alpha: 0.03748, time: 50.74784
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 30.11239, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 6.02804, qf2_loss: 5.93696, policy_loss: -136.01889, policy_entropy: -5.87452, alpha: 0.03741, time: 50.93679
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 33.87899, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 5.06007, qf2_loss: 4.97529, policy_loss: -136.67768, policy_entropy: -5.82528, alpha: 0.03707, time: 50.53509
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 233.97603, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 5.34133, qf2_loss: 5.21503, policy_loss: -137.85322, policy_entropy: -6.05185, alpha: 0.03686, time: 50.52612
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 531.50792, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 5.05749, qf2_loss: 5.05084, policy_loss: -136.91318, policy_entropy: -6.07677, alpha: 0.03709, time: 50.82539
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 429.70511, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 6.04299, qf2_loss: 5.93419, policy_loss: -136.51968, policy_entropy: -6.13331, alpha: 0.03724, time: 50.90039
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 530.15330, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 6.22646, qf2_loss: 6.20680, policy_loss: -137.41210, policy_entropy: -6.23028, alpha: 0.03755, time: 50.82631
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 566.57402, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 7.72854, qf2_loss: 7.71822, policy_loss: -136.73068, policy_entropy: -6.17132, alpha: 0.03808, time: 50.95291
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 455.67787, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 10.49133, qf2_loss: 10.43408, policy_loss: -137.10735, policy_entropy: -6.30246, alpha: 0.03857, time: 50.96515
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 574.78064, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 9.48731, qf2_loss: 9.45769, policy_loss: -138.07792, policy_entropy: -6.43562, alpha: 0.03929, time: 51.03719
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 461.76415, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 17.95606, qf2_loss: 18.03259, policy_loss: -138.30378, policy_entropy: -6.28549, alpha: 0.03986, time: 50.95685
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 558.33994, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 8.22677, qf2_loss: 8.26779, policy_loss: -138.93795, policy_entropy: -6.66948, alpha: 0.04099, time: 53.07382
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 573.66221, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 6.29365, qf2_loss: 6.29493, policy_loss: -138.82900, policy_entropy: -6.12118, alpha: 0.04190, time: 50.83345
[CW] eval: return: 537.88824, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 588.47679, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 5.58744, qf2_loss: 5.59065, policy_loss: -138.53340, policy_entropy: -6.06565, alpha: 0.04208, time: 51.09256
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 149.20647, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 5.40716, qf2_loss: 5.42106, policy_loss: -138.59058, policy_entropy: -6.11650, alpha: 0.04228, time: 51.06259
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 594.51666, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 5.59732, qf2_loss: 5.59476, policy_loss: -139.58634, policy_entropy: -6.13920, alpha: 0.04267, time: 50.82283
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 454.36327, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 6.28828, qf2_loss: 6.28552, policy_loss: -138.77303, policy_entropy: -6.01943, alpha: 0.04279, time: 50.92141
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 550.72993, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 6.82307, qf2_loss: 6.68067, policy_loss: -139.51314, policy_entropy: -6.25801, alpha: 0.04302, time: 51.00463
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 526.72912, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 5.49570, qf2_loss: 5.47248, policy_loss: -139.05870, policy_entropy: -6.12024, alpha: 0.04360, time: 50.90184
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 580.79928, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 6.79613, qf2_loss: 6.84354, policy_loss: -138.81973, policy_entropy: -5.91544, alpha: 0.04364, time: 50.97089
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 609.34804, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 5.63657, qf2_loss: 5.67494, policy_loss: -138.93029, policy_entropy: -6.04957, alpha: 0.04360, time: 50.86589
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 605.57477, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 5.70544, qf2_loss: 5.62450, policy_loss: -139.99687, policy_entropy: -6.10832, alpha: 0.04369, time: 50.75940
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 387.31155, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 9.36138, qf2_loss: 9.49553, policy_loss: -139.81727, policy_entropy: -6.08743, alpha: 0.04396, time: 50.78228
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 520.80039, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 9.82594, qf2_loss: 9.94103, policy_loss: -140.05479, policy_entropy: -6.25885, alpha: 0.04442, time: 50.76773
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 546.76038, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 6.37238, qf2_loss: 6.30982, policy_loss: -141.67786, policy_entropy: -6.21055, alpha: 0.04502, time: 50.92775
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 550.50573, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 5.82781, qf2_loss: 5.81319, policy_loss: -139.90788, policy_entropy: -6.19469, alpha: 0.04563, time: 50.87057
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 596.42276, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 5.76093, qf2_loss: 5.60743, policy_loss: -141.35625, policy_entropy: -6.12096, alpha: 0.04605, time: 51.42083
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 585.74124, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 5.83383, qf2_loss: 5.76251, policy_loss: -141.93473, policy_entropy: -6.14641, alpha: 0.04624, time: 50.89861
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 574.61291, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 5.62256, qf2_loss: 5.57582, policy_loss: -140.41510, policy_entropy: -6.05832, alpha: 0.04653, time: 50.79934
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 609.56770, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 5.99239, qf2_loss: 6.07485, policy_loss: -140.26457, policy_entropy: -5.99305, alpha: 0.04664, time: 50.68400
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 52.35313, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 6.24679, qf2_loss: 6.17034, policy_loss: -141.51296, policy_entropy: -6.04775, alpha: 0.04676, time: 50.55761
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 558.14857, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 5.73312, qf2_loss: 5.67306, policy_loss: -141.90861, policy_entropy: -6.11053, alpha: 0.04688, time: 50.71644
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 537.35259, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 11.61721, qf2_loss: 11.86444, policy_loss: -141.39243, policy_entropy: -5.92887, alpha: 0.04703, time: 50.74095
[CW] eval: return: 540.16656, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 530.33677, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 7.72981, qf2_loss: 7.75501, policy_loss: -142.21027, policy_entropy: -6.19588, alpha: 0.04712, time: 51.75863
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 591.98106, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 6.60916, qf2_loss: 6.61662, policy_loss: -142.12924, policy_entropy: -6.13536, alpha: 0.04767, time: 50.70565
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 583.50877, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 6.00904, qf2_loss: 5.98825, policy_loss: -141.93489, policy_entropy: -6.04227, alpha: 0.04785, time: 50.50553
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 565.68027, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 5.16064, qf2_loss: 5.15269, policy_loss: -143.43027, policy_entropy: -6.06912, alpha: 0.04801, time: 50.79544
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 564.48336, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 5.94385, qf2_loss: 5.93994, policy_loss: -143.47613, policy_entropy: -5.98489, alpha: 0.04816, time: 50.57840
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 572.04029, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 7.11691, qf2_loss: 6.96764, policy_loss: -143.72371, policy_entropy: -5.87839, alpha: 0.04792, time: 50.60712
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 491.72222, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 5.96905, qf2_loss: 6.00101, policy_loss: -143.97835, policy_entropy: -5.92933, alpha: 0.04762, time: 50.75130
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 598.40071, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 5.86420, qf2_loss: 5.78556, policy_loss: -144.94018, policy_entropy: -5.99915, alpha: 0.04757, time: 50.62417
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 577.27984, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 6.12712, qf2_loss: 6.19061, policy_loss: -143.48171, policy_entropy: -5.94139, alpha: 0.04753, time: 50.97540
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 274.67308, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 6.41690, qf2_loss: 6.39004, policy_loss: -144.63096, policy_entropy: -5.93927, alpha: 0.04734, time: 50.76424
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 557.01572, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 6.90656, qf2_loss: 6.84624, policy_loss: -142.88645, policy_entropy: -5.85093, alpha: 0.04697, time: 50.84906
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 589.94375, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 5.74508, qf2_loss: 5.68188, policy_loss: -143.85297, policy_entropy: -5.97064, alpha: 0.04680, time: 50.54001
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 565.74750, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 8.25832, qf2_loss: 8.32672, policy_loss: -143.11141, policy_entropy: -5.86289, alpha: 0.04659, time: 50.69413
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 551.95968, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 10.51529, qf2_loss: 10.81019, policy_loss: -144.36276, policy_entropy: -6.11273, alpha: 0.04631, time: 50.50214
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 546.77588, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 7.72460, qf2_loss: 7.88404, policy_loss: -146.71925, policy_entropy: -6.23669, alpha: 0.04691, time: 50.68818
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 590.99378, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 6.67219, qf2_loss: 6.67622, policy_loss: -145.99146, policy_entropy: -5.98099, alpha: 0.04726, time: 50.92926
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 577.47547, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 6.70995, qf2_loss: 6.80018, policy_loss: -145.15380, policy_entropy: -6.01909, alpha: 0.04723, time: 50.68869
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 610.36432, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 6.55357, qf2_loss: 6.57299, policy_loss: -145.29232, policy_entropy: -6.02059, alpha: 0.04733, time: 50.55944
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 602.29802, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 6.52778, qf2_loss: 6.56270, policy_loss: -144.87218, policy_entropy: -5.94379, alpha: 0.04716, time: 50.45140
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 567.57270, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 7.62303, qf2_loss: 7.81632, policy_loss: -146.56751, policy_entropy: -6.00874, alpha: 0.04722, time: 50.65508
[CW] eval: return: 582.83191, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 575.80537, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 7.39424, qf2_loss: 7.45555, policy_loss: -146.43929, policy_entropy: -5.94356, alpha: 0.04707, time: 50.83741
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 459.36717, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 6.65709, qf2_loss: 6.52547, policy_loss: -145.53774, policy_entropy: -5.94161, alpha: 0.04700, time: 50.64612
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 563.83760, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 6.31258, qf2_loss: 6.32177, policy_loss: -146.58626, policy_entropy: -5.93169, alpha: 0.04673, time: 50.64233
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 563.62420, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 6.29846, qf2_loss: 6.31786, policy_loss: -146.49144, policy_entropy: -6.03343, alpha: 0.04671, time: 50.74719
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 552.09626, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 5.76095, qf2_loss: 5.73734, policy_loss: -146.33020, policy_entropy: -5.93178, alpha: 0.04670, time: 50.52485
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 566.22852, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 6.93191, qf2_loss: 6.98643, policy_loss: -148.18440, policy_entropy: -6.09447, alpha: 0.04675, time: 50.82336
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 524.23379, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 7.41915, qf2_loss: 7.55173, policy_loss: -145.27216, policy_entropy: -5.85209, alpha: 0.04670, time: 50.90839
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 568.87754, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 6.85865, qf2_loss: 6.79151, policy_loss: -145.22825, policy_entropy: -5.89477, alpha: 0.04630, time: 50.61496
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 487.76381, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 9.62075, qf2_loss: 9.75662, policy_loss: -148.34007, policy_entropy: -6.22592, alpha: 0.04649, time: 50.74941
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 599.22148, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 6.66435, qf2_loss: 6.62930, policy_loss: -149.01474, policy_entropy: -6.08691, alpha: 0.04691, time: 50.66052
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 552.06664, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 6.91520, qf2_loss: 6.94566, policy_loss: -149.15706, policy_entropy: -6.04379, alpha: 0.04706, time: 50.62802
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 594.74835, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 7.59980, qf2_loss: 7.58184, policy_loss: -146.03029, policy_entropy: -5.90985, alpha: 0.04701, time: 50.65750
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 570.20326, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 6.68050, qf2_loss: 6.65401, policy_loss: -148.82205, policy_entropy: -6.03210, alpha: 0.04690, time: 51.42798
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 228.47992, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 7.33933, qf2_loss: 7.37489, policy_loss: -148.17060, policy_entropy: -5.97173, alpha: 0.04694, time: 50.79062
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 563.38212, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 6.95174, qf2_loss: 6.88488, policy_loss: -150.09375, policy_entropy: -6.11254, alpha: 0.04703, time: 50.67580
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 572.71240, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 8.55165, qf2_loss: 8.46758, policy_loss: -148.98918, policy_entropy: -5.99872, alpha: 0.04721, time: 50.88214
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 590.96042, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 40.08611, qf2_loss: 40.10764, policy_loss: -148.83059, policy_entropy: -6.22806, alpha: 0.04722, time: 51.07610
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 499.64092, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 13.35240, qf2_loss: 13.55668, policy_loss: -148.49731, policy_entropy: -6.41092, alpha: 0.04844, time: 50.63263
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 620.45173, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 9.08214, qf2_loss: 9.25511, policy_loss: -148.28520, policy_entropy: -6.12641, alpha: 0.04909, time: 50.88064
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 565.70350, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 7.16344, qf2_loss: 7.22856, policy_loss: -150.93878, policy_entropy: -6.09169, alpha: 0.04946, time: 50.54102
[CW] eval: return: 566.44228, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 625.47557, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 6.58518, qf2_loss: 6.51353, policy_loss: -151.87487, policy_entropy: -5.86884, alpha: 0.04937, time: 50.64763
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 589.38663, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 6.67826, qf2_loss: 6.65773, policy_loss: -150.72044, policy_entropy: -5.87935, alpha: 0.04909, time: 50.60316
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 613.28500, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 6.47859, qf2_loss: 6.51176, policy_loss: -150.83861, policy_entropy: -5.94900, alpha: 0.04876, time: 50.57205
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 601.69809, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 6.49386, qf2_loss: 6.56887, policy_loss: -150.48386, policy_entropy: -6.08751, alpha: 0.04867, time: 50.50032
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 620.49806, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 7.33767, qf2_loss: 7.26276, policy_loss: -151.81475, policy_entropy: -6.10222, alpha: 0.04894, time: 50.55571
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 632.18861, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 7.84228, qf2_loss: 7.72946, policy_loss: -151.60328, policy_entropy: -6.02599, alpha: 0.04924, time: 53.77839
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 606.80512, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 7.87040, qf2_loss: 7.80401, policy_loss: -152.55733, policy_entropy: -5.92631, alpha: 0.04922, time: 50.49381
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 612.24122, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 8.31780, qf2_loss: 8.50733, policy_loss: -151.62781, policy_entropy: -6.03379, alpha: 0.04907, time: 50.65146
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 625.41303, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 14.93710, qf2_loss: 14.68801, policy_loss: -151.10814, policy_entropy: -5.96075, alpha: 0.04920, time: 50.45564
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 556.91326, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 11.85996, qf2_loss: 11.99125, policy_loss: -150.84915, policy_entropy: -6.04396, alpha: 0.04918, time: 50.61215
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 608.02044, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 9.30300, qf2_loss: 9.28667, policy_loss: -153.29408, policy_entropy: -6.06234, alpha: 0.04938, time: 50.82885
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 588.41809, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 10.30566, qf2_loss: 10.31535, policy_loss: -153.14589, policy_entropy: -5.92891, alpha: 0.04935, time: 50.66073
[CW] ---------------------------
[CW] ---- Iteration:   673 ----
[CW] collect: return: 499.15459, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 11.45800, qf2_loss: 11.49668, policy_loss: -153.81290, policy_entropy: -5.85398, alpha: 0.04910, time: 50.52025
[CW] ---------------------------
[CW] ---- Iteration:   674 ----
[CW] collect: return: 593.34389, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 22.34915, qf2_loss: 21.83874, policy_loss: -152.75708, policy_entropy: -5.95774, alpha: 0.04862, time: 50.55654
[CW] ---------------------------
[CW] ---- Iteration:   675 ----
[CW] collect: return: 591.82990, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 13.02458, qf2_loss: 13.05032, policy_loss: -153.41993, policy_entropy: -6.20809, alpha: 0.04887, time: 50.66737
[CW] ---------------------------
