{"collect/return": 849.9509802183602, "collect/steps": 1000.0, "collect/total_steps": 554000.0, "train/qf1_loss": 34.002037372589115, "train/qf2_loss": 34.1647561454773, "train/policy_loss": -514.21400390625, "train/policy_entropy": -0.9813093495368957, "train/alpha": 0.5599794811010361, "train/time": 51.53498888015747, "eval/return": 845.6127119190248, "eval/steps": 1000.0, "_timestamp": 1678342219.2627933, "_runtime": 28764.813808202744, "_step": 548}