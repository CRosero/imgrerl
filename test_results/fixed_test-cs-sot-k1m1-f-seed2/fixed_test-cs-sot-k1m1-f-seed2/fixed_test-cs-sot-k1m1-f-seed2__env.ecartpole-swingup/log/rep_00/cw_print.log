[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 167.69149, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.79466, qf2_loss: 1.79693, policy_loss: -2.78364, policy_entropy: 0.68270, alpha: 0.98504, time: 50.35119
[CW] eval: return: 113.35793, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 47.45614, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.10087, qf2_loss: 0.10115, policy_loss: -3.19801, policy_entropy: 0.68121, alpha: 0.95627, time: 49.81065
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 193.44003, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.09266, qf2_loss: 0.09397, policy_loss: -3.84855, policy_entropy: 0.67618, alpha: 0.92876, time: 49.93594
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 86.33836, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.10032, qf2_loss: 0.10257, policy_loss: -4.40480, policy_entropy: 0.67081, alpha: 0.90245, time: 50.07123
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 197.08100, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.13323, qf2_loss: 0.13584, policy_loss: -5.11540, policy_entropy: 0.66572, alpha: 0.87725, time: 50.46089
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 91.30015, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.16815, qf2_loss: 0.17096, policy_loss: -5.75031, policy_entropy: 0.66209, alpha: 0.85308, time: 50.28768
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 97.34469, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.19388, qf2_loss: 0.19647, policy_loss: -6.35126, policy_entropy: 0.65976, alpha: 0.82986, time: 50.26961
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 91.82244, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.23594, qf2_loss: 0.23868, policy_loss: -6.98114, policy_entropy: 0.65437, alpha: 0.80753, time: 50.79498
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 131.56545, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.30774, qf2_loss: 0.31124, policy_loss: -7.69721, policy_entropy: 0.64845, alpha: 0.78607, time: 50.62281
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 72.64566, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.30068, qf2_loss: 0.30286, policy_loss: -8.21229, policy_entropy: 0.64289, alpha: 0.76543, time: 50.68149
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 60.38752, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.43589, qf2_loss: 0.43839, policy_loss: -8.64825, policy_entropy: 0.63938, alpha: 0.74555, time: 50.70880
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 91.06641, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.41521, qf2_loss: 0.41416, policy_loss: -9.21043, policy_entropy: 0.63328, alpha: 0.72638, time: 50.50993
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 157.52017, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.53793, qf2_loss: 0.53781, policy_loss: -9.98461, policy_entropy: 0.62038, alpha: 0.70793, time: 50.99327
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 88.13225, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.53550, qf2_loss: 0.53549, policy_loss: -10.48532, policy_entropy: 0.61139, alpha: 0.69017, time: 50.65574
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 284.42517, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.69169, qf2_loss: 0.69094, policy_loss: -11.48116, policy_entropy: 0.58619, alpha: 0.67312, time: 50.70918
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 76.17007, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.64119, qf2_loss: 0.63923, policy_loss: -11.96229, policy_entropy: 0.56374, alpha: 0.65680, time: 50.98930
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 186.43991, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.80588, qf2_loss: 0.80219, policy_loss: -12.74313, policy_entropy: 0.53439, alpha: 0.64115, time: 51.15343
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 126.41464, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.77190, qf2_loss: 0.76507, policy_loss: -13.44942, policy_entropy: 0.50653, alpha: 0.62618, time: 51.15943
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 115.68477, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.76516, qf2_loss: 0.76109, policy_loss: -14.17494, policy_entropy: 0.48182, alpha: 0.61180, time: 50.80950
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 251.59515, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.94021, qf2_loss: 0.93849, policy_loss: -15.18379, policy_entropy: 0.44326, alpha: 0.59800, time: 51.01556
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 239.19171, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 1.03339, qf2_loss: 1.03385, policy_loss: -16.08364, policy_entropy: 0.41497, alpha: 0.58481, time: 50.73754
[CW] eval: return: 220.44652, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 174.05654, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 1.05793, qf2_loss: 1.05902, policy_loss: -16.90255, policy_entropy: 0.38745, alpha: 0.57210, time: 50.93228
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 220.72737, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 1.17478, qf2_loss: 1.16537, policy_loss: -17.75516, policy_entropy: 0.36298, alpha: 0.55983, time: 50.91610
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 203.54874, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 1.07444, qf2_loss: 1.07188, policy_loss: -18.56005, policy_entropy: 0.32749, alpha: 0.54799, time: 50.68592
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 151.09258, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 1.32990, qf2_loss: 1.32483, policy_loss: -19.35255, policy_entropy: 0.29509, alpha: 0.53663, time: 50.91318
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 206.72790, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 2.16028, qf2_loss: 2.15973, policy_loss: -20.45233, policy_entropy: 0.27846, alpha: 0.52566, time: 51.03093
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 213.30158, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 1.53029, qf2_loss: 1.52206, policy_loss: -21.43641, policy_entropy: 0.24546, alpha: 0.51496, time: 50.86585
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 299.62818, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 1.46114, qf2_loss: 1.45928, policy_loss: -22.65772, policy_entropy: 0.21071, alpha: 0.50475, time: 51.32737
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 296.14865, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 1.61998, qf2_loss: 1.61393, policy_loss: -23.86743, policy_entropy: 0.18751, alpha: 0.49482, time: 50.99376
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 165.60585, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 1.58381, qf2_loss: 1.58203, policy_loss: -24.56447, policy_entropy: 0.15978, alpha: 0.48522, time: 51.02911
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 296.67410, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 1.58267, qf2_loss: 1.58108, policy_loss: -25.90219, policy_entropy: 0.12690, alpha: 0.47590, time: 51.32088
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 208.01831, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 1.59288, qf2_loss: 1.59114, policy_loss: -26.77278, policy_entropy: 0.09287, alpha: 0.46697, time: 51.01742
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 233.51386, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 1.55840, qf2_loss: 1.56152, policy_loss: -27.87757, policy_entropy: 0.07798, alpha: 0.45828, time: 51.11102
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 223.71342, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 1.60682, qf2_loss: 1.59887, policy_loss: -28.67194, policy_entropy: 0.05483, alpha: 0.44977, time: 51.06811
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 279.57282, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 1.74857, qf2_loss: 1.74312, policy_loss: -30.06280, policy_entropy: 0.02776, alpha: 0.44153, time: 50.81999
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 231.54619, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 1.70435, qf2_loss: 1.70838, policy_loss: -30.80754, policy_entropy: 0.00406, alpha: 0.43351, time: 50.96213
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 198.22503, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 2.05213, qf2_loss: 2.04268, policy_loss: -31.72744, policy_entropy: -0.00958, alpha: 0.42567, time: 51.11058
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 185.21946, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 1.88726, qf2_loss: 1.88002, policy_loss: -33.17750, policy_entropy: -0.02289, alpha: 0.41795, time: 50.96160
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 181.28969, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 1.88205, qf2_loss: 1.87194, policy_loss: -34.00368, policy_entropy: -0.04305, alpha: 0.41042, time: 51.03747
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 208.83375, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 1.83495, qf2_loss: 1.83330, policy_loss: -34.93443, policy_entropy: -0.05871, alpha: 0.40306, time: 50.96548
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 205.42701, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 1.90038, qf2_loss: 1.89989, policy_loss: -36.08105, policy_entropy: -0.08271, alpha: 0.39584, time: 50.84754
[CW] eval: return: 226.15733, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 202.01988, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 1.99848, qf2_loss: 2.00041, policy_loss: -36.63180, policy_entropy: -0.11091, alpha: 0.38888, time: 51.14545
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 308.05028, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 2.54429, qf2_loss: 2.53378, policy_loss: -37.59316, policy_entropy: -0.14408, alpha: 0.38217, time: 51.07096
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 205.34564, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 2.06491, qf2_loss: 2.05772, policy_loss: -38.92946, policy_entropy: -0.14200, alpha: 0.37558, time: 50.71385
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 196.37528, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 2.15907, qf2_loss: 2.13238, policy_loss: -39.89465, policy_entropy: -0.18298, alpha: 0.36911, time: 51.18697
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 265.67812, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 2.22880, qf2_loss: 2.22877, policy_loss: -40.97002, policy_entropy: -0.18775, alpha: 0.36290, time: 50.91376
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 210.91881, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 2.17953, qf2_loss: 2.16550, policy_loss: -41.95406, policy_entropy: -0.18244, alpha: 0.35670, time: 50.91100
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 320.93828, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 2.37791, qf2_loss: 2.36716, policy_loss: -42.94544, policy_entropy: -0.20283, alpha: 0.35046, time: 51.20646
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 244.97178, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 2.48187, qf2_loss: 2.47638, policy_loss: -44.12042, policy_entropy: -0.21637, alpha: 0.34444, time: 50.99494
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 287.98088, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 2.28073, qf2_loss: 2.27809, policy_loss: -45.02197, policy_entropy: -0.21547, alpha: 0.33844, time: 50.98549
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 205.50391, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 2.65714, qf2_loss: 2.63669, policy_loss: -46.13811, policy_entropy: -0.23891, alpha: 0.33254, time: 51.05655
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 235.66162, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 2.27971, qf2_loss: 2.26614, policy_loss: -46.90436, policy_entropy: -0.25445, alpha: 0.32679, time: 50.76841
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 291.89170, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 2.40542, qf2_loss: 2.38672, policy_loss: -48.10327, policy_entropy: -0.24217, alpha: 0.32110, time: 50.76554
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 362.22670, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 2.53529, qf2_loss: 2.52770, policy_loss: -48.95691, policy_entropy: -0.27233, alpha: 0.31551, time: 51.05451
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 297.01857, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 2.71510, qf2_loss: 2.71025, policy_loss: -50.47584, policy_entropy: -0.26801, alpha: 0.30999, time: 50.77771
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 268.14163, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 2.84751, qf2_loss: 2.84606, policy_loss: -50.94327, policy_entropy: -0.30686, alpha: 0.30461, time: 50.96598
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 277.69924, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 2.53390, qf2_loss: 2.53287, policy_loss: -52.11741, policy_entropy: -0.30576, alpha: 0.29936, time: 51.19071
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 239.77135, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 2.48138, qf2_loss: 2.48539, policy_loss: -52.97700, policy_entropy: -0.30567, alpha: 0.29415, time: 50.93058
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 350.43182, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 2.66131, qf2_loss: 2.64512, policy_loss: -53.76324, policy_entropy: -0.30745, alpha: 0.28896, time: 50.94096
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 338.13637, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 2.75743, qf2_loss: 2.73690, policy_loss: -54.98525, policy_entropy: -0.31750, alpha: 0.28381, time: 51.10224
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 326.17008, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 2.48333, qf2_loss: 2.48774, policy_loss: -55.79749, policy_entropy: -0.34649, alpha: 0.27884, time: 50.67979
[CW] eval: return: 233.34624, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 251.94136, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 2.55797, qf2_loss: 2.54536, policy_loss: -56.94188, policy_entropy: -0.35298, alpha: 0.27404, time: 50.82041
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 276.54498, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 2.98254, qf2_loss: 2.95136, policy_loss: -57.96641, policy_entropy: -0.35152, alpha: 0.26926, time: 51.15020
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 213.23149, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 2.65227, qf2_loss: 2.65655, policy_loss: -58.62182, policy_entropy: -0.36262, alpha: 0.26446, time: 50.81438
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 216.30525, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 2.75921, qf2_loss: 2.76241, policy_loss: -59.49033, policy_entropy: -0.35352, alpha: 0.25980, time: 50.84177
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 243.08130, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 2.88459, qf2_loss: 2.91343, policy_loss: -60.31551, policy_entropy: -0.35189, alpha: 0.25506, time: 50.98735
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 247.97419, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 2.95046, qf2_loss: 2.92795, policy_loss: -60.90959, policy_entropy: -0.36657, alpha: 0.25039, time: 50.97410
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 244.90976, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 2.87977, qf2_loss: 2.86662, policy_loss: -62.25192, policy_entropy: -0.36931, alpha: 0.24583, time: 50.87752
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 351.68960, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 2.94354, qf2_loss: 2.94671, policy_loss: -63.04517, policy_entropy: -0.38148, alpha: 0.24128, time: 51.06545
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 328.67167, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 2.93103, qf2_loss: 2.92086, policy_loss: -63.96121, policy_entropy: -0.39460, alpha: 0.23692, time: 50.95424
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 245.01227, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 2.98866, qf2_loss: 3.00172, policy_loss: -64.88394, policy_entropy: -0.40870, alpha: 0.23265, time: 50.90168
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 355.40819, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 3.37384, qf2_loss: 3.37775, policy_loss: -66.07957, policy_entropy: -0.40614, alpha: 0.22848, time: 51.08557
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 338.38603, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 3.46531, qf2_loss: 3.47572, policy_loss: -66.99000, policy_entropy: -0.42286, alpha: 0.22436, time: 50.97971
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 415.39291, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 3.17601, qf2_loss: 3.17118, policy_loss: -67.79541, policy_entropy: -0.45486, alpha: 0.22041, time: 50.85388
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 332.21671, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 3.14030, qf2_loss: 3.14474, policy_loss: -68.87307, policy_entropy: -0.46422, alpha: 0.21662, time: 51.15233
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 275.71378, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 3.07337, qf2_loss: 3.08131, policy_loss: -70.00310, policy_entropy: -0.47353, alpha: 0.21286, time: 50.95193
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 286.49186, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 3.21213, qf2_loss: 3.20959, policy_loss: -71.08037, policy_entropy: -0.51066, alpha: 0.20933, time: 50.93355
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 385.45795, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 3.27346, qf2_loss: 3.28015, policy_loss: -71.80377, policy_entropy: -0.52161, alpha: 0.20596, time: 50.78161
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 264.51305, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 4.02440, qf2_loss: 4.04159, policy_loss: -72.84567, policy_entropy: -0.52930, alpha: 0.20263, time: 51.08094
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 323.87672, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 3.34843, qf2_loss: 3.36306, policy_loss: -73.96736, policy_entropy: -0.54947, alpha: 0.19936, time: 50.88953
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 286.99860, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 3.29803, qf2_loss: 3.32829, policy_loss: -75.13673, policy_entropy: -0.56940, alpha: 0.19625, time: 50.86926
[CW] eval: return: 321.77311, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 350.74154, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 3.41628, qf2_loss: 3.39934, policy_loss: -75.56782, policy_entropy: -0.59519, alpha: 0.19322, time: 51.05217
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 373.80618, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 3.65821, qf2_loss: 3.64125, policy_loss: -76.92612, policy_entropy: -0.60237, alpha: 0.19040, time: 50.77962
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 380.57033, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 3.64155, qf2_loss: 3.62096, policy_loss: -78.04110, policy_entropy: -0.59484, alpha: 0.18755, time: 50.96701
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 337.23935, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 3.46052, qf2_loss: 3.46106, policy_loss: -78.96299, policy_entropy: -0.63509, alpha: 0.18472, time: 51.26775
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 274.96384, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 3.68334, qf2_loss: 3.66421, policy_loss: -79.85524, policy_entropy: -0.65776, alpha: 0.18215, time: 50.98593
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 321.51823, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 3.54232, qf2_loss: 3.51689, policy_loss: -80.98839, policy_entropy: -0.66916, alpha: 0.17965, time: 50.69161
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 280.49949, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 3.72441, qf2_loss: 3.71468, policy_loss: -81.91035, policy_entropy: -0.70173, alpha: 0.17727, time: 51.08552
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 236.45544, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 3.43041, qf2_loss: 3.39519, policy_loss: -82.81046, policy_entropy: -0.70812, alpha: 0.17498, time: 51.10379
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 254.09561, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 3.51874, qf2_loss: 3.50342, policy_loss: -83.56773, policy_entropy: -0.71976, alpha: 0.17279, time: 50.98535
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 282.73341, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 3.48661, qf2_loss: 3.47580, policy_loss: -84.68340, policy_entropy: -0.74003, alpha: 0.17069, time: 51.16766
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 301.28004, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 3.71254, qf2_loss: 3.68825, policy_loss: -85.54237, policy_entropy: -0.75020, alpha: 0.16869, time: 51.16080
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 234.25784, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 3.76130, qf2_loss: 3.78203, policy_loss: -86.21075, policy_entropy: -0.75727, alpha: 0.16667, time: 56.13218
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 363.95485, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 3.77657, qf2_loss: 3.74286, policy_loss: -87.40227, policy_entropy: -0.78401, alpha: 0.16480, time: 50.69330
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 328.40957, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 3.61906, qf2_loss: 3.61582, policy_loss: -88.31874, policy_entropy: -0.80089, alpha: 0.16299, time: 53.13151
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 363.37071, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 3.80046, qf2_loss: 3.80267, policy_loss: -88.97412, policy_entropy: -0.80890, alpha: 0.16132, time: 50.71851
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 375.32471, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 3.98336, qf2_loss: 3.95853, policy_loss: -90.15369, policy_entropy: -0.81689, alpha: 0.15966, time: 50.53325
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 354.86299, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 3.81931, qf2_loss: 3.80758, policy_loss: -90.91929, policy_entropy: -0.82613, alpha: 0.15806, time: 51.15235
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 284.69735, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 3.75081, qf2_loss: 3.74927, policy_loss: -91.85545, policy_entropy: -0.85709, alpha: 0.15655, time: 50.86554
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 275.85599, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 3.89746, qf2_loss: 3.87174, policy_loss: -92.36273, policy_entropy: -0.87589, alpha: 0.15525, time: 51.02009
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 396.82831, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 3.86688, qf2_loss: 3.84841, policy_loss: -93.66552, policy_entropy: -0.88000, alpha: 0.15414, time: 51.03886
[CW] eval: return: 378.55922, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 420.95557, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 4.04755, qf2_loss: 4.05133, policy_loss: -94.72448, policy_entropy: -0.88946, alpha: 0.15293, time: 50.87702
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 361.23354, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 3.90466, qf2_loss: 3.89080, policy_loss: -95.90210, policy_entropy: -0.90497, alpha: 0.15184, time: 50.74549
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 366.89192, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 4.45785, qf2_loss: 4.40988, policy_loss: -96.56052, policy_entropy: -0.89437, alpha: 0.15077, time: 50.86090
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 385.35807, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 4.41153, qf2_loss: 4.38705, policy_loss: -97.54223, policy_entropy: -0.89436, alpha: 0.14960, time: 51.18745
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 448.05557, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 4.28149, qf2_loss: 4.25498, policy_loss: -98.92170, policy_entropy: -0.89425, alpha: 0.14839, time: 50.96799
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 301.14501, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 4.56037, qf2_loss: 4.51560, policy_loss: -99.96586, policy_entropy: -0.92106, alpha: 0.14739, time: 50.57030
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 360.65095, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 4.03547, qf2_loss: 4.02920, policy_loss: -100.91470, policy_entropy: -0.92395, alpha: 0.14636, time: 51.06457
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 373.88319, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 4.89780, qf2_loss: 4.85018, policy_loss: -101.64198, policy_entropy: -0.92474, alpha: 0.14547, time: 50.73230
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 389.53879, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 4.71640, qf2_loss: 4.71249, policy_loss: -102.96085, policy_entropy: -0.92521, alpha: 0.14444, time: 50.73253
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 461.78352, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 5.30324, qf2_loss: 5.30651, policy_loss: -103.52858, policy_entropy: -0.94060, alpha: 0.14356, time: 51.10446
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 437.51807, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 4.51577, qf2_loss: 4.51879, policy_loss: -104.78982, policy_entropy: -0.96602, alpha: 0.14288, time: 51.02698
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 305.02544, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 4.76942, qf2_loss: 4.74175, policy_loss: -105.69973, policy_entropy: -0.97150, alpha: 0.14236, time: 50.80184
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 464.11646, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 4.74180, qf2_loss: 4.73981, policy_loss: -106.78759, policy_entropy: -0.95907, alpha: 0.14187, time: 50.74417
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 457.32422, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 5.89714, qf2_loss: 5.88131, policy_loss: -108.25859, policy_entropy: -0.97425, alpha: 0.14138, time: 51.06788
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 473.89132, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 4.86703, qf2_loss: 4.83725, policy_loss: -108.56320, policy_entropy: -1.00413, alpha: 0.14113, time: 51.00865
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 330.53791, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 6.31348, qf2_loss: 6.25481, policy_loss: -109.76600, policy_entropy: -0.97164, alpha: 0.14098, time: 50.74636
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 400.92389, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 5.88731, qf2_loss: 5.85018, policy_loss: -110.92918, policy_entropy: -0.99514, alpha: 0.14069, time: 51.23499
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 345.42137, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 4.92077, qf2_loss: 4.91407, policy_loss: -112.19713, policy_entropy: -1.00668, alpha: 0.14066, time: 50.98706
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 472.89623, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 6.29121, qf2_loss: 6.27691, policy_loss: -112.96810, policy_entropy: -1.02187, alpha: 0.14088, time: 50.71315
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 387.46930, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 5.24721, qf2_loss: 5.21435, policy_loss: -115.18369, policy_entropy: -1.02279, alpha: 0.14132, time: 50.92193
[CW] eval: return: 444.98793, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 382.83210, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 5.25438, qf2_loss: 5.20657, policy_loss: -115.56479, policy_entropy: -1.03598, alpha: 0.14203, time: 50.96996
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 444.29582, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 5.58686, qf2_loss: 5.57537, policy_loss: -116.07956, policy_entropy: -1.04657, alpha: 0.14296, time: 50.66182
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 451.25358, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 8.26055, qf2_loss: 8.17259, policy_loss: -117.15889, policy_entropy: -1.02203, alpha: 0.14388, time: 50.78808
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 395.06883, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 6.90105, qf2_loss: 6.90732, policy_loss: -118.37008, policy_entropy: -1.03309, alpha: 0.14447, time: 51.04071
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 467.12510, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 5.59130, qf2_loss: 5.54656, policy_loss: -119.95686, policy_entropy: -1.03716, alpha: 0.14527, time: 50.93900
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 424.97097, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 5.42045, qf2_loss: 5.39508, policy_loss: -120.30728, policy_entropy: -1.04064, alpha: 0.14657, time: 50.63215
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 462.37413, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 5.45923, qf2_loss: 5.42567, policy_loss: -122.11923, policy_entropy: -1.05546, alpha: 0.14788, time: 51.16718
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 364.19464, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 5.45187, qf2_loss: 5.46286, policy_loss: -123.07601, policy_entropy: -1.05836, alpha: 0.14976, time: 50.99669
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 376.74458, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 6.15381, qf2_loss: 6.17771, policy_loss: -124.10673, policy_entropy: -1.06299, alpha: 0.15162, time: 50.84565
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 373.96093, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 5.82444, qf2_loss: 5.83585, policy_loss: -125.05083, policy_entropy: -1.06288, alpha: 0.15410, time: 50.63931
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 453.73868, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 5.97039, qf2_loss: 5.94809, policy_loss: -126.27947, policy_entropy: -1.06673, alpha: 0.15659, time: 51.12134
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 443.44341, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 6.40894, qf2_loss: 6.40029, policy_loss: -127.36230, policy_entropy: -1.04955, alpha: 0.15893, time: 50.80913
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 348.66198, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 6.18209, qf2_loss: 6.14997, policy_loss: -128.44968, policy_entropy: -1.04775, alpha: 0.16087, time: 50.67321
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 447.41706, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 5.92312, qf2_loss: 5.92925, policy_loss: -129.04886, policy_entropy: -1.05030, alpha: 0.16320, time: 50.62033
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 463.30302, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 7.18579, qf2_loss: 7.07626, policy_loss: -130.14329, policy_entropy: -1.04804, alpha: 0.16549, time: 51.20419
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 377.98335, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 6.18927, qf2_loss: 6.14552, policy_loss: -131.51320, policy_entropy: -1.05184, alpha: 0.16795, time: 50.77985
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 434.99156, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 6.13792, qf2_loss: 6.10398, policy_loss: -132.25155, policy_entropy: -1.04585, alpha: 0.17070, time: 50.69356
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 396.08592, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 7.10761, qf2_loss: 7.08858, policy_loss: -134.65362, policy_entropy: -1.03520, alpha: 0.17284, time: 50.81978
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 505.94114, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 6.87819, qf2_loss: 6.86291, policy_loss: -134.43534, policy_entropy: -1.03181, alpha: 0.17500, time: 51.04773
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 360.15449, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 6.52587, qf2_loss: 6.48613, policy_loss: -136.18783, policy_entropy: -1.04298, alpha: 0.17700, time: 50.76786
[CW] eval: return: 419.25350, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 438.96710, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 6.73482, qf2_loss: 6.73474, policy_loss: -137.33225, policy_entropy: -1.03707, alpha: 0.17976, time: 50.77482
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 471.33005, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 6.00397, qf2_loss: 5.95931, policy_loss: -138.00985, policy_entropy: -1.04233, alpha: 0.18246, time: 50.76629
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 365.41839, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 7.19483, qf2_loss: 7.17098, policy_loss: -139.11531, policy_entropy: -1.02564, alpha: 0.18521, time: 50.99170
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 408.39763, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 7.54533, qf2_loss: 7.48194, policy_loss: -140.09934, policy_entropy: -1.02813, alpha: 0.18696, time: 50.91667
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 414.53511, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 7.43231, qf2_loss: 7.34199, policy_loss: -140.72714, policy_entropy: -1.02550, alpha: 0.18932, time: 50.54732
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 478.29695, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 7.12015, qf2_loss: 7.06704, policy_loss: -142.52894, policy_entropy: -1.02261, alpha: 0.19086, time: 50.65936
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 480.99131, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 6.84239, qf2_loss: 6.83526, policy_loss: -143.60255, policy_entropy: -1.02564, alpha: 0.19294, time: 51.11789
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 461.97341, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 7.82667, qf2_loss: 7.76749, policy_loss: -144.92902, policy_entropy: -1.02742, alpha: 0.19544, time: 50.77696
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 385.25176, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 6.94043, qf2_loss: 6.90261, policy_loss: -145.69250, policy_entropy: -1.02470, alpha: 0.19780, time: 50.52207
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 404.81064, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 9.61122, qf2_loss: 9.69112, policy_loss: -146.68965, policy_entropy: -1.02142, alpha: 0.20020, time: 50.57722
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 456.11365, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 9.81950, qf2_loss: 9.66617, policy_loss: -148.50113, policy_entropy: -1.00756, alpha: 0.20142, time: 51.03189
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 451.30849, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 7.37128, qf2_loss: 7.32907, policy_loss: -148.44275, policy_entropy: -1.02897, alpha: 0.20271, time: 50.99686
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 486.74165, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 7.24667, qf2_loss: 7.19192, policy_loss: -149.84468, policy_entropy: -1.03233, alpha: 0.20592, time: 50.59570
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 453.70110, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 8.27994, qf2_loss: 8.21965, policy_loss: -150.28228, policy_entropy: -1.04092, alpha: 0.20944, time: 50.66416
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 458.86549, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 8.03470, qf2_loss: 7.99300, policy_loss: -152.26205, policy_entropy: -1.00676, alpha: 0.21260, time: 50.94888
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 454.96470, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 7.58676, qf2_loss: 7.57143, policy_loss: -153.63814, policy_entropy: -1.02695, alpha: 0.21383, time: 50.88990
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 396.01868, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 8.34900, qf2_loss: 8.21338, policy_loss: -154.04015, policy_entropy: -1.00921, alpha: 0.21589, time: 50.92323
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 519.65909, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 9.17627, qf2_loss: 9.13404, policy_loss: -154.52603, policy_entropy: -1.01816, alpha: 0.21735, time: 50.80355
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 460.48475, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 9.27966, qf2_loss: 9.20663, policy_loss: -155.81242, policy_entropy: -1.03717, alpha: 0.22065, time: 50.55954
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 475.76799, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 7.74864, qf2_loss: 7.73341, policy_loss: -157.32877, policy_entropy: -1.02137, alpha: 0.22401, time: 50.67942
[CW] eval: return: 406.56411, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 386.93828, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 8.53008, qf2_loss: 8.41288, policy_loss: -157.96387, policy_entropy: -1.01679, alpha: 0.22648, time: 50.70623
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 463.94949, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 9.62298, qf2_loss: 9.53883, policy_loss: -158.94507, policy_entropy: -1.01922, alpha: 0.22916, time: 50.70044
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 400.02885, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 8.22120, qf2_loss: 8.22193, policy_loss: -160.47306, policy_entropy: -1.02365, alpha: 0.23117, time: 50.47770
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 438.71534, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 8.95250, qf2_loss: 8.83155, policy_loss: -161.78766, policy_entropy: -1.01035, alpha: 0.23357, time: 50.46896
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 398.73606, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 9.45915, qf2_loss: 9.42795, policy_loss: -162.78771, policy_entropy: -1.01746, alpha: 0.23538, time: 50.53856
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 447.35800, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 9.52841, qf2_loss: 9.43044, policy_loss: -163.52087, policy_entropy: -1.01679, alpha: 0.23736, time: 54.99243
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 452.62974, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 9.10791, qf2_loss: 8.98706, policy_loss: -165.21523, policy_entropy: -1.01153, alpha: 0.23948, time: 50.64942
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 525.54995, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 9.47051, qf2_loss: 9.47282, policy_loss: -166.27286, policy_entropy: -1.00773, alpha: 0.24092, time: 52.01304
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 461.27050, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 8.68064, qf2_loss: 8.60959, policy_loss: -166.98247, policy_entropy: -1.01482, alpha: 0.24285, time: 50.72777
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 466.55634, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 8.85539, qf2_loss: 8.78459, policy_loss: -168.19947, policy_entropy: -1.03030, alpha: 0.24571, time: 50.71079
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 545.17030, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 9.04025, qf2_loss: 8.96215, policy_loss: -168.57833, policy_entropy: -1.00922, alpha: 0.24880, time: 50.49418
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 458.48933, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 8.73979, qf2_loss: 8.73225, policy_loss: -170.67856, policy_entropy: -1.01400, alpha: 0.24996, time: 50.60201
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 474.16300, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 8.77639, qf2_loss: 8.70028, policy_loss: -170.71629, policy_entropy: -1.01167, alpha: 0.25195, time: 50.43060
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 461.32231, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 8.31340, qf2_loss: 8.25082, policy_loss: -172.01913, policy_entropy: -1.00578, alpha: 0.25341, time: 50.43416
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 473.15645, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 8.52665, qf2_loss: 8.48560, policy_loss: -173.94898, policy_entropy: -0.99822, alpha: 0.25412, time: 50.44656
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 454.02149, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 9.53243, qf2_loss: 9.49893, policy_loss: -174.06290, policy_entropy: -1.02652, alpha: 0.25574, time: 50.36792
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 505.63824, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 9.53156, qf2_loss: 9.54920, policy_loss: -174.75575, policy_entropy: -1.00006, alpha: 0.25748, time: 50.45328
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 467.56889, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 8.80760, qf2_loss: 8.75644, policy_loss: -176.81051, policy_entropy: -1.01261, alpha: 0.25852, time: 50.46205
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 425.55329, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 9.59470, qf2_loss: 9.58527, policy_loss: -177.82278, policy_entropy: -0.99857, alpha: 0.25965, time: 50.47200
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 479.52628, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 7.70124, qf2_loss: 7.68451, policy_loss: -178.61895, policy_entropy: -0.99970, alpha: 0.25987, time: 50.66532
[CW] eval: return: 439.04251, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 475.89757, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 8.15273, qf2_loss: 8.16347, policy_loss: -179.04078, policy_entropy: -1.00639, alpha: 0.25973, time: 50.64414
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 509.97224, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 7.76817, qf2_loss: 7.78081, policy_loss: -180.37076, policy_entropy: -0.99690, alpha: 0.26021, time: 50.49421
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 413.59646, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 7.53298, qf2_loss: 7.56695, policy_loss: -182.05948, policy_entropy: -1.01507, alpha: 0.26085, time: 50.49079
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 538.68726, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 10.99944, qf2_loss: 10.97644, policy_loss: -183.51203, policy_entropy: -0.99352, alpha: 0.26189, time: 50.55585
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 508.77064, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 8.52287, qf2_loss: 8.48625, policy_loss: -184.05893, policy_entropy: -1.00290, alpha: 0.26126, time: 50.46749
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 534.93279, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 7.97232, qf2_loss: 7.93403, policy_loss: -185.06204, policy_entropy: -1.00988, alpha: 0.26254, time: 50.47565
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 395.44796, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 8.50098, qf2_loss: 8.49607, policy_loss: -185.73362, policy_entropy: -1.00101, alpha: 0.26354, time: 50.50432
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 478.37348, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 9.07583, qf2_loss: 9.01735, policy_loss: -185.79873, policy_entropy: -1.01421, alpha: 0.26393, time: 50.51905
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 473.44564, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 9.66627, qf2_loss: 9.49908, policy_loss: -188.71871, policy_entropy: -0.99340, alpha: 0.26533, time: 50.44909
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 471.53023, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 8.18175, qf2_loss: 8.19134, policy_loss: -189.20912, policy_entropy: -1.00723, alpha: 0.26505, time: 50.56025
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 508.25165, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 8.18862, qf2_loss: 8.10136, policy_loss: -190.56738, policy_entropy: -1.00496, alpha: 0.26635, time: 50.55181
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 448.36378, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 7.56839, qf2_loss: 7.53054, policy_loss: -190.95549, policy_entropy: -1.00659, alpha: 0.26710, time: 50.48233
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 501.36497, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 7.79181, qf2_loss: 7.73091, policy_loss: -192.19572, policy_entropy: -1.01342, alpha: 0.26857, time: 50.45848
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 498.74734, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 8.08212, qf2_loss: 8.05041, policy_loss: -193.21464, policy_entropy: -0.99757, alpha: 0.26970, time: 50.56310
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 528.29315, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 9.59817, qf2_loss: 9.64415, policy_loss: -194.46934, policy_entropy: -0.99264, alpha: 0.26974, time: 50.50828
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 521.15164, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 8.08658, qf2_loss: 8.09724, policy_loss: -195.15033, policy_entropy: -1.00246, alpha: 0.26854, time: 50.47334
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 485.88003, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 7.64810, qf2_loss: 7.57416, policy_loss: -197.14173, policy_entropy: -1.01816, alpha: 0.27013, time: 50.48970
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 612.01855, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 7.28055, qf2_loss: 7.26295, policy_loss: -197.32443, policy_entropy: -1.01681, alpha: 0.27292, time: 50.72074
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 487.37000, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 8.18704, qf2_loss: 8.21962, policy_loss: -198.40567, policy_entropy: -1.00334, alpha: 0.27496, time: 50.46440
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 540.60424, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 8.95943, qf2_loss: 8.88699, policy_loss: -199.13882, policy_entropy: -1.01183, alpha: 0.27610, time: 50.49962
[CW] eval: return: 483.04334, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 497.22314, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 7.91428, qf2_loss: 7.96480, policy_loss: -200.04080, policy_entropy: -1.00716, alpha: 0.27733, time: 50.55653
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 534.97071, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 7.86093, qf2_loss: 7.80975, policy_loss: -200.92058, policy_entropy: -1.00641, alpha: 0.27897, time: 50.56312
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 530.30825, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 7.82625, qf2_loss: 7.79109, policy_loss: -201.88289, policy_entropy: -0.99645, alpha: 0.27944, time: 50.63872
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 516.41590, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 7.88910, qf2_loss: 7.86933, policy_loss: -202.50548, policy_entropy: -0.99103, alpha: 0.27881, time: 50.49065
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 526.86925, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 8.41956, qf2_loss: 8.38235, policy_loss: -205.48173, policy_entropy: -0.99626, alpha: 0.27697, time: 50.54415
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 521.30429, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 8.04259, qf2_loss: 8.06767, policy_loss: -205.27725, policy_entropy: -1.00371, alpha: 0.27636, time: 50.59236
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 547.41251, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 8.42520, qf2_loss: 8.31822, policy_loss: -205.05448, policy_entropy: -1.00140, alpha: 0.27761, time: 50.47106
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 538.55940, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 8.23446, qf2_loss: 8.27125, policy_loss: -207.44067, policy_entropy: -1.00058, alpha: 0.27734, time: 50.56393
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 573.04666, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 8.65516, qf2_loss: 8.68961, policy_loss: -208.07805, policy_entropy: -1.00289, alpha: 0.27747, time: 50.66063
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 461.80053, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 8.39610, qf2_loss: 8.35211, policy_loss: -209.19510, policy_entropy: -1.00491, alpha: 0.27838, time: 50.59524
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 527.84580, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 8.69297, qf2_loss: 8.74928, policy_loss: -210.11411, policy_entropy: -1.00170, alpha: 0.27897, time: 50.51818
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 533.84590, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 8.70194, qf2_loss: 8.67313, policy_loss: -210.89815, policy_entropy: -1.01073, alpha: 0.28021, time: 50.54535
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 480.51438, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 8.39079, qf2_loss: 8.32058, policy_loss: -212.13033, policy_entropy: -0.99736, alpha: 0.28061, time: 50.71436
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 554.24048, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 8.36344, qf2_loss: 8.37238, policy_loss: -213.13378, policy_entropy: -1.01730, alpha: 0.28231, time: 50.71018
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 475.69018, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 8.19184, qf2_loss: 8.06378, policy_loss: -214.05335, policy_entropy: -1.01477, alpha: 0.28501, time: 50.47523
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 609.89067, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 8.64596, qf2_loss: 8.59810, policy_loss: -215.43686, policy_entropy: -1.01145, alpha: 0.28863, time: 50.45071
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 594.89398, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 8.50554, qf2_loss: 8.38125, policy_loss: -216.18997, policy_entropy: -1.01438, alpha: 0.29088, time: 50.51355
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 466.01228, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 8.54229, qf2_loss: 8.46260, policy_loss: -217.54805, policy_entropy: -1.00959, alpha: 0.29279, time: 50.38960
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 542.93167, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 9.61102, qf2_loss: 9.56047, policy_loss: -218.40502, policy_entropy: -1.00031, alpha: 0.29464, time: 50.49568
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 525.92717, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 8.55329, qf2_loss: 8.51469, policy_loss: -218.35532, policy_entropy: -0.99218, alpha: 0.29320, time: 50.52131
[CW] eval: return: 529.30764, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 539.81002, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 8.83277, qf2_loss: 8.84291, policy_loss: -219.53887, policy_entropy: -1.00407, alpha: 0.29318, time: 50.49045
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 544.52831, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 8.95229, qf2_loss: 8.88042, policy_loss: -221.02479, policy_entropy: -1.00480, alpha: 0.29358, time: 50.71921
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 580.81699, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 8.63517, qf2_loss: 8.67503, policy_loss: -222.60859, policy_entropy: -1.01443, alpha: 0.29454, time: 50.61989
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 593.86235, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 9.47995, qf2_loss: 9.43254, policy_loss: -221.82323, policy_entropy: -0.99303, alpha: 0.29655, time: 50.60321
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 542.64911, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 19.67516, qf2_loss: 19.56240, policy_loss: -223.84718, policy_entropy: -0.98705, alpha: 0.29524, time: 50.63130
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 527.29619, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 10.15155, qf2_loss: 10.12184, policy_loss: -225.47015, policy_entropy: -1.01109, alpha: 0.29375, time: 50.54463
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 591.83632, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 8.01018, qf2_loss: 8.03152, policy_loss: -227.26434, policy_entropy: -1.01781, alpha: 0.29652, time: 50.55576
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 606.14588, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 8.28991, qf2_loss: 8.23816, policy_loss: -227.06182, policy_entropy: -1.01427, alpha: 0.30003, time: 50.56865
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 595.36001, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 8.08571, qf2_loss: 8.07210, policy_loss: -229.20673, policy_entropy: -1.00725, alpha: 0.30272, time: 50.53189
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 584.40241, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 7.88430, qf2_loss: 7.90673, policy_loss: -229.03649, policy_entropy: -1.01891, alpha: 0.30500, time: 50.47004
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 535.77114, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 8.16263, qf2_loss: 8.14762, policy_loss: -230.59360, policy_entropy: -1.00493, alpha: 0.30755, time: 50.53577
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 540.33557, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 8.82172, qf2_loss: 8.77621, policy_loss: -231.72598, policy_entropy: -1.00321, alpha: 0.30827, time: 50.51531
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 576.12843, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 8.66523, qf2_loss: 8.59090, policy_loss: -232.73484, policy_entropy: -0.99994, alpha: 0.30947, time: 52.26920
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 533.85662, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 9.20438, qf2_loss: 9.18558, policy_loss: -233.28712, policy_entropy: -1.00697, alpha: 0.30933, time: 50.53853
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 548.70090, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 8.56130, qf2_loss: 8.63452, policy_loss: -234.09988, policy_entropy: -1.00015, alpha: 0.30941, time: 50.63271
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 536.24621, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 8.82867, qf2_loss: 8.78529, policy_loss: -235.45938, policy_entropy: -1.01297, alpha: 0.31154, time: 50.51839
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 540.17630, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 9.20657, qf2_loss: 9.19487, policy_loss: -236.78127, policy_entropy: -0.99351, alpha: 0.31315, time: 50.57694
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 553.31302, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 9.41818, qf2_loss: 9.40643, policy_loss: -237.97956, policy_entropy: -1.00716, alpha: 0.31215, time: 50.62653
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 594.68788, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 9.53606, qf2_loss: 9.51845, policy_loss: -239.64852, policy_entropy: -1.00083, alpha: 0.31326, time: 50.65452
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 509.66265, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 8.64393, qf2_loss: 8.52477, policy_loss: -240.14122, policy_entropy: -1.00423, alpha: 0.31367, time: 50.53091
[CW] eval: return: 548.77545, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 461.58175, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 8.66533, qf2_loss: 8.65176, policy_loss: -240.02968, policy_entropy: -1.00089, alpha: 0.31414, time: 50.87108
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 542.31670, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 8.23173, qf2_loss: 8.17775, policy_loss: -240.67091, policy_entropy: -1.00534, alpha: 0.31525, time: 51.06507
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 686.56963, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 8.78536, qf2_loss: 8.69912, policy_loss: -242.48046, policy_entropy: -0.99882, alpha: 0.31594, time: 50.79366
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 605.81755, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 8.90118, qf2_loss: 8.89307, policy_loss: -243.63967, policy_entropy: -1.01048, alpha: 0.31621, time: 50.16414
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 548.12588, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 8.93532, qf2_loss: 8.88112, policy_loss: -244.68366, policy_entropy: -1.01754, alpha: 0.31930, time: 50.59961
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 557.28544, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 8.58754, qf2_loss: 8.51931, policy_loss: -246.03047, policy_entropy: -1.00469, alpha: 0.32206, time: 50.13320
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 598.74297, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 9.27243, qf2_loss: 9.29118, policy_loss: -246.68099, policy_entropy: -1.01130, alpha: 0.32346, time: 49.96825
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 533.67890, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 9.17111, qf2_loss: 9.17269, policy_loss: -248.00083, policy_entropy: -1.00522, alpha: 0.32483, time: 49.54987
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 541.58077, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 8.87540, qf2_loss: 8.87515, policy_loss: -248.43706, policy_entropy: -1.00571, alpha: 0.32632, time: 49.40933
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 527.74817, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 8.99561, qf2_loss: 8.95786, policy_loss: -249.73243, policy_entropy: -1.01539, alpha: 0.32837, time: 50.46650
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 573.93951, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 11.36006, qf2_loss: 11.33444, policy_loss: -250.31753, policy_entropy: -1.01073, alpha: 0.33165, time: 50.80283
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 541.96780, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 9.33807, qf2_loss: 9.28253, policy_loss: -251.67771, policy_entropy: -1.00078, alpha: 0.33243, time: 50.40494
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 600.42993, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 9.07261, qf2_loss: 8.98893, policy_loss: -252.25480, policy_entropy: -1.01456, alpha: 0.33426, time: 50.78989
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 526.26992, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 9.98535, qf2_loss: 10.06181, policy_loss: -252.98046, policy_entropy: -0.99130, alpha: 0.33583, time: 50.72211
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 695.38463, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 11.80261, qf2_loss: 11.78629, policy_loss: -254.70111, policy_entropy: -1.00517, alpha: 0.33448, time: 50.75779
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 558.24526, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 9.54690, qf2_loss: 9.54144, policy_loss: -255.41513, policy_entropy: -1.00740, alpha: 0.33488, time: 50.67901
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 546.86204, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 8.80948, qf2_loss: 8.77637, policy_loss: -256.45357, policy_entropy: -1.00383, alpha: 0.33680, time: 50.79654
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 556.24669, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 9.33536, qf2_loss: 9.34217, policy_loss: -256.92505, policy_entropy: -1.00682, alpha: 0.33838, time: 50.87881
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 605.42691, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 9.04678, qf2_loss: 8.97856, policy_loss: -258.47886, policy_entropy: -1.00492, alpha: 0.33950, time: 50.70322
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 613.16204, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 8.77783, qf2_loss: 8.66935, policy_loss: -259.20628, policy_entropy: -1.01210, alpha: 0.34176, time: 50.88477
[CW] eval: return: 572.68874, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 605.68966, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 9.10657, qf2_loss: 9.14159, policy_loss: -259.34420, policy_entropy: -1.00617, alpha: 0.34383, time: 50.96043
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 595.37170, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 9.67769, qf2_loss: 9.71666, policy_loss: -260.38466, policy_entropy: -1.01034, alpha: 0.34576, time: 50.65588
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 567.02297, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 10.46344, qf2_loss: 10.42784, policy_loss: -261.73662, policy_entropy: -0.99830, alpha: 0.34670, time: 50.60509
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 629.95659, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 10.08926, qf2_loss: 10.06766, policy_loss: -262.94014, policy_entropy: -1.00419, alpha: 0.34643, time: 50.80008
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 555.43771, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 9.40385, qf2_loss: 9.34797, policy_loss: -264.32485, policy_entropy: -1.00550, alpha: 0.34778, time: 50.80738
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 558.62655, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 9.38469, qf2_loss: 9.32784, policy_loss: -265.43775, policy_entropy: -1.00585, alpha: 0.34998, time: 50.75633
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 550.51564, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 10.14742, qf2_loss: 10.19101, policy_loss: -265.28261, policy_entropy: -0.99935, alpha: 0.35068, time: 50.82935
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 514.25399, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 11.38653, qf2_loss: 11.33696, policy_loss: -266.73921, policy_entropy: -1.00410, alpha: 0.35027, time: 50.74543
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 557.90448, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 8.76443, qf2_loss: 8.72666, policy_loss: -268.29499, policy_entropy: -1.00092, alpha: 0.35133, time: 50.77759
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 547.78949, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 10.37724, qf2_loss: 10.39417, policy_loss: -268.73589, policy_entropy: -1.00819, alpha: 0.35236, time: 50.70280
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 540.80929, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 11.31622, qf2_loss: 11.26284, policy_loss: -270.00758, policy_entropy: -1.00075, alpha: 0.35311, time: 50.65589
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 540.27284, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 10.42145, qf2_loss: 10.48384, policy_loss: -269.91141, policy_entropy: -1.00723, alpha: 0.35311, time: 50.75134
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 594.12226, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 9.29898, qf2_loss: 9.25319, policy_loss: -271.21472, policy_entropy: -1.00952, alpha: 0.35581, time: 50.74928
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 498.41285, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 11.56416, qf2_loss: 11.47861, policy_loss: -271.87853, policy_entropy: -1.00528, alpha: 0.35753, time: 50.79593
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 573.98636, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 9.98548, qf2_loss: 9.90735, policy_loss: -273.17014, policy_entropy: -1.00427, alpha: 0.35899, time: 50.67390
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 682.47771, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 10.21122, qf2_loss: 10.18148, policy_loss: -274.73081, policy_entropy: -1.00521, alpha: 0.36023, time: 50.78874
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 600.12530, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 11.12947, qf2_loss: 11.22639, policy_loss: -275.69633, policy_entropy: -1.00827, alpha: 0.36217, time: 50.82212
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 552.70258, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 10.54246, qf2_loss: 10.51835, policy_loss: -276.62848, policy_entropy: -0.99420, alpha: 0.36255, time: 50.87947
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 582.39065, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 12.24913, qf2_loss: 12.06967, policy_loss: -276.18940, policy_entropy: -1.00989, alpha: 0.36291, time: 50.83917
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 535.90345, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 11.42550, qf2_loss: 11.38859, policy_loss: -277.36304, policy_entropy: -1.00198, alpha: 0.36375, time: 50.86797
[CW] eval: return: 560.92537, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 515.59714, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 11.21706, qf2_loss: 11.14890, policy_loss: -277.30924, policy_entropy: -0.99498, alpha: 0.36386, time: 50.90212
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 679.96323, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 10.01685, qf2_loss: 10.12426, policy_loss: -279.99840, policy_entropy: -1.01478, alpha: 0.36444, time: 50.70716
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 605.22209, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 10.58503, qf2_loss: 10.61157, policy_loss: -279.79425, policy_entropy: -1.00668, alpha: 0.36789, time: 50.60972
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 536.54779, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 10.96435, qf2_loss: 10.96503, policy_loss: -281.33707, policy_entropy: -1.01082, alpha: 0.37060, time: 50.76398
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 666.94474, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 11.39397, qf2_loss: 11.38595, policy_loss: -282.47020, policy_entropy: -1.00479, alpha: 0.37184, time: 50.61559
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 738.90430, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 11.20526, qf2_loss: 11.27739, policy_loss: -283.08921, policy_entropy: -1.00868, alpha: 0.37308, time: 50.63247
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 610.70291, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 11.01403, qf2_loss: 10.96893, policy_loss: -284.74683, policy_entropy: -1.00873, alpha: 0.37616, time: 50.52138
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 610.02205, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 10.76579, qf2_loss: 10.74027, policy_loss: -285.04847, policy_entropy: -1.00064, alpha: 0.37717, time: 50.70969
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 615.24791, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 11.49328, qf2_loss: 11.54723, policy_loss: -286.08587, policy_entropy: -1.00850, alpha: 0.37827, time: 50.58582
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 563.26287, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 11.51196, qf2_loss: 11.48716, policy_loss: -287.27526, policy_entropy: -0.99542, alpha: 0.37884, time: 50.69815
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 635.15084, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 11.27267, qf2_loss: 11.19792, policy_loss: -288.36908, policy_entropy: -0.99684, alpha: 0.37803, time: 50.84918
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 602.97400, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 12.80926, qf2_loss: 12.82151, policy_loss: -288.29792, policy_entropy: -0.99343, alpha: 0.37678, time: 50.66607
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 596.16563, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 11.03730, qf2_loss: 11.05239, policy_loss: -289.46075, policy_entropy: -1.01094, alpha: 0.37739, time: 50.73116
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 588.41357, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 11.09468, qf2_loss: 11.05402, policy_loss: -289.55437, policy_entropy: -1.00794, alpha: 0.37984, time: 50.58825
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 628.35306, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 11.05908, qf2_loss: 11.04030, policy_loss: -290.68469, policy_entropy: -0.99789, alpha: 0.38042, time: 50.67855
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 616.46767, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 10.80046, qf2_loss: 10.76099, policy_loss: -292.35015, policy_entropy: -1.00365, alpha: 0.38001, time: 50.66354
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 676.27567, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 10.57686, qf2_loss: 10.58306, policy_loss: -293.00502, policy_entropy: -1.00533, alpha: 0.38227, time: 50.52245
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 682.05981, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 11.67783, qf2_loss: 11.64834, policy_loss: -293.22587, policy_entropy: -1.00117, alpha: 0.38352, time: 52.05853
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 620.67619, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 11.38655, qf2_loss: 11.28591, policy_loss: -294.80795, policy_entropy: -1.00545, alpha: 0.38379, time: 50.77331
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 616.88979, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 11.81513, qf2_loss: 11.88949, policy_loss: -294.43296, policy_entropy: -0.99909, alpha: 0.38414, time: 50.56121
[CW] eval: return: 637.28444, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 629.96454, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 10.65743, qf2_loss: 10.59281, policy_loss: -296.83240, policy_entropy: -1.00619, alpha: 0.38479, time: 50.77710
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 553.84430, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 11.57027, qf2_loss: 11.58141, policy_loss: -296.48124, policy_entropy: -0.99861, alpha: 0.38571, time: 50.53961
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 646.04679, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 11.76290, qf2_loss: 11.79235, policy_loss: -298.31650, policy_entropy: -1.00995, alpha: 0.38638, time: 50.60536
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 620.69777, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 11.57649, qf2_loss: 11.49585, policy_loss: -299.02779, policy_entropy: -0.99970, alpha: 0.38782, time: 50.66328
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 552.04070, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 12.34190, qf2_loss: 12.40035, policy_loss: -300.03147, policy_entropy: -0.99769, alpha: 0.38697, time: 50.77974
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 624.10494, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 11.74580, qf2_loss: 11.69230, policy_loss: -299.52476, policy_entropy: -1.00980, alpha: 0.38824, time: 50.86112
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 613.15794, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 11.26995, qf2_loss: 11.27818, policy_loss: -302.37485, policy_entropy: -1.00197, alpha: 0.39021, time: 50.73074
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 605.80434, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 16.35533, qf2_loss: 16.35590, policy_loss: -302.97551, policy_entropy: -0.98599, alpha: 0.39021, time: 51.80490
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 618.51425, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 11.59059, qf2_loss: 11.60799, policy_loss: -302.66299, policy_entropy: -1.00277, alpha: 0.38699, time: 50.75443
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 624.02800, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 11.23984, qf2_loss: 11.18995, policy_loss: -303.47010, policy_entropy: -1.01911, alpha: 0.38906, time: 50.78749
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 585.90784, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 14.72759, qf2_loss: 14.79119, policy_loss: -304.70300, policy_entropy: -1.01170, alpha: 0.39376, time: 50.88144
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 686.37775, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 11.80398, qf2_loss: 11.84365, policy_loss: -305.89422, policy_entropy: -1.01140, alpha: 0.39683, time: 50.74274
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 687.45969, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 11.20061, qf2_loss: 11.20999, policy_loss: -307.78007, policy_entropy: -1.01654, alpha: 0.40074, time: 50.70102
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 687.36733, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 11.63107, qf2_loss: 11.67244, policy_loss: -308.95241, policy_entropy: -1.00831, alpha: 0.40394, time: 51.52030
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 590.33811, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 12.39678, qf2_loss: 12.47345, policy_loss: -307.72041, policy_entropy: -0.98732, alpha: 0.40334, time: 50.73288
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 598.76328, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 12.14661, qf2_loss: 12.03989, policy_loss: -309.05978, policy_entropy: -1.01320, alpha: 0.40352, time: 50.78021
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 641.74923, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 12.59543, qf2_loss: 12.60727, policy_loss: -308.98130, policy_entropy: -1.00684, alpha: 0.40560, time: 51.14049
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 680.08614, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 11.40686, qf2_loss: 11.35102, policy_loss: -310.88952, policy_entropy: -1.00694, alpha: 0.40692, time: 50.72006
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 692.24039, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 12.86314, qf2_loss: 12.87629, policy_loss: -310.82756, policy_entropy: -0.99958, alpha: 0.40990, time: 50.77349
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 544.78190, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 13.05883, qf2_loss: 12.99480, policy_loss: -313.47750, policy_entropy: -0.99970, alpha: 0.40896, time: 50.51511
[CW] eval: return: 619.85982, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 556.64034, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 12.96599, qf2_loss: 13.01927, policy_loss: -313.07650, policy_entropy: -1.00272, alpha: 0.40879, time: 50.79931
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 670.40428, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 12.37528, qf2_loss: 12.46213, policy_loss: -314.11106, policy_entropy: -1.00483, alpha: 0.40973, time: 50.74071
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 718.07229, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 13.18950, qf2_loss: 13.09477, policy_loss: -316.14312, policy_entropy: -1.00898, alpha: 0.41181, time: 51.05576
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 611.19096, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 13.50465, qf2_loss: 13.59997, policy_loss: -317.18961, policy_entropy: -1.00204, alpha: 0.41310, time: 50.84313
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 771.79520, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 13.10879, qf2_loss: 13.12902, policy_loss: -317.56587, policy_entropy: -0.99665, alpha: 0.41352, time: 53.84860
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 609.78095, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 14.34906, qf2_loss: 14.20677, policy_loss: -317.91770, policy_entropy: -0.99343, alpha: 0.41217, time: 50.65810
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 764.13199, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 13.60687, qf2_loss: 13.76592, policy_loss: -319.38473, policy_entropy: -1.00564, alpha: 0.41223, time: 50.59747
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 575.28780, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 12.17539, qf2_loss: 12.13239, policy_loss: -319.50136, policy_entropy: -0.99982, alpha: 0.41262, time: 50.53531
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 546.78976, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 12.01237, qf2_loss: 12.00165, policy_loss: -320.77273, policy_entropy: -1.01054, alpha: 0.41280, time: 50.73103
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 601.46121, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 12.72954, qf2_loss: 12.85785, policy_loss: -320.06620, policy_entropy: -1.00172, alpha: 0.41458, time: 50.59931
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 687.74640, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 15.14088, qf2_loss: 15.01145, policy_loss: -321.83542, policy_entropy: -0.99894, alpha: 0.41658, time: 50.67976
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 620.08142, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 14.34251, qf2_loss: 14.38561, policy_loss: -321.85967, policy_entropy: -0.99719, alpha: 0.41462, time: 50.84117
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 625.95337, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 14.36817, qf2_loss: 14.31761, policy_loss: -324.13504, policy_entropy: -0.99808, alpha: 0.41446, time: 50.66544
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 610.59404, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 14.62014, qf2_loss: 14.61048, policy_loss: -323.83338, policy_entropy: -1.00682, alpha: 0.41499, time: 50.87021
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 617.88997, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 14.86121, qf2_loss: 14.88612, policy_loss: -324.57304, policy_entropy: -0.99852, alpha: 0.41627, time: 50.60601
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 594.90878, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 15.02882, qf2_loss: 14.98857, policy_loss: -326.48732, policy_entropy: -1.00397, alpha: 0.41624, time: 50.73924
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 722.73804, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 12.65338, qf2_loss: 12.70318, policy_loss: -328.72336, policy_entropy: -1.00805, alpha: 0.41707, time: 50.83386
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 697.60690, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 13.75430, qf2_loss: 13.65727, policy_loss: -329.42837, policy_entropy: -0.99958, alpha: 0.41808, time: 50.81651
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 540.37053, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 12.28324, qf2_loss: 12.30740, policy_loss: -327.87873, policy_entropy: -1.01598, alpha: 0.42053, time: 50.78986
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 668.06127, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 13.58171, qf2_loss: 13.54360, policy_loss: -330.61492, policy_entropy: -1.00064, alpha: 0.42267, time: 51.00834
[CW] eval: return: 690.97867, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 695.76517, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 17.05844, qf2_loss: 17.15926, policy_loss: -329.36639, policy_entropy: -1.00953, alpha: 0.42428, time: 51.01624
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 596.16988, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 13.99716, qf2_loss: 14.06955, policy_loss: -331.45346, policy_entropy: -1.00196, alpha: 0.42667, time: 50.80709
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 691.15498, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 13.97272, qf2_loss: 13.87685, policy_loss: -331.29249, policy_entropy: -1.00675, alpha: 0.42709, time: 50.69053
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 621.39758, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 15.30656, qf2_loss: 15.38792, policy_loss: -333.28833, policy_entropy: -0.99804, alpha: 0.42914, time: 50.63803
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 599.61425, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 14.84244, qf2_loss: 14.91851, policy_loss: -333.84743, policy_entropy: -0.99411, alpha: 0.42748, time: 50.93533
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 810.54480, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 14.97630, qf2_loss: 15.01635, policy_loss: -334.33157, policy_entropy: -1.01517, alpha: 0.42935, time: 50.89304
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 833.75078, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 17.57026, qf2_loss: 17.66281, policy_loss: -334.39938, policy_entropy: -1.00195, alpha: 0.43099, time: 50.99800
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 763.28120, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 17.27801, qf2_loss: 17.08601, policy_loss: -336.74523, policy_entropy: -0.99522, alpha: 0.43056, time: 50.87763
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 728.30366, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 14.01118, qf2_loss: 13.93763, policy_loss: -337.91575, policy_entropy: -1.00557, alpha: 0.43123, time: 51.11192
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 659.85179, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 14.36425, qf2_loss: 14.36982, policy_loss: -338.41652, policy_entropy: -1.02280, alpha: 0.43457, time: 50.99162
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 672.38793, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 15.53759, qf2_loss: 15.51387, policy_loss: -338.78739, policy_entropy: -0.99790, alpha: 0.43758, time: 50.88585
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 758.63657, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 14.84311, qf2_loss: 14.78956, policy_loss: -340.44431, policy_entropy: -0.99957, alpha: 0.43772, time: 51.14208
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 693.35163, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 15.85114, qf2_loss: 15.98998, policy_loss: -340.31273, policy_entropy: -0.99996, alpha: 0.43839, time: 51.60383
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 748.48538, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 15.32653, qf2_loss: 15.22064, policy_loss: -342.34132, policy_entropy: -1.00521, alpha: 0.43846, time: 50.96610
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 570.21876, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 14.78128, qf2_loss: 14.72603, policy_loss: -341.34689, policy_entropy: -1.00877, alpha: 0.44048, time: 50.86029
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 816.29492, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 18.25426, qf2_loss: 18.35521, policy_loss: -343.14819, policy_entropy: -1.00651, alpha: 0.44329, time: 50.83471
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 542.88303, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 21.08133, qf2_loss: 20.92506, policy_loss: -344.09079, policy_entropy: -0.98946, alpha: 0.44158, time: 50.87924
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 764.30205, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 15.01057, qf2_loss: 14.99419, policy_loss: -346.64417, policy_entropy: -1.00315, alpha: 0.44164, time: 50.77028
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 604.96473, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 14.24168, qf2_loss: 14.31367, policy_loss: -346.64345, policy_entropy: -1.00634, alpha: 0.44202, time: 50.85434
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 766.12926, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 15.83402, qf2_loss: 16.04864, policy_loss: -347.43981, policy_entropy: -1.01234, alpha: 0.44502, time: 51.00325
[CW] eval: return: 596.83287, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 534.02794, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 19.26970, qf2_loss: 19.19759, policy_loss: -347.19413, policy_entropy: -1.00759, alpha: 0.44934, time: 50.85273
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 680.13313, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 16.24566, qf2_loss: 16.02142, policy_loss: -347.87744, policy_entropy: -0.99738, alpha: 0.44938, time: 50.93026
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 738.08660, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 15.45246, qf2_loss: 15.41459, policy_loss: -350.29513, policy_entropy: -1.00701, alpha: 0.45080, time: 51.05940
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 843.31550, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 17.20008, qf2_loss: 17.29052, policy_loss: -350.91717, policy_entropy: -1.00289, alpha: 0.45270, time: 51.06947
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 691.67562, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 16.56721, qf2_loss: 16.41282, policy_loss: -350.79267, policy_entropy: -1.00504, alpha: 0.45362, time: 51.04795
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 821.94186, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 16.10690, qf2_loss: 16.15794, policy_loss: -353.04582, policy_entropy: -1.00282, alpha: 0.45370, time: 50.93042
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 671.46015, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 16.62854, qf2_loss: 16.60093, policy_loss: -353.24765, policy_entropy: -0.99751, alpha: 0.45340, time: 50.89814
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 676.89526, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 17.19175, qf2_loss: 17.11473, policy_loss: -353.22948, policy_entropy: -1.01016, alpha: 0.45576, time: 50.78187
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 682.89355, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 19.50974, qf2_loss: 19.45390, policy_loss: -355.08463, policy_entropy: -1.00362, alpha: 0.45732, time: 51.02511
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 806.95705, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 18.37528, qf2_loss: 18.69171, policy_loss: -354.55773, policy_entropy: -1.00186, alpha: 0.45913, time: 50.81841
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 615.87131, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 18.72263, qf2_loss: 18.67708, policy_loss: -354.50670, policy_entropy: -0.99792, alpha: 0.45895, time: 50.89746
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 821.30387, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 19.82151, qf2_loss: 19.74246, policy_loss: -356.31910, policy_entropy: -1.00957, alpha: 0.46015, time: 50.91766
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 827.68403, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 18.92888, qf2_loss: 19.07694, policy_loss: -357.14940, policy_entropy: -1.00296, alpha: 0.46258, time: 50.80356
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 808.02456, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 18.08396, qf2_loss: 17.97607, policy_loss: -360.08242, policy_entropy: -1.01033, alpha: 0.46455, time: 50.81884
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 815.04529, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 20.62741, qf2_loss: 20.69773, policy_loss: -360.28786, policy_entropy: -1.00046, alpha: 0.46706, time: 51.03731
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 580.55869, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 19.80051, qf2_loss: 19.72027, policy_loss: -361.55934, policy_entropy: -1.00108, alpha: 0.46744, time: 50.98938
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 610.20198, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 20.32385, qf2_loss: 20.43547, policy_loss: -362.38655, policy_entropy: -0.99702, alpha: 0.46652, time: 51.38247
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 604.84559, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 21.43013, qf2_loss: 21.58467, policy_loss: -362.70455, policy_entropy: -0.99104, alpha: 0.46371, time: 51.06370
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 819.92696, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 17.28496, qf2_loss: 17.35677, policy_loss: -362.97163, policy_entropy: -1.00532, alpha: 0.46220, time: 51.03174
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 796.81073, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 18.29199, qf2_loss: 18.16450, policy_loss: -364.23189, policy_entropy: -1.01163, alpha: 0.46581, time: 51.00969
[CW] eval: return: 798.83491, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 801.41213, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 17.70785, qf2_loss: 17.77917, policy_loss: -365.95919, policy_entropy: -1.01304, alpha: 0.46974, time: 51.14178
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 678.36664, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 18.01775, qf2_loss: 18.02234, policy_loss: -365.43677, policy_entropy: -1.00169, alpha: 0.47327, time: 52.43813
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 733.75463, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 17.61266, qf2_loss: 17.68650, policy_loss: -366.53559, policy_entropy: -1.01206, alpha: 0.47470, time: 50.84016
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 678.80374, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 19.99205, qf2_loss: 19.82515, policy_loss: -368.31413, policy_entropy: -1.00024, alpha: 0.47776, time: 51.05151
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 730.84947, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 19.38074, qf2_loss: 19.39997, policy_loss: -369.66358, policy_entropy: -1.00717, alpha: 0.47866, time: 50.89199
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 839.99467, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 18.71071, qf2_loss: 18.68904, policy_loss: -369.37981, policy_entropy: -0.99672, alpha: 0.47885, time: 50.99367
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 803.82301, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 19.38620, qf2_loss: 19.14279, policy_loss: -369.59020, policy_entropy: -1.00564, alpha: 0.48039, time: 51.10603
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 836.58055, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 20.15726, qf2_loss: 20.35580, policy_loss: -370.95606, policy_entropy: -1.00284, alpha: 0.48133, time: 52.19247
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 832.62384, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 18.88282, qf2_loss: 18.82898, policy_loss: -374.75712, policy_entropy: -1.00490, alpha: 0.48350, time: 51.04729
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 807.29826, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 20.66371, qf2_loss: 20.59094, policy_loss: -374.41919, policy_entropy: -0.98620, alpha: 0.48127, time: 50.89976
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 835.39084, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 19.17232, qf2_loss: 19.32353, policy_loss: -374.11360, policy_entropy: -1.01070, alpha: 0.48026, time: 51.19009
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 814.97727, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 21.75898, qf2_loss: 21.89022, policy_loss: -375.06594, policy_entropy: -1.00771, alpha: 0.48423, time: 51.03379
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 839.47201, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 21.58101, qf2_loss: 21.46361, policy_loss: -376.57897, policy_entropy: -1.00618, alpha: 0.48634, time: 50.92457
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 839.40167, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 19.29288, qf2_loss: 19.29158, policy_loss: -376.68256, policy_entropy: -1.00344, alpha: 0.48901, time: 51.08127
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 841.83738, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 25.14037, qf2_loss: 25.13190, policy_loss: -377.42880, policy_entropy: -1.00683, alpha: 0.49055, time: 51.07421
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 763.46536, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 21.45365, qf2_loss: 21.44651, policy_loss: -378.76954, policy_entropy: -1.00136, alpha: 0.49112, time: 51.02928
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 815.47746, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 20.23534, qf2_loss: 20.12610, policy_loss: -380.54380, policy_entropy: -0.99947, alpha: 0.49167, time: 51.17582
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 844.64652, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 20.28030, qf2_loss: 20.34568, policy_loss: -381.59048, policy_entropy: -0.99608, alpha: 0.49188, time: 50.95803
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 681.19235, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 23.42296, qf2_loss: 23.13246, policy_loss: -381.86174, policy_entropy: -1.00468, alpha: 0.49141, time: 51.99001
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 842.86534, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 23.43570, qf2_loss: 23.50486, policy_loss: -383.08831, policy_entropy: -0.99857, alpha: 0.49219, time: 51.16430
[CW] eval: return: 616.25049, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 621.15672, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 35.23255, qf2_loss: 35.44475, policy_loss: -383.85062, policy_entropy: -0.98414, alpha: 0.48936, time: 51.48539
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 833.71935, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 31.58418, qf2_loss: 31.62561, policy_loss: -384.52131, policy_entropy: -1.00217, alpha: 0.48717, time: 51.11578
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 845.61317, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 20.22632, qf2_loss: 20.25944, policy_loss: -386.90625, policy_entropy: -1.01205, alpha: 0.48874, time: 50.92909
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 830.62270, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 19.22541, qf2_loss: 19.15855, policy_loss: -386.05172, policy_entropy: -1.02487, alpha: 0.49484, time: 51.09887
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 823.17127, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 20.26601, qf2_loss: 20.06717, policy_loss: -387.77556, policy_entropy: -1.01218, alpha: 0.50181, time: 51.10481
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 839.94735, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 21.38705, qf2_loss: 21.34212, policy_loss: -389.60727, policy_entropy: -1.00766, alpha: 0.50530, time: 51.12679
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 824.84178, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 21.67399, qf2_loss: 21.70490, policy_loss: -388.42374, policy_entropy: -1.00811, alpha: 0.50903, time: 51.05021
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 824.28787, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 21.65557, qf2_loss: 21.57712, policy_loss: -390.28910, policy_entropy: -1.00235, alpha: 0.51118, time: 50.91957
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 826.08452, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 24.84695, qf2_loss: 24.61499, policy_loss: -390.52766, policy_entropy: -0.99550, alpha: 0.51178, time: 50.95123
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 843.08931, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 23.57302, qf2_loss: 23.83429, policy_loss: -393.28927, policy_entropy: -1.00813, alpha: 0.51139, time: 50.95354
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 845.82621, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 28.15303, qf2_loss: 28.33878, policy_loss: -393.30060, policy_entropy: -1.00198, alpha: 0.51375, time: 51.12857
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 676.81913, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 23.97969, qf2_loss: 24.06259, policy_loss: -393.29417, policy_entropy: -1.00276, alpha: 0.51323, time: 51.48783
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 846.55097, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 21.95379, qf2_loss: 21.70315, policy_loss: -395.41449, policy_entropy: -1.00583, alpha: 0.51567, time: 51.03812
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 842.19348, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 25.10350, qf2_loss: 25.20361, policy_loss: -395.93748, policy_entropy: -0.99776, alpha: 0.51745, time: 51.10129
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 849.15624, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 33.75685, qf2_loss: 33.63736, policy_loss: -399.18329, policy_entropy: -0.97975, alpha: 0.51454, time: 51.20638
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 673.50749, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 24.76825, qf2_loss: 24.78148, policy_loss: -397.27552, policy_entropy: -1.00248, alpha: 0.50877, time: 51.21464
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 687.07752, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 24.33732, qf2_loss: 24.42811, policy_loss: -398.71062, policy_entropy: -1.01266, alpha: 0.51130, time: 51.01255
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 848.72222, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 25.21666, qf2_loss: 25.16898, policy_loss: -399.59377, policy_entropy: -1.00438, alpha: 0.51511, time: 50.94389
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 821.02428, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 24.55984, qf2_loss: 24.44139, policy_loss: -402.45565, policy_entropy: -1.00584, alpha: 0.51726, time: 50.91073
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 536.59894, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 23.27254, qf2_loss: 23.39356, policy_loss: -401.79777, policy_entropy: -1.01416, alpha: 0.51990, time: 51.10207
[CW] eval: return: 830.33921, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 847.33943, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 24.61596, qf2_loss: 24.67067, policy_loss: -402.51859, policy_entropy: -0.99981, alpha: 0.52353, time: 51.20033
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 819.57316, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 25.73172, qf2_loss: 25.65246, policy_loss: -403.35759, policy_entropy: -1.00378, alpha: 0.52396, time: 51.01119
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 837.43849, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 27.14035, qf2_loss: 27.53554, policy_loss: -405.46154, policy_entropy: -1.00972, alpha: 0.52489, time: 51.09785
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 827.24407, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 24.42941, qf2_loss: 24.29190, policy_loss: -405.54229, policy_entropy: -1.00871, alpha: 0.52940, time: 51.15095
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 750.16607, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 36.34151, qf2_loss: 36.39575, policy_loss: -405.31827, policy_entropy: -0.99364, alpha: 0.53155, time: 51.22309
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 810.66053, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 27.08689, qf2_loss: 27.32605, policy_loss: -406.78812, policy_entropy: -1.00965, alpha: 0.53039, time: 50.87120
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 608.19665, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 24.73369, qf2_loss: 24.66718, policy_loss: -407.92800, policy_entropy: -1.00148, alpha: 0.53373, time: 51.38115
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 828.89771, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 23.52766, qf2_loss: 23.49285, policy_loss: -409.90190, policy_entropy: -1.00563, alpha: 0.53491, time: 51.62610
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 830.01619, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 23.59696, qf2_loss: 23.81960, policy_loss: -408.22471, policy_entropy: -1.00271, alpha: 0.53654, time: 51.26666
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 835.93863, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 24.56867, qf2_loss: 24.26955, policy_loss: -412.48738, policy_entropy: -0.99830, alpha: 0.53617, time: 51.11534
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 844.38109, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 28.63438, qf2_loss: 28.71813, policy_loss: -410.75155, policy_entropy: -1.00151, alpha: 0.53728, time: 51.14679
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 825.67377, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 28.43300, qf2_loss: 28.13844, policy_loss: -414.62320, policy_entropy: -0.99876, alpha: 0.53690, time: 51.13042
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 840.38605, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 25.18075, qf2_loss: 25.23330, policy_loss: -414.85418, policy_entropy: -0.99597, alpha: 0.53498, time: 51.01721
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 850.52049, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 25.34744, qf2_loss: 25.38317, policy_loss: -415.08773, policy_entropy: -1.00893, alpha: 0.53627, time: 50.96007
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 850.72605, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 27.55722, qf2_loss: 27.51101, policy_loss: -414.54101, policy_entropy: -0.99411, alpha: 0.53760, time: 50.88541
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 830.73426, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 28.41979, qf2_loss: 28.51700, policy_loss: -415.82108, policy_entropy: -0.99785, alpha: 0.53449, time: 51.15296
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 617.44487, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 27.20270, qf2_loss: 27.34655, policy_loss: -417.56400, policy_entropy: -1.00459, alpha: 0.53477, time: 51.09037
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 850.32029, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 28.28179, qf2_loss: 28.16603, policy_loss: -418.12682, policy_entropy: -0.99707, alpha: 0.53706, time: 50.92846
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 826.29251, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 27.85409, qf2_loss: 27.62752, policy_loss: -420.65953, policy_entropy: -1.00899, alpha: 0.53834, time: 51.35105
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 839.07542, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 25.76497, qf2_loss: 25.74038, policy_loss: -420.00036, policy_entropy: -1.01070, alpha: 0.54179, time: 51.49114
[CW] eval: return: 814.14569, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 674.63113, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 26.88572, qf2_loss: 26.76110, policy_loss: -422.69868, policy_entropy: -1.00194, alpha: 0.54308, time: 51.06245
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 766.03347, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 27.25983, qf2_loss: 27.08035, policy_loss: -423.63530, policy_entropy: -1.01641, alpha: 0.54626, time: 51.13776
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 847.81037, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 32.50110, qf2_loss: 32.51475, policy_loss: -422.30537, policy_entropy: -0.99561, alpha: 0.54980, time: 54.86286
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 840.15562, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 27.73007, qf2_loss: 28.13449, policy_loss: -424.01340, policy_entropy: -1.00686, alpha: 0.55059, time: 50.99735
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 695.05825, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 27.40700, qf2_loss: 27.52280, policy_loss: -424.98599, policy_entropy: -0.99667, alpha: 0.55134, time: 51.07325
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 846.37089, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 26.80434, qf2_loss: 26.60997, policy_loss: -426.49974, policy_entropy: -1.01570, alpha: 0.55300, time: 51.11477
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 851.99555, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 28.01863, qf2_loss: 28.18867, policy_loss: -426.72628, policy_entropy: -0.99899, alpha: 0.55704, time: 51.32692
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 842.55198, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 27.60237, qf2_loss: 27.30531, policy_loss: -426.86936, policy_entropy: -0.99986, alpha: 0.55579, time: 51.55436
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 835.07388, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 27.62092, qf2_loss: 27.56075, policy_loss: -427.97986, policy_entropy: -1.00391, alpha: 0.55571, time: 50.83217
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 838.79324, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 28.27432, qf2_loss: 28.34226, policy_loss: -429.81861, policy_entropy: -1.00042, alpha: 0.55802, time: 51.06091
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 849.83852, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 29.10086, qf2_loss: 29.15083, policy_loss: -429.73008, policy_entropy: -1.01042, alpha: 0.55991, time: 50.87095
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 752.06193, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 27.33518, qf2_loss: 27.36223, policy_loss: -430.62092, policy_entropy: -1.00210, alpha: 0.56162, time: 51.15699
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 831.81217, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 28.78392, qf2_loss: 28.94303, policy_loss: -431.53439, policy_entropy: -0.99479, alpha: 0.56218, time: 51.11018
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 840.62039, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 29.11679, qf2_loss: 28.79487, policy_loss: -433.49189, policy_entropy: -1.00390, alpha: 0.56164, time: 51.01426
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 834.91696, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 31.37465, qf2_loss: 31.39414, policy_loss: -435.82569, policy_entropy: -1.01093, alpha: 0.56571, time: 50.89657
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 843.95375, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 30.27043, qf2_loss: 30.01391, policy_loss: -434.85671, policy_entropy: -1.00175, alpha: 0.56955, time: 51.01750
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 842.72036, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 30.04074, qf2_loss: 30.18558, policy_loss: -435.79159, policy_entropy: -1.00170, alpha: 0.56821, time: 52.76670
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 847.93983, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 30.89336, qf2_loss: 30.76783, policy_loss: -437.67070, policy_entropy: -0.99970, alpha: 0.56821, time: 51.20632
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 845.73219, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 31.57278, qf2_loss: 31.71826, policy_loss: -440.07306, policy_entropy: -0.99526, alpha: 0.56675, time: 51.26629
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 846.66215, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 29.39477, qf2_loss: 29.24443, policy_loss: -436.64153, policy_entropy: -0.99488, alpha: 0.56547, time: 51.46360
[CW] eval: return: 846.76877, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 847.97944, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 29.33301, qf2_loss: 29.38852, policy_loss: -439.81023, policy_entropy: -1.00581, alpha: 0.56621, time: 51.20028
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 843.07549, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 28.75848, qf2_loss: 28.70450, policy_loss: -442.16616, policy_entropy: -0.99877, alpha: 0.56725, time: 51.26309
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 849.53936, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 29.47172, qf2_loss: 29.48324, policy_loss: -444.83932, policy_entropy: -1.00390, alpha: 0.56671, time: 51.05271
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 697.18844, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 34.87902, qf2_loss: 35.03508, policy_loss: -441.38687, policy_entropy: -0.99742, alpha: 0.56753, time: 51.30903
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 832.90094, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 29.46089, qf2_loss: 29.73761, policy_loss: -442.64417, policy_entropy: -0.98577, alpha: 0.56364, time: 51.05184
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 762.20158, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 28.67641, qf2_loss: 28.78186, policy_loss: -444.82729, policy_entropy: -1.00065, alpha: 0.56145, time: 51.40564
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 845.67114, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 30.66402, qf2_loss: 30.77145, policy_loss: -443.01211, policy_entropy: -1.01695, alpha: 0.56396, time: 51.00307
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 825.83479, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 29.69960, qf2_loss: 29.84795, policy_loss: -446.57836, policy_entropy: -1.00287, alpha: 0.56917, time: 50.95711
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 836.31846, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 31.70328, qf2_loss: 31.98404, policy_loss: -446.51745, policy_entropy: -0.99791, alpha: 0.56787, time: 51.01086
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 836.63886, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 31.71177, qf2_loss: 31.76129, policy_loss: -447.23725, policy_entropy: -0.99613, alpha: 0.56714, time: 50.97210
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 842.37203, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 31.05491, qf2_loss: 31.19173, policy_loss: -450.26707, policy_entropy: -0.99193, alpha: 0.56625, time: 51.30667
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 835.43266, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 28.94805, qf2_loss: 28.84927, policy_loss: -448.09303, policy_entropy: -0.99953, alpha: 0.56351, time: 51.26528
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 825.45085, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 30.92780, qf2_loss: 30.73353, policy_loss: -452.11564, policy_entropy: -0.99762, alpha: 0.56306, time: 52.65476
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 677.23043, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 35.47960, qf2_loss: 35.67224, policy_loss: -452.24136, policy_entropy: -1.00717, alpha: 0.56457, time: 50.90874
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 684.79683, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 34.49867, qf2_loss: 34.65915, policy_loss: -454.40567, policy_entropy: -0.99658, alpha: 0.56541, time: 51.28862
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 765.52258, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 29.11484, qf2_loss: 29.09870, policy_loss: -456.09262, policy_entropy: -1.00281, alpha: 0.56361, time: 51.05452
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 849.68290, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 31.53993, qf2_loss: 31.28736, policy_loss: -452.93664, policy_entropy: -1.00015, alpha: 0.56615, time: 51.51261
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 847.60468, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 39.47096, qf2_loss: 39.52196, policy_loss: -454.39782, policy_entropy: -1.00465, alpha: 0.56697, time: 51.06106
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 843.66673, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 34.11213, qf2_loss: 34.39146, policy_loss: -455.02936, policy_entropy: -0.98883, alpha: 0.56606, time: 50.93781
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 844.26587, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 32.35631, qf2_loss: 32.64790, policy_loss: -456.92493, policy_entropy: -1.00807, alpha: 0.56433, time: 50.99392
[CW] eval: return: 800.61862, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 736.30610, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 31.77803, qf2_loss: 31.42309, policy_loss: -458.99947, policy_entropy: -1.00715, alpha: 0.56690, time: 51.15521
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 844.08494, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 34.02433, qf2_loss: 33.70767, policy_loss: -460.12539, policy_entropy: -1.00539, alpha: 0.56882, time: 51.27820
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 688.90210, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 31.10508, qf2_loss: 31.08645, policy_loss: -458.45232, policy_entropy: -1.00159, alpha: 0.57190, time: 51.26446
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 833.23541, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 29.65989, qf2_loss: 29.88213, policy_loss: -458.68556, policy_entropy: -1.01318, alpha: 0.57223, time: 50.95474
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 842.87085, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 30.57093, qf2_loss: 30.30645, policy_loss: -461.00371, policy_entropy: -0.99317, alpha: 0.57450, time: 51.06792
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 846.90117, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 29.17706, qf2_loss: 29.15038, policy_loss: -462.68195, policy_entropy: -1.00316, alpha: 0.57439, time: 51.16941
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 830.79650, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 33.17641, qf2_loss: 33.14717, policy_loss: -462.51384, policy_entropy: -0.99346, alpha: 0.57211, time: 50.98239
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 838.43348, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 34.06047, qf2_loss: 33.93961, policy_loss: -464.91577, policy_entropy: -0.99737, alpha: 0.57130, time: 51.33020
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 837.37665, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 35.31038, qf2_loss: 35.70970, policy_loss: -465.72617, policy_entropy: -1.00756, alpha: 0.57261, time: 51.02504
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 605.46802, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 32.63557, qf2_loss: 32.29479, policy_loss: -467.72209, policy_entropy: -0.99816, alpha: 0.57349, time: 51.35879
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 848.63416, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 33.46635, qf2_loss: 33.40448, policy_loss: -465.12579, policy_entropy: -0.99619, alpha: 0.57282, time: 51.33738
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 620.20906, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 37.71561, qf2_loss: 37.65028, policy_loss: -467.23906, policy_entropy: -0.99763, alpha: 0.57121, time: 51.13599
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 845.73014, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 39.63125, qf2_loss: 39.61648, policy_loss: -466.91906, policy_entropy: -1.00322, alpha: 0.57230, time: 51.56603
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 816.87274, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 66.13363, qf2_loss: 65.44734, policy_loss: -468.65204, policy_entropy: -0.98204, alpha: 0.56977, time: 50.95472
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 606.48061, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 85.19502, qf2_loss: 86.12264, policy_loss: -473.16491, policy_entropy: -0.98490, alpha: 0.56446, time: 51.02155
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 692.83315, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 36.12634, qf2_loss: 36.11376, policy_loss: -471.42108, policy_entropy: -0.99480, alpha: 0.55945, time: 51.02448
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 835.94346, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 29.61154, qf2_loss: 29.36620, policy_loss: -472.26131, policy_entropy: -1.00283, alpha: 0.55911, time: 50.79460
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 836.35412, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 27.71741, qf2_loss: 27.59170, policy_loss: -475.26638, policy_entropy: -0.99037, alpha: 0.55872, time: 50.65909
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 846.04484, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 28.85896, qf2_loss: 28.69987, policy_loss: -472.88911, policy_entropy: -1.02376, alpha: 0.56071, time: 51.53584
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 845.60462, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 29.92355, qf2_loss: 29.89195, policy_loss: -475.15647, policy_entropy: -1.02229, alpha: 0.56738, time: 51.05870
[CW] eval: return: 791.28933, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 553.85975, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 36.41278, qf2_loss: 36.54711, policy_loss: -472.81554, policy_entropy: -1.00612, alpha: 0.57308, time: 52.24213
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 842.74198, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 29.04164, qf2_loss: 28.86498, policy_loss: -475.95468, policy_entropy: -1.01056, alpha: 0.57472, time: 51.33235
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 844.28660, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 29.67997, qf2_loss: 29.53623, policy_loss: -475.66555, policy_entropy: -0.99686, alpha: 0.57846, time: 50.99960
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 846.81663, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 29.74647, qf2_loss: 29.56262, policy_loss: -476.20719, policy_entropy: -1.00737, alpha: 0.57878, time: 51.04379
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 843.45624, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 31.18107, qf2_loss: 31.33807, policy_loss: -479.51125, policy_entropy: -1.00279, alpha: 0.58104, time: 51.19271
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 841.22403, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 30.12306, qf2_loss: 29.95640, policy_loss: -481.01097, policy_entropy: -1.00585, alpha: 0.58092, time: 51.23666
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 834.24782, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 31.51732, qf2_loss: 31.31527, policy_loss: -478.61281, policy_entropy: -1.00181, alpha: 0.58336, time: 51.28145
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 843.83494, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 32.97498, qf2_loss: 32.93500, policy_loss: -480.57518, policy_entropy: -1.00044, alpha: 0.58423, time: 51.19357
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 688.25335, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 39.16912, qf2_loss: 39.27405, policy_loss: -482.60334, policy_entropy: -0.99282, alpha: 0.58326, time: 51.31471
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 842.44693, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 37.57656, qf2_loss: 37.67242, policy_loss: -482.52079, policy_entropy: -0.99110, alpha: 0.57898, time: 51.09702
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 843.97829, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 34.12648, qf2_loss: 34.31171, policy_loss: -483.26182, policy_entropy: -1.00090, alpha: 0.57812, time: 51.28084
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 842.11825, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 33.98887, qf2_loss: 34.17304, policy_loss: -487.12556, policy_entropy: -0.98983, alpha: 0.57639, time: 51.35984
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 846.01598, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 41.09376, qf2_loss: 41.33083, policy_loss: -484.63872, policy_entropy: -0.98957, alpha: 0.57265, time: 51.39836
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 842.13204, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 35.21775, qf2_loss: 35.26237, policy_loss: -483.25564, policy_entropy: -1.00283, alpha: 0.57140, time: 51.16108
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 841.63007, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 37.18349, qf2_loss: 36.95929, policy_loss: -487.69003, policy_entropy: -0.99033, alpha: 0.57034, time: 51.07454
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 826.90598, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 31.71078, qf2_loss: 31.39764, policy_loss: -486.81065, policy_entropy: -1.00414, alpha: 0.56880, time: 51.08444
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 834.43252, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 30.99509, qf2_loss: 30.80140, policy_loss: -487.51698, policy_entropy: -0.99689, alpha: 0.56877, time: 52.55655
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 845.47886, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 33.28238, qf2_loss: 33.61113, policy_loss: -490.22113, policy_entropy: -1.00587, alpha: 0.57005, time: 51.01390
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 838.05091, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 37.87238, qf2_loss: 37.79033, policy_loss: -491.94898, policy_entropy: -1.00261, alpha: 0.57107, time: 51.19831
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 841.95351, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 40.60019, qf2_loss: 40.72212, policy_loss: -491.22220, policy_entropy: -0.99344, alpha: 0.57109, time: 51.12278
[CW] eval: return: 820.45132, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 828.17516, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 31.28946, qf2_loss: 31.26678, policy_loss: -492.08675, policy_entropy: -0.99968, alpha: 0.56938, time: 51.28268
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 845.36332, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 37.10727, qf2_loss: 37.14171, policy_loss: -497.06073, policy_entropy: -0.99039, alpha: 0.56759, time: 51.16434
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 842.24030, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 34.13820, qf2_loss: 34.02616, policy_loss: -492.48320, policy_entropy: -1.00926, alpha: 0.56700, time: 51.43631
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 840.04853, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 34.96802, qf2_loss: 35.35665, policy_loss: -494.86161, policy_entropy: -1.00359, alpha: 0.57003, time: 53.86914
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 844.17419, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 34.42069, qf2_loss: 34.51077, policy_loss: -496.25193, policy_entropy: -1.00575, alpha: 0.57129, time: 50.56674
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 675.28101, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 36.77739, qf2_loss: 36.33104, policy_loss: -496.67341, policy_entropy: -1.00673, alpha: 0.57319, time: 50.81087
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 842.93384, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 34.31812, qf2_loss: 34.39557, policy_loss: -497.50747, policy_entropy: -1.00260, alpha: 0.57454, time: 50.90204
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 833.95125, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 34.74178, qf2_loss: 34.58171, policy_loss: -495.24017, policy_entropy: -0.99948, alpha: 0.57522, time: 50.81444
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 841.91620, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 39.93496, qf2_loss: 40.23634, policy_loss: -500.47264, policy_entropy: -1.00381, alpha: 0.57496, time: 50.85403
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 850.65355, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 43.45169, qf2_loss: 43.25201, policy_loss: -498.88194, policy_entropy: -0.99619, alpha: 0.57549, time: 50.91155
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 843.51297, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 41.01045, qf2_loss: 40.48921, policy_loss: -500.35409, policy_entropy: -0.98969, alpha: 0.57381, time: 51.29785
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 838.19767, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 35.13283, qf2_loss: 35.30600, policy_loss: -503.70438, policy_entropy: -0.99939, alpha: 0.57035, time: 50.85858
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 847.69756, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 37.31097, qf2_loss: 37.38108, policy_loss: -502.18620, policy_entropy: -0.99530, alpha: 0.57068, time: 50.84579
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 846.81823, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 35.60887, qf2_loss: 35.70570, policy_loss: -501.94185, policy_entropy: -0.99459, alpha: 0.56871, time: 50.88643
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 829.64479, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 37.02058, qf2_loss: 37.23066, policy_loss: -503.10896, policy_entropy: -0.99627, alpha: 0.56793, time: 50.90123
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 837.64444, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 34.69988, qf2_loss: 34.38610, policy_loss: -503.47654, policy_entropy: -0.99541, alpha: 0.56711, time: 50.91244
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 754.77843, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 32.41441, qf2_loss: 32.15457, policy_loss: -505.69738, policy_entropy: -0.98898, alpha: 0.56410, time: 50.98485
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 637.37615, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 40.13750, qf2_loss: 40.91952, policy_loss: -505.97442, policy_entropy: -1.00318, alpha: 0.56353, time: 50.86511
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 836.15973, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 36.74095, qf2_loss: 36.78102, policy_loss: -505.57712, policy_entropy: -0.98904, alpha: 0.56285, time: 50.66063
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 839.67451, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 37.70876, qf2_loss: 37.36627, policy_loss: -505.45731, policy_entropy: -1.00740, alpha: 0.56118, time: 50.82422
[CW] eval: return: 845.61271, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 844.83713, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 32.84146, qf2_loss: 32.81366, policy_loss: -509.01346, policy_entropy: -0.99497, alpha: 0.56009, time: 50.96638
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 845.93032, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 33.38430, qf2_loss: 33.46321, policy_loss: -507.53544, policy_entropy: -1.01209, alpha: 0.56238, time: 50.95173
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 841.72254, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 33.39617, qf2_loss: 33.36610, policy_loss: -509.86653, policy_entropy: -1.00818, alpha: 0.56614, time: 51.01362
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 831.29415, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 34.07473, qf2_loss: 33.94780, policy_loss: -511.98470, policy_entropy: -0.99279, alpha: 0.56648, time: 51.10161
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 846.50663, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 45.96773, qf2_loss: 46.06989, policy_loss: -507.53377, policy_entropy: -1.00304, alpha: 0.56484, time: 50.91543
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 843.87775, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 35.46177, qf2_loss: 35.23326, policy_loss: -511.15910, policy_entropy: -0.99504, alpha: 0.56601, time: 51.07264
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 841.03266, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 34.60123, qf2_loss: 34.74744, policy_loss: -512.48235, policy_entropy: -0.99327, alpha: 0.56406, time: 50.94029
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 849.95098, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 34.00204, qf2_loss: 34.16476, policy_loss: -514.21400, policy_entropy: -0.98131, alpha: 0.55998, time: 51.53499
[CW] ---------------------------
