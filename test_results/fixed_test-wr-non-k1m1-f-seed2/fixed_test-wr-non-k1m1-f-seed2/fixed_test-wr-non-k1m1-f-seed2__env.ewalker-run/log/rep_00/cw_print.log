[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 23.74455, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.66651, qf2_loss: 1.68657, policy_loss: -7.83331, policy_entropy: 4.09775, alpha: 0.98504, time: 38.43144
[CW] eval: return: 24.93697, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 22.23560, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.08231, qf2_loss: 0.08233, policy_loss: -8.52171, policy_entropy: 4.09996, alpha: 0.95626, time: 31.62279
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 24.55719, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.07664, qf2_loss: 0.07677, policy_loss: -9.22799, policy_entropy: 4.10078, alpha: 0.92871, time: 31.95159
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 26.78935, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.06878, qf2_loss: 0.06884, policy_loss: -10.17467, policy_entropy: 4.10108, alpha: 0.90231, time: 32.17807
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 22.51528, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06430, qf2_loss: 0.06447, policy_loss: -11.22397, policy_entropy: 4.10121, alpha: 0.87698, time: 31.97078
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 20.10559, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.05977, qf2_loss: 0.06009, policy_loss: -12.32855, policy_entropy: 4.10087, alpha: 0.85267, time: 32.17773
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 24.10791, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.06447, qf2_loss: 0.06538, policy_loss: -13.48500, policy_entropy: 4.09968, alpha: 0.82930, time: 32.42183
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 20.12463, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.07607, qf2_loss: 0.07650, policy_loss: -14.69906, policy_entropy: 4.10139, alpha: 0.80683, time: 31.84404
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 25.91622, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.08390, qf2_loss: 0.08403, policy_loss: -15.92501, policy_entropy: 4.10181, alpha: 0.78520, time: 32.21473
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 25.81844, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.07236, qf2_loss: 0.07250, policy_loss: -17.15023, policy_entropy: 4.10230, alpha: 0.76435, time: 32.02353
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 22.14327, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.07542, qf2_loss: 0.07558, policy_loss: -18.34517, policy_entropy: 4.10200, alpha: 0.74426, time: 32.00878
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 26.39835, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.08333, qf2_loss: 0.08366, policy_loss: -19.51126, policy_entropy: 4.10167, alpha: 0.72488, time: 32.20054
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 25.43697, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.08861, qf2_loss: 0.08902, policy_loss: -20.64646, policy_entropy: 4.10152, alpha: 0.70616, time: 32.34202
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 25.04038, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.06168, qf2_loss: 0.06194, policy_loss: -21.77423, policy_entropy: 4.10233, alpha: 0.68809, time: 32.28242
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 25.36328, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.08420, qf2_loss: 0.08467, policy_loss: -22.85630, policy_entropy: 4.10122, alpha: 0.67061, time: 32.12457
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 25.37418, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.05915, qf2_loss: 0.05942, policy_loss: -23.92270, policy_entropy: 4.10129, alpha: 0.65371, time: 31.96905
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 27.94744, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.08238, qf2_loss: 0.08287, policy_loss: -24.94936, policy_entropy: 4.10256, alpha: 0.63736, time: 32.37942
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 22.48477, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.07751, qf2_loss: 0.07796, policy_loss: -25.95902, policy_entropy: 4.10170, alpha: 0.62152, time: 31.89641
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 25.34399, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.08690, qf2_loss: 0.08748, policy_loss: -26.93231, policy_entropy: 4.10167, alpha: 0.60618, time: 32.48008
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 24.47678, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.07377, qf2_loss: 0.07479, policy_loss: -27.88026, policy_entropy: 4.10118, alpha: 0.59131, time: 32.37367
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 30.01740, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.06655, qf2_loss: 0.06672, policy_loss: -28.80157, policy_entropy: 4.10075, alpha: 0.57689, time: 32.37437
[CW] eval: return: 26.39557, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 26.72067, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.08107, qf2_loss: 0.08155, policy_loss: -29.70285, policy_entropy: 4.10187, alpha: 0.56290, time: 32.43709
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 22.95590, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.06382, qf2_loss: 0.06428, policy_loss: -30.57129, policy_entropy: 4.10254, alpha: 0.54933, time: 32.45403
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 28.44488, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.08029, qf2_loss: 0.08112, policy_loss: -31.41532, policy_entropy: 4.10102, alpha: 0.53614, time: 32.44297
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 23.92247, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.05406, qf2_loss: 0.05441, policy_loss: -32.23269, policy_entropy: 4.10090, alpha: 0.52334, time: 32.38609
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 31.66059, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.08970, qf2_loss: 0.09044, policy_loss: -33.02382, policy_entropy: 4.10207, alpha: 0.51090, time: 32.25556
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 26.12353, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.05683, qf2_loss: 0.05731, policy_loss: -33.79397, policy_entropy: 4.10159, alpha: 0.49880, time: 32.23394
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 32.02672, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.06706, qf2_loss: 0.06766, policy_loss: -34.54135, policy_entropy: 4.10179, alpha: 0.48704, time: 32.55245
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 21.25955, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.07210, qf2_loss: 0.07290, policy_loss: -35.25561, policy_entropy: 4.10101, alpha: 0.47561, time: 32.60351
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 25.11428, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.06204, qf2_loss: 0.06258, policy_loss: -35.95661, policy_entropy: 4.10166, alpha: 0.46448, time: 32.10867
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 27.57115, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.09645, qf2_loss: 0.09763, policy_loss: -36.63734, policy_entropy: 4.10112, alpha: 0.45364, time: 32.41080
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 22.90647, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.04249, qf2_loss: 0.04266, policy_loss: -37.29115, policy_entropy: 4.10074, alpha: 0.44310, time: 32.48667
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 26.27793, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.06080, qf2_loss: 0.06161, policy_loss: -37.92315, policy_entropy: 4.10132, alpha: 0.43283, time: 32.43184
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 24.05097, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.07506, qf2_loss: 0.07597, policy_loss: -38.53939, policy_entropy: 4.10102, alpha: 0.42283, time: 32.66764
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 29.57897, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.04817, qf2_loss: 0.04867, policy_loss: -39.13084, policy_entropy: 4.10220, alpha: 0.41309, time: 32.91306
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 30.78607, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.08964, qf2_loss: 0.09087, policy_loss: -39.70542, policy_entropy: 4.10209, alpha: 0.40359, time: 32.78530
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 25.11966, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.04441, qf2_loss: 0.04494, policy_loss: -40.26061, policy_entropy: 4.10147, alpha: 0.39434, time: 32.53011
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 21.48928, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.06261, qf2_loss: 0.06346, policy_loss: -40.78964, policy_entropy: 4.10204, alpha: 0.38532, time: 32.15775
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 23.96880, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.07446, qf2_loss: 0.07540, policy_loss: -41.30011, policy_entropy: 4.10077, alpha: 0.37652, time: 32.81591
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 25.33322, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.03236, qf2_loss: 0.03266, policy_loss: -41.80390, policy_entropy: 4.10080, alpha: 0.36795, time: 32.82333
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 25.03714, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.07411, qf2_loss: 0.07524, policy_loss: -42.27411, policy_entropy: 4.10089, alpha: 0.35958, time: 32.52662
[CW] eval: return: 26.15016, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 24.23588, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.05258, qf2_loss: 0.05327, policy_loss: -42.73913, policy_entropy: 4.10168, alpha: 0.35142, time: 32.03990
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 24.03199, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.07679, qf2_loss: 0.07791, policy_loss: -43.18293, policy_entropy: 4.10091, alpha: 0.34346, time: 32.69219
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 30.26738, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.03853, qf2_loss: 0.03899, policy_loss: -43.61203, policy_entropy: 4.10226, alpha: 0.33569, time: 32.68410
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 22.82297, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.04388, qf2_loss: 0.04300, policy_loss: -44.01903, policy_entropy: 4.10155, alpha: 0.32810, time: 32.66531
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 24.79983, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.06966, qf2_loss: 0.07164, policy_loss: -44.42178, policy_entropy: 4.10153, alpha: 0.32070, time: 32.71689
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 28.05969, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.05109, qf2_loss: 0.05209, policy_loss: -44.80396, policy_entropy: 4.10077, alpha: 0.31348, time: 32.68220
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 24.15509, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.05198, qf2_loss: 0.05270, policy_loss: -45.17037, policy_entropy: 4.10046, alpha: 0.30643, time: 32.92261
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 22.32677, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.07747, qf2_loss: 0.07874, policy_loss: -45.52203, policy_entropy: 4.10272, alpha: 0.29954, time: 31.98304
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 28.63143, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.02320, qf2_loss: 0.02343, policy_loss: -45.86080, policy_entropy: 4.10128, alpha: 0.29281, time: 31.66912
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 23.39943, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.05239, qf2_loss: 0.05332, policy_loss: -46.18543, policy_entropy: 4.10231, alpha: 0.28625, time: 32.51645
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 21.97863, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.05438, qf2_loss: 0.05528, policy_loss: -46.49259, policy_entropy: 4.10128, alpha: 0.27983, time: 32.73423
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 27.54898, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.04200, qf2_loss: 0.04253, policy_loss: -46.79208, policy_entropy: 4.10155, alpha: 0.27357, time: 32.53962
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 26.53608, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.07521, qf2_loss: 0.07640, policy_loss: -47.07753, policy_entropy: 4.10142, alpha: 0.26745, time: 32.75453
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 21.29943, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.03208, qf2_loss: 0.03252, policy_loss: -47.35530, policy_entropy: 4.10133, alpha: 0.26147, time: 32.76984
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 25.13697, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.05458, qf2_loss: 0.05546, policy_loss: -47.61093, policy_entropy: 4.10204, alpha: 0.25563, time: 32.84875
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 24.00451, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.06531, qf2_loss: 0.06631, policy_loss: -47.85769, policy_entropy: 4.10234, alpha: 0.24993, time: 32.91377
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 25.49227, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.02347, qf2_loss: 0.02383, policy_loss: -48.09620, policy_entropy: 4.10146, alpha: 0.24435, time: 32.75809
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 27.33061, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.06271, qf2_loss: 0.06357, policy_loss: -48.31894, policy_entropy: 4.10173, alpha: 0.23890, time: 32.83391
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 23.94234, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.06583, qf2_loss: 0.06711, policy_loss: -48.53113, policy_entropy: 4.10087, alpha: 0.23358, time: 32.90127
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 26.99388, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.02192, qf2_loss: 0.02229, policy_loss: -48.73433, policy_entropy: 4.10229, alpha: 0.22838, time: 32.88752
[CW] eval: return: 25.09178, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 24.51521, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.05779, qf2_loss: 0.05868, policy_loss: -48.93239, policy_entropy: 4.10110, alpha: 0.22330, time: 32.35310
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 23.32411, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.02743, qf2_loss: 0.02797, policy_loss: -49.11142, policy_entropy: 4.10113, alpha: 0.21833, time: 32.36369
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 22.24453, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.04533, qf2_loss: 0.04605, policy_loss: -49.28865, policy_entropy: 4.10277, alpha: 0.21348, time: 32.40874
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 21.50420, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.04429, qf2_loss: 0.04511, policy_loss: -49.45310, policy_entropy: 4.10215, alpha: 0.20873, time: 32.38649
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 27.55000, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.04996, qf2_loss: 0.05066, policy_loss: -49.60935, policy_entropy: 4.10114, alpha: 0.20409, time: 32.42506
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 25.39395, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.04258, qf2_loss: 0.04346, policy_loss: -49.74880, policy_entropy: 4.10072, alpha: 0.19956, time: 32.39890
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 39.74576, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.03908, qf2_loss: 0.03960, policy_loss: -49.89823, policy_entropy: 4.10218, alpha: 0.19513, time: 32.33768
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 20.17111, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.04141, qf2_loss: 0.04202, policy_loss: -50.02088, policy_entropy: 4.10228, alpha: 0.19079, time: 32.43443
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 24.22514, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.05028, qf2_loss: 0.05071, policy_loss: -50.14195, policy_entropy: 4.10103, alpha: 0.18656, time: 32.52885
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 26.31111, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.04018, qf2_loss: 0.03987, policy_loss: -50.24055, policy_entropy: 4.10057, alpha: 0.18242, time: 32.53015
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 22.52973, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.07398, qf2_loss: 0.07582, policy_loss: -50.34915, policy_entropy: 4.10273, alpha: 0.17838, time: 32.76289
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 24.64349, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.01290, qf2_loss: 0.01315, policy_loss: -50.44478, policy_entropy: 4.10172, alpha: 0.17442, time: 32.90094
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 24.35503, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.06425, qf2_loss: 0.06279, policy_loss: -50.52622, policy_entropy: 4.10131, alpha: 0.17055, time: 32.64976
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 23.82821, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.02278, qf2_loss: 0.02328, policy_loss: -50.60535, policy_entropy: 4.10031, alpha: 0.16677, time: 32.38394
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 27.17353, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.04900, qf2_loss: 0.05019, policy_loss: -50.68132, policy_entropy: 4.10128, alpha: 0.16308, time: 32.45903
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 25.07592, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.02437, qf2_loss: 0.02541, policy_loss: -50.75052, policy_entropy: 4.10050, alpha: 0.15946, time: 32.47031
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 25.16191, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.06029, qf2_loss: 0.06180, policy_loss: -50.80035, policy_entropy: 4.10082, alpha: 0.15593, time: 32.53121
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 24.36376, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.01900, qf2_loss: 0.01928, policy_loss: -50.85072, policy_entropy: 4.10121, alpha: 0.15248, time: 32.46156
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 20.00283, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.04528, qf2_loss: 0.04626, policy_loss: -50.89647, policy_entropy: 4.10159, alpha: 0.14910, time: 32.36819
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 26.19281, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.03069, qf2_loss: 0.03101, policy_loss: -50.93623, policy_entropy: 4.10028, alpha: 0.14580, time: 32.54207
[CW] eval: return: 24.24637, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 37.79019, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.05652, qf2_loss: 0.05765, policy_loss: -50.97392, policy_entropy: 4.10065, alpha: 0.14257, time: 32.20162
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 21.79300, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.03439, qf2_loss: 0.03470, policy_loss: -51.00416, policy_entropy: 4.10189, alpha: 0.13941, time: 32.74907
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 24.00479, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.02224, qf2_loss: 0.02274, policy_loss: -51.02083, policy_entropy: 4.10185, alpha: 0.13632, time: 32.88218
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 23.99439, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.04048, qf2_loss: 0.04094, policy_loss: -51.03684, policy_entropy: 4.10018, alpha: 0.13331, time: 32.96096
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 24.72434, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.05494, qf2_loss: 0.05624, policy_loss: -51.04105, policy_entropy: 4.10143, alpha: 0.13036, time: 32.86336
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 21.25013, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.02847, qf2_loss: 0.02838, policy_loss: -51.04498, policy_entropy: 4.10041, alpha: 0.12747, time: 33.00286
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 27.79254, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.03271, qf2_loss: 0.03313, policy_loss: -51.03624, policy_entropy: 4.10134, alpha: 0.12465, time: 32.88676
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 27.58918, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.04781, qf2_loss: 0.04864, policy_loss: -51.03237, policy_entropy: 4.10081, alpha: 0.12189, time: 32.90285
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 29.30495, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.01687, qf2_loss: 0.01702, policy_loss: -51.01934, policy_entropy: 4.10137, alpha: 0.11919, time: 32.81649
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 23.20816, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.04207, qf2_loss: 0.04257, policy_loss: -51.00281, policy_entropy: 4.10109, alpha: 0.11655, time: 33.00184
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 24.81323, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.03510, qf2_loss: 0.03550, policy_loss: -50.98550, policy_entropy: 4.10080, alpha: 0.11398, time: 32.85600
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 24.51596, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.03670, qf2_loss: 0.03710, policy_loss: -50.95563, policy_entropy: 4.10126, alpha: 0.11145, time: 32.05459
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 24.66490, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.04785, qf2_loss: 0.04885, policy_loss: -50.92846, policy_entropy: 4.10031, alpha: 0.10899, time: 32.39941
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 23.61263, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.01230, qf2_loss: 0.01216, policy_loss: -50.89847, policy_entropy: 4.10106, alpha: 0.10658, time: 32.48298
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 24.82187, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.02961, qf2_loss: 0.02971, policy_loss: -50.85042, policy_entropy: 4.10171, alpha: 0.10422, time: 32.69428
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 23.80405, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.05280, qf2_loss: 0.05448, policy_loss: -50.79545, policy_entropy: 4.10110, alpha: 0.10192, time: 32.70816
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 25.84264, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.02248, qf2_loss: 0.02240, policy_loss: -50.76317, policy_entropy: 4.10104, alpha: 0.09966, time: 32.68788
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 28.63217, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.03445, qf2_loss: 0.03446, policy_loss: -50.70561, policy_entropy: 4.10070, alpha: 0.09746, time: 32.46921
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 24.37207, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.03424, qf2_loss: 0.03482, policy_loss: -50.65614, policy_entropy: 4.09999, alpha: 0.09530, time: 32.49342
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 25.10627, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.03091, qf2_loss: 0.03123, policy_loss: -50.58892, policy_entropy: 4.10031, alpha: 0.09320, time: 32.49952
[CW] eval: return: 24.68314, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 31.01639, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.03645, qf2_loss: 0.03717, policy_loss: -50.53669, policy_entropy: 4.10004, alpha: 0.09114, time: 32.48693
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 27.38723, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.02683, qf2_loss: 0.02722, policy_loss: -50.47147, policy_entropy: 4.10042, alpha: 0.08912, time: 32.55793
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 21.13593, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.05243, qf2_loss: 0.05313, policy_loss: -50.39581, policy_entropy: 4.09985, alpha: 0.08715, time: 32.36781
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 25.13792, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.01934, qf2_loss: 0.01934, policy_loss: -50.34038, policy_entropy: 4.09970, alpha: 0.08522, time: 33.41398
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 25.42505, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.04017, qf2_loss: 0.04047, policy_loss: -50.25024, policy_entropy: 4.09941, alpha: 0.08334, time: 34.78771
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 27.03422, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.04548, qf2_loss: 0.04649, policy_loss: -50.17334, policy_entropy: 4.09905, alpha: 0.08149, time: 31.99502
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 23.35115, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.03261, qf2_loss: 0.03304, policy_loss: -50.10341, policy_entropy: 4.09847, alpha: 0.07969, time: 31.70786
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 25.62722, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.02452, qf2_loss: 0.02345, policy_loss: -49.99496, policy_entropy: 4.09873, alpha: 0.07793, time: 31.82814
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 22.82907, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.03469, qf2_loss: 0.03438, policy_loss: -49.89284, policy_entropy: 4.09754, alpha: 0.07621, time: 31.87526
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 23.30612, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.03603, qf2_loss: 0.03800, policy_loss: -49.83223, policy_entropy: 4.09904, alpha: 0.07452, time: 32.15753
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 28.58941, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.02515, qf2_loss: 0.02572, policy_loss: -49.73972, policy_entropy: 4.09732, alpha: 0.07287, time: 32.01013
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 26.04790, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.05927, qf2_loss: 0.05887, policy_loss: -49.63750, policy_entropy: 4.09723, alpha: 0.07126, time: 31.90150
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 26.24797, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.02623, qf2_loss: 0.02857, policy_loss: -49.54917, policy_entropy: 4.09632, alpha: 0.06969, time: 31.79356
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 23.89590, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.01583, qf2_loss: 0.01543, policy_loss: -49.45218, policy_entropy: 4.09630, alpha: 0.06815, time: 31.40747
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 24.44826, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.04250, qf2_loss: 0.04213, policy_loss: -49.34614, policy_entropy: 4.09437, alpha: 0.06664, time: 31.90119
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 24.05934, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.01993, qf2_loss: 0.02038, policy_loss: -49.23642, policy_entropy: 4.09641, alpha: 0.06517, time: 31.86793
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 27.11812, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.04946, qf2_loss: 0.04939, policy_loss: -49.12966, policy_entropy: 4.09265, alpha: 0.06373, time: 31.88440
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 25.31784, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.02258, qf2_loss: 0.02283, policy_loss: -49.02218, policy_entropy: 4.09379, alpha: 0.06232, time: 31.76681
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 24.01249, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.03042, qf2_loss: 0.03075, policy_loss: -48.91528, policy_entropy: 4.09435, alpha: 0.06094, time: 31.57066
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 26.39306, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.03442, qf2_loss: 0.03460, policy_loss: -48.80629, policy_entropy: 4.09348, alpha: 0.05959, time: 31.74163
[CW] eval: return: 25.71617, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 23.46868, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.03542, qf2_loss: 0.03528, policy_loss: -48.68372, policy_entropy: 4.09442, alpha: 0.05828, time: 31.60376
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 24.83057, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.01886, qf2_loss: 0.01930, policy_loss: -48.57715, policy_entropy: 4.09445, alpha: 0.05699, time: 31.57336
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 26.21160, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.05224, qf2_loss: 0.05266, policy_loss: -48.45928, policy_entropy: 4.09408, alpha: 0.05573, time: 31.39234
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 27.12052, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.01862, qf2_loss: 0.01881, policy_loss: -48.33584, policy_entropy: 4.09390, alpha: 0.05450, time: 31.52053
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 29.93605, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.02629, qf2_loss: 0.02643, policy_loss: -48.21561, policy_entropy: 4.09366, alpha: 0.05329, time: 31.56103
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 25.12661, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.05440, qf2_loss: 0.05457, policy_loss: -48.09094, policy_entropy: 4.09086, alpha: 0.05211, time: 31.65254
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 27.61765, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.01561, qf2_loss: 0.01612, policy_loss: -47.96464, policy_entropy: 4.09414, alpha: 0.05096, time: 31.48599
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 26.72677, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.02776, qf2_loss: 0.02775, policy_loss: -47.82327, policy_entropy: 4.09125, alpha: 0.04983, time: 31.58057
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 24.64377, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.05354, qf2_loss: 0.05395, policy_loss: -47.72082, policy_entropy: 4.09081, alpha: 0.04873, time: 31.75746
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 21.72842, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.01810, qf2_loss: 0.01835, policy_loss: -47.57096, policy_entropy: 4.09206, alpha: 0.04766, time: 31.45390
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 23.45071, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.03250, qf2_loss: 0.03304, policy_loss: -47.44789, policy_entropy: 4.09047, alpha: 0.04660, time: 31.55062
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 24.92376, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.03384, qf2_loss: 0.03359, policy_loss: -47.32802, policy_entropy: 4.08670, alpha: 0.04557, time: 31.53408
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 25.15925, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.02747, qf2_loss: 0.02750, policy_loss: -47.19045, policy_entropy: 4.08563, alpha: 0.04457, time: 31.55649
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 23.52619, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.02169, qf2_loss: 0.02165, policy_loss: -47.05837, policy_entropy: 4.08952, alpha: 0.04358, time: 31.62757
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 24.99941, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.07103, qf2_loss: 0.07153, policy_loss: -46.92955, policy_entropy: 4.08633, alpha: 0.04262, time: 31.73939
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 30.03290, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.01393, qf2_loss: 0.01403, policy_loss: -46.78813, policy_entropy: 4.08799, alpha: 0.04168, time: 31.55796
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 22.43692, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.01951, qf2_loss: 0.01954, policy_loss: -46.65591, policy_entropy: 4.08627, alpha: 0.04076, time: 31.64683
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 25.84719, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.02641, qf2_loss: 0.02675, policy_loss: -46.51566, policy_entropy: 4.08590, alpha: 0.03986, time: 31.74737
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 29.01639, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.03722, qf2_loss: 0.03777, policy_loss: -46.38012, policy_entropy: 4.08251, alpha: 0.03897, time: 32.20778
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 25.62376, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.01316, qf2_loss: 0.01288, policy_loss: -46.24366, policy_entropy: 4.08429, alpha: 0.03811, time: 32.34518
[CW] eval: return: 25.15429, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 21.27235, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.03058, qf2_loss: 0.03015, policy_loss: -46.09516, policy_entropy: 4.08537, alpha: 0.03727, time: 32.71576
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 31.93146, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.03719, qf2_loss: 0.03783, policy_loss: -45.96583, policy_entropy: 4.08577, alpha: 0.03645, time: 32.68799
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 21.96616, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.02609, qf2_loss: 0.02641, policy_loss: -45.82069, policy_entropy: 4.08411, alpha: 0.03564, time: 32.67945
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 25.29074, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.03041, qf2_loss: 0.03039, policy_loss: -45.67911, policy_entropy: 4.07913, alpha: 0.03486, time: 32.77558
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 26.64528, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.02992, qf2_loss: 0.02996, policy_loss: -45.53629, policy_entropy: 4.07849, alpha: 0.03409, time: 32.79062
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 23.84418, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.02750, qf2_loss: 0.02737, policy_loss: -45.39056, policy_entropy: 4.07669, alpha: 0.03333, time: 32.55056
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 31.25604, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.03632, qf2_loss: 0.03653, policy_loss: -45.25099, policy_entropy: 4.08116, alpha: 0.03260, time: 32.74676
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 27.99439, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.01576, qf2_loss: 0.01571, policy_loss: -45.11086, policy_entropy: 4.08083, alpha: 0.03188, time: 32.60671
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 24.26124, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.03160, qf2_loss: 0.03168, policy_loss: -44.95917, policy_entropy: 4.08529, alpha: 0.03117, time: 32.84519
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 27.11947, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.03429, qf2_loss: 0.03434, policy_loss: -44.80622, policy_entropy: 4.08144, alpha: 0.03048, time: 32.79427
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 25.65548, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.01994, qf2_loss: 0.02007, policy_loss: -44.66573, policy_entropy: 4.08134, alpha: 0.02981, time: 32.46717
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 23.32314, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.03487, qf2_loss: 0.03485, policy_loss: -44.50864, policy_entropy: 4.08075, alpha: 0.02915, time: 32.78757
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 24.07238, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.01734, qf2_loss: 0.01764, policy_loss: -44.36502, policy_entropy: 4.07497, alpha: 0.02851, time: 32.67840
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 21.04284, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.05232, qf2_loss: 0.05182, policy_loss: -44.21096, policy_entropy: 4.08368, alpha: 0.02788, time: 32.85951
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 32.51315, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.01214, qf2_loss: 0.01223, policy_loss: -44.07384, policy_entropy: 4.08471, alpha: 0.02726, time: 32.79491
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 27.00682, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.03462, qf2_loss: 0.03455, policy_loss: -43.92561, policy_entropy: 4.07457, alpha: 0.02666, time: 32.73164
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 22.90599, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.01190, qf2_loss: 0.01193, policy_loss: -43.77117, policy_entropy: 4.07793, alpha: 0.02607, time: 32.77289
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 25.15252, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.03013, qf2_loss: 0.02994, policy_loss: -43.63139, policy_entropy: 4.06961, alpha: 0.02549, time: 32.13213
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 40.66346, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.03311, qf2_loss: 0.03332, policy_loss: -43.48588, policy_entropy: 4.06662, alpha: 0.02493, time: 32.23227
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 25.92903, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.02377, qf2_loss: 0.02357, policy_loss: -43.32818, policy_entropy: 4.05879, alpha: 0.02438, time: 32.10138
[CW] eval: return: 26.03959, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 23.89312, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.02859, qf2_loss: 0.02871, policy_loss: -43.17167, policy_entropy: 4.05942, alpha: 0.02384, time: 32.11270
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 25.40110, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.02070, qf2_loss: 0.02061, policy_loss: -43.02661, policy_entropy: 4.06381, alpha: 0.02331, time: 32.30058
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 26.18945, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.03610, qf2_loss: 0.03568, policy_loss: -42.87962, policy_entropy: 4.05305, alpha: 0.02280, time: 32.13350
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 25.57072, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.01807, qf2_loss: 0.01822, policy_loss: -42.71797, policy_entropy: 4.05763, alpha: 0.02230, time: 32.15522
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 22.44138, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.02691, qf2_loss: 0.02725, policy_loss: -42.56751, policy_entropy: 4.05297, alpha: 0.02180, time: 32.30093
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 24.71445, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.02336, qf2_loss: 0.02274, policy_loss: -42.41655, policy_entropy: 4.02326, alpha: 0.02132, time: 32.16101
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 25.32838, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.02530, qf2_loss: 0.02504, policy_loss: -42.26495, policy_entropy: 4.05240, alpha: 0.02085, time: 32.24682
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 27.82362, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.01933, qf2_loss: 0.01951, policy_loss: -42.10249, policy_entropy: 4.04042, alpha: 0.02039, time: 32.09976
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 26.90641, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.03156, qf2_loss: 0.03163, policy_loss: -41.95823, policy_entropy: 4.04098, alpha: 0.01994, time: 32.14486
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 28.74167, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.02407, qf2_loss: 0.02342, policy_loss: -41.80315, policy_entropy: 4.02012, alpha: 0.01950, time: 32.23391
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 23.60129, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.02217, qf2_loss: 0.02213, policy_loss: -41.65418, policy_entropy: 4.03006, alpha: 0.01907, time: 32.19381
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 23.72949, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.02446, qf2_loss: 0.02430, policy_loss: -41.49740, policy_entropy: 4.02720, alpha: 0.01865, time: 32.27106
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 29.55716, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.02155, qf2_loss: 0.02139, policy_loss: -41.34719, policy_entropy: 4.00944, alpha: 0.01824, time: 31.97017
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 21.58536, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.02751, qf2_loss: 0.02736, policy_loss: -41.19061, policy_entropy: 4.00095, alpha: 0.01784, time: 32.15433
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 23.68899, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.01782, qf2_loss: 0.01750, policy_loss: -41.04414, policy_entropy: 4.01043, alpha: 0.01745, time: 32.10657
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 25.20548, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.03494, qf2_loss: 0.03502, policy_loss: -40.89037, policy_entropy: 3.99193, alpha: 0.01707, time: 32.16074
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 22.64975, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.01492, qf2_loss: 0.01470, policy_loss: -40.73924, policy_entropy: 3.98766, alpha: 0.01669, time: 32.07768
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 31.95453, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.02246, qf2_loss: 0.02238, policy_loss: -40.58896, policy_entropy: 3.96346, alpha: 0.01632, time: 32.00229
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 26.54801, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.01422, qf2_loss: 0.01431, policy_loss: -40.43257, policy_entropy: 3.98350, alpha: 0.01596, time: 32.11242
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 25.46868, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.03155, qf2_loss: 0.03054, policy_loss: -40.27875, policy_entropy: 3.92633, alpha: 0.01561, time: 31.97418
[CW] eval: return: 27.00000, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 26.08291, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.02055, qf2_loss: 0.02032, policy_loss: -40.13295, policy_entropy: 3.97735, alpha: 0.01527, time: 32.11746
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 23.39503, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.02106, qf2_loss: 0.02149, policy_loss: -39.97536, policy_entropy: 3.95644, alpha: 0.01493, time: 32.02835
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 25.99212, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.02803, qf2_loss: 0.02769, policy_loss: -39.81783, policy_entropy: 3.96336, alpha: 0.01461, time: 31.89312
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 26.44491, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.01109, qf2_loss: 0.01108, policy_loss: -39.67267, policy_entropy: 3.95271, alpha: 0.01429, time: 31.81976
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 24.81121, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.04251, qf2_loss: 0.04210, policy_loss: -39.52393, policy_entropy: 3.96923, alpha: 0.01397, time: 31.92542
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 36.92888, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 0.01278, qf2_loss: 0.01259, policy_loss: -39.37893, policy_entropy: 3.96058, alpha: 0.01366, time: 31.75504
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 26.25586, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.03993, qf2_loss: 0.03962, policy_loss: -39.21996, policy_entropy: 3.95701, alpha: 0.01336, time: 31.87843
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 22.87900, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.01359, qf2_loss: 0.01375, policy_loss: -39.08153, policy_entropy: 3.93601, alpha: 0.01307, time: 31.81375
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 26.02754, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.01299, qf2_loss: 0.01268, policy_loss: -38.93021, policy_entropy: 3.87176, alpha: 0.01278, time: 31.90701
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 29.94802, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.02365, qf2_loss: 0.02355, policy_loss: -38.79557, policy_entropy: 3.93773, alpha: 0.01250, time: 31.98067
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 21.21612, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 0.02893, qf2_loss: 0.02837, policy_loss: -38.64281, policy_entropy: 3.89763, alpha: 0.01223, time: 31.78860
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 27.36443, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.02113, qf2_loss: 0.02128, policy_loss: -38.48711, policy_entropy: 3.89877, alpha: 0.01196, time: 31.92996
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 25.78721, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.01669, qf2_loss: 0.01562, policy_loss: -38.32675, policy_entropy: 3.90213, alpha: 0.01169, time: 31.70009
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 27.17971, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.01990, qf2_loss: 0.02070, policy_loss: -38.18847, policy_entropy: 3.73174, alpha: 0.01144, time: 31.83679
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 31.02196, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.03756, qf2_loss: 0.03633, policy_loss: -38.04035, policy_entropy: 3.91208, alpha: 0.01119, time: 31.68889
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 36.08692, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.01781, qf2_loss: 0.01801, policy_loss: -37.89511, policy_entropy: 3.84348, alpha: 0.01095, time: 31.78182
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 33.26804, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.00912, qf2_loss: 0.00901, policy_loss: -37.75071, policy_entropy: 3.86933, alpha: 0.01070, time: 31.76773
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 25.95682, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.02226, qf2_loss: 0.02219, policy_loss: -37.61036, policy_entropy: 3.82043, alpha: 0.01047, time: 31.64010
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 25.13847, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.02548, qf2_loss: 0.02554, policy_loss: -37.46528, policy_entropy: 3.77841, alpha: 0.01024, time: 31.81373
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 23.83531, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 0.02042, qf2_loss: 0.01996, policy_loss: -37.29850, policy_entropy: 3.88442, alpha: 0.01002, time: 31.63328
[CW] eval: return: 27.45780, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 22.21796, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.01958, qf2_loss: 0.01944, policy_loss: -37.15927, policy_entropy: 3.83702, alpha: 0.00980, time: 31.66940
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 31.74127, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 0.03174, qf2_loss: 0.03121, policy_loss: -37.02198, policy_entropy: 3.80334, alpha: 0.00958, time: 31.70465
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 21.61558, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.01495, qf2_loss: 0.01499, policy_loss: -36.87282, policy_entropy: 3.63427, alpha: 0.00937, time: 31.73596
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 27.34892, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 0.01537, qf2_loss: 0.01517, policy_loss: -36.73393, policy_entropy: 3.72993, alpha: 0.00917, time: 32.07873
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 24.67367, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 0.01757, qf2_loss: 0.01720, policy_loss: -36.59026, policy_entropy: 3.72500, alpha: 0.00897, time: 32.83978
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 30.81741, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 0.02507, qf2_loss: 0.02492, policy_loss: -36.44698, policy_entropy: 3.63636, alpha: 0.00878, time: 32.19774
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 25.11659, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 0.03236, qf2_loss: 0.03171, policy_loss: -36.30380, policy_entropy: 3.80710, alpha: 0.00858, time: 32.27533
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 32.67193, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.00956, qf2_loss: 0.00957, policy_loss: -36.15386, policy_entropy: 3.87768, alpha: 0.00839, time: 32.03932
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 32.09378, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.02678, qf2_loss: 0.02662, policy_loss: -36.02287, policy_entropy: 3.84685, alpha: 0.00821, time: 32.23076
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 30.57835, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 0.01140, qf2_loss: 0.01106, policy_loss: -35.87220, policy_entropy: 3.62110, alpha: 0.00803, time: 36.89279
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 20.80870, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 0.03091, qf2_loss: 0.03056, policy_loss: -35.73555, policy_entropy: 3.71995, alpha: 0.00785, time: 33.58398
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 25.32922, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 0.00890, qf2_loss: 0.00905, policy_loss: -35.57427, policy_entropy: 3.44972, alpha: 0.00768, time: 32.68195
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 25.36940, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 0.02151, qf2_loss: 0.02094, policy_loss: -35.44174, policy_entropy: 3.51204, alpha: 0.00752, time: 33.94939
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 33.92757, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 0.01477, qf2_loss: 0.01453, policy_loss: -35.31455, policy_entropy: 3.07512, alpha: 0.00736, time: 33.11857
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 25.95913, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 0.01845, qf2_loss: 0.01817, policy_loss: -35.16712, policy_entropy: 3.22866, alpha: 0.00721, time: 32.80057
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 22.71838, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 0.02790, qf2_loss: 0.02763, policy_loss: -35.02462, policy_entropy: 3.18846, alpha: 0.00706, time: 32.89595
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 31.46427, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 0.01200, qf2_loss: 0.01158, policy_loss: -34.89369, policy_entropy: 3.03954, alpha: 0.00691, time: 32.92965
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 26.15954, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 0.01966, qf2_loss: 0.01964, policy_loss: -34.75193, policy_entropy: 3.29963, alpha: 0.00676, time: 32.84296
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 21.91318, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 0.02777, qf2_loss: 0.02731, policy_loss: -34.62099, policy_entropy: 3.20561, alpha: 0.00662, time: 33.24876
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 23.59664, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 0.01807, qf2_loss: 0.01793, policy_loss: -34.48201, policy_entropy: 3.23630, alpha: 0.00648, time: 33.13336
[CW] eval: return: 28.02775, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 23.64790, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 0.01574, qf2_loss: 0.01571, policy_loss: -34.33746, policy_entropy: 3.27051, alpha: 0.00634, time: 32.94532
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 22.53229, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 0.02282, qf2_loss: 0.02212, policy_loss: -34.20842, policy_entropy: 3.28565, alpha: 0.00620, time: 33.02924
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 28.07304, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 0.02082, qf2_loss: 0.02115, policy_loss: -34.07551, policy_entropy: 3.08445, alpha: 0.00607, time: 32.83568
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 25.72081, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 0.02187, qf2_loss: 0.02075, policy_loss: -33.93178, policy_entropy: 2.94181, alpha: 0.00594, time: 33.01365
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 31.29384, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 0.01930, qf2_loss: 0.01961, policy_loss: -33.80701, policy_entropy: 3.03854, alpha: 0.00581, time: 32.91852
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 36.52695, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 0.02314, qf2_loss: 0.02279, policy_loss: -33.66355, policy_entropy: 2.87135, alpha: 0.00569, time: 32.78584
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 44.08281, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 0.01191, qf2_loss: 0.01195, policy_loss: -33.53490, policy_entropy: 2.28866, alpha: 0.00558, time: 32.91879
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 28.34577, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 0.02506, qf2_loss: 0.02466, policy_loss: -33.39916, policy_entropy: 2.62362, alpha: 0.00546, time: 32.80950
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 23.69057, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 0.01921, qf2_loss: 0.01880, policy_loss: -33.26483, policy_entropy: 1.60174, alpha: 0.00536, time: 32.95323
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 47.63744, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 0.02338, qf2_loss: 0.02322, policy_loss: -33.14847, policy_entropy: 0.99408, alpha: 0.00526, time: 32.89996
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 78.01135, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 0.01998, qf2_loss: 0.01972, policy_loss: -33.01387, policy_entropy: 1.56900, alpha: 0.00517, time: 33.01679
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 30.35279, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 0.01871, qf2_loss: 0.01858, policy_loss: -32.89107, policy_entropy: 0.05583, alpha: 0.00508, time: 32.85999
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 62.01872, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 0.03230, qf2_loss: 0.03189, policy_loss: -32.75912, policy_entropy: 0.54718, alpha: 0.00500, time: 32.79188
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 25.41168, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 0.01728, qf2_loss: 0.01702, policy_loss: -32.61421, policy_entropy: 1.29331, alpha: 0.00491, time: 32.58037
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 45.58413, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 0.01235, qf2_loss: 0.01238, policy_loss: -32.50263, policy_entropy: 0.76188, alpha: 0.00482, time: 32.45896
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 47.61818, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 0.02042, qf2_loss: 0.02006, policy_loss: -32.38077, policy_entropy: 0.39146, alpha: 0.00473, time: 32.67819
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 23.52918, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 0.03569, qf2_loss: 0.03504, policy_loss: -32.25765, policy_entropy: 0.48556, alpha: 0.00465, time: 32.63508
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 46.46934, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 0.01738, qf2_loss: 0.01764, policy_loss: -32.13197, policy_entropy: -0.07888, alpha: 0.00457, time: 32.69260
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 31.53962, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 0.01388, qf2_loss: 0.01369, policy_loss: -32.01783, policy_entropy: -0.16225, alpha: 0.00450, time: 32.37050
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 68.55285, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 0.02115, qf2_loss: 0.02062, policy_loss: -31.89517, policy_entropy: -0.31163, alpha: 0.00442, time: 32.37449
[CW] eval: return: 48.44362, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 29.63022, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 0.02258, qf2_loss: 0.02234, policy_loss: -31.75284, policy_entropy: 0.06209, alpha: 0.00435, time: 32.48070
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 56.06973, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 0.01905, qf2_loss: 0.01881, policy_loss: -31.66786, policy_entropy: -0.21834, alpha: 0.00427, time: 32.41665
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 23.97944, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 0.02816, qf2_loss: 0.02778, policy_loss: -31.53908, policy_entropy: -0.37663, alpha: 0.00420, time: 32.33032
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 53.12524, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 0.01633, qf2_loss: 0.01655, policy_loss: -31.41821, policy_entropy: -0.75554, alpha: 0.00413, time: 32.40843
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 23.05949, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 0.02057, qf2_loss: 0.02005, policy_loss: -31.29662, policy_entropy: -1.37978, alpha: 0.00407, time: 32.33548
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 23.20798, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 0.02042, qf2_loss: 0.02007, policy_loss: -31.17591, policy_entropy: -0.79745, alpha: 0.00400, time: 32.48080
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 70.27841, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 0.04327, qf2_loss: 0.04246, policy_loss: -31.06841, policy_entropy: -1.33770, alpha: 0.00394, time: 32.32706
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 9.91724, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 0.01599, qf2_loss: 0.01589, policy_loss: -30.94476, policy_entropy: -1.27736, alpha: 0.00388, time: 32.53629
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 67.19901, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 0.01544, qf2_loss: 0.01543, policy_loss: -30.83336, policy_entropy: -1.28364, alpha: 0.00381, time: 32.34446
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 65.39098, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 0.01573, qf2_loss: 0.01562, policy_loss: -30.74428, policy_entropy: -2.72597, alpha: 0.00376, time: 32.39725
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 23.33566, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 0.02331, qf2_loss: 0.02305, policy_loss: -30.61376, policy_entropy: -2.57708, alpha: 0.00372, time: 32.46057
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 26.72909, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 0.02849, qf2_loss: 0.02781, policy_loss: -30.51332, policy_entropy: -1.42486, alpha: 0.00366, time: 32.35909
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 23.65435, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 0.02005, qf2_loss: 0.01986, policy_loss: -30.40662, policy_entropy: -3.16838, alpha: 0.00361, time: 32.33736
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 59.94283, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 0.02706, qf2_loss: 0.02677, policy_loss: -30.29318, policy_entropy: -2.37166, alpha: 0.00357, time: 32.31557
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 62.42753, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 0.01796, qf2_loss: 0.01773, policy_loss: -30.18680, policy_entropy: -3.27269, alpha: 0.00352, time: 32.49020
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 69.85998, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 0.02549, qf2_loss: 0.02557, policy_loss: -30.09302, policy_entropy: -3.79061, alpha: 0.00349, time: 32.40612
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 63.14529, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 0.02328, qf2_loss: 0.02270, policy_loss: -29.97754, policy_entropy: -4.35930, alpha: 0.00346, time: 33.04465
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 24.46567, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 0.02074, qf2_loss: 0.02067, policy_loss: -29.87500, policy_entropy: -4.31157, alpha: 0.00343, time: 32.71601
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 23.86726, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 0.03610, qf2_loss: 0.03567, policy_loss: -29.76316, policy_entropy: -3.37699, alpha: 0.00340, time: 32.67596
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 62.86243, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 0.02756, qf2_loss: 0.02717, policy_loss: -29.66457, policy_entropy: -4.41244, alpha: 0.00336, time: 32.63283
[CW] eval: return: 48.64438, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 71.45484, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 0.01890, qf2_loss: 0.01866, policy_loss: -29.57606, policy_entropy: -6.23488, alpha: 0.00335, time: 32.68281
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 46.88007, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 0.01978, qf2_loss: 0.01967, policy_loss: -29.47770, policy_entropy: -6.28415, alpha: 0.00336, time: 32.48297
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 36.50249, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 0.05225, qf2_loss: 0.05154, policy_loss: -29.35996, policy_entropy: -6.12076, alpha: 0.00336, time: 32.49448
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 39.97719, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 0.02004, qf2_loss: 0.02001, policy_loss: -29.26650, policy_entropy: -6.15983, alpha: 0.00336, time: 32.23596
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 60.68412, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 0.01890, qf2_loss: 0.01886, policy_loss: -29.15521, policy_entropy: -6.64794, alpha: 0.00337, time: 32.30244
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 69.88327, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 0.03588, qf2_loss: 0.03558, policy_loss: -29.06080, policy_entropy: -6.35084, alpha: 0.00339, time: 32.17285
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 76.06825, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 0.02074, qf2_loss: 0.02058, policy_loss: -28.97786, policy_entropy: -6.14466, alpha: 0.00339, time: 32.44500
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 59.38538, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 0.02420, qf2_loss: 0.02410, policy_loss: -28.91554, policy_entropy: -5.79360, alpha: 0.00339, time: 32.13715
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 59.80559, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 0.03361, qf2_loss: 0.03311, policy_loss: -28.78735, policy_entropy: -5.50623, alpha: 0.00338, time: 32.31921
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 71.30253, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 0.02963, qf2_loss: 0.02936, policy_loss: -28.70483, policy_entropy: -5.65675, alpha: 0.00337, time: 32.21386
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 59.33694, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 0.04172, qf2_loss: 0.04137, policy_loss: -28.61861, policy_entropy: -5.21795, alpha: 0.00336, time: 32.29762
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 50.92528, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 0.02954, qf2_loss: 0.02926, policy_loss: -28.51932, policy_entropy: -5.06820, alpha: 0.00333, time: 32.36694
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 15.63400, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 0.02057, qf2_loss: 0.02064, policy_loss: -28.42098, policy_entropy: -4.15446, alpha: 0.00329, time: 32.17102
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 47.35194, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 0.02666, qf2_loss: 0.02655, policy_loss: -28.34301, policy_entropy: -4.96205, alpha: 0.00325, time: 32.28014
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 66.32199, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 0.02969, qf2_loss: 0.02972, policy_loss: -28.24770, policy_entropy: -4.83111, alpha: 0.00322, time: 32.04953
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 76.56174, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 0.03942, qf2_loss: 0.03879, policy_loss: -28.15267, policy_entropy: -4.95032, alpha: 0.00319, time: 32.23084
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 40.19363, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 0.02401, qf2_loss: 0.02394, policy_loss: -28.05516, policy_entropy: -4.30296, alpha: 0.00314, time: 32.16707
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 63.01902, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 0.03058, qf2_loss: 0.03012, policy_loss: -27.96374, policy_entropy: -4.52061, alpha: 0.00310, time: 32.43479
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 32.35053, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 0.02781, qf2_loss: 0.02782, policy_loss: -27.86715, policy_entropy: -4.26550, alpha: 0.00305, time: 32.43525
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 46.79121, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 0.03691, qf2_loss: 0.03654, policy_loss: -27.77794, policy_entropy: -4.11501, alpha: 0.00299, time: 32.33871
[CW] eval: return: 62.17526, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 67.77271, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 0.02794, qf2_loss: 0.02776, policy_loss: -27.70239, policy_entropy: -4.54494, alpha: 0.00295, time: 32.26367
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 20.22478, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 0.03730, qf2_loss: 0.03643, policy_loss: -27.60108, policy_entropy: -4.06452, alpha: 0.00290, time: 32.19884
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 57.18229, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 0.03197, qf2_loss: 0.03194, policy_loss: -27.51093, policy_entropy: -4.24113, alpha: 0.00283, time: 32.21046
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 85.29315, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 0.03394, qf2_loss: 0.03367, policy_loss: -27.41531, policy_entropy: -4.51455, alpha: 0.00279, time: 32.37122
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 44.82128, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 0.03823, qf2_loss: 0.03759, policy_loss: -27.33728, policy_entropy: -4.96717, alpha: 0.00275, time: 32.17096
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 23.90332, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 0.03203, qf2_loss: 0.03188, policy_loss: -27.25713, policy_entropy: -5.31677, alpha: 0.00273, time: 32.32131
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 56.84987, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 0.04571, qf2_loss: 0.04525, policy_loss: -27.19437, policy_entropy: -5.24894, alpha: 0.00271, time: 32.14032
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 56.34457, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 0.06912, qf2_loss: 0.06938, policy_loss: -27.09034, policy_entropy: -5.36750, alpha: 0.00268, time: 32.24347
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 23.99863, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 0.05530, qf2_loss: 0.05374, policy_loss: -26.91772, policy_entropy: -1.29028, alpha: 0.00262, time: 32.25185
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 11.41237, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 0.02548, qf2_loss: 0.02542, policy_loss: -26.81798, policy_entropy: -2.83504, alpha: 0.00251, time: 32.28157
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 33.02548, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 0.02334, qf2_loss: 0.02316, policy_loss: -26.73823, policy_entropy: -3.38946, alpha: 0.00244, time: 32.06778
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 27.50348, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 0.02606, qf2_loss: 0.02578, policy_loss: -26.64923, policy_entropy: -2.72591, alpha: 0.00238, time: 32.50670
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 17.57655, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 0.02434, qf2_loss: 0.02391, policy_loss: -26.54992, policy_entropy: -3.45984, alpha: 0.00231, time: 32.33542
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 26.63507, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 0.02581, qf2_loss: 0.02577, policy_loss: -26.44443, policy_entropy: -3.89338, alpha: 0.00226, time: 32.59075
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 22.27113, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 0.02657, qf2_loss: 0.02646, policy_loss: -26.36075, policy_entropy: -2.88837, alpha: 0.00221, time: 32.64220
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 23.39841, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 0.04424, qf2_loss: 0.04301, policy_loss: -26.27345, policy_entropy: -2.80816, alpha: 0.00215, time: 32.48876
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 24.25147, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 0.02479, qf2_loss: 0.02459, policy_loss: -26.17837, policy_entropy: -2.62114, alpha: 0.00209, time: 32.27472
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 78.99576, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 0.02291, qf2_loss: 0.02290, policy_loss: -26.08744, policy_entropy: -4.27467, alpha: 0.00205, time: 32.17598
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 27.37126, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 0.02672, qf2_loss: 0.02666, policy_loss: -25.99039, policy_entropy: -4.04334, alpha: 0.00201, time: 32.22713
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 3.72385, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 0.03306, qf2_loss: 0.03257, policy_loss: -25.92259, policy_entropy: -3.43752, alpha: 0.00198, time: 32.22542
[CW] eval: return: 33.11610, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 31.95630, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 0.02642, qf2_loss: 0.02609, policy_loss: -25.81448, policy_entropy: -3.53210, alpha: 0.00193, time: 32.06367
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 27.95721, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 0.02666, qf2_loss: 0.02632, policy_loss: -25.74030, policy_entropy: -4.18058, alpha: 0.00190, time: 32.28598
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 10.76211, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 0.02844, qf2_loss: 0.02782, policy_loss: -25.66225, policy_entropy: -4.73943, alpha: 0.00187, time: 32.17810
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 24.01752, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 0.02599, qf2_loss: 0.02611, policy_loss: -25.56933, policy_entropy: -3.96296, alpha: 0.00184, time: 32.21432
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 26.80913, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 0.03014, qf2_loss: 0.02988, policy_loss: -25.46786, policy_entropy: -3.66469, alpha: 0.00181, time: 32.10642
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 26.60946, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 0.02275, qf2_loss: 0.02274, policy_loss: -25.38888, policy_entropy: -3.51701, alpha: 0.00176, time: 32.16552
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 38.83675, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 0.02197, qf2_loss: 0.02168, policy_loss: -25.31254, policy_entropy: -4.12170, alpha: 0.00173, time: 32.31723
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 27.73890, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 0.02188, qf2_loss: 0.02155, policy_loss: -25.20769, policy_entropy: -3.86948, alpha: 0.00170, time: 32.20693
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 30.72149, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 0.03991, qf2_loss: 0.03924, policy_loss: -25.12080, policy_entropy: -3.67554, alpha: 0.00166, time: 32.23892
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 25.54853, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 0.02149, qf2_loss: 0.02134, policy_loss: -25.04323, policy_entropy: -8.26863, alpha: 0.00166, time: 32.21077
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 22.31161, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 0.02400, qf2_loss: 0.02385, policy_loss: -24.98238, policy_entropy: -8.11757, alpha: 0.00170, time: 34.81011
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 21.89948, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 0.02245, qf2_loss: 0.02226, policy_loss: -24.88934, policy_entropy: -8.13746, alpha: 0.00173, time: 32.27789
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 22.53678, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 0.03293, qf2_loss: 0.03271, policy_loss: -24.82188, policy_entropy: -7.34251, alpha: 0.00176, time: 32.35658
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 22.30510, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 0.02761, qf2_loss: 0.02701, policy_loss: -24.70454, policy_entropy: -6.57914, alpha: 0.00179, time: 32.28811
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 22.28612, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 0.02378, qf2_loss: 0.02351, policy_loss: -24.63426, policy_entropy: -7.35177, alpha: 0.00180, time: 32.29843
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 25.61802, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 0.02194, qf2_loss: 0.02166, policy_loss: -24.56554, policy_entropy: -7.19192, alpha: 0.00183, time: 32.31341
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 59.83450, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 0.02421, qf2_loss: 0.02422, policy_loss: -24.46417, policy_entropy: -6.20813, alpha: 0.00185, time: 37.98194
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 5.49727, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 0.02569, qf2_loss: 0.02521, policy_loss: -24.40342, policy_entropy: -5.85629, alpha: 0.00184, time: 32.60535
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 27.06543, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 0.05247, qf2_loss: 0.05184, policy_loss: -24.40972, policy_entropy: -10.13357, alpha: 0.00188, time: 32.53201
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 23.86351, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 0.03460, qf2_loss: 0.03442, policy_loss: -24.34351, policy_entropy: -9.13528, alpha: 0.00198, time: 32.56450
[CW] eval: return: 23.12032, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 24.08386, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 0.03091, qf2_loss: 0.03103, policy_loss: -24.29106, policy_entropy: -9.05349, alpha: 0.00205, time: 32.27334
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 23.16995, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 0.03275, qf2_loss: 0.03275, policy_loss: -24.22768, policy_entropy: -8.01433, alpha: 0.00213, time: 32.17057
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 57.88977, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 0.06130, qf2_loss: 0.06017, policy_loss: -24.15183, policy_entropy: -7.49748, alpha: 0.00217, time: 32.37502
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 30.17381, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 0.04429, qf2_loss: 0.04443, policy_loss: -24.05966, policy_entropy: -7.09743, alpha: 0.00221, time: 32.20357
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 26.16960, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 0.03488, qf2_loss: 0.03475, policy_loss: -24.03265, policy_entropy: -6.53424, alpha: 0.00224, time: 32.37648
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 59.08515, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 0.03064, qf2_loss: 0.03050, policy_loss: -23.93961, policy_entropy: -5.57489, alpha: 0.00224, time: 32.28563
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 79.90563, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 0.03438, qf2_loss: 0.03432, policy_loss: -23.88957, policy_entropy: -5.15571, alpha: 0.00222, time: 32.42020
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 36.88905, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 0.04866, qf2_loss: 0.04806, policy_loss: -23.80895, policy_entropy: -5.02617, alpha: 0.00219, time: 32.32537
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 27.01652, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 0.03134, qf2_loss: 0.03144, policy_loss: -23.76347, policy_entropy: -5.05114, alpha: 0.00216, time: 32.26406
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 22.97175, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 0.02978, qf2_loss: 0.02972, policy_loss: -23.67193, policy_entropy: -5.19151, alpha: 0.00213, time: 32.27416
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 25.80799, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 0.03485, qf2_loss: 0.03457, policy_loss: -23.62297, policy_entropy: -5.09483, alpha: 0.00210, time: 32.21406
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 32.30714, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 0.03158, qf2_loss: 0.03134, policy_loss: -23.50733, policy_entropy: -4.81202, alpha: 0.00206, time: 32.39066
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 28.55893, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 0.02844, qf2_loss: 0.02852, policy_loss: -23.44553, policy_entropy: -5.48802, alpha: 0.00203, time: 32.23633
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 34.17394, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 0.03313, qf2_loss: 0.03283, policy_loss: -23.36301, policy_entropy: -6.69272, alpha: 0.00204, time: 32.39395
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 23.33397, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 0.05320, qf2_loss: 0.05268, policy_loss: -23.28850, policy_entropy: -4.74612, alpha: 0.00203, time: 32.29445
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 22.70709, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 0.03226, qf2_loss: 0.03208, policy_loss: -23.21384, policy_entropy: -6.43352, alpha: 0.00201, time: 32.42551
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 22.33004, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 0.03603, qf2_loss: 0.03555, policy_loss: -23.16201, policy_entropy: -6.44334, alpha: 0.00203, time: 32.18302
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 22.46036, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 0.02697, qf2_loss: 0.02691, policy_loss: -23.08597, policy_entropy: -5.07126, alpha: 0.00202, time: 32.37192
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 55.46045, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 0.02410, qf2_loss: 0.02417, policy_loss: -23.03216, policy_entropy: -6.07491, alpha: 0.00201, time: 32.18942
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 25.05459, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 0.02515, qf2_loss: 0.02527, policy_loss: -22.93243, policy_entropy: -4.54154, alpha: 0.00197, time: 32.36054
[CW] eval: return: 30.80036, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 22.86771, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 0.02658, qf2_loss: 0.02635, policy_loss: -22.86792, policy_entropy: -4.81661, alpha: 0.00193, time: 32.19991
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 33.81643, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 0.03146, qf2_loss: 0.03138, policy_loss: -22.81252, policy_entropy: -4.70671, alpha: 0.00188, time: 32.04553
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 53.31779, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 0.03912, qf2_loss: 0.03921, policy_loss: -22.73932, policy_entropy: -5.31647, alpha: 0.00185, time: 32.27309
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 41.08458, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 0.03048, qf2_loss: 0.02988, policy_loss: -22.69978, policy_entropy: -4.95940, alpha: 0.00182, time: 32.24228
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 28.01461, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 0.02811, qf2_loss: 0.02804, policy_loss: -22.63213, policy_entropy: -5.36782, alpha: 0.00180, time: 32.30711
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 25.58375, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 0.03240, qf2_loss: 0.03218, policy_loss: -22.56231, policy_entropy: -5.78323, alpha: 0.00178, time: 32.32526
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 48.92551, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 0.02352, qf2_loss: 0.02354, policy_loss: -22.46369, policy_entropy: -6.02742, alpha: 0.00178, time: 32.22296
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 28.66609, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 0.02986, qf2_loss: 0.02978, policy_loss: -22.38866, policy_entropy: -6.44384, alpha: 0.00178, time: 32.51881
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 24.71129, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 0.03025, qf2_loss: 0.02973, policy_loss: -22.35513, policy_entropy: -6.56775, alpha: 0.00180, time: 32.34981
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 59.20429, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 0.03118, qf2_loss: 0.03102, policy_loss: -22.26231, policy_entropy: -6.64169, alpha: 0.00183, time: 32.52562
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 45.14169, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 0.02661, qf2_loss: 0.02675, policy_loss: -22.22907, policy_entropy: -6.58688, alpha: 0.00185, time: 32.25658
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 28.49335, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 0.02346, qf2_loss: 0.02337, policy_loss: -22.17972, policy_entropy: -6.30585, alpha: 0.00187, time: 32.48784
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 51.50554, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 0.02556, qf2_loss: 0.02560, policy_loss: -22.07871, policy_entropy: -6.39556, alpha: 0.00189, time: 32.34830
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 34.07221, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 0.03702, qf2_loss: 0.03622, policy_loss: -22.05193, policy_entropy: -6.49542, alpha: 0.00190, time: 32.54599
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 70.67633, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 0.02961, qf2_loss: 0.02923, policy_loss: -21.93200, policy_entropy: -6.24502, alpha: 0.00193, time: 32.43165
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 27.52822, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 0.02687, qf2_loss: 0.02704, policy_loss: -21.92018, policy_entropy: -6.15682, alpha: 0.00194, time: 32.39520
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 28.91148, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 0.04731, qf2_loss: 0.04686, policy_loss: -21.81477, policy_entropy: -5.66446, alpha: 0.00193, time: 32.24637
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 82.05594, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 0.02955, qf2_loss: 0.02964, policy_loss: -21.73219, policy_entropy: -6.05355, alpha: 0.00193, time: 32.28533
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 81.00609, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 0.03209, qf2_loss: 0.03120, policy_loss: -21.69562, policy_entropy: -5.91675, alpha: 0.00193, time: 32.36536
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 79.50508, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 0.02958, qf2_loss: 0.02947, policy_loss: -21.67877, policy_entropy: -5.32179, alpha: 0.00190, time: 32.26207
[CW] eval: return: 52.45332, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 27.50404, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 0.02878, qf2_loss: 0.02863, policy_loss: -21.62281, policy_entropy: -5.40223, alpha: 0.00187, time: 32.46766
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 68.15820, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 0.02351, qf2_loss: 0.02339, policy_loss: -21.52286, policy_entropy: -5.52600, alpha: 0.00184, time: 32.82275
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 75.85202, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 0.02412, qf2_loss: 0.02409, policy_loss: -21.47772, policy_entropy: -5.92717, alpha: 0.00183, time: 32.64249
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 73.53044, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 0.03821, qf2_loss: 0.03782, policy_loss: -21.42417, policy_entropy: -5.15264, alpha: 0.00181, time: 32.83166
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 36.93781, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 0.02241, qf2_loss: 0.02230, policy_loss: -21.34802, policy_entropy: -6.64802, alpha: 0.00179, time: 32.63583
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 60.58724, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 0.02793, qf2_loss: 0.02777, policy_loss: -21.27928, policy_entropy: -6.61485, alpha: 0.00183, time: 32.81797
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 72.02341, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 0.03976, qf2_loss: 0.03932, policy_loss: -21.22160, policy_entropy: -6.57596, alpha: 0.00186, time: 32.73340
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 83.39465, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 0.02375, qf2_loss: 0.02366, policy_loss: -21.16410, policy_entropy: -6.46924, alpha: 0.00189, time: 32.72752
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 59.97292, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 0.02303, qf2_loss: 0.02300, policy_loss: -21.12130, policy_entropy: -6.73166, alpha: 0.00193, time: 32.55149
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 21.36962, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 0.02504, qf2_loss: 0.02472, policy_loss: -21.06910, policy_entropy: -6.19254, alpha: 0.00196, time: 32.56869
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 35.76269, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 0.02841, qf2_loss: 0.02807, policy_loss: -21.01332, policy_entropy: -6.48520, alpha: 0.00198, time: 32.41495
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 50.33024, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 0.02811, qf2_loss: 0.02804, policy_loss: -20.94508, policy_entropy: -6.23680, alpha: 0.00200, time: 32.15913
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 81.95223, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 0.02579, qf2_loss: 0.02552, policy_loss: -20.87376, policy_entropy: -6.19744, alpha: 0.00203, time: 32.27123
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 73.33600, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 0.02588, qf2_loss: 0.02595, policy_loss: -20.82986, policy_entropy: -6.40374, alpha: 0.00205, time: 32.09691
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 71.30508, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 0.03088, qf2_loss: 0.03037, policy_loss: -20.80299, policy_entropy: -6.10634, alpha: 0.00207, time: 32.35991
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 46.09170, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 0.03209, qf2_loss: 0.03203, policy_loss: -20.75873, policy_entropy: -6.49090, alpha: 0.00208, time: 32.27492
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 76.95952, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 0.03213, qf2_loss: 0.03170, policy_loss: -20.67683, policy_entropy: -6.16268, alpha: 0.00211, time: 32.46506
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 53.77896, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 0.18107, qf2_loss: 0.17699, policy_loss: -20.59472, policy_entropy: -4.83743, alpha: 0.00214, time: 32.31326
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 35.75119, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 0.02640, qf2_loss: 0.02634, policy_loss: -20.47525, policy_entropy: -3.62686, alpha: 0.00203, time: 32.51202
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 26.52104, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 0.01966, qf2_loss: 0.01954, policy_loss: -20.38761, policy_entropy: -5.37322, alpha: 0.00196, time: 32.28296
[CW] eval: return: 38.64713, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 23.21920, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 0.01889, qf2_loss: 0.01882, policy_loss: -20.37877, policy_entropy: -6.25114, alpha: 0.00196, time: 32.51673
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 46.44460, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 0.01728, qf2_loss: 0.01717, policy_loss: -20.29880, policy_entropy: -6.24954, alpha: 0.00197, time: 33.00763
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 22.96558, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 0.01727, qf2_loss: 0.01718, policy_loss: -20.23035, policy_entropy: -6.44440, alpha: 0.00198, time: 32.90434
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 35.23058, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 0.01834, qf2_loss: 0.01819, policy_loss: -20.25444, policy_entropy: -6.23027, alpha: 0.00199, time: 32.90641
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 46.94833, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 0.01948, qf2_loss: 0.01940, policy_loss: -20.18125, policy_entropy: -6.54714, alpha: 0.00201, time: 32.90670
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 75.21189, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 0.02005, qf2_loss: 0.01997, policy_loss: -20.08530, policy_entropy: -6.85118, alpha: 0.00205, time: 32.97063
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 38.32836, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 0.02738, qf2_loss: 0.02739, policy_loss: -20.07658, policy_entropy: -7.32084, alpha: 0.00209, time: 32.94072
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 28.37574, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 0.02430, qf2_loss: 0.02409, policy_loss: -19.99532, policy_entropy: -7.20446, alpha: 0.00216, time: 32.76227
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 30.44994, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 0.02291, qf2_loss: 0.02280, policy_loss: -19.97179, policy_entropy: -6.96005, alpha: 0.00221, time: 32.83130
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 30.23757, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 0.02237, qf2_loss: 0.02241, policy_loss: -19.90605, policy_entropy: -6.53720, alpha: 0.00225, time: 32.80921
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 71.46844, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 0.02308, qf2_loss: 0.02293, policy_loss: -19.87146, policy_entropy: -6.86722, alpha: 0.00230, time: 32.99080
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 63.00198, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 0.04316, qf2_loss: 0.04234, policy_loss: -19.82745, policy_entropy: -6.61849, alpha: 0.00234, time: 33.36825
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 65.70062, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 0.02632, qf2_loss: 0.02613, policy_loss: -19.73014, policy_entropy: -6.22812, alpha: 0.00236, time: 32.90600
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 55.17033, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 0.02484, qf2_loss: 0.02458, policy_loss: -19.69227, policy_entropy: -6.57040, alpha: 0.00238, time: 32.75787
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 53.09706, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 0.02584, qf2_loss: 0.02570, policy_loss: -19.65280, policy_entropy: -6.52733, alpha: 0.00243, time: 33.00948
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 71.44369, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 0.03388, qf2_loss: 0.03349, policy_loss: -19.62728, policy_entropy: -6.37661, alpha: 0.00245, time: 32.88063
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 91.34630, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 0.02693, qf2_loss: 0.02668, policy_loss: -19.55286, policy_entropy: -6.33713, alpha: 0.00247, time: 33.03692
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 93.54392, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 0.02990, qf2_loss: 0.02958, policy_loss: -19.51672, policy_entropy: -5.88688, alpha: 0.00249, time: 32.86465
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 66.63992, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 0.02812, qf2_loss: 0.02809, policy_loss: -19.51763, policy_entropy: -6.07435, alpha: 0.00248, time: 32.90214
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 68.54130, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 0.04269, qf2_loss: 0.04167, policy_loss: -19.41014, policy_entropy: -5.94052, alpha: 0.00248, time: 32.78924
[CW] eval: return: 71.28116, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 83.31481, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 0.03406, qf2_loss: 0.03389, policy_loss: -19.40805, policy_entropy: -6.52418, alpha: 0.00249, time: 32.94805
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 43.36953, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 0.03221, qf2_loss: 0.03198, policy_loss: -19.32314, policy_entropy: -6.34553, alpha: 0.00254, time: 33.01189
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 90.76433, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 0.03367, qf2_loss: 0.03311, policy_loss: -19.30132, policy_entropy: -6.62730, alpha: 0.00258, time: 32.77700
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 78.11940, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 0.02566, qf2_loss: 0.02559, policy_loss: -19.25277, policy_entropy: -6.04450, alpha: 0.00261, time: 32.84273
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 69.15770, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 0.03738, qf2_loss: 0.03690, policy_loss: -19.24250, policy_entropy: -6.59097, alpha: 0.00263, time: 32.80861
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 99.58346, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 0.03092, qf2_loss: 0.03041, policy_loss: -19.15558, policy_entropy: -5.68395, alpha: 0.00265, time: 32.68597
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 90.23128, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 0.03041, qf2_loss: 0.03006, policy_loss: -19.16279, policy_entropy: -6.57536, alpha: 0.00267, time: 32.71661
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 95.48739, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 0.03197, qf2_loss: 0.03190, policy_loss: -19.11734, policy_entropy: -6.91079, alpha: 0.00273, time: 32.75401
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 31.47612, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 0.02782, qf2_loss: 0.02756, policy_loss: -19.11392, policy_entropy: -6.13124, alpha: 0.00280, time: 32.96060
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 39.78537, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 0.02562, qf2_loss: 0.02550, policy_loss: -18.98002, policy_entropy: -5.97301, alpha: 0.00281, time: 32.86959
[CW] ---------------------------
[CW] ---- Iteration:   411 ----
[CW] collect: return: 64.90314, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 0.03958, qf2_loss: 0.03921, policy_loss: -18.96386, policy_entropy: -5.56531, alpha: 0.00277, time: 32.95806
[CW] ---------------------------
[CW] ---- Iteration:   412 ----
[CW] collect: return: 75.90561, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 0.02975, qf2_loss: 0.02949, policy_loss: -18.89574, policy_entropy: -6.16453, alpha: 0.00276, time: 32.68384
[CW] ---------------------------
[CW] ---- Iteration:   413 ----
[CW] collect: return: 99.68166, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 0.03179, qf2_loss: 0.03141, policy_loss: -18.92371, policy_entropy: -5.74475, alpha: 0.00277, time: 32.90919
[CW] ---------------------------
[CW] ---- Iteration:   414 ----
[CW] collect: return: 21.62570, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 0.03380, qf2_loss: 0.03356, policy_loss: -18.81567, policy_entropy: -5.54971, alpha: 0.00271, time: 32.86364
[CW] ---------------------------
[CW] ---- Iteration:   415 ----
[CW] collect: return: 96.94937, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 0.02586, qf2_loss: 0.02568, policy_loss: -18.76811, policy_entropy: -5.90633, alpha: 0.00270, time: 33.01068
[CW] ---------------------------
[CW] ---- Iteration:   416 ----
[CW] collect: return: 101.07262, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 0.02677, qf2_loss: 0.02661, policy_loss: -18.78491, policy_entropy: -6.12099, alpha: 0.00269, time: 32.75901
[CW] ---------------------------
[CW] ---- Iteration:   417 ----
[CW] collect: return: 95.54207, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 0.04195, qf2_loss: 0.04155, policy_loss: -18.74640, policy_entropy: -5.61334, alpha: 0.00267, time: 37.99416
[CW] ---------------------------
[CW] ---- Iteration:   418 ----
[CW] collect: return: 75.56596, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 0.02951, qf2_loss: 0.02918, policy_loss: -18.72713, policy_entropy: -6.09823, alpha: 0.00266, time: 33.06768
[CW] ---------------------------
[CW] ---- Iteration:   419 ----
[CW] collect: return: 59.54034, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 0.02833, qf2_loss: 0.02824, policy_loss: -18.68076, policy_entropy: -6.01127, alpha: 0.00267, time: 32.97590
[CW] ---------------------------
[CW] ---- Iteration:   420 ----
[CW] collect: return: 89.02147, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 0.03262, qf2_loss: 0.03243, policy_loss: -18.64940, policy_entropy: -5.99117, alpha: 0.00267, time: 32.79517
[CW] eval: return: 85.36588, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   421 ----
[CW] collect: return: 99.15932, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 0.03181, qf2_loss: 0.03158, policy_loss: -18.59334, policy_entropy: -5.82856, alpha: 0.00266, time: 32.51897
[CW] ---------------------------
[CW] ---- Iteration:   422 ----
[CW] collect: return: 73.29483, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 0.04136, qf2_loss: 0.04083, policy_loss: -18.55611, policy_entropy: -5.42301, alpha: 0.00263, time: 34.24224
[CW] ---------------------------
[CW] ---- Iteration:   423 ----
[CW] collect: return: 70.16030, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 0.05299, qf2_loss: 0.05232, policy_loss: -18.48632, policy_entropy: -4.97962, alpha: 0.00256, time: 32.85146
[CW] ---------------------------
[CW] ---- Iteration:   424 ----
[CW] collect: return: 65.18719, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 0.04620, qf2_loss: 0.04552, policy_loss: -18.52346, policy_entropy: -5.12152, alpha: 0.00246, time: 32.57347
[CW] ---------------------------
[CW] ---- Iteration:   425 ----
[CW] collect: return: 47.12078, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 0.02935, qf2_loss: 0.02924, policy_loss: -18.48056, policy_entropy: -6.47953, alpha: 0.00244, time: 32.31837
[CW] ---------------------------
[CW] ---- Iteration:   426 ----
[CW] collect: return: 94.84185, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 0.03278, qf2_loss: 0.03262, policy_loss: -18.40230, policy_entropy: -5.93391, alpha: 0.00246, time: 32.36243
[CW] ---------------------------
[CW] ---- Iteration:   427 ----
[CW] collect: return: 87.29474, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 0.02907, qf2_loss: 0.02887, policy_loss: -18.39307, policy_entropy: -5.92484, alpha: 0.00246, time: 32.37879
[CW] ---------------------------
[CW] ---- Iteration:   428 ----
[CW] collect: return: 74.82955, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 0.03000, qf2_loss: 0.02977, policy_loss: -18.32898, policy_entropy: -5.93395, alpha: 0.00245, time: 32.37167
[CW] ---------------------------
[CW] ---- Iteration:   429 ----
[CW] collect: return: 84.75592, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 0.03226, qf2_loss: 0.03215, policy_loss: -18.33255, policy_entropy: -6.01136, alpha: 0.00245, time: 32.39178
[CW] ---------------------------
[CW] ---- Iteration:   430 ----
[CW] collect: return: 87.15108, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 0.03256, qf2_loss: 0.03228, policy_loss: -18.25602, policy_entropy: -5.97942, alpha: 0.00245, time: 32.28405
[CW] ---------------------------
[CW] ---- Iteration:   431 ----
[CW] collect: return: 92.38180, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 0.03443, qf2_loss: 0.03405, policy_loss: -18.22221, policy_entropy: -6.12554, alpha: 0.00245, time: 32.50249
[CW] ---------------------------
[CW] ---- Iteration:   432 ----
[CW] collect: return: 84.95537, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 0.03240, qf2_loss: 0.03242, policy_loss: -18.28971, policy_entropy: -6.10155, alpha: 0.00246, time: 32.58227
[CW] ---------------------------
[CW] ---- Iteration:   433 ----
[CW] collect: return: 83.18763, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 0.03819, qf2_loss: 0.03789, policy_loss: -18.13386, policy_entropy: -6.56746, alpha: 0.00248, time: 33.08666
[CW] ---------------------------
[CW] ---- Iteration:   434 ----
[CW] collect: return: 38.20465, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 0.03476, qf2_loss: 0.03463, policy_loss: -18.09255, policy_entropy: -5.85261, alpha: 0.00252, time: 32.84580
[CW] ---------------------------
[CW] ---- Iteration:   435 ----
[CW] collect: return: 22.64844, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 0.05312, qf2_loss: 0.05246, policy_loss: -18.09282, policy_entropy: -5.53437, alpha: 0.00250, time: 33.05657
[CW] ---------------------------
[CW] ---- Iteration:   436 ----
[CW] collect: return: 88.81523, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 0.05132, qf2_loss: 0.05044, policy_loss: -18.07491, policy_entropy: -5.82622, alpha: 0.00243, time: 32.79157
[CW] ---------------------------
[CW] ---- Iteration:   437 ----
[CW] collect: return: 39.57038, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 0.03495, qf2_loss: 0.03452, policy_loss: -18.06968, policy_entropy: -6.55853, alpha: 0.00247, time: 32.81366
[CW] ---------------------------
[CW] ---- Iteration:   438 ----
[CW] collect: return: 74.70691, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 0.02976, qf2_loss: 0.02973, policy_loss: -17.99021, policy_entropy: -5.67210, alpha: 0.00248, time: 32.42735
[CW] ---------------------------
[CW] ---- Iteration:   439 ----
[CW] collect: return: 78.32851, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 0.03055, qf2_loss: 0.03051, policy_loss: -17.95200, policy_entropy: -5.75168, alpha: 0.00245, time: 32.39855
[CW] ---------------------------
[CW] ---- Iteration:   440 ----
[CW] collect: return: 88.06155, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 0.03009, qf2_loss: 0.02999, policy_loss: -17.99299, policy_entropy: -6.43537, alpha: 0.00246, time: 32.24597
[CW] eval: return: 87.48972, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   441 ----
[CW] collect: return: 95.21994, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 0.03363, qf2_loss: 0.03349, policy_loss: -17.95205, policy_entropy: -6.40091, alpha: 0.00249, time: 32.23573
[CW] ---------------------------
[CW] ---- Iteration:   442 ----
[CW] collect: return: 77.10741, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 0.03480, qf2_loss: 0.03458, policy_loss: -17.90637, policy_entropy: -6.70206, alpha: 0.00254, time: 32.24897
[CW] ---------------------------
[CW] ---- Iteration:   443 ----
[CW] collect: return: 83.80117, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 0.04484, qf2_loss: 0.04450, policy_loss: -17.85965, policy_entropy: -6.15774, alpha: 0.00258, time: 32.26940
[CW] ---------------------------
[CW] ---- Iteration:   444 ----
[CW] collect: return: 90.81500, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 0.04066, qf2_loss: 0.04040, policy_loss: -17.83715, policy_entropy: -6.55608, alpha: 0.00262, time: 32.31049
[CW] ---------------------------
[CW] ---- Iteration:   445 ----
[CW] collect: return: 79.97025, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 0.05438, qf2_loss: 0.05383, policy_loss: -17.85223, policy_entropy: -6.31313, alpha: 0.00265, time: 32.45126
[CW] ---------------------------
[CW] ---- Iteration:   446 ----
[CW] collect: return: 92.10758, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 0.05477, qf2_loss: 0.05400, policy_loss: -17.82062, policy_entropy: -5.71832, alpha: 0.00265, time: 32.30335
[CW] ---------------------------
[CW] ---- Iteration:   447 ----
[CW] collect: return: 81.78472, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 0.03357, qf2_loss: 0.03357, policy_loss: -17.66622, policy_entropy: -6.47855, alpha: 0.00267, time: 32.39519
[CW] ---------------------------
[CW] ---- Iteration:   448 ----
[CW] collect: return: 94.94505, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 0.03153, qf2_loss: 0.03147, policy_loss: -17.69107, policy_entropy: -6.19456, alpha: 0.00270, time: 32.36982
[CW] ---------------------------
[CW] ---- Iteration:   449 ----
[CW] collect: return: 89.72725, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 0.02976, qf2_loss: 0.02978, policy_loss: -17.69933, policy_entropy: -6.09589, alpha: 0.00271, time: 32.63837
[CW] ---------------------------
[CW] ---- Iteration:   450 ----
[CW] collect: return: 85.76283, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 0.04085, qf2_loss: 0.04041, policy_loss: -17.63633, policy_entropy: -5.52129, alpha: 0.00271, time: 32.32517
[CW] ---------------------------
[CW] ---- Iteration:   451 ----
[CW] collect: return: 89.64041, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 0.03525, qf2_loss: 0.03508, policy_loss: -17.60196, policy_entropy: -5.19370, alpha: 0.00263, time: 32.44004
[CW] ---------------------------
[CW] ---- Iteration:   452 ----
[CW] collect: return: 95.04252, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 0.04149, qf2_loss: 0.04113, policy_loss: -17.52897, policy_entropy: -5.88884, alpha: 0.00259, time: 32.23376
[CW] ---------------------------
[CW] ---- Iteration:   453 ----
[CW] collect: return: 88.07097, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 0.03357, qf2_loss: 0.03339, policy_loss: -17.54466, policy_entropy: -5.90646, alpha: 0.00259, time: 32.25095
[CW] ---------------------------
[CW] ---- Iteration:   454 ----
[CW] collect: return: 87.18267, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 0.03229, qf2_loss: 0.03217, policy_loss: -17.57050, policy_entropy: -5.99164, alpha: 0.00258, time: 32.36654
[CW] ---------------------------
[CW] ---- Iteration:   455 ----
[CW] collect: return: 90.84311, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 0.04341, qf2_loss: 0.04288, policy_loss: -17.50238, policy_entropy: -5.86394, alpha: 0.00259, time: 32.53842
[CW] ---------------------------
[CW] ---- Iteration:   456 ----
[CW] collect: return: 102.19417, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 0.05277, qf2_loss: 0.05239, policy_loss: -17.48418, policy_entropy: -5.52384, alpha: 0.00254, time: 33.22526
[CW] ---------------------------
[CW] ---- Iteration:   457 ----
[CW] collect: return: 95.50336, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 0.03381, qf2_loss: 0.03366, policy_loss: -17.46279, policy_entropy: -6.17449, alpha: 0.00253, time: 32.75473
[CW] ---------------------------
[CW] ---- Iteration:   458 ----
[CW] collect: return: 99.12956, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 0.03824, qf2_loss: 0.03800, policy_loss: -17.48235, policy_entropy: -6.08769, alpha: 0.00255, time: 32.84162
[CW] ---------------------------
[CW] ---- Iteration:   459 ----
[CW] collect: return: 86.10017, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 0.03144, qf2_loss: 0.03128, policy_loss: -17.45071, policy_entropy: -6.42633, alpha: 0.00257, time: 32.76332
[CW] ---------------------------
[CW] ---- Iteration:   460 ----
[CW] collect: return: 82.03854, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 0.03585, qf2_loss: 0.03572, policy_loss: -17.41848, policy_entropy: -6.14365, alpha: 0.00259, time: 32.95786
[CW] eval: return: 93.28151, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   461 ----
[CW] collect: return: 95.38580, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 0.03568, qf2_loss: 0.03564, policy_loss: -17.40670, policy_entropy: -6.27100, alpha: 0.00261, time: 32.66577
[CW] ---------------------------
[CW] ---- Iteration:   462 ----
[CW] collect: return: 106.26542, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 0.03762, qf2_loss: 0.03752, policy_loss: -17.39197, policy_entropy: -6.34461, alpha: 0.00265, time: 32.98406
[CW] ---------------------------
[CW] ---- Iteration:   463 ----
[CW] collect: return: 94.16983, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 0.03489, qf2_loss: 0.03480, policy_loss: -17.31281, policy_entropy: -6.21433, alpha: 0.00267, time: 33.22518
[CW] ---------------------------
[CW] ---- Iteration:   464 ----
[CW] collect: return: 105.86084, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 0.03333, qf2_loss: 0.03327, policy_loss: -17.28156, policy_entropy: -6.39517, alpha: 0.00271, time: 33.38935
[CW] ---------------------------
[CW] ---- Iteration:   465 ----
[CW] collect: return: 100.22920, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 0.04283, qf2_loss: 0.04236, policy_loss: -17.30924, policy_entropy: -6.47709, alpha: 0.00276, time: 33.63077
[CW] ---------------------------
[CW] ---- Iteration:   466 ----
[CW] collect: return: 93.95173, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 0.04101, qf2_loss: 0.04064, policy_loss: -17.32320, policy_entropy: -6.36036, alpha: 0.00282, time: 33.55755
[CW] ---------------------------
[CW] ---- Iteration:   467 ----
[CW] collect: return: 87.79952, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 0.04070, qf2_loss: 0.04041, policy_loss: -17.22018, policy_entropy: -5.65598, alpha: 0.00281, time: 33.67728
[CW] ---------------------------
[CW] ---- Iteration:   468 ----
[CW] collect: return: 102.92354, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 0.04732, qf2_loss: 0.04692, policy_loss: -17.23340, policy_entropy: -6.02788, alpha: 0.00280, time: 33.57863
[CW] ---------------------------
[CW] ---- Iteration:   469 ----
[CW] collect: return: 98.88854, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 0.04488, qf2_loss: 0.04450, policy_loss: -17.22283, policy_entropy: -5.97110, alpha: 0.00279, time: 33.60812
[CW] ---------------------------
[CW] ---- Iteration:   470 ----
[CW] collect: return: 93.25694, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 0.03912, qf2_loss: 0.03900, policy_loss: -17.22752, policy_entropy: -6.34871, alpha: 0.00281, time: 33.44262
[CW] ---------------------------
[CW] ---- Iteration:   471 ----
[CW] collect: return: 86.42199, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 0.03318, qf2_loss: 0.03306, policy_loss: -17.24034, policy_entropy: -6.24281, alpha: 0.00285, time: 34.07788
[CW] ---------------------------
[CW] ---- Iteration:   472 ----
[CW] collect: return: 109.83294, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 0.04093, qf2_loss: 0.04062, policy_loss: -17.12476, policy_entropy: -5.87849, alpha: 0.00285, time: 33.50279
[CW] ---------------------------
[CW] ---- Iteration:   473 ----
[CW] collect: return: 88.14447, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 0.03788, qf2_loss: 0.03763, policy_loss: -17.19573, policy_entropy: -6.20004, alpha: 0.00286, time: 33.58629
[CW] ---------------------------
[CW] ---- Iteration:   474 ----
[CW] collect: return: 106.22563, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 0.04038, qf2_loss: 0.03998, policy_loss: -17.10493, policy_entropy: -5.94133, alpha: 0.00287, time: 33.36327
[CW] ---------------------------
[CW] ---- Iteration:   475 ----
[CW] collect: return: 95.56050, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 0.04101, qf2_loss: 0.04084, policy_loss: -17.14943, policy_entropy: -6.24921, alpha: 0.00288, time: 33.56344
[CW] ---------------------------
[CW] ---- Iteration:   476 ----
[CW] collect: return: 100.29209, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 0.03716, qf2_loss: 0.03701, policy_loss: -17.07161, policy_entropy: -6.05723, alpha: 0.00291, time: 33.48045
[CW] ---------------------------
[CW] ---- Iteration:   477 ----
[CW] collect: return: 81.63606, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 0.04522, qf2_loss: 0.04474, policy_loss: -17.05868, policy_entropy: -6.07272, alpha: 0.00292, time: 33.62165
[CW] ---------------------------
[CW] ---- Iteration:   478 ----
[CW] collect: return: 103.35902, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 0.04029, qf2_loss: 0.04008, policy_loss: -17.10523, policy_entropy: -6.26519, alpha: 0.00294, time: 33.46612
[CW] ---------------------------
[CW] ---- Iteration:   479 ----
[CW] collect: return: 102.51991, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 0.03932, qf2_loss: 0.03915, policy_loss: -17.04915, policy_entropy: -5.95326, alpha: 0.00295, time: 33.51610
[CW] ---------------------------
[CW] ---- Iteration:   480 ----
[CW] collect: return: 102.99117, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 0.06010, qf2_loss: 0.05928, policy_loss: -17.08311, policy_entropy: -6.08075, alpha: 0.00295, time: 33.43894
[CW] eval: return: 89.90209, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   481 ----
[CW] collect: return: 101.11683, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 0.04034, qf2_loss: 0.04025, policy_loss: -16.99598, policy_entropy: -6.66446, alpha: 0.00300, time: 33.52229
[CW] ---------------------------
[CW] ---- Iteration:   482 ----
[CW] collect: return: 95.64545, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 0.03744, qf2_loss: 0.03721, policy_loss: -16.99285, policy_entropy: -6.29701, alpha: 0.00307, time: 33.65901
[CW] ---------------------------
[CW] ---- Iteration:   483 ----
[CW] collect: return: 103.55403, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 0.03560, qf2_loss: 0.03540, policy_loss: -17.01757, policy_entropy: -6.22469, alpha: 0.00311, time: 33.47977
[CW] ---------------------------
[CW] ---- Iteration:   484 ----
[CW] collect: return: 85.77738, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 0.03438, qf2_loss: 0.03425, policy_loss: -16.96063, policy_entropy: -6.28007, alpha: 0.00315, time: 33.53383
[CW] ---------------------------
[CW] ---- Iteration:   485 ----
[CW] collect: return: 97.36662, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 0.03811, qf2_loss: 0.03788, policy_loss: -17.01152, policy_entropy: -6.50824, alpha: 0.00321, time: 33.48273
[CW] ---------------------------
[CW] ---- Iteration:   486 ----
[CW] collect: return: 99.37688, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 0.04457, qf2_loss: 0.04427, policy_loss: -16.92965, policy_entropy: -6.13404, alpha: 0.00326, time: 33.55330
[CW] ---------------------------
[CW] ---- Iteration:   487 ----
[CW] collect: return: 101.97083, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 0.05306, qf2_loss: 0.05248, policy_loss: -17.01298, policy_entropy: -6.17080, alpha: 0.00327, time: 33.42533
[CW] ---------------------------
[CW] ---- Iteration:   488 ----
[CW] collect: return: 91.71953, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 0.03933, qf2_loss: 0.03913, policy_loss: -16.84831, policy_entropy: -6.19183, alpha: 0.00331, time: 33.66732
[CW] ---------------------------
[CW] ---- Iteration:   489 ----
[CW] collect: return: 93.28948, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 0.03730, qf2_loss: 0.03717, policy_loss: -16.90351, policy_entropy: -6.05000, alpha: 0.00334, time: 33.54903
[CW] ---------------------------
[CW] ---- Iteration:   490 ----
[CW] collect: return: 93.47143, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 0.04228, qf2_loss: 0.04235, policy_loss: -16.91045, policy_entropy: -6.31144, alpha: 0.00337, time: 33.66383
[CW] ---------------------------
[CW] ---- Iteration:   491 ----
[CW] collect: return: 101.79864, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 0.03788, qf2_loss: 0.03759, policy_loss: -16.94094, policy_entropy: -6.27768, alpha: 0.00341, time: 33.50099
[CW] ---------------------------
[CW] ---- Iteration:   492 ----
[CW] collect: return: 107.47455, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 0.05306, qf2_loss: 0.05225, policy_loss: -16.92278, policy_entropy: -6.17123, alpha: 0.00345, time: 33.56265
[CW] ---------------------------
[CW] ---- Iteration:   493 ----
[CW] collect: return: 98.74720, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 0.05875, qf2_loss: 0.05837, policy_loss: -16.86343, policy_entropy: -5.92904, alpha: 0.00346, time: 33.55909
[CW] ---------------------------
[CW] ---- Iteration:   494 ----
[CW] collect: return: 99.68085, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 0.03888, qf2_loss: 0.03871, policy_loss: -16.90805, policy_entropy: -6.26569, alpha: 0.00347, time: 33.50975
[CW] ---------------------------
[CW] ---- Iteration:   495 ----
[CW] collect: return: 108.21656, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 0.03675, qf2_loss: 0.03669, policy_loss: -16.83147, policy_entropy: -6.15018, alpha: 0.00352, time: 33.58563
[CW] ---------------------------
[CW] ---- Iteration:   496 ----
[CW] collect: return: 100.48831, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 0.03590, qf2_loss: 0.03571, policy_loss: -16.84089, policy_entropy: -5.91196, alpha: 0.00352, time: 33.57348
[CW] ---------------------------
[CW] ---- Iteration:   497 ----
[CW] collect: return: 89.64518, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 0.04512, qf2_loss: 0.04484, policy_loss: -16.84863, policy_entropy: -5.82919, alpha: 0.00350, time: 35.06400
[CW] ---------------------------
[CW] ---- Iteration:   498 ----
[CW] collect: return: 95.21719, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 0.05548, qf2_loss: 0.05479, policy_loss: -16.90102, policy_entropy: -6.26925, alpha: 0.00350, time: 33.53242
[CW] ---------------------------
[CW] ---- Iteration:   499 ----
[CW] collect: return: 106.52581, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 0.04851, qf2_loss: 0.04842, policy_loss: -16.81744, policy_entropy: -6.30197, alpha: 0.00356, time: 33.58074
[CW] ---------------------------
[CW] ---- Iteration:   500 ----
[CW] collect: return: 99.37809, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 0.04910, qf2_loss: 0.04868, policy_loss: -16.80729, policy_entropy: -6.04868, alpha: 0.00359, time: 34.80586
[CW] eval: return: 94.84884, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   501 ----
[CW] collect: return: 94.52160, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 0.04848, qf2_loss: 0.04812, policy_loss: -16.76546, policy_entropy: -6.15505, alpha: 0.00360, time: 33.21097
[CW] ---------------------------
[CW] ---- Iteration:   502 ----
[CW] collect: return: 104.73516, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 0.04312, qf2_loss: 0.04285, policy_loss: -16.74783, policy_entropy: -6.02790, alpha: 0.00363, time: 33.12536
[CW] ---------------------------
[CW] ---- Iteration:   503 ----
[CW] collect: return: 99.19168, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 0.03711, qf2_loss: 0.03705, policy_loss: -16.76322, policy_entropy: -6.04822, alpha: 0.00363, time: 33.05469
[CW] ---------------------------
[CW] ---- Iteration:   504 ----
[CW] collect: return: 96.07566, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 0.03405, qf2_loss: 0.03397, policy_loss: -16.75665, policy_entropy: -5.92594, alpha: 0.00362, time: 33.15990
[CW] ---------------------------
[CW] ---- Iteration:   505 ----
[CW] collect: return: 107.30056, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 0.03964, qf2_loss: 0.03937, policy_loss: -16.80993, policy_entropy: -6.18969, alpha: 0.00363, time: 33.12881
[CW] ---------------------------
[CW] ---- Iteration:   506 ----
[CW] collect: return: 100.78667, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 0.03678, qf2_loss: 0.03681, policy_loss: -16.70384, policy_entropy: -6.18067, alpha: 0.00367, time: 33.17867
[CW] ---------------------------
[CW] ---- Iteration:   507 ----
[CW] collect: return: 107.47560, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 0.03954, qf2_loss: 0.03938, policy_loss: -16.69729, policy_entropy: -6.03248, alpha: 0.00370, time: 33.10016
[CW] ---------------------------
[CW] ---- Iteration:   508 ----
[CW] collect: return: 95.87584, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 0.08902, qf2_loss: 0.08765, policy_loss: -16.66742, policy_entropy: -5.71764, alpha: 0.00368, time: 33.13427
[CW] ---------------------------
[CW] ---- Iteration:   509 ----
[CW] collect: return: 98.62870, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 0.05481, qf2_loss: 0.05447, policy_loss: -16.71263, policy_entropy: -6.07785, alpha: 0.00365, time: 32.99047
[CW] ---------------------------
[CW] ---- Iteration:   510 ----
[CW] collect: return: 104.73877, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 0.04053, qf2_loss: 0.04046, policy_loss: -16.77475, policy_entropy: -6.04818, alpha: 0.00366, time: 33.15185
[CW] ---------------------------
[CW] ---- Iteration:   511 ----
[CW] collect: return: 105.42459, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 0.04094, qf2_loss: 0.04084, policy_loss: -16.75166, policy_entropy: -6.20153, alpha: 0.00368, time: 33.14055
[CW] ---------------------------
[CW] ---- Iteration:   512 ----
[CW] collect: return: 93.72436, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 0.03464, qf2_loss: 0.03456, policy_loss: -16.64767, policy_entropy: -6.25408, alpha: 0.00372, time: 33.16267
[CW] ---------------------------
[CW] ---- Iteration:   513 ----
[CW] collect: return: 109.28120, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 0.03637, qf2_loss: 0.03624, policy_loss: -16.71446, policy_entropy: -6.33771, alpha: 0.00379, time: 33.19418
[CW] ---------------------------
[CW] ---- Iteration:   514 ----
[CW] collect: return: 111.00079, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 0.04496, qf2_loss: 0.04474, policy_loss: -16.64364, policy_entropy: -6.23274, alpha: 0.00384, time: 33.22022
[CW] ---------------------------
[CW] ---- Iteration:   515 ----
[CW] collect: return: 105.58108, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 0.04046, qf2_loss: 0.04023, policy_loss: -16.65138, policy_entropy: -6.31716, alpha: 0.00390, time: 32.98428
[CW] ---------------------------
[CW] ---- Iteration:   516 ----
[CW] collect: return: 115.62146, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 0.05030, qf2_loss: 0.04987, policy_loss: -16.56493, policy_entropy: -6.21633, alpha: 0.00396, time: 33.14453
[CW] ---------------------------
[CW] ---- Iteration:   517 ----
[CW] collect: return: 105.48317, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 0.04449, qf2_loss: 0.04425, policy_loss: -16.62286, policy_entropy: -6.16187, alpha: 0.00401, time: 33.20597
[CW] ---------------------------
[CW] ---- Iteration:   518 ----
[CW] collect: return: 109.62815, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 0.04181, qf2_loss: 0.04161, policy_loss: -16.64084, policy_entropy: -5.82061, alpha: 0.00401, time: 33.36105
[CW] ---------------------------
[CW] ---- Iteration:   519 ----
[CW] collect: return: 92.85537, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 0.05032, qf2_loss: 0.05019, policy_loss: -16.66155, policy_entropy: -5.85002, alpha: 0.00397, time: 33.13306
[CW] ---------------------------
[CW] ---- Iteration:   520 ----
[CW] collect: return: 118.16787, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 0.05730, qf2_loss: 0.05694, policy_loss: -16.61500, policy_entropy: -6.34906, alpha: 0.00398, time: 33.35457
[CW] eval: return: 104.68143, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   521 ----
[CW] collect: return: 108.05112, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 0.05289, qf2_loss: 0.05280, policy_loss: -16.64639, policy_entropy: -6.27840, alpha: 0.00406, time: 36.36604
[CW] ---------------------------
[CW] ---- Iteration:   522 ----
[CW] collect: return: 97.57345, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 0.04031, qf2_loss: 0.04008, policy_loss: -16.58471, policy_entropy: -6.11206, alpha: 0.00410, time: 33.44166
[CW] ---------------------------
[CW] ---- Iteration:   523 ----
[CW] collect: return: 98.53324, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 0.03973, qf2_loss: 0.03962, policy_loss: -16.61358, policy_entropy: -6.27979, alpha: 0.00415, time: 33.62068
[CW] ---------------------------
[CW] ---- Iteration:   524 ----
[CW] collect: return: 100.01761, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 0.04209, qf2_loss: 0.04182, policy_loss: -16.62381, policy_entropy: -6.30751, alpha: 0.00422, time: 33.41694
[CW] ---------------------------
[CW] ---- Iteration:   525 ----
[CW] collect: return: 103.91294, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 0.03760, qf2_loss: 0.03741, policy_loss: -16.57807, policy_entropy: -6.25295, alpha: 0.00427, time: 33.40356
[CW] ---------------------------
[CW] ---- Iteration:   526 ----
[CW] collect: return: 111.28709, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 0.04991, qf2_loss: 0.04931, policy_loss: -16.53179, policy_entropy: -6.12532, alpha: 0.00432, time: 33.19930
[CW] ---------------------------
[CW] ---- Iteration:   527 ----
[CW] collect: return: 100.50677, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 0.04016, qf2_loss: 0.04004, policy_loss: -16.55028, policy_entropy: -6.10452, alpha: 0.00436, time: 33.46044
[CW] ---------------------------
[CW] ---- Iteration:   528 ----
[CW] collect: return: 109.28024, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 0.04249, qf2_loss: 0.04217, policy_loss: -16.52388, policy_entropy: -6.15021, alpha: 0.00439, time: 33.82637
[CW] ---------------------------
[CW] ---- Iteration:   529 ----
[CW] collect: return: 99.93392, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 0.05719, qf2_loss: 0.05709, policy_loss: -16.57209, policy_entropy: -6.20000, alpha: 0.00444, time: 33.46181
[CW] ---------------------------
[CW] ---- Iteration:   530 ----
[CW] collect: return: 75.92163, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 0.04680, qf2_loss: 0.04660, policy_loss: -16.58365, policy_entropy: -5.99572, alpha: 0.00447, time: 33.46189
[CW] ---------------------------
[CW] ---- Iteration:   531 ----
[CW] collect: return: 109.12350, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 0.04470, qf2_loss: 0.04452, policy_loss: -16.58837, policy_entropy: -6.20220, alpha: 0.00448, time: 33.59231
[CW] ---------------------------
[CW] ---- Iteration:   532 ----
[CW] collect: return: 105.05640, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 0.04986, qf2_loss: 0.04954, policy_loss: -16.56868, policy_entropy: -6.32321, alpha: 0.00454, time: 33.50289
[CW] ---------------------------
[CW] ---- Iteration:   533 ----
[CW] collect: return: 100.90561, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 0.04023, qf2_loss: 0.04010, policy_loss: -16.50372, policy_entropy: -6.06895, alpha: 0.00461, time: 33.51225
[CW] ---------------------------
[CW] ---- Iteration:   534 ----
[CW] collect: return: 107.27374, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 0.04480, qf2_loss: 0.04457, policy_loss: -16.54723, policy_entropy: -6.15636, alpha: 0.00462, time: 33.46065
[CW] ---------------------------
[CW] ---- Iteration:   535 ----
[CW] collect: return: 112.28191, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 0.05680, qf2_loss: 0.05631, policy_loss: -16.53923, policy_entropy: -5.82451, alpha: 0.00465, time: 33.48497
[CW] ---------------------------
[CW] ---- Iteration:   536 ----
[CW] collect: return: 113.58805, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 0.04762, qf2_loss: 0.04747, policy_loss: -16.56900, policy_entropy: -6.50208, alpha: 0.00464, time: 33.44665
[CW] ---------------------------
[CW] ---- Iteration:   537 ----
[CW] collect: return: 108.52551, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 0.04557, qf2_loss: 0.04536, policy_loss: -16.54677, policy_entropy: -6.06096, alpha: 0.00477, time: 33.42339
[CW] ---------------------------
[CW] ---- Iteration:   538 ----
[CW] collect: return: 95.32175, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 0.04774, qf2_loss: 0.04751, policy_loss: -16.47327, policy_entropy: -6.04122, alpha: 0.00477, time: 33.55106
[CW] ---------------------------
[CW] ---- Iteration:   539 ----
[CW] collect: return: 49.70513, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 0.04458, qf2_loss: 0.04434, policy_loss: -16.46235, policy_entropy: -6.17742, alpha: 0.00479, time: 33.50312
[CW] ---------------------------
[CW] ---- Iteration:   540 ----
[CW] collect: return: 97.09250, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 0.04777, qf2_loss: 0.04753, policy_loss: -16.54328, policy_entropy: -5.76529, alpha: 0.00480, time: 33.51844
[CW] eval: return: 93.20454, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   541 ----
[CW] collect: return: 89.18030, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 0.04656, qf2_loss: 0.04638, policy_loss: -16.50484, policy_entropy: -6.07125, alpha: 0.00476, time: 33.37136
[CW] ---------------------------
[CW] ---- Iteration:   542 ----
[CW] collect: return: 111.02815, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 0.04309, qf2_loss: 0.04297, policy_loss: -16.51009, policy_entropy: -5.99232, alpha: 0.00479, time: 33.14316
[CW] ---------------------------
[CW] ---- Iteration:   543 ----
[CW] collect: return: 103.82259, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 0.05194, qf2_loss: 0.05163, policy_loss: -16.48107, policy_entropy: -6.04467, alpha: 0.00476, time: 33.12115
[CW] ---------------------------
[CW] ---- Iteration:   544 ----
[CW] collect: return: 102.61501, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 0.05560, qf2_loss: 0.05515, policy_loss: -16.48386, policy_entropy: -5.79083, alpha: 0.00475, time: 32.98221
[CW] ---------------------------
[CW] ---- Iteration:   545 ----
[CW] collect: return: 102.87977, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 0.04584, qf2_loss: 0.04569, policy_loss: -16.50215, policy_entropy: -5.70312, alpha: 0.00470, time: 33.17965
[CW] ---------------------------
[CW] ---- Iteration:   546 ----
[CW] collect: return: 84.79960, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 0.04710, qf2_loss: 0.04692, policy_loss: -16.42153, policy_entropy: -5.97218, alpha: 0.00465, time: 33.09388
[CW] ---------------------------
[CW] ---- Iteration:   547 ----
[CW] collect: return: 93.51383, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 0.04874, qf2_loss: 0.04858, policy_loss: -16.43397, policy_entropy: -6.09398, alpha: 0.00467, time: 33.21009
[CW] ---------------------------
[CW] ---- Iteration:   548 ----
[CW] collect: return: 106.25304, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 0.04566, qf2_loss: 0.04545, policy_loss: -16.51674, policy_entropy: -6.12583, alpha: 0.00468, time: 33.01027
[CW] ---------------------------
[CW] ---- Iteration:   549 ----
[CW] collect: return: 69.28827, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 0.04797, qf2_loss: 0.04775, policy_loss: -16.44514, policy_entropy: -6.06375, alpha: 0.00470, time: 33.25091
[CW] ---------------------------
[CW] ---- Iteration:   550 ----
[CW] collect: return: 112.31327, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 0.04760, qf2_loss: 0.04760, policy_loss: -16.57332, policy_entropy: -6.01450, alpha: 0.00472, time: 33.04537
[CW] ---------------------------
[CW] ---- Iteration:   551 ----
[CW] collect: return: 110.97446, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 0.04684, qf2_loss: 0.04666, policy_loss: -16.49688, policy_entropy: -6.09159, alpha: 0.00474, time: 33.21942
[CW] ---------------------------
[CW] ---- Iteration:   552 ----
[CW] collect: return: 114.26409, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 0.04716, qf2_loss: 0.04704, policy_loss: -16.40319, policy_entropy: -5.99083, alpha: 0.00475, time: 32.95576
[CW] ---------------------------
[CW] ---- Iteration:   553 ----
[CW] collect: return: 110.11314, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 0.05503, qf2_loss: 0.05444, policy_loss: -16.46609, policy_entropy: -6.23091, alpha: 0.00476, time: 33.24123
[CW] ---------------------------
[CW] ---- Iteration:   554 ----
[CW] collect: return: 114.07089, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 0.05858, qf2_loss: 0.05833, policy_loss: -16.51016, policy_entropy: -6.19789, alpha: 0.00483, time: 33.15299
[CW] ---------------------------
[CW] ---- Iteration:   555 ----
[CW] collect: return: 110.74228, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 0.06704, qf2_loss: 0.06687, policy_loss: -16.40643, policy_entropy: -6.18807, alpha: 0.00489, time: 33.22759
[CW] ---------------------------
[CW] ---- Iteration:   556 ----
[CW] collect: return: 111.05143, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 0.04431, qf2_loss: 0.04423, policy_loss: -16.47259, policy_entropy: -5.73185, alpha: 0.00488, time: 33.06060
[CW] ---------------------------
[CW] ---- Iteration:   557 ----
[CW] collect: return: 100.37979, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 0.04664, qf2_loss: 0.04655, policy_loss: -16.47011, policy_entropy: -5.89812, alpha: 0.00482, time: 33.23139
[CW] ---------------------------
[CW] ---- Iteration:   558 ----
[CW] collect: return: 103.83591, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 0.05006, qf2_loss: 0.04971, policy_loss: -16.43541, policy_entropy: -6.11746, alpha: 0.00483, time: 33.18094
[CW] ---------------------------
[CW] ---- Iteration:   559 ----
[CW] collect: return: 103.83778, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 0.05198, qf2_loss: 0.05159, policy_loss: -16.48637, policy_entropy: -6.02666, alpha: 0.00485, time: 32.99075
[CW] ---------------------------
[CW] ---- Iteration:   560 ----
[CW] collect: return: 109.32673, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 0.04647, qf2_loss: 0.04627, policy_loss: -16.48100, policy_entropy: -5.90025, alpha: 0.00485, time: 33.44326
[CW] eval: return: 102.98140, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   561 ----
[CW] collect: return: 96.16124, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 0.04144, qf2_loss: 0.04135, policy_loss: -16.36922, policy_entropy: -5.93850, alpha: 0.00481, time: 33.32878
[CW] ---------------------------
[CW] ---- Iteration:   562 ----
[CW] collect: return: 110.35866, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 0.04833, qf2_loss: 0.04803, policy_loss: -16.36055, policy_entropy: -6.15232, alpha: 0.00481, time: 33.27694
[CW] ---------------------------
[CW] ---- Iteration:   563 ----
[CW] collect: return: 103.05923, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 0.05853, qf2_loss: 0.05797, policy_loss: -16.43913, policy_entropy: -6.08509, alpha: 0.00485, time: 33.05588
[CW] ---------------------------
[CW] ---- Iteration:   564 ----
[CW] collect: return: 94.17589, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 0.05141, qf2_loss: 0.05108, policy_loss: -16.35944, policy_entropy: -5.75629, alpha: 0.00485, time: 33.12621
[CW] ---------------------------
[CW] ---- Iteration:   565 ----
[CW] collect: return: 109.73789, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 0.04433, qf2_loss: 0.04426, policy_loss: -16.40709, policy_entropy: -5.91279, alpha: 0.00480, time: 33.67049
[CW] ---------------------------
[CW] ---- Iteration:   566 ----
[CW] collect: return: 108.79720, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 0.04639, qf2_loss: 0.04622, policy_loss: -16.46478, policy_entropy: -5.86377, alpha: 0.00477, time: 33.22620
[CW] ---------------------------
[CW] ---- Iteration:   567 ----
[CW] collect: return: 120.67848, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 0.05332, qf2_loss: 0.05303, policy_loss: -16.46064, policy_entropy: -5.87506, alpha: 0.00471, time: 33.15649
[CW] ---------------------------
[CW] ---- Iteration:   568 ----
[CW] collect: return: 119.18951, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 0.05597, qf2_loss: 0.05546, policy_loss: -16.37584, policy_entropy: -6.12257, alpha: 0.00473, time: 33.13211
[CW] ---------------------------
[CW] ---- Iteration:   569 ----
[CW] collect: return: 105.65027, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 0.04362, qf2_loss: 0.04349, policy_loss: -16.40988, policy_entropy: -5.96467, alpha: 0.00472, time: 32.89440
[CW] ---------------------------
[CW] ---- Iteration:   570 ----
[CW] collect: return: 95.12910, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 0.04382, qf2_loss: 0.04371, policy_loss: -16.40307, policy_entropy: -5.84563, alpha: 0.00469, time: 32.97907
[CW] ---------------------------
[CW] ---- Iteration:   571 ----
[CW] collect: return: 125.72944, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 0.04524, qf2_loss: 0.04499, policy_loss: -16.43270, policy_entropy: -6.16361, alpha: 0.00471, time: 32.88263
[CW] ---------------------------
[CW] ---- Iteration:   572 ----
[CW] collect: return: 107.79473, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 0.04818, qf2_loss: 0.04794, policy_loss: -16.47027, policy_entropy: -6.14882, alpha: 0.00475, time: 33.01158
[CW] ---------------------------
[CW] ---- Iteration:   573 ----
[CW] collect: return: 92.46690, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 0.05193, qf2_loss: 0.05187, policy_loss: -16.57462, policy_entropy: -6.20120, alpha: 0.00480, time: 32.93548
[CW] ---------------------------
[CW] ---- Iteration:   574 ----
[CW] collect: return: 113.46940, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 0.04338, qf2_loss: 0.04331, policy_loss: -16.38484, policy_entropy: -6.14092, alpha: 0.00484, time: 32.82130
[CW] ---------------------------
[CW] ---- Iteration:   575 ----
[CW] collect: return: 102.49474, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 0.04415, qf2_loss: 0.04400, policy_loss: -16.44606, policy_entropy: -6.21879, alpha: 0.00489, time: 32.96552
[CW] ---------------------------
[CW] ---- Iteration:   576 ----
[CW] collect: return: 126.48958, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 0.05750, qf2_loss: 0.05713, policy_loss: -16.40348, policy_entropy: -6.32733, alpha: 0.00497, time: 32.80738
[CW] ---------------------------
[CW] ---- Iteration:   577 ----
[CW] collect: return: 112.65655, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 0.06592, qf2_loss: 0.06540, policy_loss: -16.45487, policy_entropy: -6.06245, alpha: 0.00506, time: 32.95787
[CW] ---------------------------
[CW] ---- Iteration:   578 ----
[CW] collect: return: 109.71757, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 0.04809, qf2_loss: 0.04809, policy_loss: -16.47209, policy_entropy: -5.91522, alpha: 0.00504, time: 32.78737
[CW] ---------------------------
[CW] ---- Iteration:   579 ----
[CW] collect: return: 104.11776, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 0.04234, qf2_loss: 0.04227, policy_loss: -16.37563, policy_entropy: -6.14909, alpha: 0.00504, time: 32.90565
[CW] ---------------------------
[CW] ---- Iteration:   580 ----
[CW] collect: return: 120.80752, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 0.04382, qf2_loss: 0.04365, policy_loss: -16.46795, policy_entropy: -6.16999, alpha: 0.00509, time: 33.00867
[CW] eval: return: 112.45325, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   581 ----
[CW] collect: return: 111.75835, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 0.04101, qf2_loss: 0.04084, policy_loss: -16.43121, policy_entropy: -5.87736, alpha: 0.00513, time: 33.42547
[CW] ---------------------------
[CW] ---- Iteration:   582 ----
[CW] collect: return: 84.81111, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 0.04140, qf2_loss: 0.04122, policy_loss: -16.37606, policy_entropy: -5.92380, alpha: 0.00506, time: 33.63061
[CW] ---------------------------
[CW] ---- Iteration:   583 ----
[CW] collect: return: 122.62427, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 0.05785, qf2_loss: 0.05750, policy_loss: -16.38228, policy_entropy: -5.71179, alpha: 0.00505, time: 33.49145
[CW] ---------------------------
[CW] ---- Iteration:   584 ----
[CW] collect: return: 126.35665, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 0.04804, qf2_loss: 0.04773, policy_loss: -16.42259, policy_entropy: -5.62401, alpha: 0.00494, time: 33.66061
[CW] ---------------------------
[CW] ---- Iteration:   585 ----
[CW] collect: return: 103.62664, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 0.04382, qf2_loss: 0.04363, policy_loss: -16.39394, policy_entropy: -5.96608, alpha: 0.00486, time: 33.64748
[CW] ---------------------------
[CW] ---- Iteration:   586 ----
[CW] collect: return: 110.32273, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 0.04632, qf2_loss: 0.04611, policy_loss: -16.36779, policy_entropy: -6.08632, alpha: 0.00486, time: 33.69339
[CW] ---------------------------
[CW] ---- Iteration:   587 ----
[CW] collect: return: 103.04120, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 0.04666, qf2_loss: 0.04647, policy_loss: -16.38138, policy_entropy: -6.05084, alpha: 0.00490, time: 33.63547
[CW] ---------------------------
[CW] ---- Iteration:   588 ----
[CW] collect: return: 110.21286, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 0.05769, qf2_loss: 0.05761, policy_loss: -16.30330, policy_entropy: -6.07732, alpha: 0.00491, time: 33.65080
[CW] ---------------------------
[CW] ---- Iteration:   589 ----
[CW] collect: return: 101.70514, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 0.05648, qf2_loss: 0.05614, policy_loss: -16.33435, policy_entropy: -5.93653, alpha: 0.00493, time: 36.93609
[CW] ---------------------------
[CW] ---- Iteration:   590 ----
[CW] collect: return: 116.30658, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 0.05032, qf2_loss: 0.05013, policy_loss: -16.34984, policy_entropy: -5.77337, alpha: 0.00487, time: 33.61101
[CW] ---------------------------
[CW] ---- Iteration:   591 ----
[CW] collect: return: 114.29493, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 0.04682, qf2_loss: 0.04674, policy_loss: -16.30759, policy_entropy: -5.90450, alpha: 0.00482, time: 33.54404
[CW] ---------------------------
[CW] ---- Iteration:   592 ----
[CW] collect: return: 97.61456, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 0.04326, qf2_loss: 0.04313, policy_loss: -16.43135, policy_entropy: -6.13247, alpha: 0.00483, time: 33.70636
[CW] ---------------------------
[CW] ---- Iteration:   593 ----
[CW] collect: return: 121.51201, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 0.04781, qf2_loss: 0.04774, policy_loss: -16.36977, policy_entropy: -5.93846, alpha: 0.00483, time: 33.64548
[CW] ---------------------------
[CW] ---- Iteration:   594 ----
[CW] collect: return: 117.12713, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 0.05155, qf2_loss: 0.05132, policy_loss: -16.45853, policy_entropy: -5.89915, alpha: 0.00483, time: 33.79700
[CW] ---------------------------
[CW] ---- Iteration:   595 ----
[CW] collect: return: 89.75680, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 0.07372, qf2_loss: 0.07334, policy_loss: -16.34484, policy_entropy: -5.92758, alpha: 0.00479, time: 33.67230
[CW] ---------------------------
[CW] ---- Iteration:   596 ----
[CW] collect: return: 108.52086, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 0.05365, qf2_loss: 0.05339, policy_loss: -16.31717, policy_entropy: -6.11446, alpha: 0.00479, time: 33.73238
[CW] ---------------------------
[CW] ---- Iteration:   597 ----
[CW] collect: return: 114.97611, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 0.04433, qf2_loss: 0.04426, policy_loss: -16.36718, policy_entropy: -6.05518, alpha: 0.00483, time: 33.51218
[CW] ---------------------------
[CW] ---- Iteration:   598 ----
[CW] collect: return: 127.78760, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 0.04494, qf2_loss: 0.04485, policy_loss: -16.35264, policy_entropy: -6.00249, alpha: 0.00481, time: 33.57816
[CW] ---------------------------
[CW] ---- Iteration:   599 ----
[CW] collect: return: 112.47626, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 0.04201, qf2_loss: 0.04198, policy_loss: -16.28852, policy_entropy: -5.89365, alpha: 0.00482, time: 33.64661
[CW] ---------------------------
[CW] ---- Iteration:   600 ----
[CW] collect: return: 129.11497, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 0.04416, qf2_loss: 0.04396, policy_loss: -16.39167, policy_entropy: -6.20717, alpha: 0.00483, time: 33.43176
[CW] eval: return: 110.79249, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   601 ----
[CW] collect: return: 111.30253, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 0.04522, qf2_loss: 0.04532, policy_loss: -16.31709, policy_entropy: -6.18031, alpha: 0.00489, time: 34.96519
[CW] ---------------------------
[CW] ---- Iteration:   602 ----
[CW] collect: return: 122.65182, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 0.05233, qf2_loss: 0.05205, policy_loss: -16.37832, policy_entropy: -6.25770, alpha: 0.00496, time: 33.26510
[CW] ---------------------------
[CW] ---- Iteration:   603 ----
[CW] collect: return: 119.58175, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 0.05287, qf2_loss: 0.05259, policy_loss: -16.42897, policy_entropy: -6.07321, alpha: 0.00500, time: 33.23744
[CW] ---------------------------
[CW] ---- Iteration:   604 ----
[CW] collect: return: 105.28959, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 0.07104, qf2_loss: 0.07068, policy_loss: -16.37072, policy_entropy: -5.96975, alpha: 0.00503, time: 33.21443
[CW] ---------------------------
[CW] ---- Iteration:   605 ----
[CW] collect: return: 102.57911, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 0.05648, qf2_loss: 0.05615, policy_loss: -16.31967, policy_entropy: -6.20088, alpha: 0.00503, time: 33.26357
[CW] ---------------------------
[CW] ---- Iteration:   606 ----
[CW] collect: return: 127.77579, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 0.05228, qf2_loss: 0.05221, policy_loss: -16.29240, policy_entropy: -6.28255, alpha: 0.00510, time: 33.10215
[CW] ---------------------------
[CW] ---- Iteration:   607 ----
[CW] collect: return: 114.86625, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 0.04853, qf2_loss: 0.04861, policy_loss: -16.38653, policy_entropy: -6.00891, alpha: 0.00517, time: 33.15049
[CW] ---------------------------
[CW] ---- Iteration:   608 ----
[CW] collect: return: 122.86687, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 0.04463, qf2_loss: 0.04449, policy_loss: -16.34489, policy_entropy: -6.02483, alpha: 0.00516, time: 33.15807
[CW] ---------------------------
[CW] ---- Iteration:   609 ----
[CW] collect: return: 127.53263, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 0.05585, qf2_loss: 0.05559, policy_loss: -16.32568, policy_entropy: -6.20337, alpha: 0.00519, time: 33.22538
[CW] ---------------------------
[CW] ---- Iteration:   610 ----
[CW] collect: return: 119.90258, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 0.05254, qf2_loss: 0.05225, policy_loss: -16.38224, policy_entropy: -6.36616, alpha: 0.00530, time: 33.24808
[CW] ---------------------------
[CW] ---- Iteration:   611 ----
[CW] collect: return: 110.78680, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 0.05465, qf2_loss: 0.05428, policy_loss: -16.36950, policy_entropy: -6.07940, alpha: 0.00537, time: 33.17108
[CW] ---------------------------
[CW] ---- Iteration:   612 ----
[CW] collect: return: 126.37449, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 0.04689, qf2_loss: 0.04683, policy_loss: -16.40542, policy_entropy: -5.97201, alpha: 0.00539, time: 33.02552
[CW] ---------------------------
[CW] ---- Iteration:   613 ----
[CW] collect: return: 122.39507, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 0.05254, qf2_loss: 0.05241, policy_loss: -16.44017, policy_entropy: -6.06392, alpha: 0.00538, time: 32.96962
[CW] ---------------------------
[CW] ---- Iteration:   614 ----
[CW] collect: return: 119.11366, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 0.04812, qf2_loss: 0.04807, policy_loss: -16.34479, policy_entropy: -5.94321, alpha: 0.00540, time: 32.85938
[CW] ---------------------------
[CW] ---- Iteration:   615 ----
[CW] collect: return: 108.35593, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 0.04909, qf2_loss: 0.04909, policy_loss: -16.38564, policy_entropy: -6.20602, alpha: 0.00540, time: 32.91329
[CW] ---------------------------
[CW] ---- Iteration:   616 ----
[CW] collect: return: 118.44875, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 0.04913, qf2_loss: 0.04886, policy_loss: -16.31059, policy_entropy: -6.31973, alpha: 0.00549, time: 32.75523
[CW] ---------------------------
[CW] ---- Iteration:   617 ----
[CW] collect: return: 112.52786, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 0.05202, qf2_loss: 0.05179, policy_loss: -16.43819, policy_entropy: -6.33521, alpha: 0.00561, time: 32.88728
[CW] ---------------------------
[CW] ---- Iteration:   618 ----
[CW] collect: return: 101.05308, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 0.05495, qf2_loss: 0.05486, policy_loss: -16.48060, policy_entropy: -6.06734, alpha: 0.00571, time: 32.88730
[CW] ---------------------------
[CW] ---- Iteration:   619 ----
[CW] collect: return: 99.92714, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 0.08690, qf2_loss: 0.08617, policy_loss: -16.44173, policy_entropy: -5.96818, alpha: 0.00569, time: 33.00562
[CW] ---------------------------
[CW] ---- Iteration:   620 ----
[CW] collect: return: 110.29629, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 0.06151, qf2_loss: 0.06147, policy_loss: -16.36635, policy_entropy: -6.01370, alpha: 0.00568, time: 33.01476
[CW] eval: return: 109.13537, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   621 ----
[CW] collect: return: 108.29801, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 0.04950, qf2_loss: 0.04940, policy_loss: -16.38385, policy_entropy: -6.13897, alpha: 0.00570, time: 33.17557
[CW] ---------------------------
[CW] ---- Iteration:   622 ----
[CW] collect: return: 132.59888, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 0.05317, qf2_loss: 0.05309, policy_loss: -16.36421, policy_entropy: -6.08269, alpha: 0.00576, time: 33.21721
[CW] ---------------------------
[CW] ---- Iteration:   623 ----
[CW] collect: return: 115.40872, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 0.05043, qf2_loss: 0.05026, policy_loss: -16.36966, policy_entropy: -6.13532, alpha: 0.00581, time: 33.16155
[CW] ---------------------------
[CW] ---- Iteration:   624 ----
[CW] collect: return: 116.98251, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 0.06103, qf2_loss: 0.06070, policy_loss: -16.45229, policy_entropy: -6.16047, alpha: 0.00587, time: 33.13953
[CW] ---------------------------
[CW] ---- Iteration:   625 ----
[CW] collect: return: 120.57576, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 0.05124, qf2_loss: 0.05114, policy_loss: -16.50803, policy_entropy: -6.05791, alpha: 0.00590, time: 33.12865
[CW] ---------------------------
[CW] ---- Iteration:   626 ----
[CW] collect: return: 120.60771, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 0.05338, qf2_loss: 0.05329, policy_loss: -16.45746, policy_entropy: -6.13157, alpha: 0.00594, time: 33.24933
[CW] ---------------------------
[CW] ---- Iteration:   627 ----
[CW] collect: return: 116.48910, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 0.04882, qf2_loss: 0.04876, policy_loss: -16.37450, policy_entropy: -5.92344, alpha: 0.00594, time: 33.20221
[CW] ---------------------------
[CW] ---- Iteration:   628 ----
[CW] collect: return: 117.58667, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 0.04992, qf2_loss: 0.04988, policy_loss: -16.35664, policy_entropy: -5.91863, alpha: 0.00594, time: 33.06668
[CW] ---------------------------
[CW] ---- Iteration:   629 ----
[CW] collect: return: 109.98693, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 0.05541, qf2_loss: 0.05533, policy_loss: -16.44231, policy_entropy: -5.73435, alpha: 0.00587, time: 33.00514
[CW] ---------------------------
[CW] ---- Iteration:   630 ----
[CW] collect: return: 117.72985, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 0.05472, qf2_loss: 0.05460, policy_loss: -16.50058, policy_entropy: -6.01771, alpha: 0.00582, time: 32.92141
[CW] ---------------------------
[CW] ---- Iteration:   631 ----
[CW] collect: return: 117.23205, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 0.04963, qf2_loss: 0.04954, policy_loss: -16.54523, policy_entropy: -6.06866, alpha: 0.00582, time: 32.98559
[CW] ---------------------------
[CW] ---- Iteration:   632 ----
[CW] collect: return: 128.44519, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 0.05272, qf2_loss: 0.05257, policy_loss: -16.45816, policy_entropy: -6.06927, alpha: 0.00586, time: 33.55627
[CW] ---------------------------
[CW] ---- Iteration:   633 ----
[CW] collect: return: 118.16995, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 0.06185, qf2_loss: 0.06172, policy_loss: -16.38496, policy_entropy: -6.27329, alpha: 0.00588, time: 32.95819
[CW] ---------------------------
[CW] ---- Iteration:   634 ----
[CW] collect: return: 92.02545, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 0.06303, qf2_loss: 0.06288, policy_loss: -16.51087, policy_entropy: -6.25318, alpha: 0.00599, time: 33.02879
[CW] ---------------------------
[CW] ---- Iteration:   635 ----
[CW] collect: return: 125.17560, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 0.06478, qf2_loss: 0.06460, policy_loss: -16.46204, policy_entropy: -5.97493, alpha: 0.00608, time: 32.94042
[CW] ---------------------------
[CW] ---- Iteration:   636 ----
[CW] collect: return: 118.97918, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 0.06551, qf2_loss: 0.06507, policy_loss: -16.41140, policy_entropy: -6.12907, alpha: 0.00607, time: 33.01589
[CW] ---------------------------
[CW] ---- Iteration:   637 ----
[CW] collect: return: 119.26078, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 0.05657, qf2_loss: 0.05624, policy_loss: -16.49287, policy_entropy: -6.05524, alpha: 0.00611, time: 33.10393
[CW] ---------------------------
[CW] ---- Iteration:   638 ----
[CW] collect: return: 128.94796, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 0.05409, qf2_loss: 0.05411, policy_loss: -16.52583, policy_entropy: -6.12090, alpha: 0.00615, time: 33.09136
[CW] ---------------------------
[CW] ---- Iteration:   639 ----
[CW] collect: return: 110.14105, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 0.05270, qf2_loss: 0.05256, policy_loss: -16.49140, policy_entropy: -5.86459, alpha: 0.00616, time: 33.12326
[CW] ---------------------------
[CW] ---- Iteration:   640 ----
[CW] collect: return: 127.67316, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 0.05787, qf2_loss: 0.05748, policy_loss: -16.37566, policy_entropy: -6.13677, alpha: 0.00614, time: 33.11691
[CW] eval: return: 118.29113, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   641 ----
[CW] collect: return: 111.49957, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 0.07948, qf2_loss: 0.07924, policy_loss: -16.49360, policy_entropy: -6.10212, alpha: 0.00620, time: 33.09802
[CW] ---------------------------
[CW] ---- Iteration:   642 ----
[CW] collect: return: 88.23434, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 0.06120, qf2_loss: 0.06110, policy_loss: -16.43553, policy_entropy: -5.96922, alpha: 0.00620, time: 33.29971
[CW] ---------------------------
[CW] ---- Iteration:   643 ----
[CW] collect: return: 130.89370, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 0.05395, qf2_loss: 0.05394, policy_loss: -16.50641, policy_entropy: -5.99655, alpha: 0.00620, time: 33.16337
[CW] ---------------------------
[CW] ---- Iteration:   644 ----
[CW] collect: return: 115.68379, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 0.05641, qf2_loss: 0.05628, policy_loss: -16.52991, policy_entropy: -5.90691, alpha: 0.00618, time: 33.45047
[CW] ---------------------------
[CW] ---- Iteration:   645 ----
[CW] collect: return: 113.39790, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 0.05383, qf2_loss: 0.05376, policy_loss: -16.47925, policy_entropy: -6.12963, alpha: 0.00619, time: 33.31427
[CW] ---------------------------
[CW] ---- Iteration:   646 ----
[CW] collect: return: 100.78663, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 0.05290, qf2_loss: 0.05285, policy_loss: -16.46301, policy_entropy: -6.01688, alpha: 0.00618, time: 33.28329
[CW] ---------------------------
[CW] ---- Iteration:   647 ----
[CW] collect: return: 131.37711, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 0.05654, qf2_loss: 0.05650, policy_loss: -16.61178, policy_entropy: -5.98891, alpha: 0.00623, time: 33.29030
[CW] ---------------------------
[CW] ---- Iteration:   648 ----
[CW] collect: return: 101.65537, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 0.06449, qf2_loss: 0.06421, policy_loss: -16.47183, policy_entropy: -6.14045, alpha: 0.00622, time: 33.40984
[CW] ---------------------------
[CW] ---- Iteration:   649 ----
[CW] collect: return: 115.09372, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 0.06872, qf2_loss: 0.06855, policy_loss: -16.51066, policy_entropy: -5.94739, alpha: 0.00627, time: 33.38382
[CW] ---------------------------
[CW] ---- Iteration:   650 ----
[CW] collect: return: 121.38743, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 0.06408, qf2_loss: 0.06366, policy_loss: -16.51429, policy_entropy: -5.79383, alpha: 0.00623, time: 33.62491
[CW] ---------------------------
[CW] ---- Iteration:   651 ----
[CW] collect: return: 121.74953, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 0.05912, qf2_loss: 0.05904, policy_loss: -16.53186, policy_entropy: -5.71962, alpha: 0.00612, time: 33.48739
[CW] ---------------------------
[CW] ---- Iteration:   652 ----
[CW] collect: return: 125.90879, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 0.05904, qf2_loss: 0.05888, policy_loss: -16.53288, policy_entropy: -6.12730, alpha: 0.00609, time: 33.61560
[CW] ---------------------------
[CW] ---- Iteration:   653 ----
[CW] collect: return: 96.75965, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 0.06562, qf2_loss: 0.06546, policy_loss: -16.54765, policy_entropy: -6.43056, alpha: 0.00616, time: 33.73852
[CW] ---------------------------
[CW] ---- Iteration:   654 ----
[CW] collect: return: 125.86537, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 0.06012, qf2_loss: 0.05994, policy_loss: -16.59459, policy_entropy: -6.33684, alpha: 0.00632, time: 33.81648
[CW] ---------------------------
[CW] ---- Iteration:   655 ----
[CW] collect: return: 121.34265, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 0.06404, qf2_loss: 0.06378, policy_loss: -16.59413, policy_entropy: -6.12534, alpha: 0.00642, time: 33.95710
[CW] ---------------------------
[CW] ---- Iteration:   656 ----
[CW] collect: return: 114.55724, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 0.05982, qf2_loss: 0.05988, policy_loss: -16.58806, policy_entropy: -5.94747, alpha: 0.00640, time: 33.70299
[CW] ---------------------------
[CW] ---- Iteration:   657 ----
[CW] collect: return: 112.77364, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 0.06740, qf2_loss: 0.06721, policy_loss: -16.55670, policy_entropy: -5.81175, alpha: 0.00638, time: 33.39778
[CW] ---------------------------
[CW] ---- Iteration:   658 ----
[CW] collect: return: 119.30361, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 0.05898, qf2_loss: 0.05899, policy_loss: -16.56960, policy_entropy: -5.91928, alpha: 0.00632, time: 33.47534
[CW] ---------------------------
[CW] ---- Iteration:   659 ----
[CW] collect: return: 130.15936, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 0.06097, qf2_loss: 0.06091, policy_loss: -16.59813, policy_entropy: -5.78540, alpha: 0.00629, time: 33.20750
[CW] ---------------------------
[CW] ---- Iteration:   660 ----
[CW] collect: return: 125.22531, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 0.06761, qf2_loss: 0.06754, policy_loss: -16.71093, policy_entropy: -6.22605, alpha: 0.00626, time: 33.19216
[CW] eval: return: 118.19295, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   661 ----
[CW] collect: return: 102.49377, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 0.06827, qf2_loss: 0.06790, policy_loss: -16.58789, policy_entropy: -6.21523, alpha: 0.00635, time: 33.25376
[CW] ---------------------------
[CW] ---- Iteration:   662 ----
[CW] collect: return: 123.77580, steps: 1000.00000, total_steps: 668000.00000
[CW] train: qf1_loss: 0.06083, qf2_loss: 0.06069, policy_loss: -16.58652, policy_entropy: -6.01232, alpha: 0.00638, time: 33.02440
[CW] ---------------------------
[CW] ---- Iteration:   663 ----
[CW] collect: return: 139.17649, steps: 1000.00000, total_steps: 669000.00000
[CW] train: qf1_loss: 0.05883, qf2_loss: 0.05868, policy_loss: -16.64965, policy_entropy: -6.10560, alpha: 0.00640, time: 33.10202
[CW] ---------------------------
[CW] ---- Iteration:   664 ----
[CW] collect: return: 130.96995, steps: 1000.00000, total_steps: 670000.00000
[CW] train: qf1_loss: 0.06551, qf2_loss: 0.06544, policy_loss: -16.55686, policy_entropy: -6.09005, alpha: 0.00645, time: 32.93064
[CW] ---------------------------
[CW] ---- Iteration:   665 ----
[CW] collect: return: 109.77484, steps: 1000.00000, total_steps: 671000.00000
[CW] train: qf1_loss: 0.06109, qf2_loss: 0.06088, policy_loss: -16.63209, policy_entropy: -6.15454, alpha: 0.00648, time: 32.89057
[CW] ---------------------------
[CW] ---- Iteration:   666 ----
[CW] collect: return: 119.89421, steps: 1000.00000, total_steps: 672000.00000
[CW] train: qf1_loss: 0.06668, qf2_loss: 0.06656, policy_loss: -16.68455, policy_entropy: -5.86070, alpha: 0.00652, time: 32.93150
[CW] ---------------------------
[CW] ---- Iteration:   667 ----
[CW] collect: return: 131.89825, steps: 1000.00000, total_steps: 673000.00000
[CW] train: qf1_loss: 0.07218, qf2_loss: 0.07192, policy_loss: -16.77830, policy_entropy: -5.93187, alpha: 0.00646, time: 33.00955
[CW] ---------------------------
[CW] ---- Iteration:   668 ----
[CW] collect: return: 66.07526, steps: 1000.00000, total_steps: 674000.00000
[CW] train: qf1_loss: 0.11964, qf2_loss: 0.11872, policy_loss: -16.67043, policy_entropy: -6.04257, alpha: 0.00645, time: 32.98647
[CW] ---------------------------
[CW] ---- Iteration:   669 ----
[CW] collect: return: 100.63544, steps: 1000.00000, total_steps: 675000.00000
[CW] train: qf1_loss: 0.06371, qf2_loss: 0.06370, policy_loss: -16.73779, policy_entropy: -5.58072, alpha: 0.00640, time: 32.91981
[CW] ---------------------------
[CW] ---- Iteration:   670 ----
[CW] collect: return: 110.07116, steps: 1000.00000, total_steps: 676000.00000
[CW] train: qf1_loss: 0.06326, qf2_loss: 0.06308, policy_loss: -16.68325, policy_entropy: -5.63821, alpha: 0.00625, time: 32.85884
[CW] ---------------------------
[CW] ---- Iteration:   671 ----
[CW] collect: return: 121.61899, steps: 1000.00000, total_steps: 677000.00000
[CW] train: qf1_loss: 0.06028, qf2_loss: 0.06028, policy_loss: -16.66733, policy_entropy: -5.94388, alpha: 0.00617, time: 33.03365
[CW] ---------------------------
[CW] ---- Iteration:   672 ----
[CW] collect: return: 120.42574, steps: 1000.00000, total_steps: 678000.00000
[CW] train: qf1_loss: 0.06398, qf2_loss: 0.06398, policy_loss: -16.69510, policy_entropy: -6.16318, alpha: 0.00620, time: 32.97007
[CW] ---------------------------
[CW] ---- Iteration:   673 ----
[CW] collect: return: 116.14231, steps: 1000.00000, total_steps: 679000.00000
[CW] train: qf1_loss: 0.06970, qf2_loss: 0.06952, policy_loss: -16.71145, policy_entropy: -5.98194, alpha: 0.00622, time: 33.12064
[CW] ---------------------------
[CW] ---- Iteration:   674 ----
[CW] collect: return: 106.70600, steps: 1000.00000, total_steps: 680000.00000
[CW] train: qf1_loss: 0.06527, qf2_loss: 0.06526, policy_loss: -16.76997, policy_entropy: -6.09251, alpha: 0.00623, time: 33.01467
[CW] ---------------------------
[CW] ---- Iteration:   675 ----
[CW] collect: return: 77.92569, steps: 1000.00000, total_steps: 681000.00000
[CW] train: qf1_loss: 0.06348, qf2_loss: 0.06345, policy_loss: -16.80377, policy_entropy: -5.93897, alpha: 0.00624, time: 33.12535
[CW] ---------------------------
[CW] ---- Iteration:   676 ----
[CW] collect: return: 107.03097, steps: 1000.00000, total_steps: 682000.00000
[CW] train: qf1_loss: 0.07977, qf2_loss: 0.07938, policy_loss: -16.73815, policy_entropy: -6.16772, alpha: 0.00625, time: 33.05929
[CW] ---------------------------
[CW] ---- Iteration:   677 ----
[CW] collect: return: 95.76524, steps: 1000.00000, total_steps: 683000.00000
[CW] train: qf1_loss: 0.07472, qf2_loss: 0.07454, policy_loss: -16.72508, policy_entropy: -6.02918, alpha: 0.00631, time: 33.13174
[CW] ---------------------------
[CW] ---- Iteration:   678 ----
[CW] collect: return: 102.98817, steps: 1000.00000, total_steps: 684000.00000
[CW] train: qf1_loss: 0.07090, qf2_loss: 0.07064, policy_loss: -16.69553, policy_entropy: -5.60185, alpha: 0.00623, time: 32.89230
[CW] ---------------------------
[CW] ---- Iteration:   679 ----
[CW] collect: return: 88.31183, steps: 1000.00000, total_steps: 685000.00000
[CW] train: qf1_loss: 0.08123, qf2_loss: 0.08110, policy_loss: -16.79786, policy_entropy: -5.75996, alpha: 0.00614, time: 34.82278
[CW] ---------------------------
[CW] ---- Iteration:   680 ----
[CW] collect: return: 106.91558, steps: 1000.00000, total_steps: 686000.00000
[CW] train: qf1_loss: 0.06730, qf2_loss: 0.06712, policy_loss: -16.69709, policy_entropy: -5.70641, alpha: 0.00604, time: 33.50430
[CW] eval: return: 107.24996, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   681 ----
[CW] collect: return: 114.49085, steps: 1000.00000, total_steps: 687000.00000
[CW] train: qf1_loss: 0.06529, qf2_loss: 0.06517, policy_loss: -16.78558, policy_entropy: -5.98535, alpha: 0.00599, time: 33.64926
[CW] ---------------------------
[CW] ---- Iteration:   682 ----
[CW] collect: return: 114.66428, steps: 1000.00000, total_steps: 688000.00000
[CW] train: qf1_loss: 0.06585, qf2_loss: 0.06569, policy_loss: -16.80495, policy_entropy: -6.08025, alpha: 0.00600, time: 33.49386
[CW] ---------------------------
[CW] ---- Iteration:   683 ----
[CW] collect: return: 109.84813, steps: 1000.00000, total_steps: 689000.00000
[CW] train: qf1_loss: 0.06856, qf2_loss: 0.06858, policy_loss: -16.79439, policy_entropy: -6.12925, alpha: 0.00604, time: 33.51096
[CW] ---------------------------
[CW] ---- Iteration:   684 ----
[CW] collect: return: 109.36583, steps: 1000.00000, total_steps: 690000.00000
[CW] train: qf1_loss: 0.06826, qf2_loss: 0.06820, policy_loss: -16.82148, policy_entropy: -5.99415, alpha: 0.00606, time: 33.62636
[CW] ---------------------------
[CW] ---- Iteration:   685 ----
[CW] collect: return: 136.78469, steps: 1000.00000, total_steps: 691000.00000
[CW] train: qf1_loss: 0.06828, qf2_loss: 0.06812, policy_loss: -16.80054, policy_entropy: -5.83497, alpha: 0.00602, time: 33.37539
[CW] ---------------------------
[CW] ---- Iteration:   686 ----
[CW] collect: return: 97.34074, steps: 1000.00000, total_steps: 692000.00000
[CW] train: qf1_loss: 0.07458, qf2_loss: 0.07446, policy_loss: -16.87175, policy_entropy: -5.95613, alpha: 0.00598, time: 33.44988
[CW] ---------------------------
[CW] ---- Iteration:   687 ----
[CW] collect: return: 112.91834, steps: 1000.00000, total_steps: 693000.00000
[CW] train: qf1_loss: 0.08883, qf2_loss: 0.08841, policy_loss: -16.78626, policy_entropy: -6.02906, alpha: 0.00598, time: 33.32569
[CW] ---------------------------
[CW] ---- Iteration:   688 ----
[CW] collect: return: 136.54161, steps: 1000.00000, total_steps: 694000.00000
[CW] train: qf1_loss: 0.07470, qf2_loss: 0.07448, policy_loss: -16.84456, policy_entropy: -5.89403, alpha: 0.00598, time: 33.37827
[CW] ---------------------------
[CW] ---- Iteration:   689 ----
[CW] collect: return: 52.37934, steps: 1000.00000, total_steps: 695000.00000
[CW] train: qf1_loss: 0.06659, qf2_loss: 0.06644, policy_loss: -16.82984, policy_entropy: -6.09364, alpha: 0.00597, time: 33.35484
[CW] ---------------------------
[CW] ---- Iteration:   690 ----
[CW] collect: return: 128.89270, steps: 1000.00000, total_steps: 696000.00000
[CW] train: qf1_loss: 0.06638, qf2_loss: 0.06636, policy_loss: -16.85266, policy_entropy: -5.92828, alpha: 0.00597, time: 33.30622
[CW] ---------------------------
[CW] ---- Iteration:   691 ----
[CW] collect: return: 101.51829, steps: 1000.00000, total_steps: 697000.00000
[CW] train: qf1_loss: 0.08309, qf2_loss: 0.08257, policy_loss: -16.77140, policy_entropy: -5.84704, alpha: 0.00593, time: 33.32145
[CW] ---------------------------
[CW] ---- Iteration:   692 ----
[CW] collect: return: 96.74416, steps: 1000.00000, total_steps: 698000.00000
[CW] train: qf1_loss: 0.07656, qf2_loss: 0.07643, policy_loss: -16.80873, policy_entropy: -6.11859, alpha: 0.00593, time: 33.19509
[CW] ---------------------------
[CW] ---- Iteration:   693 ----
[CW] collect: return: 62.63801, steps: 1000.00000, total_steps: 699000.00000
[CW] train: qf1_loss: 0.07508, qf2_loss: 0.07506, policy_loss: -16.75147, policy_entropy: -6.03244, alpha: 0.00595, time: 34.40822
[CW] ---------------------------
[CW] ---- Iteration:   694 ----
[CW] collect: return: 133.33976, steps: 1000.00000, total_steps: 700000.00000
[CW] train: qf1_loss: 0.07231, qf2_loss: 0.07207, policy_loss: -16.91601, policy_entropy: -6.04070, alpha: 0.00595, time: 32.97796
[CW] ---------------------------
[CW] ---- Iteration:   695 ----
[CW] collect: return: 113.85330, steps: 1000.00000, total_steps: 701000.00000
[CW] train: qf1_loss: 0.07635, qf2_loss: 0.07604, policy_loss: -16.84002, policy_entropy: -6.06968, alpha: 0.00599, time: 32.99945
[CW] ---------------------------
[CW] ---- Iteration:   696 ----
[CW] collect: return: 124.93097, steps: 1000.00000, total_steps: 702000.00000
[CW] train: qf1_loss: 0.08546, qf2_loss: 0.08541, policy_loss: -16.80859, policy_entropy: -6.23004, alpha: 0.00601, time: 32.98171
[CW] ---------------------------
[CW] ---- Iteration:   697 ----
[CW] collect: return: 128.64513, steps: 1000.00000, total_steps: 703000.00000
[CW] train: qf1_loss: 0.07404, qf2_loss: 0.07396, policy_loss: -16.84132, policy_entropy: -6.01039, alpha: 0.00607, time: 33.01995
[CW] ---------------------------
[CW] ---- Iteration:   698 ----
[CW] collect: return: 120.58720, steps: 1000.00000, total_steps: 704000.00000
[CW] train: qf1_loss: 0.09478, qf2_loss: 0.09442, policy_loss: -16.84893, policy_entropy: -6.08793, alpha: 0.00610, time: 32.97730
[CW] ---------------------------
[CW] ---- Iteration:   699 ----
[CW] collect: return: 100.87193, steps: 1000.00000, total_steps: 705000.00000
[CW] train: qf1_loss: 0.07462, qf2_loss: 0.07443, policy_loss: -16.86626, policy_entropy: -5.93467, alpha: 0.00610, time: 33.02896
[CW] ---------------------------
[CW] ---- Iteration:   700 ----
[CW] collect: return: 126.48818, steps: 1000.00000, total_steps: 706000.00000
[CW] train: qf1_loss: 0.07146, qf2_loss: 0.07130, policy_loss: -16.89432, policy_entropy: -6.06638, alpha: 0.00608, time: 32.96326
[CW] eval: return: 113.49243, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   701 ----
[CW] collect: return: 128.18319, steps: 1000.00000, total_steps: 707000.00000
[CW] train: qf1_loss: 0.07439, qf2_loss: 0.07425, policy_loss: -16.83141, policy_entropy: -5.95960, alpha: 0.00611, time: 32.87340
[CW] ---------------------------
[CW] ---- Iteration:   702 ----
[CW] collect: return: 128.69245, steps: 1000.00000, total_steps: 708000.00000
[CW] train: qf1_loss: 0.06970, qf2_loss: 0.06955, policy_loss: -16.94961, policy_entropy: -6.09349, alpha: 0.00611, time: 32.92290
[CW] ---------------------------
[CW] ---- Iteration:   703 ----
[CW] collect: return: 118.19497, steps: 1000.00000, total_steps: 709000.00000
[CW] train: qf1_loss: 0.07496, qf2_loss: 0.07476, policy_loss: -16.90788, policy_entropy: -5.99457, alpha: 0.00612, time: 32.70810
[CW] ---------------------------
[CW] ---- Iteration:   704 ----
[CW] collect: return: 125.16095, steps: 1000.00000, total_steps: 710000.00000
[CW] train: qf1_loss: 0.08030, qf2_loss: 0.08020, policy_loss: -17.02898, policy_entropy: -5.93365, alpha: 0.00612, time: 33.18504
[CW] ---------------------------
[CW] ---- Iteration:   705 ----
[CW] collect: return: 129.84036, steps: 1000.00000, total_steps: 711000.00000
[CW] train: qf1_loss: 0.06982, qf2_loss: 0.06974, policy_loss: -16.82570, policy_entropy: -5.92160, alpha: 0.00610, time: 35.37255
[CW] ---------------------------
[CW] ---- Iteration:   706 ----
[CW] collect: return: 97.78221, steps: 1000.00000, total_steps: 712000.00000
[CW] train: qf1_loss: 0.07488, qf2_loss: 0.07513, policy_loss: -16.87856, policy_entropy: -5.90436, alpha: 0.00606, time: 32.91163
[CW] ---------------------------
[CW] ---- Iteration:   707 ----
[CW] collect: return: 107.71923, steps: 1000.00000, total_steps: 713000.00000
[CW] train: qf1_loss: 0.07647, qf2_loss: 0.07644, policy_loss: -16.97644, policy_entropy: -6.03347, alpha: 0.00605, time: 32.82746
[CW] ---------------------------
[CW] ---- Iteration:   708 ----
[CW] collect: return: 132.83020, steps: 1000.00000, total_steps: 714000.00000
[CW] train: qf1_loss: 0.07474, qf2_loss: 0.07454, policy_loss: -16.95228, policy_entropy: -6.03792, alpha: 0.00606, time: 32.95108
[CW] ---------------------------
[CW] ---- Iteration:   709 ----
[CW] collect: return: 111.46272, steps: 1000.00000, total_steps: 715000.00000
[CW] train: qf1_loss: 0.07936, qf2_loss: 0.07892, policy_loss: -16.94318, policy_entropy: -6.24970, alpha: 0.00609, time: 32.86211
[CW] ---------------------------
[CW] ---- Iteration:   710 ----
[CW] collect: return: 109.11214, steps: 1000.00000, total_steps: 716000.00000
[CW] train: qf1_loss: 0.07941, qf2_loss: 0.07919, policy_loss: -16.88475, policy_entropy: -6.20107, alpha: 0.00621, time: 32.81578
[CW] ---------------------------
[CW] ---- Iteration:   711 ----
[CW] collect: return: 122.69978, steps: 1000.00000, total_steps: 717000.00000
[CW] train: qf1_loss: 0.07779, qf2_loss: 0.07739, policy_loss: -16.90332, policy_entropy: -6.03238, alpha: 0.00624, time: 32.86217
[CW] ---------------------------
[CW] ---- Iteration:   712 ----
[CW] collect: return: 120.96235, steps: 1000.00000, total_steps: 718000.00000
[CW] train: qf1_loss: 0.08347, qf2_loss: 0.08342, policy_loss: -16.90551, policy_entropy: -5.89582, alpha: 0.00625, time: 33.01170
[CW] ---------------------------
[CW] ---- Iteration:   713 ----
[CW] collect: return: 142.72004, steps: 1000.00000, total_steps: 719000.00000
[CW] train: qf1_loss: 0.09083, qf2_loss: 0.09091, policy_loss: -16.93438, policy_entropy: -6.08958, alpha: 0.00622, time: 32.84468
[CW] ---------------------------
[CW] ---- Iteration:   714 ----
[CW] collect: return: 127.65361, steps: 1000.00000, total_steps: 720000.00000
[CW] train: qf1_loss: 0.07495, qf2_loss: 0.07483, policy_loss: -16.99837, policy_entropy: -6.13788, alpha: 0.00628, time: 32.95339
[CW] ---------------------------
[CW] ---- Iteration:   715 ----
[CW] collect: return: 121.25154, steps: 1000.00000, total_steps: 721000.00000
[CW] train: qf1_loss: 0.07075, qf2_loss: 0.07057, policy_loss: -17.03393, policy_entropy: -5.97004, alpha: 0.00629, time: 33.65777
[CW] ---------------------------
[CW] ---- Iteration:   716 ----
[CW] collect: return: 134.63861, steps: 1000.00000, total_steps: 722000.00000
[CW] train: qf1_loss: 0.07859, qf2_loss: 0.07834, policy_loss: -16.96017, policy_entropy: -5.79404, alpha: 0.00626, time: 33.02205
[CW] ---------------------------
[CW] ---- Iteration:   717 ----
[CW] collect: return: 131.20382, steps: 1000.00000, total_steps: 723000.00000
[CW] train: qf1_loss: 0.10496, qf2_loss: 0.10453, policy_loss: -17.07744, policy_entropy: -5.93979, alpha: 0.00620, time: 32.87079
[CW] ---------------------------
[CW] ---- Iteration:   718 ----
[CW] collect: return: 126.85106, steps: 1000.00000, total_steps: 724000.00000
[CW] train: qf1_loss: 0.11630, qf2_loss: 0.11596, policy_loss: -16.98421, policy_entropy: -6.15310, alpha: 0.00622, time: 32.90243
[CW] ---------------------------
[CW] ---- Iteration:   719 ----
[CW] collect: return: 34.51672, steps: 1000.00000, total_steps: 725000.00000
[CW] train: qf1_loss: 0.08010, qf2_loss: 0.08005, policy_loss: -16.95865, policy_entropy: -6.01092, alpha: 0.00624, time: 32.68143
[CW] ---------------------------
[CW] ---- Iteration:   720 ----
[CW] collect: return: 141.16212, steps: 1000.00000, total_steps: 726000.00000
[CW] train: qf1_loss: 0.07507, qf2_loss: 0.07483, policy_loss: -17.05466, policy_entropy: -5.86363, alpha: 0.00622, time: 32.60415
[CW] eval: return: 111.56195, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   721 ----
[CW] collect: return: 104.90109, steps: 1000.00000, total_steps: 727000.00000
[CW] train: qf1_loss: 0.07342, qf2_loss: 0.07331, policy_loss: -17.07275, policy_entropy: -5.97303, alpha: 0.00620, time: 32.71470
[CW] ---------------------------
[CW] ---- Iteration:   722 ----
[CW] collect: return: 111.67090, steps: 1000.00000, total_steps: 728000.00000
[CW] train: qf1_loss: 0.07750, qf2_loss: 0.07732, policy_loss: -16.97584, policy_entropy: -6.10602, alpha: 0.00621, time: 32.66379
[CW] ---------------------------
[CW] ---- Iteration:   723 ----
[CW] collect: return: 107.49473, steps: 1000.00000, total_steps: 729000.00000
[CW] train: qf1_loss: 0.07786, qf2_loss: 0.07790, policy_loss: -16.96783, policy_entropy: -6.06496, alpha: 0.00624, time: 32.90263
[CW] ---------------------------
[CW] ---- Iteration:   724 ----
[CW] collect: return: 102.71384, steps: 1000.00000, total_steps: 730000.00000
[CW] train: qf1_loss: 0.07272, qf2_loss: 0.07267, policy_loss: -17.06945, policy_entropy: -5.94364, alpha: 0.00625, time: 32.86784
[CW] ---------------------------
[CW] ---- Iteration:   725 ----
[CW] collect: return: 113.46793, steps: 1000.00000, total_steps: 731000.00000
[CW] train: qf1_loss: 0.07864, qf2_loss: 0.07841, policy_loss: -17.16220, policy_entropy: -6.16023, alpha: 0.00626, time: 32.75424
[CW] ---------------------------
[CW] ---- Iteration:   726 ----
[CW] collect: return: 106.70821, steps: 1000.00000, total_steps: 732000.00000
[CW] train: qf1_loss: 0.08230, qf2_loss: 0.08225, policy_loss: -16.97761, policy_entropy: -5.94882, alpha: 0.00629, time: 32.65426
[CW] ---------------------------
[CW] ---- Iteration:   727 ----
[CW] collect: return: 135.67966, steps: 1000.00000, total_steps: 733000.00000
[CW] train: qf1_loss: 0.08163, qf2_loss: 0.08174, policy_loss: -17.06126, policy_entropy: -6.10700, alpha: 0.00631, time: 33.07587
[CW] ---------------------------
[CW] ---- Iteration:   728 ----
[CW] collect: return: 96.85046, steps: 1000.00000, total_steps: 734000.00000
[CW] train: qf1_loss: 0.09189, qf2_loss: 0.09172, policy_loss: -17.11452, policy_entropy: -5.88214, alpha: 0.00626, time: 32.88620
[CW] ---------------------------
[CW] ---- Iteration:   729 ----
[CW] collect: return: 104.52427, steps: 1000.00000, total_steps: 735000.00000
[CW] train: qf1_loss: 0.08089, qf2_loss: 0.08068, policy_loss: -17.05652, policy_entropy: -6.14228, alpha: 0.00629, time: 32.93480
[CW] ---------------------------
[CW] ---- Iteration:   730 ----
[CW] collect: return: 111.61095, steps: 1000.00000, total_steps: 736000.00000
[CW] train: qf1_loss: 0.08649, qf2_loss: 0.08638, policy_loss: -17.08065, policy_entropy: -6.04934, alpha: 0.00633, time: 32.72998
[CW] ---------------------------
[CW] ---- Iteration:   731 ----
[CW] collect: return: 130.36023, steps: 1000.00000, total_steps: 737000.00000
[CW] train: qf1_loss: 0.09732, qf2_loss: 0.09722, policy_loss: -17.01155, policy_entropy: -6.08733, alpha: 0.00634, time: 32.63102
[CW] ---------------------------
[CW] ---- Iteration:   732 ----
[CW] collect: return: 131.65787, steps: 1000.00000, total_steps: 738000.00000
[CW] train: qf1_loss: 0.08368, qf2_loss: 0.08366, policy_loss: -17.10121, policy_entropy: -6.12033, alpha: 0.00641, time: 32.56268
[CW] ---------------------------
[CW] ---- Iteration:   733 ----
[CW] collect: return: 145.56835, steps: 1000.00000, total_steps: 739000.00000
[CW] train: qf1_loss: 0.10085, qf2_loss: 0.10047, policy_loss: -17.02397, policy_entropy: -5.91381, alpha: 0.00641, time: 32.52959
[CW] ---------------------------
[CW] ---- Iteration:   734 ----
[CW] collect: return: 115.09064, steps: 1000.00000, total_steps: 740000.00000
[CW] train: qf1_loss: 0.07760, qf2_loss: 0.07753, policy_loss: -17.19910, policy_entropy: -6.16804, alpha: 0.00641, time: 32.58578
[CW] ---------------------------
[CW] ---- Iteration:   735 ----
[CW] collect: return: 115.40723, steps: 1000.00000, total_steps: 741000.00000
[CW] train: qf1_loss: 0.08005, qf2_loss: 0.07984, policy_loss: -17.04566, policy_entropy: -6.04193, alpha: 0.00647, time: 32.73170
[CW] ---------------------------
[CW] ---- Iteration:   736 ----
[CW] collect: return: 130.09034, steps: 1000.00000, total_steps: 742000.00000
[CW] train: qf1_loss: 0.07876, qf2_loss: 0.07871, policy_loss: -17.07314, policy_entropy: -5.92827, alpha: 0.00646, time: 33.01680
[CW] ---------------------------
[CW] ---- Iteration:   737 ----
[CW] collect: return: 127.31981, steps: 1000.00000, total_steps: 743000.00000
[CW] train: qf1_loss: 0.08601, qf2_loss: 0.08607, policy_loss: -17.16021, policy_entropy: -6.07691, alpha: 0.00644, time: 32.89996
[CW] ---------------------------
[CW] ---- Iteration:   738 ----
[CW] collect: return: 126.05404, steps: 1000.00000, total_steps: 744000.00000
[CW] train: qf1_loss: 0.08402, qf2_loss: 0.08407, policy_loss: -17.16198, policy_entropy: -6.40944, alpha: 0.00654, time: 32.72840
[CW] ---------------------------
[CW] ---- Iteration:   739 ----
[CW] collect: return: 116.90698, steps: 1000.00000, total_steps: 745000.00000
[CW] train: qf1_loss: 0.08250, qf2_loss: 0.08241, policy_loss: -17.07807, policy_entropy: -5.86645, alpha: 0.00664, time: 32.80495
[CW] ---------------------------
[CW] ---- Iteration:   740 ----
[CW] collect: return: 146.00086, steps: 1000.00000, total_steps: 746000.00000
[CW] train: qf1_loss: 0.08341, qf2_loss: 0.08321, policy_loss: -17.16227, policy_entropy: -6.11621, alpha: 0.00661, time: 32.63391
[CW] eval: return: 125.56604, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   741 ----
[CW] collect: return: 134.48441, steps: 1000.00000, total_steps: 747000.00000
[CW] train: qf1_loss: 0.07639, qf2_loss: 0.07624, policy_loss: -17.12238, policy_entropy: -6.03555, alpha: 0.00664, time: 32.72453
[CW] ---------------------------
[CW] ---- Iteration:   742 ----
[CW] collect: return: 138.17908, steps: 1000.00000, total_steps: 748000.00000
[CW] train: qf1_loss: 0.09009, qf2_loss: 0.08980, policy_loss: -17.16011, policy_entropy: -6.02096, alpha: 0.00668, time: 32.67533
[CW] ---------------------------
[CW] ---- Iteration:   743 ----
[CW] collect: return: 117.01033, steps: 1000.00000, total_steps: 749000.00000
[CW] train: qf1_loss: 0.10346, qf2_loss: 0.10345, policy_loss: -17.20500, policy_entropy: -6.05597, alpha: 0.00668, time: 32.97579
[CW] ---------------------------
[CW] ---- Iteration:   744 ----
[CW] collect: return: 124.31199, steps: 1000.00000, total_steps: 750000.00000
[CW] train: qf1_loss: 0.09303, qf2_loss: 0.09300, policy_loss: -17.21910, policy_entropy: -5.73257, alpha: 0.00666, time: 32.42644
[CW] ---------------------------
[CW] ---- Iteration:   745 ----
[CW] collect: return: 133.89144, steps: 1000.00000, total_steps: 751000.00000
[CW] train: qf1_loss: 0.09866, qf2_loss: 0.09834, policy_loss: -17.20847, policy_entropy: -5.77560, alpha: 0.00653, time: 32.49729
[CW] ---------------------------
[CW] ---- Iteration:   746 ----
[CW] collect: return: 128.91660, steps: 1000.00000, total_steps: 752000.00000
[CW] train: qf1_loss: 0.11696, qf2_loss: 0.11727, policy_loss: -17.16246, policy_entropy: -5.97331, alpha: 0.00648, time: 32.74774
[CW] ---------------------------
[CW] ---- Iteration:   747 ----
[CW] collect: return: 116.51279, steps: 1000.00000, total_steps: 753000.00000
[CW] train: qf1_loss: 0.09285, qf2_loss: 0.09257, policy_loss: -17.20744, policy_entropy: -6.04591, alpha: 0.00648, time: 32.94010
[CW] ---------------------------
[CW] ---- Iteration:   748 ----
[CW] collect: return: 131.59552, steps: 1000.00000, total_steps: 754000.00000
[CW] train: qf1_loss: 0.07969, qf2_loss: 0.07952, policy_loss: -17.21614, policy_entropy: -6.10592, alpha: 0.00652, time: 33.27615
[CW] ---------------------------
[CW] ---- Iteration:   749 ----
[CW] collect: return: 120.80661, steps: 1000.00000, total_steps: 755000.00000
[CW] train: qf1_loss: 0.08243, qf2_loss: 0.08233, policy_loss: -17.22568, policy_entropy: -6.02752, alpha: 0.00656, time: 33.35510
[CW] ---------------------------
[CW] ---- Iteration:   750 ----
[CW] collect: return: 140.94134, steps: 1000.00000, total_steps: 756000.00000
[CW] train: qf1_loss: 0.08312, qf2_loss: 0.08282, policy_loss: -17.24789, policy_entropy: -6.03016, alpha: 0.00657, time: 33.35421
[CW] ---------------------------
[CW] ---- Iteration:   751 ----
[CW] collect: return: 118.36689, steps: 1000.00000, total_steps: 757000.00000
[CW] train: qf1_loss: 0.08109, qf2_loss: 0.08102, policy_loss: -17.26964, policy_entropy: -6.05101, alpha: 0.00657, time: 33.43915
[CW] ---------------------------
[CW] ---- Iteration:   752 ----
[CW] collect: return: 120.75993, steps: 1000.00000, total_steps: 758000.00000
[CW] train: qf1_loss: 0.08837, qf2_loss: 0.08802, policy_loss: -17.29316, policy_entropy: -5.93281, alpha: 0.00660, time: 33.42683
[CW] ---------------------------
[CW] ---- Iteration:   753 ----
[CW] collect: return: 137.58218, steps: 1000.00000, total_steps: 759000.00000
[CW] train: qf1_loss: 0.10524, qf2_loss: 0.10497, policy_loss: -17.35293, policy_entropy: -5.90121, alpha: 0.00654, time: 33.43744
[CW] ---------------------------
[CW] ---- Iteration:   754 ----
[CW] collect: return: 132.39420, steps: 1000.00000, total_steps: 760000.00000
[CW] train: qf1_loss: 0.08813, qf2_loss: 0.08785, policy_loss: -17.23596, policy_entropy: -5.94606, alpha: 0.00651, time: 33.44875
[CW] ---------------------------
[CW] ---- Iteration:   755 ----
[CW] collect: return: 89.91012, steps: 1000.00000, total_steps: 761000.00000
[CW] train: qf1_loss: 0.09384, qf2_loss: 0.09363, policy_loss: -17.26399, policy_entropy: -5.78668, alpha: 0.00645, time: 33.42452
[CW] ---------------------------
[CW] ---- Iteration:   756 ----
[CW] collect: return: 111.93701, steps: 1000.00000, total_steps: 762000.00000
[CW] train: qf1_loss: 0.09192, qf2_loss: 0.09190, policy_loss: -17.33031, policy_entropy: -6.01395, alpha: 0.00642, time: 33.39282
[CW] ---------------------------
[CW] ---- Iteration:   757 ----
[CW] collect: return: 121.08061, steps: 1000.00000, total_steps: 763000.00000
[CW] train: qf1_loss: 0.09043, qf2_loss: 0.09031, policy_loss: -17.27181, policy_entropy: -6.00643, alpha: 0.00643, time: 33.45650
[CW] ---------------------------
[CW] ---- Iteration:   758 ----
[CW] collect: return: 126.74365, steps: 1000.00000, total_steps: 764000.00000
[CW] train: qf1_loss: 0.08300, qf2_loss: 0.08298, policy_loss: -17.34353, policy_entropy: -5.84203, alpha: 0.00639, time: 33.32826
[CW] ---------------------------
[CW] ---- Iteration:   759 ----
[CW] collect: return: 134.34353, steps: 1000.00000, total_steps: 765000.00000
[CW] train: qf1_loss: 0.08010, qf2_loss: 0.08005, policy_loss: -17.31538, policy_entropy: -6.07192, alpha: 0.00638, time: 33.13422
[CW] ---------------------------
[CW] ---- Iteration:   760 ----
[CW] collect: return: 132.83164, steps: 1000.00000, total_steps: 766000.00000
[CW] train: qf1_loss: 0.08722, qf2_loss: 0.08706, policy_loss: -17.36684, policy_entropy: -6.14260, alpha: 0.00641, time: 33.57330
[CW] eval: return: 120.16516, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   761 ----
[CW] collect: return: 124.78546, steps: 1000.00000, total_steps: 767000.00000
[CW] train: qf1_loss: 0.10794, qf2_loss: 0.10771, policy_loss: -17.22153, policy_entropy: -5.93815, alpha: 0.00644, time: 33.81430
[CW] ---------------------------
[CW] ---- Iteration:   762 ----
[CW] collect: return: 134.25518, steps: 1000.00000, total_steps: 768000.00000
[CW] train: qf1_loss: 0.09374, qf2_loss: 0.09358, policy_loss: -17.33824, policy_entropy: -6.11700, alpha: 0.00644, time: 33.85581
[CW] ---------------------------
[CW] ---- Iteration:   763 ----
[CW] collect: return: 99.64517, steps: 1000.00000, total_steps: 769000.00000
[CW] train: qf1_loss: 0.09434, qf2_loss: 0.09399, policy_loss: -17.33242, policy_entropy: -5.93875, alpha: 0.00648, time: 36.11673
[CW] ---------------------------
[CW] ---- Iteration:   764 ----
[CW] collect: return: 131.46741, steps: 1000.00000, total_steps: 770000.00000
[CW] train: qf1_loss: 0.11535, qf2_loss: 0.11544, policy_loss: -17.40992, policy_entropy: -5.88854, alpha: 0.00645, time: 33.80213
[CW] ---------------------------
[CW] ---- Iteration:   765 ----
[CW] collect: return: 118.73846, steps: 1000.00000, total_steps: 771000.00000
[CW] train: qf1_loss: 0.09556, qf2_loss: 0.09529, policy_loss: -17.41510, policy_entropy: -5.91933, alpha: 0.00639, time: 33.69212
[CW] ---------------------------
[CW] ---- Iteration:   766 ----
[CW] collect: return: 132.63671, steps: 1000.00000, total_steps: 772000.00000
[CW] train: qf1_loss: 0.08762, qf2_loss: 0.08742, policy_loss: -17.41006, policy_entropy: -5.86726, alpha: 0.00636, time: 33.84104
[CW] ---------------------------
[CW] ---- Iteration:   767 ----
[CW] collect: return: 117.25105, steps: 1000.00000, total_steps: 773000.00000
[CW] train: qf1_loss: 0.10383, qf2_loss: 0.10356, policy_loss: -17.31320, policy_entropy: -6.20219, alpha: 0.00636, time: 33.85085
[CW] ---------------------------
[CW] ---- Iteration:   768 ----
[CW] collect: return: 138.95410, steps: 1000.00000, total_steps: 774000.00000
[CW] train: qf1_loss: 0.08972, qf2_loss: 0.08966, policy_loss: -17.30194, policy_entropy: -6.04713, alpha: 0.00642, time: 33.93459
[CW] ---------------------------
[CW] ---- Iteration:   769 ----
[CW] collect: return: 112.14132, steps: 1000.00000, total_steps: 775000.00000
[CW] train: qf1_loss: 0.09710, qf2_loss: 0.09706, policy_loss: -17.46225, policy_entropy: -5.99017, alpha: 0.00641, time: 33.75064
[CW] ---------------------------
[CW] ---- Iteration:   770 ----
[CW] collect: return: 120.30436, steps: 1000.00000, total_steps: 776000.00000
[CW] train: qf1_loss: 0.09140, qf2_loss: 0.09127, policy_loss: -17.46977, policy_entropy: -5.95629, alpha: 0.00641, time: 33.65571
[CW] ---------------------------
[CW] ---- Iteration:   771 ----
[CW] collect: return: 114.99883, steps: 1000.00000, total_steps: 777000.00000
[CW] train: qf1_loss: 0.09822, qf2_loss: 0.09826, policy_loss: -17.41534, policy_entropy: -6.02838, alpha: 0.00640, time: 33.59882
[CW] ---------------------------
[CW] ---- Iteration:   772 ----
[CW] collect: return: 133.76136, steps: 1000.00000, total_steps: 778000.00000
[CW] train: qf1_loss: 0.09220, qf2_loss: 0.09200, policy_loss: -17.44021, policy_entropy: -5.83991, alpha: 0.00637, time: 33.78573
[CW] ---------------------------
[CW] ---- Iteration:   773 ----
[CW] collect: return: 132.27333, steps: 1000.00000, total_steps: 779000.00000
[CW] train: qf1_loss: 0.09355, qf2_loss: 0.09339, policy_loss: -17.47504, policy_entropy: -6.19394, alpha: 0.00638, time: 33.73885
[CW] ---------------------------
[CW] ---- Iteration:   774 ----
[CW] collect: return: 108.10589, steps: 1000.00000, total_steps: 780000.00000
[CW] train: qf1_loss: 0.09166, qf2_loss: 0.09158, policy_loss: -17.42906, policy_entropy: -5.86951, alpha: 0.00640, time: 33.77896
[CW] ---------------------------
[CW] ---- Iteration:   775 ----
[CW] collect: return: 131.51751, steps: 1000.00000, total_steps: 781000.00000
[CW] train: qf1_loss: 0.08992, qf2_loss: 0.08975, policy_loss: -17.36375, policy_entropy: -6.01658, alpha: 0.00638, time: 33.81829
[CW] ---------------------------
[CW] ---- Iteration:   776 ----
[CW] collect: return: 140.89603, steps: 1000.00000, total_steps: 782000.00000
[CW] train: qf1_loss: 0.09209, qf2_loss: 0.09209, policy_loss: -17.45946, policy_entropy: -6.13052, alpha: 0.00640, time: 33.76223
[CW] ---------------------------
[CW] ---- Iteration:   777 ----
[CW] collect: return: 113.60950, steps: 1000.00000, total_steps: 783000.00000
[CW] train: qf1_loss: 0.09933, qf2_loss: 0.09900, policy_loss: -17.57361, policy_entropy: -6.13788, alpha: 0.00644, time: 33.66512
[CW] ---------------------------
[CW] ---- Iteration:   778 ----
[CW] collect: return: 128.22474, steps: 1000.00000, total_steps: 784000.00000
[CW] train: qf1_loss: 0.09633, qf2_loss: 0.09638, policy_loss: -17.48350, policy_entropy: -6.13031, alpha: 0.00651, time: 33.74642
[CW] ---------------------------
[CW] ---- Iteration:   779 ----
[CW] collect: return: 130.21904, steps: 1000.00000, total_steps: 785000.00000
[CW] train: qf1_loss: 0.09994, qf2_loss: 0.10022, policy_loss: -17.45984, policy_entropy: -5.91238, alpha: 0.00655, time: 33.88041
[CW] ---------------------------
[CW] ---- Iteration:   780 ----
[CW] collect: return: 121.05965, steps: 1000.00000, total_steps: 786000.00000
[CW] train: qf1_loss: 0.10624, qf2_loss: 0.10582, policy_loss: -17.55039, policy_entropy: -5.95396, alpha: 0.00649, time: 33.80349
[CW] eval: return: 125.67217, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   781 ----
[CW] collect: return: 125.10625, steps: 1000.00000, total_steps: 787000.00000
[CW] train: qf1_loss: 0.09725, qf2_loss: 0.09706, policy_loss: -17.38612, policy_entropy: -6.10278, alpha: 0.00651, time: 34.14046
[CW] ---------------------------
[CW] ---- Iteration:   782 ----
[CW] collect: return: 127.06160, steps: 1000.00000, total_steps: 788000.00000
[CW] train: qf1_loss: 0.09390, qf2_loss: 0.09367, policy_loss: -17.40560, policy_entropy: -5.90657, alpha: 0.00650, time: 33.97322
[CW] ---------------------------
[CW] ---- Iteration:   783 ----
[CW] collect: return: 132.76725, steps: 1000.00000, total_steps: 789000.00000
[CW] train: qf1_loss: 0.09264, qf2_loss: 0.09267, policy_loss: -17.42921, policy_entropy: -6.01161, alpha: 0.00648, time: 33.85516
[CW] ---------------------------
[CW] ---- Iteration:   784 ----
[CW] collect: return: 117.00536, steps: 1000.00000, total_steps: 790000.00000
[CW] train: qf1_loss: 0.10062, qf2_loss: 0.10032, policy_loss: -17.40719, policy_entropy: -5.85179, alpha: 0.00647, time: 33.79429
[CW] ---------------------------
[CW] ---- Iteration:   785 ----
[CW] collect: return: 136.55896, steps: 1000.00000, total_steps: 791000.00000
[CW] train: qf1_loss: 0.09493, qf2_loss: 0.09483, policy_loss: -17.55462, policy_entropy: -6.02151, alpha: 0.00643, time: 33.78917
[CW] ---------------------------
[CW] ---- Iteration:   786 ----
[CW] collect: return: 125.03428, steps: 1000.00000, total_steps: 792000.00000
[CW] train: qf1_loss: 0.09463, qf2_loss: 0.09446, policy_loss: -17.47590, policy_entropy: -6.19286, alpha: 0.00648, time: 33.81280
[CW] ---------------------------
[CW] ---- Iteration:   787 ----
[CW] collect: return: 133.71580, steps: 1000.00000, total_steps: 793000.00000
[CW] train: qf1_loss: 0.09202, qf2_loss: 0.09191, policy_loss: -17.40287, policy_entropy: -5.88889, alpha: 0.00650, time: 33.80169
[CW] ---------------------------
[CW] ---- Iteration:   788 ----
[CW] collect: return: 127.11133, steps: 1000.00000, total_steps: 794000.00000
[CW] train: qf1_loss: 0.08833, qf2_loss: 0.08824, policy_loss: -17.46507, policy_entropy: -5.76060, alpha: 0.00644, time: 33.91726
[CW] ---------------------------
[CW] ---- Iteration:   789 ----
[CW] collect: return: 128.63392, steps: 1000.00000, total_steps: 795000.00000
[CW] train: qf1_loss: 0.12251, qf2_loss: 0.12244, policy_loss: -17.54776, policy_entropy: -5.83401, alpha: 0.00637, time: 33.68760
[CW] ---------------------------
[CW] ---- Iteration:   790 ----
[CW] collect: return: 100.50240, steps: 1000.00000, total_steps: 796000.00000
[CW] train: qf1_loss: 0.10792, qf2_loss: 0.10782, policy_loss: -17.58575, policy_entropy: -6.12088, alpha: 0.00633, time: 33.77121
[CW] ---------------------------
[CW] ---- Iteration:   791 ----
[CW] collect: return: 113.99755, steps: 1000.00000, total_steps: 797000.00000
[CW] train: qf1_loss: 0.10015, qf2_loss: 0.10014, policy_loss: -17.67519, policy_entropy: -6.31602, alpha: 0.00643, time: 33.77448
[CW] ---------------------------
[CW] ---- Iteration:   792 ----
[CW] collect: return: 102.60423, steps: 1000.00000, total_steps: 798000.00000
[CW] train: qf1_loss: 0.09665, qf2_loss: 0.09643, policy_loss: -17.50204, policy_entropy: -5.92699, alpha: 0.00647, time: 33.85304
[CW] ---------------------------
[CW] ---- Iteration:   793 ----
[CW] collect: return: 107.77003, steps: 1000.00000, total_steps: 799000.00000
[CW] train: qf1_loss: 0.10010, qf2_loss: 0.10007, policy_loss: -17.55179, policy_entropy: -5.81750, alpha: 0.00642, time: 33.79307
[CW] ---------------------------
[CW] ---- Iteration:   794 ----
[CW] collect: return: 133.33678, steps: 1000.00000, total_steps: 800000.00000
[CW] train: qf1_loss: 0.10627, qf2_loss: 0.10598, policy_loss: -17.62377, policy_entropy: -6.08350, alpha: 0.00640, time: 33.83713
[CW] ---------------------------
[CW] ---- Iteration:   795 ----
[CW] collect: return: 129.06882, steps: 1000.00000, total_steps: 801000.00000
[CW] train: qf1_loss: 0.11485, qf2_loss: 0.11457, policy_loss: -17.62033, policy_entropy: -6.25858, alpha: 0.00646, time: 33.83259
[CW] ---------------------------
[CW] ---- Iteration:   796 ----
[CW] collect: return: 126.53001, steps: 1000.00000, total_steps: 802000.00000
[CW] train: qf1_loss: 0.10787, qf2_loss: 0.10795, policy_loss: -17.74648, policy_entropy: -6.06999, alpha: 0.00653, time: 33.91861
[CW] ---------------------------
[CW] ---- Iteration:   797 ----
[CW] collect: return: 26.68760, steps: 1000.00000, total_steps: 803000.00000
[CW] train: qf1_loss: 0.09805, qf2_loss: 0.09784, policy_loss: -17.62058, policy_entropy: -5.94202, alpha: 0.00652, time: 33.88392
[CW] ---------------------------
[CW] ---- Iteration:   798 ----
[CW] collect: return: 150.10007, steps: 1000.00000, total_steps: 804000.00000
[CW] train: qf1_loss: 0.09526, qf2_loss: 0.09523, policy_loss: -17.67708, policy_entropy: -5.97574, alpha: 0.00652, time: 34.50773
[CW] ---------------------------
[CW] ---- Iteration:   799 ----
[CW] collect: return: 131.09002, steps: 1000.00000, total_steps: 805000.00000
[CW] train: qf1_loss: 0.09663, qf2_loss: 0.09649, policy_loss: -17.70035, policy_entropy: -5.83529, alpha: 0.00647, time: 33.94207
[CW] ---------------------------
[CW] ---- Iteration:   800 ----
[CW] collect: return: 129.28757, steps: 1000.00000, total_steps: 806000.00000
[CW] train: qf1_loss: 0.11809, qf2_loss: 0.11791, policy_loss: -17.59314, policy_entropy: -5.91542, alpha: 0.00643, time: 33.86209
[CW] eval: return: 126.73389, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   801 ----
[CW] collect: return: 113.83853, steps: 1000.00000, total_steps: 807000.00000
[CW] train: qf1_loss: 0.12446, qf2_loss: 0.12420, policy_loss: -17.53724, policy_entropy: -5.88801, alpha: 0.00639, time: 33.89270
[CW] ---------------------------
[CW] ---- Iteration:   802 ----
[CW] collect: return: 123.83751, steps: 1000.00000, total_steps: 808000.00000
[CW] train: qf1_loss: 0.10815, qf2_loss: 0.10809, policy_loss: -17.64267, policy_entropy: -6.29526, alpha: 0.00644, time: 33.94148
[CW] ---------------------------
[CW] ---- Iteration:   803 ----
[CW] collect: return: 115.58607, steps: 1000.00000, total_steps: 809000.00000
[CW] train: qf1_loss: 0.10254, qf2_loss: 0.10234, policy_loss: -17.64988, policy_entropy: -6.05140, alpha: 0.00649, time: 33.87424
[CW] ---------------------------
[CW] ---- Iteration:   804 ----
[CW] collect: return: 117.17707, steps: 1000.00000, total_steps: 810000.00000
[CW] train: qf1_loss: 0.09449, qf2_loss: 0.09427, policy_loss: -17.63280, policy_entropy: -6.03480, alpha: 0.00651, time: 33.95734
[CW] ---------------------------
[CW] ---- Iteration:   805 ----
[CW] collect: return: 113.61667, steps: 1000.00000, total_steps: 811000.00000
[CW] train: qf1_loss: 0.10254, qf2_loss: 0.10239, policy_loss: -17.66677, policy_entropy: -5.95964, alpha: 0.00651, time: 33.76577
[CW] ---------------------------
[CW] ---- Iteration:   806 ----
[CW] collect: return: 117.21758, steps: 1000.00000, total_steps: 812000.00000
[CW] train: qf1_loss: 0.10464, qf2_loss: 0.10474, policy_loss: -17.64204, policy_entropy: -5.86336, alpha: 0.00648, time: 33.91505
[CW] ---------------------------
[CW] ---- Iteration:   807 ----
[CW] collect: return: 129.68318, steps: 1000.00000, total_steps: 813000.00000
[CW] train: qf1_loss: 0.10887, qf2_loss: 0.10869, policy_loss: -17.60249, policy_entropy: -6.02207, alpha: 0.00645, time: 33.77035
[CW] ---------------------------
[CW] ---- Iteration:   808 ----
[CW] collect: return: 114.02490, steps: 1000.00000, total_steps: 814000.00000
[CW] train: qf1_loss: 0.10209, qf2_loss: 0.10200, policy_loss: -17.66895, policy_entropy: -6.01374, alpha: 0.00647, time: 33.91295
[CW] ---------------------------
[CW] ---- Iteration:   809 ----
[CW] collect: return: 140.59874, steps: 1000.00000, total_steps: 815000.00000
[CW] train: qf1_loss: 0.10563, qf2_loss: 0.10536, policy_loss: -17.61408, policy_entropy: -5.85899, alpha: 0.00643, time: 33.67033
[CW] ---------------------------
[CW] ---- Iteration:   810 ----
[CW] collect: return: 126.54755, steps: 1000.00000, total_steps: 816000.00000
[CW] train: qf1_loss: 0.11116, qf2_loss: 0.11097, policy_loss: -17.72554, policy_entropy: -6.02006, alpha: 0.00641, time: 33.76943
[CW] ---------------------------
[CW] ---- Iteration:   811 ----
[CW] collect: return: 117.74266, steps: 1000.00000, total_steps: 817000.00000
[CW] train: qf1_loss: 0.12341, qf2_loss: 0.12306, policy_loss: -17.73892, policy_entropy: -5.98987, alpha: 0.00643, time: 33.74069
[CW] ---------------------------
[CW] ---- Iteration:   812 ----
[CW] collect: return: 110.17557, steps: 1000.00000, total_steps: 818000.00000
[CW] train: qf1_loss: 0.11594, qf2_loss: 0.11607, policy_loss: -17.69397, policy_entropy: -6.06078, alpha: 0.00642, time: 33.93766
[CW] ---------------------------
[CW] ---- Iteration:   813 ----
[CW] collect: return: 128.44273, steps: 1000.00000, total_steps: 819000.00000
[CW] train: qf1_loss: 0.12979, qf2_loss: 0.12975, policy_loss: -17.65306, policy_entropy: -6.06928, alpha: 0.00644, time: 33.88163
[CW] ---------------------------
[CW] ---- Iteration:   814 ----
[CW] collect: return: 124.85183, steps: 1000.00000, total_steps: 820000.00000
[CW] train: qf1_loss: 0.11515, qf2_loss: 0.11539, policy_loss: -17.75905, policy_entropy: -6.16420, alpha: 0.00650, time: 33.89237
[CW] ---------------------------
[CW] ---- Iteration:   815 ----
[CW] collect: return: 125.15944, steps: 1000.00000, total_steps: 821000.00000
[CW] train: qf1_loss: 0.10254, qf2_loss: 0.10253, policy_loss: -17.68576, policy_entropy: -5.99847, alpha: 0.00653, time: 34.34477
[CW] ---------------------------
[CW] ---- Iteration:   816 ----
[CW] collect: return: 133.37249, steps: 1000.00000, total_steps: 822000.00000
[CW] train: qf1_loss: 0.10025, qf2_loss: 0.10019, policy_loss: -17.69085, policy_entropy: -5.85919, alpha: 0.00649, time: 33.95443
[CW] ---------------------------
[CW] ---- Iteration:   817 ----
[CW] collect: return: 140.48245, steps: 1000.00000, total_steps: 823000.00000
[CW] train: qf1_loss: 0.10217, qf2_loss: 0.10203, policy_loss: -17.73501, policy_entropy: -5.90494, alpha: 0.00647, time: 33.56746
[CW] ---------------------------
[CW] ---- Iteration:   818 ----
[CW] collect: return: 126.87904, steps: 1000.00000, total_steps: 824000.00000
[CW] train: qf1_loss: 0.10387, qf2_loss: 0.10384, policy_loss: -17.83153, policy_entropy: -6.00751, alpha: 0.00644, time: 33.38551
[CW] ---------------------------
[CW] ---- Iteration:   819 ----
[CW] collect: return: 112.07780, steps: 1000.00000, total_steps: 825000.00000
[CW] train: qf1_loss: 0.09925, qf2_loss: 0.09927, policy_loss: -17.81980, policy_entropy: -6.06034, alpha: 0.00644, time: 33.58521
[CW] ---------------------------
[CW] ---- Iteration:   820 ----
[CW] collect: return: 109.01780, steps: 1000.00000, total_steps: 826000.00000
[CW] train: qf1_loss: 0.10456, qf2_loss: 0.10452, policy_loss: -17.90602, policy_entropy: -6.10334, alpha: 0.00648, time: 33.47312
[CW] eval: return: 123.05317, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   821 ----
[CW] collect: return: 118.43438, steps: 1000.00000, total_steps: 827000.00000
[CW] train: qf1_loss: 0.10915, qf2_loss: 0.10916, policy_loss: -17.85065, policy_entropy: -5.97394, alpha: 0.00650, time: 33.58162
[CW] ---------------------------
[CW] ---- Iteration:   822 ----
[CW] collect: return: 123.77864, steps: 1000.00000, total_steps: 828000.00000
[CW] train: qf1_loss: 0.13377, qf2_loss: 0.13362, policy_loss: -17.80020, policy_entropy: -5.99102, alpha: 0.00648, time: 33.48730
[CW] ---------------------------
[CW] ---- Iteration:   823 ----
[CW] collect: return: 118.96710, steps: 1000.00000, total_steps: 829000.00000
[CW] train: qf1_loss: 0.13170, qf2_loss: 0.13165, policy_loss: -17.79535, policy_entropy: -6.06040, alpha: 0.00651, time: 33.34757
[CW] ---------------------------
[CW] ---- Iteration:   824 ----
[CW] collect: return: 109.81030, steps: 1000.00000, total_steps: 830000.00000
[CW] train: qf1_loss: 0.11768, qf2_loss: 0.11710, policy_loss: -17.82243, policy_entropy: -5.94810, alpha: 0.00649, time: 33.53941
[CW] ---------------------------
[CW] ---- Iteration:   825 ----
[CW] collect: return: 130.44008, steps: 1000.00000, total_steps: 831000.00000
[CW] train: qf1_loss: 0.11049, qf2_loss: 0.11032, policy_loss: -17.87124, policy_entropy: -5.96731, alpha: 0.00648, time: 33.44651
[CW] ---------------------------
[CW] ---- Iteration:   826 ----
[CW] collect: return: 102.60844, steps: 1000.00000, total_steps: 832000.00000
[CW] train: qf1_loss: 0.10542, qf2_loss: 0.10547, policy_loss: -17.83192, policy_entropy: -5.96999, alpha: 0.00645, time: 33.55526
[CW] ---------------------------
[CW] ---- Iteration:   827 ----
[CW] collect: return: 93.74831, steps: 1000.00000, total_steps: 833000.00000
[CW] train: qf1_loss: 0.10907, qf2_loss: 0.10891, policy_loss: -17.95183, policy_entropy: -6.08097, alpha: 0.00649, time: 33.32751
[CW] ---------------------------
[CW] ---- Iteration:   828 ----
[CW] collect: return: 119.88034, steps: 1000.00000, total_steps: 834000.00000
[CW] train: qf1_loss: 0.11766, qf2_loss: 0.11769, policy_loss: -17.84052, policy_entropy: -5.75465, alpha: 0.00644, time: 33.40731
[CW] ---------------------------
[CW] ---- Iteration:   829 ----
[CW] collect: return: 121.80867, steps: 1000.00000, total_steps: 835000.00000
[CW] train: qf1_loss: 0.10813, qf2_loss: 0.10788, policy_loss: -17.93146, policy_entropy: -5.91719, alpha: 0.00640, time: 33.50922
[CW] ---------------------------
[CW] ---- Iteration:   830 ----
[CW] collect: return: 141.28761, steps: 1000.00000, total_steps: 836000.00000
[CW] train: qf1_loss: 0.11512, qf2_loss: 0.11503, policy_loss: -17.78470, policy_entropy: -5.53507, alpha: 0.00632, time: 33.57337
[CW] ---------------------------
[CW] ---- Iteration:   831 ----
[CW] collect: return: 136.19786, steps: 1000.00000, total_steps: 837000.00000
[CW] train: qf1_loss: 0.10995, qf2_loss: 0.10979, policy_loss: -17.98153, policy_entropy: -5.99510, alpha: 0.00620, time: 33.73527
[CW] ---------------------------
[CW] ---- Iteration:   832 ----
[CW] collect: return: 140.46840, steps: 1000.00000, total_steps: 838000.00000
[CW] train: qf1_loss: 0.10665, qf2_loss: 0.10642, policy_loss: -18.01968, policy_entropy: -6.19961, alpha: 0.00620, time: 33.52058
[CW] ---------------------------
[CW] ---- Iteration:   833 ----
[CW] collect: return: 133.18041, steps: 1000.00000, total_steps: 839000.00000
[CW] train: qf1_loss: 0.11871, qf2_loss: 0.11868, policy_loss: -17.90448, policy_entropy: -6.05194, alpha: 0.00628, time: 33.40478
[CW] ---------------------------
[CW] ---- Iteration:   834 ----
[CW] collect: return: 130.04689, steps: 1000.00000, total_steps: 840000.00000
[CW] train: qf1_loss: 0.11431, qf2_loss: 0.11428, policy_loss: -18.03201, policy_entropy: -6.24486, alpha: 0.00632, time: 33.52766
[CW] ---------------------------
[CW] ---- Iteration:   835 ----
[CW] collect: return: 26.82335, steps: 1000.00000, total_steps: 841000.00000
[CW] train: qf1_loss: 0.11063, qf2_loss: 0.11050, policy_loss: -17.95817, policy_entropy: -6.00866, alpha: 0.00640, time: 33.42996
[CW] ---------------------------
[CW] ---- Iteration:   836 ----
[CW] collect: return: 133.32621, steps: 1000.00000, total_steps: 842000.00000
[CW] train: qf1_loss: 0.10759, qf2_loss: 0.10757, policy_loss: -17.79262, policy_entropy: -5.76067, alpha: 0.00635, time: 33.62805
[CW] ---------------------------
[CW] ---- Iteration:   837 ----
[CW] collect: return: 105.59628, steps: 1000.00000, total_steps: 843000.00000
[CW] train: qf1_loss: 0.17834, qf2_loss: 0.17729, policy_loss: -17.95123, policy_entropy: -6.03152, alpha: 0.00630, time: 33.71125
[CW] ---------------------------
[CW] ---- Iteration:   838 ----
[CW] collect: return: 114.36266, steps: 1000.00000, total_steps: 844000.00000
[CW] train: qf1_loss: 0.11254, qf2_loss: 0.11264, policy_loss: -18.06323, policy_entropy: -6.09308, alpha: 0.00631, time: 33.73906
[CW] ---------------------------
[CW] ---- Iteration:   839 ----
[CW] collect: return: 121.69082, steps: 1000.00000, total_steps: 845000.00000
[CW] train: qf1_loss: 0.10198, qf2_loss: 0.10214, policy_loss: -17.86988, policy_entropy: -5.97133, alpha: 0.00633, time: 33.56958
[CW] ---------------------------
[CW] ---- Iteration:   840 ----
[CW] collect: return: 123.68897, steps: 1000.00000, total_steps: 846000.00000
[CW] train: qf1_loss: 0.10257, qf2_loss: 0.10269, policy_loss: -17.80763, policy_entropy: -5.95445, alpha: 0.00631, time: 33.59192
[CW] eval: return: 125.96190, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   841 ----
[CW] collect: return: 116.04133, steps: 1000.00000, total_steps: 847000.00000
[CW] train: qf1_loss: 0.11717, qf2_loss: 0.11688, policy_loss: -17.84525, policy_entropy: -6.05765, alpha: 0.00632, time: 33.74213
[CW] ---------------------------
[CW] ---- Iteration:   842 ----
[CW] collect: return: 103.85538, steps: 1000.00000, total_steps: 848000.00000
[CW] train: qf1_loss: 0.10507, qf2_loss: 0.10496, policy_loss: -17.96910, policy_entropy: -6.15636, alpha: 0.00636, time: 33.68526
[CW] ---------------------------
[CW] ---- Iteration:   843 ----
[CW] collect: return: 111.47911, steps: 1000.00000, total_steps: 849000.00000
[CW] train: qf1_loss: 0.10142, qf2_loss: 0.10147, policy_loss: -18.10845, policy_entropy: -5.91043, alpha: 0.00637, time: 33.51710
[CW] ---------------------------
[CW] ---- Iteration:   844 ----
[CW] collect: return: 133.13301, steps: 1000.00000, total_steps: 850000.00000
[CW] train: qf1_loss: 0.15143, qf2_loss: 0.15165, policy_loss: -18.01200, policy_entropy: -6.07586, alpha: 0.00636, time: 33.60174
[CW] ---------------------------
[CW] ---- Iteration:   845 ----
[CW] collect: return: 129.75348, steps: 1000.00000, total_steps: 851000.00000
[CW] train: qf1_loss: 0.11549, qf2_loss: 0.11542, policy_loss: -17.87880, policy_entropy: -6.03543, alpha: 0.00639, time: 33.43143
[CW] ---------------------------
[CW] ---- Iteration:   846 ----
[CW] collect: return: 101.84833, steps: 1000.00000, total_steps: 852000.00000
[CW] train: qf1_loss: 0.11938, qf2_loss: 0.11927, policy_loss: -18.01682, policy_entropy: -5.90467, alpha: 0.00638, time: 34.00161
[CW] ---------------------------
[CW] ---- Iteration:   847 ----
[CW] collect: return: 138.20788, steps: 1000.00000, total_steps: 853000.00000
[CW] train: qf1_loss: 0.10883, qf2_loss: 0.10903, policy_loss: -18.06944, policy_entropy: -6.12968, alpha: 0.00639, time: 33.46416
[CW] ---------------------------
[CW] ---- Iteration:   848 ----
[CW] collect: return: 124.03458, steps: 1000.00000, total_steps: 854000.00000
[CW] train: qf1_loss: 0.11675, qf2_loss: 0.11675, policy_loss: -18.01453, policy_entropy: -6.16695, alpha: 0.00643, time: 33.50404
[CW] ---------------------------
[CW] ---- Iteration:   849 ----
[CW] collect: return: 127.47295, steps: 1000.00000, total_steps: 855000.00000
[CW] train: qf1_loss: 0.11339, qf2_loss: 0.11328, policy_loss: -18.02594, policy_entropy: -6.19043, alpha: 0.00650, time: 33.41944
[CW] ---------------------------
[CW] ---- Iteration:   850 ----
[CW] collect: return: 140.74051, steps: 1000.00000, total_steps: 856000.00000
[CW] train: qf1_loss: 0.11159, qf2_loss: 0.11162, policy_loss: -17.95379, policy_entropy: -6.04062, alpha: 0.00657, time: 33.56764
[CW] ---------------------------
[CW] ---- Iteration:   851 ----
[CW] collect: return: 134.12453, steps: 1000.00000, total_steps: 857000.00000
[CW] train: qf1_loss: 0.11411, qf2_loss: 0.11395, policy_loss: -17.99659, policy_entropy: -6.01021, alpha: 0.00657, time: 33.43004
[CW] ---------------------------
[CW] ---- Iteration:   852 ----
[CW] collect: return: 123.22059, steps: 1000.00000, total_steps: 858000.00000
[CW] train: qf1_loss: 0.10895, qf2_loss: 0.10891, policy_loss: -18.00608, policy_entropy: -6.19727, alpha: 0.00659, time: 33.49267
[CW] ---------------------------
[CW] ---- Iteration:   853 ----
[CW] collect: return: 150.45870, steps: 1000.00000, total_steps: 859000.00000
[CW] train: qf1_loss: 0.11843, qf2_loss: 0.11826, policy_loss: -18.09916, policy_entropy: -6.17796, alpha: 0.00668, time: 33.42776
[CW] ---------------------------
[CW] ---- Iteration:   854 ----
[CW] collect: return: 121.72258, steps: 1000.00000, total_steps: 860000.00000
[CW] train: qf1_loss: 0.12107, qf2_loss: 0.12088, policy_loss: -18.03546, policy_entropy: -5.79980, alpha: 0.00669, time: 33.53364
[CW] ---------------------------
[CW] ---- Iteration:   855 ----
[CW] collect: return: 114.49607, steps: 1000.00000, total_steps: 861000.00000
[CW] train: qf1_loss: 0.11758, qf2_loss: 0.11756, policy_loss: -17.87385, policy_entropy: -5.78277, alpha: 0.00660, time: 33.43935
[CW] ---------------------------
[CW] ---- Iteration:   856 ----
[CW] collect: return: 104.73907, steps: 1000.00000, total_steps: 862000.00000
[CW] train: qf1_loss: 0.12940, qf2_loss: 0.12919, policy_loss: -17.97613, policy_entropy: -5.72107, alpha: 0.00650, time: 33.83590
[CW] ---------------------------
[CW] ---- Iteration:   857 ----
[CW] collect: return: 90.04078, steps: 1000.00000, total_steps: 863000.00000
[CW] train: qf1_loss: 0.15349, qf2_loss: 0.15337, policy_loss: -17.98954, policy_entropy: -5.81970, alpha: 0.00642, time: 33.76927
[CW] ---------------------------
[CW] ---- Iteration:   858 ----
[CW] collect: return: 103.16744, steps: 1000.00000, total_steps: 864000.00000
[CW] train: qf1_loss: 0.14853, qf2_loss: 0.14835, policy_loss: -18.04220, policy_entropy: -5.90910, alpha: 0.00634, time: 33.78742
[CW] ---------------------------
[CW] ---- Iteration:   859 ----
[CW] collect: return: 145.03642, steps: 1000.00000, total_steps: 865000.00000
[CW] train: qf1_loss: 0.11535, qf2_loss: 0.11515, policy_loss: -18.03692, policy_entropy: -6.17636, alpha: 0.00639, time: 33.73883
[CW] ---------------------------
[CW] ---- Iteration:   860 ----
[CW] collect: return: 129.49505, steps: 1000.00000, total_steps: 866000.00000
[CW] train: qf1_loss: 0.11437, qf2_loss: 0.11421, policy_loss: -18.06942, policy_entropy: -6.11069, alpha: 0.00645, time: 33.73080
[CW] eval: return: 126.33640, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   861 ----
[CW] collect: return: 145.88894, steps: 1000.00000, total_steps: 867000.00000
[CW] train: qf1_loss: 0.11124, qf2_loss: 0.11110, policy_loss: -18.18348, policy_entropy: -6.18453, alpha: 0.00648, time: 34.59528
[CW] ---------------------------
[CW] ---- Iteration:   862 ----
[CW] collect: return: 128.07474, steps: 1000.00000, total_steps: 868000.00000
[CW] train: qf1_loss: 0.10687, qf2_loss: 0.10671, policy_loss: -18.00555, policy_entropy: -6.05783, alpha: 0.00654, time: 33.90000
[CW] ---------------------------
[CW] ---- Iteration:   863 ----
[CW] collect: return: 117.96300, steps: 1000.00000, total_steps: 869000.00000
[CW] train: qf1_loss: 0.10379, qf2_loss: 0.10365, policy_loss: -18.07809, policy_entropy: -5.85745, alpha: 0.00654, time: 33.78915
[CW] ---------------------------
[CW] ---- Iteration:   864 ----
[CW] collect: return: 106.97237, steps: 1000.00000, total_steps: 870000.00000
[CW] train: qf1_loss: 0.10403, qf2_loss: 0.10393, policy_loss: -18.04636, policy_entropy: -5.91644, alpha: 0.00647, time: 33.92839
[CW] ---------------------------
[CW] ---- Iteration:   865 ----
[CW] collect: return: 127.19335, steps: 1000.00000, total_steps: 871000.00000
[CW] train: qf1_loss: 0.11932, qf2_loss: 0.11920, policy_loss: -17.97585, policy_entropy: -5.94048, alpha: 0.00646, time: 33.83376
[CW] ---------------------------
[CW] ---- Iteration:   866 ----
[CW] collect: return: 130.22629, steps: 1000.00000, total_steps: 872000.00000
[CW] train: qf1_loss: 0.11158, qf2_loss: 0.11145, policy_loss: -18.08322, policy_entropy: -5.83055, alpha: 0.00642, time: 36.20609
[CW] ---------------------------
[CW] ---- Iteration:   867 ----
[CW] collect: return: 117.19124, steps: 1000.00000, total_steps: 873000.00000
[CW] train: qf1_loss: 0.13666, qf2_loss: 0.13653, policy_loss: -18.18130, policy_entropy: -5.94978, alpha: 0.00637, time: 33.86639
[CW] ---------------------------
[CW] ---- Iteration:   868 ----
[CW] collect: return: 130.99321, steps: 1000.00000, total_steps: 874000.00000
[CW] train: qf1_loss: 0.12225, qf2_loss: 0.12238, policy_loss: -18.17651, policy_entropy: -5.93888, alpha: 0.00635, time: 33.99957
[CW] ---------------------------
[CW] ---- Iteration:   869 ----
[CW] collect: return: 127.95925, steps: 1000.00000, total_steps: 875000.00000
[CW] train: qf1_loss: 0.12564, qf2_loss: 0.12545, policy_loss: -18.01666, policy_entropy: -5.81076, alpha: 0.00630, time: 33.90350
[CW] ---------------------------
[CW] ---- Iteration:   870 ----
[CW] collect: return: 127.81791, steps: 1000.00000, total_steps: 876000.00000
[CW] train: qf1_loss: 0.11685, qf2_loss: 0.11656, policy_loss: -18.16717, policy_entropy: -5.87686, alpha: 0.00624, time: 33.78353
[CW] ---------------------------
[CW] ---- Iteration:   871 ----
[CW] collect: return: 128.06191, steps: 1000.00000, total_steps: 877000.00000
[CW] train: qf1_loss: 0.10972, qf2_loss: 0.10966, policy_loss: -18.17033, policy_entropy: -5.86134, alpha: 0.00621, time: 33.72164
[CW] ---------------------------
[CW] ---- Iteration:   872 ----
[CW] collect: return: 135.57265, steps: 1000.00000, total_steps: 878000.00000
[CW] train: qf1_loss: 0.11039, qf2_loss: 0.11031, policy_loss: -18.17323, policy_entropy: -6.00863, alpha: 0.00617, time: 33.87513
[CW] ---------------------------
[CW] ---- Iteration:   873 ----
[CW] collect: return: 121.50603, steps: 1000.00000, total_steps: 879000.00000
[CW] train: qf1_loss: 0.12054, qf2_loss: 0.12016, policy_loss: -18.24133, policy_entropy: -6.07252, alpha: 0.00619, time: 33.76302
[CW] ---------------------------
[CW] ---- Iteration:   874 ----
[CW] collect: return: 115.12452, steps: 1000.00000, total_steps: 880000.00000
[CW] train: qf1_loss: 0.11690, qf2_loss: 0.11652, policy_loss: -18.14098, policy_entropy: -5.96908, alpha: 0.00621, time: 33.79265
[CW] ---------------------------
[CW] ---- Iteration:   875 ----
[CW] collect: return: 129.83518, steps: 1000.00000, total_steps: 881000.00000
[CW] train: qf1_loss: 0.11919, qf2_loss: 0.11935, policy_loss: -18.32180, policy_entropy: -6.09236, alpha: 0.00620, time: 33.75256
[CW] ---------------------------
[CW] ---- Iteration:   876 ----
[CW] collect: return: 115.46109, steps: 1000.00000, total_steps: 882000.00000
[CW] train: qf1_loss: 0.11220, qf2_loss: 0.11187, policy_loss: -18.09067, policy_entropy: -6.18243, alpha: 0.00626, time: 33.68329
[CW] ---------------------------
[CW] ---- Iteration:   877 ----
[CW] collect: return: 136.98377, steps: 1000.00000, total_steps: 883000.00000
[CW] train: qf1_loss: 0.12043, qf2_loss: 0.12012, policy_loss: -18.15017, policy_entropy: -5.99615, alpha: 0.00630, time: 33.94378
[CW] ---------------------------
[CW] ---- Iteration:   878 ----
[CW] collect: return: 136.87633, steps: 1000.00000, total_steps: 884000.00000
[CW] train: qf1_loss: 0.15145, qf2_loss: 0.15146, policy_loss: -18.27144, policy_entropy: -5.99217, alpha: 0.00632, time: 33.83056
[CW] ---------------------------
[CW] ---- Iteration:   879 ----
[CW] collect: return: 107.45818, steps: 1000.00000, total_steps: 885000.00000
[CW] train: qf1_loss: 0.13800, qf2_loss: 0.13782, policy_loss: -18.23104, policy_entropy: -5.84290, alpha: 0.00625, time: 33.81679
[CW] ---------------------------
[CW] ---- Iteration:   880 ----
[CW] collect: return: 124.39376, steps: 1000.00000, total_steps: 886000.00000
[CW] train: qf1_loss: 0.12409, qf2_loss: 0.12374, policy_loss: -18.20574, policy_entropy: -5.86822, alpha: 0.00622, time: 33.81655
[CW] eval: return: 127.84179, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   881 ----
[CW] collect: return: 124.92909, steps: 1000.00000, total_steps: 887000.00000
[CW] train: qf1_loss: 0.10985, qf2_loss: 0.10951, policy_loss: -18.20502, policy_entropy: -5.92391, alpha: 0.00619, time: 33.94056
[CW] ---------------------------
[CW] ---- Iteration:   882 ----
[CW] collect: return: 111.56221, steps: 1000.00000, total_steps: 888000.00000
[CW] train: qf1_loss: 0.11278, qf2_loss: 0.11257, policy_loss: -18.14915, policy_entropy: -5.72651, alpha: 0.00612, time: 33.97807
[CW] ---------------------------
[CW] ---- Iteration:   883 ----
[CW] collect: return: 159.85994, steps: 1000.00000, total_steps: 889000.00000
[CW] train: qf1_loss: 0.11634, qf2_loss: 0.11622, policy_loss: -18.28110, policy_entropy: -5.95605, alpha: 0.00606, time: 34.05579
[CW] ---------------------------
[CW] ---- Iteration:   884 ----
[CW] collect: return: 137.96137, steps: 1000.00000, total_steps: 890000.00000
[CW] train: qf1_loss: 0.11805, qf2_loss: 0.11764, policy_loss: -18.29607, policy_entropy: -6.24203, alpha: 0.00610, time: 34.01845
[CW] ---------------------------
[CW] ---- Iteration:   885 ----
[CW] collect: return: 128.30748, steps: 1000.00000, total_steps: 891000.00000
[CW] train: qf1_loss: 0.12723, qf2_loss: 0.12693, policy_loss: -18.24039, policy_entropy: -5.95551, alpha: 0.00612, time: 33.77434
[CW] ---------------------------
[CW] ---- Iteration:   886 ----
[CW] collect: return: 151.32057, steps: 1000.00000, total_steps: 892000.00000
[CW] train: qf1_loss: 0.11637, qf2_loss: 0.11624, policy_loss: -18.27613, policy_entropy: -6.01995, alpha: 0.00614, time: 33.84832
[CW] ---------------------------
[CW] ---- Iteration:   887 ----
[CW] collect: return: 126.28917, steps: 1000.00000, total_steps: 893000.00000
[CW] train: qf1_loss: 0.14893, qf2_loss: 0.14851, policy_loss: -18.36839, policy_entropy: -5.87984, alpha: 0.00610, time: 33.81831
[CW] ---------------------------
[CW] ---- Iteration:   888 ----
[CW] collect: return: 131.86543, steps: 1000.00000, total_steps: 894000.00000
[CW] train: qf1_loss: 0.11898, qf2_loss: 0.11881, policy_loss: -18.40706, policy_entropy: -5.98335, alpha: 0.00608, time: 33.83165
[CW] ---------------------------
[CW] ---- Iteration:   889 ----
[CW] collect: return: 134.34786, steps: 1000.00000, total_steps: 895000.00000
[CW] train: qf1_loss: 0.13296, qf2_loss: 0.13293, policy_loss: -18.20224, policy_entropy: -5.95660, alpha: 0.00607, time: 33.83253
[CW] ---------------------------
[CW] ---- Iteration:   890 ----
[CW] collect: return: 130.20171, steps: 1000.00000, total_steps: 896000.00000
[CW] train: qf1_loss: 0.12675, qf2_loss: 0.12607, policy_loss: -18.29010, policy_entropy: -6.26802, alpha: 0.00609, time: 34.03717
[CW] ---------------------------
[CW] ---- Iteration:   891 ----
[CW] collect: return: 144.64773, steps: 1000.00000, total_steps: 897000.00000
[CW] train: qf1_loss: 0.11473, qf2_loss: 0.11442, policy_loss: -18.27099, policy_entropy: -6.23158, alpha: 0.00620, time: 33.87482
[CW] ---------------------------
[CW] ---- Iteration:   892 ----
[CW] collect: return: 127.57216, steps: 1000.00000, total_steps: 898000.00000
[CW] train: qf1_loss: 0.11934, qf2_loss: 0.11917, policy_loss: -18.19094, policy_entropy: -6.23611, alpha: 0.00629, time: 33.83207
[CW] ---------------------------
[CW] ---- Iteration:   893 ----
[CW] collect: return: 125.15156, steps: 1000.00000, total_steps: 899000.00000
[CW] train: qf1_loss: 0.11681, qf2_loss: 0.11646, policy_loss: -18.26258, policy_entropy: -6.11779, alpha: 0.00637, time: 33.90391
[CW] ---------------------------
[CW] ---- Iteration:   894 ----
[CW] collect: return: 124.25072, steps: 1000.00000, total_steps: 900000.00000
[CW] train: qf1_loss: 0.11396, qf2_loss: 0.11394, policy_loss: -18.26130, policy_entropy: -5.90666, alpha: 0.00635, time: 34.00796
[CW] ---------------------------
[CW] ---- Iteration:   895 ----
[CW] collect: return: 116.36717, steps: 1000.00000, total_steps: 901000.00000
[CW] train: qf1_loss: 0.11853, qf2_loss: 0.11838, policy_loss: -18.37231, policy_entropy: -5.99472, alpha: 0.00633, time: 33.77798
[CW] ---------------------------
[CW] ---- Iteration:   896 ----
[CW] collect: return: 124.34575, steps: 1000.00000, total_steps: 902000.00000
[CW] train: qf1_loss: 0.12507, qf2_loss: 0.12495, policy_loss: -18.40561, policy_entropy: -6.08982, alpha: 0.00635, time: 33.87501
[CW] ---------------------------
[CW] ---- Iteration:   897 ----
[CW] collect: return: 140.72132, steps: 1000.00000, total_steps: 903000.00000
[CW] train: qf1_loss: 0.11991, qf2_loss: 0.11970, policy_loss: -18.34314, policy_entropy: -6.15867, alpha: 0.00640, time: 33.88035
[CW] ---------------------------
[CW] ---- Iteration:   898 ----
[CW] collect: return: 151.56067, steps: 1000.00000, total_steps: 904000.00000
[CW] train: qf1_loss: 0.11940, qf2_loss: 0.11920, policy_loss: -18.40631, policy_entropy: -6.26839, alpha: 0.00648, time: 33.82881
[CW] ---------------------------
[CW] ---- Iteration:   899 ----
[CW] collect: return: 134.56523, steps: 1000.00000, total_steps: 905000.00000
[CW] train: qf1_loss: 0.13762, qf2_loss: 0.13722, policy_loss: -18.34833, policy_entropy: -6.15257, alpha: 0.00655, time: 33.88923
[CW] ---------------------------
[CW] ---- Iteration:   900 ----
[CW] collect: return: 118.78306, steps: 1000.00000, total_steps: 906000.00000
[CW] train: qf1_loss: 0.17385, qf2_loss: 0.17332, policy_loss: -18.29320, policy_entropy: -5.74083, alpha: 0.00657, time: 36.30782
[CW] eval: return: 109.82542, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   901 ----
[CW] collect: return: 107.07664, steps: 1000.00000, total_steps: 907000.00000
[CW] train: qf1_loss: 0.15011, qf2_loss: 0.14952, policy_loss: -18.44865, policy_entropy: -5.93765, alpha: 0.00648, time: 33.90719
[CW] ---------------------------
[CW] ---- Iteration:   902 ----
[CW] collect: return: 129.05037, steps: 1000.00000, total_steps: 908000.00000
[CW] train: qf1_loss: 0.11161, qf2_loss: 0.11140, policy_loss: -18.47732, policy_entropy: -5.97431, alpha: 0.00647, time: 33.93287
[CW] ---------------------------
[CW] ---- Iteration:   903 ----
[CW] collect: return: 127.58985, steps: 1000.00000, total_steps: 909000.00000
[CW] train: qf1_loss: 0.10856, qf2_loss: 0.10821, policy_loss: -18.45472, policy_entropy: -6.00930, alpha: 0.00647, time: 33.85663
[CW] ---------------------------
[CW] ---- Iteration:   904 ----
[CW] collect: return: 135.89778, steps: 1000.00000, total_steps: 910000.00000
[CW] train: qf1_loss: 0.11273, qf2_loss: 0.11245, policy_loss: -18.34837, policy_entropy: -6.02883, alpha: 0.00648, time: 33.89835
[CW] ---------------------------
[CW] ---- Iteration:   905 ----
[CW] collect: return: 137.92342, steps: 1000.00000, total_steps: 911000.00000
[CW] train: qf1_loss: 0.11329, qf2_loss: 0.11304, policy_loss: -18.37841, policy_entropy: -5.97060, alpha: 0.00646, time: 34.01705
[CW] ---------------------------
[CW] ---- Iteration:   906 ----
[CW] collect: return: 155.43902, steps: 1000.00000, total_steps: 912000.00000
[CW] train: qf1_loss: 0.10827, qf2_loss: 0.10807, policy_loss: -18.34224, policy_entropy: -6.09155, alpha: 0.00648, time: 33.91922
[CW] ---------------------------
[CW] ---- Iteration:   907 ----
[CW] collect: return: 148.94497, steps: 1000.00000, total_steps: 913000.00000
[CW] train: qf1_loss: 0.11159, qf2_loss: 0.11140, policy_loss: -18.49438, policy_entropy: -6.04036, alpha: 0.00650, time: 33.87037
[CW] ---------------------------
[CW] ---- Iteration:   908 ----
[CW] collect: return: 120.14561, steps: 1000.00000, total_steps: 914000.00000
[CW] train: qf1_loss: 0.11076, qf2_loss: 0.11060, policy_loss: -18.44679, policy_entropy: -5.86229, alpha: 0.00650, time: 34.00443
[CW] ---------------------------
[CW] ---- Iteration:   909 ----
[CW] collect: return: 136.16355, steps: 1000.00000, total_steps: 915000.00000
[CW] train: qf1_loss: 0.12873, qf2_loss: 0.12839, policy_loss: -18.49000, policy_entropy: -5.97653, alpha: 0.00647, time: 33.84083
[CW] ---------------------------
[CW] ---- Iteration:   910 ----
[CW] collect: return: 132.16447, steps: 1000.00000, total_steps: 916000.00000
[CW] train: qf1_loss: 0.12494, qf2_loss: 0.12487, policy_loss: -18.33629, policy_entropy: -5.75824, alpha: 0.00641, time: 34.00353
[CW] ---------------------------
[CW] ---- Iteration:   911 ----
[CW] collect: return: 106.08310, steps: 1000.00000, total_steps: 917000.00000
[CW] train: qf1_loss: 0.13749, qf2_loss: 0.13729, policy_loss: -18.59302, policy_entropy: -6.05845, alpha: 0.00635, time: 33.93957
[CW] ---------------------------
[CW] ---- Iteration:   912 ----
[CW] collect: return: 147.13664, steps: 1000.00000, total_steps: 918000.00000
[CW] train: qf1_loss: 0.14928, qf2_loss: 0.14876, policy_loss: -18.42497, policy_entropy: -5.77429, alpha: 0.00634, time: 33.78655
[CW] ---------------------------
[CW] ---- Iteration:   913 ----
[CW] collect: return: 115.68402, steps: 1000.00000, total_steps: 919000.00000
[CW] train: qf1_loss: 0.12768, qf2_loss: 0.12716, policy_loss: -18.46840, policy_entropy: -6.00181, alpha: 0.00629, time: 33.98945
[CW] ---------------------------
[CW] ---- Iteration:   914 ----
[CW] collect: return: 112.34123, steps: 1000.00000, total_steps: 920000.00000
[CW] train: qf1_loss: 0.12099, qf2_loss: 0.12075, policy_loss: -18.43591, policy_entropy: -5.95771, alpha: 0.00629, time: 33.90001
[CW] ---------------------------
[CW] ---- Iteration:   915 ----
[CW] collect: return: 132.45276, steps: 1000.00000, total_steps: 921000.00000
[CW] train: qf1_loss: 0.11341, qf2_loss: 0.11312, policy_loss: -18.45787, policy_entropy: -6.12166, alpha: 0.00629, time: 33.95976
[CW] ---------------------------
[CW] ---- Iteration:   916 ----
[CW] collect: return: 121.77709, steps: 1000.00000, total_steps: 922000.00000
[CW] train: qf1_loss: 0.11304, qf2_loss: 0.11295, policy_loss: -18.48727, policy_entropy: -6.21026, alpha: 0.00636, time: 33.90539
[CW] ---------------------------
[CW] ---- Iteration:   917 ----
[CW] collect: return: 120.65652, steps: 1000.00000, total_steps: 923000.00000
[CW] train: qf1_loss: 0.10971, qf2_loss: 0.10940, policy_loss: -18.41407, policy_entropy: -6.12789, alpha: 0.00644, time: 33.86687
[CW] ---------------------------
[CW] ---- Iteration:   918 ----
[CW] collect: return: 116.96341, steps: 1000.00000, total_steps: 924000.00000
[CW] train: qf1_loss: 0.11748, qf2_loss: 0.11725, policy_loss: -18.48856, policy_entropy: -5.97116, alpha: 0.00646, time: 34.44162
[CW] ---------------------------
[CW] ---- Iteration:   919 ----
[CW] collect: return: 110.88628, steps: 1000.00000, total_steps: 925000.00000
[CW] train: qf1_loss: 0.14292, qf2_loss: 0.14207, policy_loss: -18.46989, policy_entropy: -6.02076, alpha: 0.00645, time: 33.95718
[CW] ---------------------------
[CW] ---- Iteration:   920 ----
[CW] collect: return: 145.76361, steps: 1000.00000, total_steps: 926000.00000
[CW] train: qf1_loss: 0.12660, qf2_loss: 0.12634, policy_loss: -18.63347, policy_entropy: -6.03230, alpha: 0.00647, time: 34.35783
[CW] eval: return: 121.49529, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   921 ----
[CW] collect: return: 143.65044, steps: 1000.00000, total_steps: 927000.00000
[CW] train: qf1_loss: 0.14768, qf2_loss: 0.14751, policy_loss: -18.51385, policy_entropy: -5.98895, alpha: 0.00646, time: 33.40485
[CW] ---------------------------
[CW] ---- Iteration:   922 ----
[CW] collect: return: 139.38334, steps: 1000.00000, total_steps: 928000.00000
[CW] train: qf1_loss: 0.14764, qf2_loss: 0.14708, policy_loss: -18.48411, policy_entropy: -6.15874, alpha: 0.00648, time: 33.23502
[CW] ---------------------------
[CW] ---- Iteration:   923 ----
[CW] collect: return: 110.15402, steps: 1000.00000, total_steps: 929000.00000
[CW] train: qf1_loss: 0.12652, qf2_loss: 0.12618, policy_loss: -18.49253, policy_entropy: -6.09431, alpha: 0.00654, time: 33.18740
[CW] ---------------------------
[CW] ---- Iteration:   924 ----
[CW] collect: return: 134.90148, steps: 1000.00000, total_steps: 930000.00000
[CW] train: qf1_loss: 0.12703, qf2_loss: 0.12684, policy_loss: -18.53316, policy_entropy: -6.13124, alpha: 0.00657, time: 33.32148
[CW] ---------------------------
[CW] ---- Iteration:   925 ----
[CW] collect: return: 141.49987, steps: 1000.00000, total_steps: 931000.00000
[CW] train: qf1_loss: 0.12466, qf2_loss: 0.12439, policy_loss: -18.47666, policy_entropy: -6.09023, alpha: 0.00663, time: 33.33745
[CW] ---------------------------
[CW] ---- Iteration:   926 ----
[CW] collect: return: 126.82907, steps: 1000.00000, total_steps: 932000.00000
[CW] train: qf1_loss: 0.11263, qf2_loss: 0.11231, policy_loss: -18.51106, policy_entropy: -5.99561, alpha: 0.00666, time: 33.87222
[CW] ---------------------------
[CW] ---- Iteration:   927 ----
[CW] collect: return: 109.21213, steps: 1000.00000, total_steps: 933000.00000
[CW] train: qf1_loss: 0.11336, qf2_loss: 0.11321, policy_loss: -18.52020, policy_entropy: -6.00958, alpha: 0.00666, time: 33.67569
[CW] ---------------------------
[CW] ---- Iteration:   928 ----
[CW] collect: return: 122.52233, steps: 1000.00000, total_steps: 934000.00000
[CW] train: qf1_loss: 0.12748, qf2_loss: 0.12739, policy_loss: -18.56662, policy_entropy: -6.01981, alpha: 0.00665, time: 33.52835
[CW] ---------------------------
[CW] ---- Iteration:   929 ----
[CW] collect: return: 140.46719, steps: 1000.00000, total_steps: 935000.00000
[CW] train: qf1_loss: 0.11555, qf2_loss: 0.11525, policy_loss: -18.53730, policy_entropy: -5.92559, alpha: 0.00666, time: 33.58249
[CW] ---------------------------
[CW] ---- Iteration:   930 ----
[CW] collect: return: 116.68662, steps: 1000.00000, total_steps: 936000.00000
[CW] train: qf1_loss: 0.11660, qf2_loss: 0.11638, policy_loss: -18.68217, policy_entropy: -6.02731, alpha: 0.00665, time: 33.51493
[CW] ---------------------------
[CW] ---- Iteration:   931 ----
[CW] collect: return: 35.91568, steps: 1000.00000, total_steps: 937000.00000
[CW] train: qf1_loss: 0.11531, qf2_loss: 0.11517, policy_loss: -18.60941, policy_entropy: -6.10851, alpha: 0.00666, time: 33.62690
[CW] ---------------------------
[CW] ---- Iteration:   932 ----
[CW] collect: return: 130.38793, steps: 1000.00000, total_steps: 938000.00000
[CW] train: qf1_loss: 0.11631, qf2_loss: 0.11612, policy_loss: -18.58423, policy_entropy: -6.11902, alpha: 0.00673, time: 33.55896
[CW] ---------------------------
[CW] ---- Iteration:   933 ----
[CW] collect: return: 128.47479, steps: 1000.00000, total_steps: 939000.00000
[CW] train: qf1_loss: 0.12844, qf2_loss: 0.12812, policy_loss: -18.64885, policy_entropy: -5.88518, alpha: 0.00673, time: 33.53838
[CW] ---------------------------
[CW] ---- Iteration:   934 ----
[CW] collect: return: 131.63616, steps: 1000.00000, total_steps: 940000.00000
[CW] train: qf1_loss: 0.12889, qf2_loss: 0.12885, policy_loss: -18.74633, policy_entropy: -5.94958, alpha: 0.00668, time: 33.96200
[CW] ---------------------------
[CW] ---- Iteration:   935 ----
[CW] collect: return: 126.95245, steps: 1000.00000, total_steps: 941000.00000
[CW] train: qf1_loss: 0.14976, qf2_loss: 0.14982, policy_loss: -18.60893, policy_entropy: -5.86395, alpha: 0.00664, time: 33.95832
[CW] ---------------------------
[CW] ---- Iteration:   936 ----
[CW] collect: return: 124.54601, steps: 1000.00000, total_steps: 942000.00000
[CW] train: qf1_loss: 0.12356, qf2_loss: 0.12306, policy_loss: -18.69289, policy_entropy: -6.14583, alpha: 0.00664, time: 33.97192
[CW] ---------------------------
[CW] ---- Iteration:   937 ----
[CW] collect: return: 125.94987, steps: 1000.00000, total_steps: 943000.00000
[CW] train: qf1_loss: 0.12770, qf2_loss: 0.12735, policy_loss: -18.58291, policy_entropy: -6.01783, alpha: 0.00670, time: 34.05461
[CW] ---------------------------
[CW] ---- Iteration:   938 ----
[CW] collect: return: 147.53909, steps: 1000.00000, total_steps: 944000.00000
[CW] train: qf1_loss: 0.12571, qf2_loss: 0.12527, policy_loss: -18.61946, policy_entropy: -5.93991, alpha: 0.00668, time: 33.92475
[CW] ---------------------------
[CW] ---- Iteration:   939 ----
[CW] collect: return: 119.16568, steps: 1000.00000, total_steps: 945000.00000
[CW] train: qf1_loss: 0.13250, qf2_loss: 0.13236, policy_loss: -18.63925, policy_entropy: -5.84528, alpha: 0.00663, time: 34.14146
[CW] ---------------------------
[CW] ---- Iteration:   940 ----
[CW] collect: return: 130.80323, steps: 1000.00000, total_steps: 946000.00000
[CW] train: qf1_loss: 0.11904, qf2_loss: 0.11876, policy_loss: -18.74246, policy_entropy: -5.94102, alpha: 0.00659, time: 34.05367
[CW] eval: return: 123.04973, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   941 ----
[CW] collect: return: 138.34653, steps: 1000.00000, total_steps: 947000.00000
[CW] train: qf1_loss: 0.11417, qf2_loss: 0.11391, policy_loss: -18.53078, policy_entropy: -6.04199, alpha: 0.00657, time: 34.02238
[CW] ---------------------------
[CW] ---- Iteration:   942 ----
[CW] collect: return: 87.20694, steps: 1000.00000, total_steps: 948000.00000
[CW] train: qf1_loss: 0.12006, qf2_loss: 0.11970, policy_loss: -18.70976, policy_entropy: -6.06182, alpha: 0.00661, time: 33.98452
[CW] ---------------------------
[CW] ---- Iteration:   943 ----
[CW] collect: return: 120.10920, steps: 1000.00000, total_steps: 949000.00000
[CW] train: qf1_loss: 0.12963, qf2_loss: 0.12938, policy_loss: -18.62845, policy_entropy: -5.88196, alpha: 0.00659, time: 33.96068
[CW] ---------------------------
[CW] ---- Iteration:   944 ----
[CW] collect: return: 112.76975, steps: 1000.00000, total_steps: 950000.00000
[CW] train: qf1_loss: 0.12997, qf2_loss: 0.12976, policy_loss: -18.69930, policy_entropy: -5.99793, alpha: 0.00657, time: 33.96042
[CW] ---------------------------
[CW] ---- Iteration:   945 ----
[CW] collect: return: 129.76046, steps: 1000.00000, total_steps: 951000.00000
[CW] train: qf1_loss: 0.12464, qf2_loss: 0.12441, policy_loss: -18.69472, policy_entropy: -6.19017, alpha: 0.00661, time: 33.99435
[CW] ---------------------------
[CW] ---- Iteration:   946 ----
[CW] collect: return: 120.71945, steps: 1000.00000, total_steps: 952000.00000
[CW] train: qf1_loss: 0.11413, qf2_loss: 0.11396, policy_loss: -18.87308, policy_entropy: -5.89128, alpha: 0.00662, time: 33.78912
[CW] ---------------------------
[CW] ---- Iteration:   947 ----
[CW] collect: return: 130.38534, steps: 1000.00000, total_steps: 953000.00000
[CW] train: qf1_loss: 0.12312, qf2_loss: 0.12292, policy_loss: -18.73931, policy_entropy: -6.05184, alpha: 0.00660, time: 33.45328
[CW] ---------------------------
[CW] ---- Iteration:   948 ----
[CW] collect: return: 125.26310, steps: 1000.00000, total_steps: 954000.00000
[CW] train: qf1_loss: 0.12813, qf2_loss: 0.12791, policy_loss: -18.55999, policy_entropy: -5.79956, alpha: 0.00659, time: 33.17215
[CW] ---------------------------
[CW] ---- Iteration:   949 ----
[CW] collect: return: 95.16439, steps: 1000.00000, total_steps: 955000.00000
[CW] train: qf1_loss: 0.13771, qf2_loss: 0.13743, policy_loss: -18.68143, policy_entropy: -5.75736, alpha: 0.00649, time: 33.23434
[CW] ---------------------------
[CW] ---- Iteration:   950 ----
[CW] collect: return: 145.58629, steps: 1000.00000, total_steps: 956000.00000
[CW] train: qf1_loss: 0.12405, qf2_loss: 0.12395, policy_loss: -18.78123, policy_entropy: -5.94052, alpha: 0.00643, time: 33.09088
[CW] ---------------------------
[CW] ---- Iteration:   951 ----
[CW] collect: return: 132.72190, steps: 1000.00000, total_steps: 957000.00000
[CW] train: qf1_loss: 0.11699, qf2_loss: 0.11677, policy_loss: -18.67349, policy_entropy: -6.11271, alpha: 0.00644, time: 33.18072
[CW] ---------------------------
[CW] ---- Iteration:   952 ----
[CW] collect: return: 117.78756, steps: 1000.00000, total_steps: 958000.00000
[CW] train: qf1_loss: 0.12175, qf2_loss: 0.12154, policy_loss: -18.72949, policy_entropy: -6.32076, alpha: 0.00651, time: 33.15529
[CW] ---------------------------
[CW] ---- Iteration:   953 ----
[CW] collect: return: 133.03307, steps: 1000.00000, total_steps: 959000.00000
[CW] train: qf1_loss: 0.12353, qf2_loss: 0.12335, policy_loss: -18.67331, policy_entropy: -6.05489, alpha: 0.00661, time: 33.11378
[CW] ---------------------------
[CW] ---- Iteration:   954 ----
[CW] collect: return: 124.97613, steps: 1000.00000, total_steps: 960000.00000
[CW] train: qf1_loss: 0.12317, qf2_loss: 0.12307, policy_loss: -18.74442, policy_entropy: -6.25043, alpha: 0.00664, time: 32.96335
[CW] ---------------------------
[CW] ---- Iteration:   955 ----
[CW] collect: return: 115.20869, steps: 1000.00000, total_steps: 961000.00000
[CW] train: qf1_loss: 0.12481, qf2_loss: 0.12436, policy_loss: -18.73558, policy_entropy: -6.03173, alpha: 0.00673, time: 33.12552
[CW] ---------------------------
[CW] ---- Iteration:   956 ----
[CW] collect: return: 139.72556, steps: 1000.00000, total_steps: 962000.00000
[CW] train: qf1_loss: 0.14259, qf2_loss: 0.14227, policy_loss: -18.72741, policy_entropy: -5.93761, alpha: 0.00672, time: 33.04691
[CW] ---------------------------
[CW] ---- Iteration:   957 ----
[CW] collect: return: 84.22972, steps: 1000.00000, total_steps: 963000.00000
[CW] train: qf1_loss: 0.12697, qf2_loss: 0.12653, policy_loss: -18.60399, policy_entropy: -5.87413, alpha: 0.00668, time: 33.09550
[CW] ---------------------------
[CW] ---- Iteration:   958 ----
[CW] collect: return: 136.95955, steps: 1000.00000, total_steps: 964000.00000
[CW] train: qf1_loss: 0.12196, qf2_loss: 0.12202, policy_loss: -18.75140, policy_entropy: -6.10958, alpha: 0.00667, time: 33.25177
[CW] ---------------------------
[CW] ---- Iteration:   959 ----
[CW] collect: return: 115.72963, steps: 1000.00000, total_steps: 965000.00000
[CW] train: qf1_loss: 0.12657, qf2_loss: 0.12640, policy_loss: -18.76204, policy_entropy: -5.85535, alpha: 0.00666, time: 33.37378
[CW] ---------------------------
[CW] ---- Iteration:   960 ----
[CW] collect: return: 156.36868, steps: 1000.00000, total_steps: 966000.00000
[CW] train: qf1_loss: 0.13391, qf2_loss: 0.13406, policy_loss: -18.80531, policy_entropy: -5.94124, alpha: 0.00663, time: 33.43801
[CW] eval: return: 119.12466, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   961 ----
[CW] collect: return: 137.73850, steps: 1000.00000, total_steps: 967000.00000
[CW] train: qf1_loss: 0.13221, qf2_loss: 0.13177, policy_loss: -18.74519, policy_entropy: -5.99061, alpha: 0.00659, time: 34.19382
[CW] ---------------------------
[CW] ---- Iteration:   962 ----
[CW] collect: return: 139.61433, steps: 1000.00000, total_steps: 968000.00000
[CW] train: qf1_loss: 0.12148, qf2_loss: 0.12116, policy_loss: -18.78201, policy_entropy: -6.05439, alpha: 0.00662, time: 34.10460
[CW] ---------------------------
[CW] ---- Iteration:   963 ----
[CW] collect: return: 132.97986, steps: 1000.00000, total_steps: 969000.00000
[CW] train: qf1_loss: 0.12042, qf2_loss: 0.12024, policy_loss: -18.72470, policy_entropy: -6.00872, alpha: 0.00662, time: 36.08243
[CW] ---------------------------
[CW] ---- Iteration:   964 ----
[CW] collect: return: 142.74890, steps: 1000.00000, total_steps: 970000.00000
[CW] train: qf1_loss: 0.12793, qf2_loss: 0.12769, policy_loss: -18.88209, policy_entropy: -6.12428, alpha: 0.00666, time: 33.98562
[CW] ---------------------------
[CW] ---- Iteration:   965 ----
[CW] collect: return: 141.26237, steps: 1000.00000, total_steps: 971000.00000
[CW] train: qf1_loss: 0.11798, qf2_loss: 0.11781, policy_loss: -18.89622, policy_entropy: -6.10066, alpha: 0.00670, time: 34.07687
[CW] ---------------------------
[CW] ---- Iteration:   966 ----
[CW] collect: return: 106.04901, steps: 1000.00000, total_steps: 972000.00000
[CW] train: qf1_loss: 0.11367, qf2_loss: 0.11345, policy_loss: -18.84523, policy_entropy: -5.95985, alpha: 0.00673, time: 34.07937
[CW] ---------------------------
[CW] ---- Iteration:   967 ----
[CW] collect: return: 163.68786, steps: 1000.00000, total_steps: 973000.00000
[CW] train: qf1_loss: 0.11708, qf2_loss: 0.11698, policy_loss: -19.00574, policy_entropy: -6.04001, alpha: 0.00670, time: 34.12968
[CW] ---------------------------
[CW] ---- Iteration:   968 ----
[CW] collect: return: 136.47960, steps: 1000.00000, total_steps: 974000.00000
[CW] train: qf1_loss: 0.11882, qf2_loss: 0.11843, policy_loss: -18.83917, policy_entropy: -6.19221, alpha: 0.00676, time: 34.06983
[CW] ---------------------------
[CW] ---- Iteration:   969 ----
[CW] collect: return: 152.30218, steps: 1000.00000, total_steps: 975000.00000
[CW] train: qf1_loss: 0.12285, qf2_loss: 0.12267, policy_loss: -18.92971, policy_entropy: -6.09762, alpha: 0.00683, time: 36.03250
[CW] ---------------------------
[CW] ---- Iteration:   970 ----
[CW] collect: return: 128.15697, steps: 1000.00000, total_steps: 976000.00000
[CW] train: qf1_loss: 0.11825, qf2_loss: 0.11824, policy_loss: -18.84170, policy_entropy: -5.96336, alpha: 0.00685, time: 33.96027
[CW] ---------------------------
[CW] ---- Iteration:   971 ----
[CW] collect: return: 23.67293, steps: 1000.00000, total_steps: 977000.00000
[CW] train: qf1_loss: 0.13978, qf2_loss: 0.13949, policy_loss: -18.79304, policy_entropy: -5.89299, alpha: 0.00680, time: 33.94368
[CW] ---------------------------
[CW] ---- Iteration:   972 ----
[CW] collect: return: 133.79858, steps: 1000.00000, total_steps: 978000.00000
[CW] train: qf1_loss: 0.15345, qf2_loss: 0.15346, policy_loss: -18.87673, policy_entropy: -5.99077, alpha: 0.00678, time: 33.80015
[CW] ---------------------------
[CW] ---- Iteration:   973 ----
[CW] collect: return: 131.56848, steps: 1000.00000, total_steps: 979000.00000
[CW] train: qf1_loss: 0.14226, qf2_loss: 0.14192, policy_loss: -18.98532, policy_entropy: -6.08007, alpha: 0.00680, time: 33.68829
[CW] ---------------------------
[CW] ---- Iteration:   974 ----
[CW] collect: return: 106.80600, steps: 1000.00000, total_steps: 980000.00000
[CW] train: qf1_loss: 0.12814, qf2_loss: 0.12802, policy_loss: -18.93104, policy_entropy: -6.17934, alpha: 0.00685, time: 36.60865
[CW] ---------------------------
[CW] ---- Iteration:   975 ----
[CW] collect: return: 125.84368, steps: 1000.00000, total_steps: 981000.00000
[CW] train: qf1_loss: 0.12771, qf2_loss: 0.12739, policy_loss: -18.84190, policy_entropy: -6.05771, alpha: 0.00692, time: 33.90427
[CW] ---------------------------
[CW] ---- Iteration:   976 ----
[CW] collect: return: 142.14436, steps: 1000.00000, total_steps: 982000.00000
[CW] train: qf1_loss: 0.12740, qf2_loss: 0.12747, policy_loss: -18.92667, policy_entropy: -5.85916, alpha: 0.00688, time: 33.97126
[CW] ---------------------------
[CW] ---- Iteration:   977 ----
[CW] collect: return: 132.23350, steps: 1000.00000, total_steps: 983000.00000
[CW] train: qf1_loss: 0.11779, qf2_loss: 0.11777, policy_loss: -18.90519, policy_entropy: -5.90029, alpha: 0.00685, time: 33.96954
[CW] ---------------------------
[CW] ---- Iteration:   978 ----
[CW] collect: return: 25.90386, steps: 1000.00000, total_steps: 984000.00000
[CW] train: qf1_loss: 0.12748, qf2_loss: 0.12712, policy_loss: -19.03363, policy_entropy: -5.89626, alpha: 0.00678, time: 34.08693
[CW] ---------------------------
[CW] ---- Iteration:   979 ----
[CW] collect: return: 135.92041, steps: 1000.00000, total_steps: 985000.00000
[CW] train: qf1_loss: 0.14484, qf2_loss: 0.14437, policy_loss: -18.83920, policy_entropy: -5.78257, alpha: 0.00675, time: 33.96025
[CW] ---------------------------
[CW] ---- Iteration:   980 ----
[CW] collect: return: 106.79636, steps: 1000.00000, total_steps: 986000.00000
[CW] train: qf1_loss: 0.12830, qf2_loss: 0.12789, policy_loss: -18.98270, policy_entropy: -5.89578, alpha: 0.00666, time: 33.95559
[CW] eval: return: 129.25836, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   981 ----
[CW] collect: return: 129.22962, steps: 1000.00000, total_steps: 987000.00000
[CW] train: qf1_loss: 0.11808, qf2_loss: 0.11780, policy_loss: -18.90127, policy_entropy: -5.98785, alpha: 0.00663, time: 33.95205
[CW] ---------------------------
[CW] ---- Iteration:   982 ----
[CW] collect: return: 139.57623, steps: 1000.00000, total_steps: 988000.00000
[CW] train: qf1_loss: 0.11728, qf2_loss: 0.11726, policy_loss: -18.78120, policy_entropy: -5.88291, alpha: 0.00661, time: 34.01275
[CW] ---------------------------
[CW] ---- Iteration:   983 ----
[CW] collect: return: 140.90985, steps: 1000.00000, total_steps: 989000.00000
[CW] train: qf1_loss: 0.12301, qf2_loss: 0.12290, policy_loss: -18.91861, policy_entropy: -5.88453, alpha: 0.00656, time: 34.08112
[CW] ---------------------------
[CW] ---- Iteration:   984 ----
[CW] collect: return: 135.56682, steps: 1000.00000, total_steps: 990000.00000
[CW] train: qf1_loss: 0.13506, qf2_loss: 0.13417, policy_loss: -18.97967, policy_entropy: -5.88687, alpha: 0.00650, time: 34.00665
[CW] ---------------------------
[CW] ---- Iteration:   985 ----
[CW] collect: return: 120.59200, steps: 1000.00000, total_steps: 991000.00000
[CW] train: qf1_loss: 0.12646, qf2_loss: 0.12619, policy_loss: -18.89905, policy_entropy: -6.09040, alpha: 0.00650, time: 34.01126
[CW] ---------------------------
[CW] ---- Iteration:   986 ----
[CW] collect: return: 137.76271, steps: 1000.00000, total_steps: 992000.00000
[CW] train: qf1_loss: 0.12836, qf2_loss: 0.12822, policy_loss: -18.81761, policy_entropy: -6.15719, alpha: 0.00654, time: 34.16680
[CW] ---------------------------
[CW] ---- Iteration:   987 ----
[CW] collect: return: 130.91289, steps: 1000.00000, total_steps: 993000.00000
[CW] train: qf1_loss: 0.12798, qf2_loss: 0.12792, policy_loss: -18.97311, policy_entropy: -6.23191, alpha: 0.00665, time: 34.09124
[CW] ---------------------------
[CW] ---- Iteration:   988 ----
[CW] collect: return: 123.78781, steps: 1000.00000, total_steps: 994000.00000
[CW] train: qf1_loss: 0.12437, qf2_loss: 0.12430, policy_loss: -19.01467, policy_entropy: -6.11357, alpha: 0.00670, time: 34.45062
[CW] ---------------------------
[CW] ---- Iteration:   989 ----
[CW] collect: return: 149.49527, steps: 1000.00000, total_steps: 995000.00000
[CW] train: qf1_loss: 0.11424, qf2_loss: 0.11413, policy_loss: -18.94389, policy_entropy: -6.08270, alpha: 0.00676, time: 33.94556
[CW] ---------------------------
[CW] ---- Iteration:   990 ----
[CW] collect: return: 119.75045, steps: 1000.00000, total_steps: 996000.00000
[CW] train: qf1_loss: 0.11909, qf2_loss: 0.11909, policy_loss: -19.05310, policy_entropy: -6.04166, alpha: 0.00677, time: 34.00269
[CW] ---------------------------
[CW] ---- Iteration:   991 ----
[CW] collect: return: 102.26711, steps: 1000.00000, total_steps: 997000.00000
[CW] train: qf1_loss: 0.12863, qf2_loss: 0.12849, policy_loss: -18.92908, policy_entropy: -6.12401, alpha: 0.00680, time: 34.03604
[CW] ---------------------------
[CW] ---- Iteration:   992 ----
[CW] collect: return: 145.50067, steps: 1000.00000, total_steps: 998000.00000
[CW] train: qf1_loss: 0.14413, qf2_loss: 0.14406, policy_loss: -18.97973, policy_entropy: -6.12811, alpha: 0.00686, time: 34.05869
[CW] ---------------------------
[CW] ---- Iteration:   993 ----
[CW] collect: return: 23.42763, steps: 1000.00000, total_steps: 999000.00000
[CW] train: qf1_loss: 0.12595, qf2_loss: 0.12569, policy_loss: -18.97273, policy_entropy: -6.00670, alpha: 0.00690, time: 34.13230
[CW] ---------------------------
[CW] ---- Iteration:   994 ----
[CW] collect: return: 125.15397, steps: 1000.00000, total_steps: 1000000.00000
[CW] train: qf1_loss: 0.12460, qf2_loss: 0.12429, policy_loss: -19.06221, policy_entropy: -5.98234, alpha: 0.00689, time: 34.13701
[CW] ---------------------------
[CW] ---- Iteration:   995 ----
[CW] collect: return: 120.17655, steps: 1000.00000, total_steps: 1001000.00000
[CW] train: qf1_loss: 0.12573, qf2_loss: 0.12563, policy_loss: -19.08973, policy_entropy: -5.93954, alpha: 0.00687, time: 34.14860
[CW] ---------------------------
[CW] ---- Iteration:   996 ----
[CW] collect: return: 135.23980, steps: 1000.00000, total_steps: 1002000.00000
[CW] train: qf1_loss: 0.12620, qf2_loss: 0.12589, policy_loss: -18.98195, policy_entropy: -6.00463, alpha: 0.00686, time: 34.10539
[CW] ---------------------------
[CW] ---- Iteration:   997 ----
[CW] collect: return: 109.97692, steps: 1000.00000, total_steps: 1003000.00000
[CW] train: qf1_loss: 0.12769, qf2_loss: 0.12758, policy_loss: -18.96424, policy_entropy: -6.05461, alpha: 0.00687, time: 34.13580
[CW] ---------------------------
[CW] ---- Iteration:   998 ----
[CW] collect: return: 118.57100, steps: 1000.00000, total_steps: 1004000.00000
[CW] train: qf1_loss: 0.12980, qf2_loss: 0.12962, policy_loss: -19.14401, policy_entropy: -5.98008, alpha: 0.00689, time: 34.04416
[CW] ---------------------------
[CW] ---- Iteration:   999 ----
[CW] collect: return: 121.84974, steps: 1000.00000, total_steps: 1005000.00000
[CW] train: qf1_loss: 0.13526, qf2_loss: 0.13537, policy_loss: -18.96884, policy_entropy: -5.85615, alpha: 0.00685, time: 34.16244
[CW] ---------------------------
[CW] ---- Iteration:  1000 ----
[CW] collect: return: 79.12650, steps: 1000.00000, total_steps: 1006000.00000
[CW] train: qf1_loss: 0.12625, qf2_loss: 0.12583, policy_loss: -19.03661, policy_entropy: -5.95756, alpha: 0.00681, time: 34.07396
[CW] eval: return: 128.24240, steps: 1000.00000
[CW] ---------------------------
