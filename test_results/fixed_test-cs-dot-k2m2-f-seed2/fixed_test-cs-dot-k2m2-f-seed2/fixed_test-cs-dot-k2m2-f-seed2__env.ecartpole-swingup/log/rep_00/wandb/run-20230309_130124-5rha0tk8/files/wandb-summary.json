{"collect/return": 821.7907065219188, "collect/steps": 1000.0, "collect/total_steps": 634000.0, "train/qf1_loss": 59.71745754241943, "train/qf2_loss": 59.37821937561035, "train/policy_loss": -599.5265753173828, "train/policy_entropy": -0.9850284773111343, "train/alpha": 0.4994985929131508, "train/time": 43.8447790145874, "eval/return": 841.0609744809583, "eval/steps": 1000.0, "_timestamp": 1678391934.3131526, "_runtime": 28649.805908441544, "_step": 628}