{"collect/return": 766.8041668080259, "collect/steps": 1000.0, "collect/total_steps": 540000.0, "train/qf1_loss": 37.204941759109495, "train/qf2_loss": 36.95426254272461, "train/policy_loss": -551.2679180908203, "train/policy_entropy": -1.0050282365083694, "train/alpha": 0.5334948813915252, "train/time": 51.13020944595337, "eval/return": 837.4793771553043, "eval/steps": 1000.0, "_timestamp": 1678341664.6891308, "_runtime": 28672.307143688202, "_step": 534}