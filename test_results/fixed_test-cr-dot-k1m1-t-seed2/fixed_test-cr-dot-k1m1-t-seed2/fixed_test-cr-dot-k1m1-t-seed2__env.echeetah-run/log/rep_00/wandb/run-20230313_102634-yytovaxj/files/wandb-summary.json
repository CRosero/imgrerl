{"collect/return": 39.48545184871182, "collect/steps": 1000.0, "collect/total_steps": 647000.0, "train/qf1_loss": 36.66904895782471, "train/qf2_loss": 36.7891065788269, "train/policy_loss": -257.4678852844238, "train/policy_entropy": -6.105352811813354, "train/alpha": 0.06473146013915539, "train/time": 52.7913076877594, "eval/return": 37.25285725045933, "eval/steps": 1000.0, "_timestamp": 1678735561.2697623, "_runtime": 35966.91371035576, "_step": 641}