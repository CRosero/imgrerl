Hostname: uc2n510.localdomain
Found slurm config: SLURM
Seeting default slurm config
/home/kit/stud/uprnr/imgrerl/test_results/fixed_test-cr-sot-k1m1-t-seed0/fixed_test-cr-sot-k1m1-t-seed0/fixed_test-cr-sot-k1m1-t-seed0__env.echeetah-run/log/rep_00
[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
True
params: 
 {'env': {'env': 'cheetah-run'}} 

additionalVars: 
 {'seed': 0, 'agent': {'image_augmentation_K': 1, 'image_augmentation_M': 1, 'image_augmentation_type': <AugmentationType.SAME_OVER_TIME: 2>, 'image_augmentation_actor_critic_same_aug': True}}
conf_dict: 
 --------Config-------- 
seed: 0
cuda_id: 0
Subconfig: env
	env: cheetah-run
	obs_type: image
Subconfig: agent
	algo_name: sac
	encoder: lstm
	action_embedding_size: 32
	observ_embedding_size: 0
	reward_embedding_size: 32
	rnn_hidden_size: 512
	dqn_layers: [512, 512, 512]
	policy_layers: [512, 512, 512]
	lr: 0.0003
	gamma: 0.99
	tau: 0.005
	entropy_alpha: 1.0
	image_augmentation_type: AugmentationType.SAME_OVER_TIME
	image_augmentation_K: 1
	image_augmentation_M: 1
	image_augmentation_actor_critic_same_aug: True
Subconfig: rl
	sampled_seq_len: 64
	buffer_size: 1000000.0
	batch_size: 32
	rollouts_per_iter: 1
	num_updates_per_iter: 100
	num_init_rollouts: 5
Subconfig: eval
	interval: 20
	num_rollouts: 20
---------------------- 
 
Init feature extractor:  6 ;  32 ;  <function relu at 0x1493b37367a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x1493b37367a0>
Init feature extractor:  6 ;  164 ;  <function relu at 0x1493b37367a0>
Critic: 
Critic_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (current_shortcut_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=164, bias=True)
  )
  (qf1): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
  (qf2): FlattenMlp(
    (fc0): Linear(in_features=776, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=1, bias=True)
  )
)
Init feature extractor:  6 ;  32 ;  <function relu at 0x1493b37367a0>
Init feature extractor:  1 ;  32 ;  <function relu at 0x1493b37367a0>
Actor: 
Actor_RNN(
  (image_encoder): ImageEncoder(
    (cnn): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU()
      (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))
      (3): ReLU()
    )
  )
  (image_encoder_embedder_A): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (image_encoder_embedder_B): ImageEncoder_Embedder(
    (linear): Sequential(
      (0): Linear(in_features=57600, out_features=200, bias=True)
      (1): ReLU()
      (2): Linear(in_features=200, out_features=100, bias=True)
      (3): ReLU()
    )
  )
  (action_embedder): FeatureExtractor(
    (fc): Linear(in_features=6, out_features=32, bias=True)
  )
  (reward_embedder): FeatureExtractor(
    (fc): Linear(in_features=1, out_features=32, bias=True)
  )
  (rnn): LSTM(164, 512)
  (policy): TanhGaussianPolicy(
    (fc0): Linear(in_features=612, out_features=512, bias=True)
    (fc1): Linear(in_features=512, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=512, bias=True)
    (last_fc): Linear(in_features=512, out_features=6, bias=True)
    (last_fc_log_std): Linear(in_features=512, out_features=6, bias=True)
  )
)
buffer RAM usage: 11.48 GB
Collecting train stats
[CW] ---- Iteration:     0 ----
[CW] collect: return: 15.82249, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 1.76489, qf2_loss: 1.75645, policy_loss: -7.82394, policy_entropy: 4.09822, alpha: 0.98504, time: 56.76476
[CW] eval: return: 13.45733, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     1 ----
[CW] collect: return: 7.27789, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.09218, qf2_loss: 0.09215, policy_loss: -8.50783, policy_entropy: 4.10116, alpha: 0.95626, time: 49.95102
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     2 ----
[CW] collect: return: 3.49730, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.08075, qf2_loss: 0.08082, policy_loss: -9.19342, policy_entropy: 4.10017, alpha: 0.92871, time: 49.98364
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     3 ----
[CW] collect: return: 11.47409, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.07429, qf2_loss: 0.07433, policy_loss: -10.10649, policy_entropy: 4.10024, alpha: 0.90231, time: 50.14769
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     4 ----
[CW] collect: return: 8.52021, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.06713, qf2_loss: 0.06710, policy_loss: -11.12914, policy_entropy: 4.10264, alpha: 0.87699, time: 50.16246
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     5 ----
[CW] collect: return: 8.84917, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.06355, qf2_loss: 0.06348, policy_loss: -12.20901, policy_entropy: 4.10131, alpha: 0.85267, time: 50.68299
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     6 ----
[CW] collect: return: 13.06827, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.07546, qf2_loss: 0.07497, policy_loss: -13.33704, policy_entropy: 4.10173, alpha: 0.82930, time: 50.16449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     7 ----
[CW] collect: return: 9.09505, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.07372, qf2_loss: 0.07347, policy_loss: -14.51737, policy_entropy: 4.10110, alpha: 0.80683, time: 50.09464
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     8 ----
[CW] collect: return: 4.68796, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.07862, qf2_loss: 0.07833, policy_loss: -15.72130, policy_entropy: 4.10030, alpha: 0.78520, time: 50.16244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:     9 ----
[CW] collect: return: 13.65429, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.08197, qf2_loss: 0.08176, policy_loss: -16.92908, policy_entropy: 4.10074, alpha: 0.76436, time: 50.31049
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    10 ----
[CW] collect: return: 10.65416, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.08868, qf2_loss: 0.08854, policy_loss: -18.11467, policy_entropy: 4.10084, alpha: 0.74427, time: 50.28271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    11 ----
[CW] collect: return: 24.02183, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.08735, qf2_loss: 0.08714, policy_loss: -19.28704, policy_entropy: 4.10186, alpha: 0.72488, time: 50.26956
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    12 ----
[CW] collect: return: 15.16644, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.09033, qf2_loss: 0.09025, policy_loss: -20.42759, policy_entropy: 4.10140, alpha: 0.70617, time: 50.57324
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    13 ----
[CW] collect: return: 13.32143, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.07341, qf2_loss: 0.07350, policy_loss: -21.54597, policy_entropy: 4.10097, alpha: 0.68809, time: 50.98620
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    14 ----
[CW] collect: return: 4.12843, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.10283, qf2_loss: 0.10277, policy_loss: -22.63168, policy_entropy: 4.10084, alpha: 0.67062, time: 50.59223
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    15 ----
[CW] collect: return: 15.93542, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 0.07607, qf2_loss: 0.07607, policy_loss: -23.69738, policy_entropy: 4.10087, alpha: 0.65372, time: 50.43894
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    16 ----
[CW] collect: return: 10.20728, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 0.09013, qf2_loss: 0.09011, policy_loss: -24.73417, policy_entropy: 4.09973, alpha: 0.63737, time: 51.38560
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    17 ----
[CW] collect: return: 11.09901, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 0.07569, qf2_loss: 0.07576, policy_loss: -25.73524, policy_entropy: 4.09909, alpha: 0.62153, time: 51.37416
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    18 ----
[CW] collect: return: 15.89581, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 0.09977, qf2_loss: 0.09987, policy_loss: -26.72117, policy_entropy: 4.09926, alpha: 0.60620, time: 51.00451
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    19 ----
[CW] collect: return: 7.19581, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 0.10043, qf2_loss: 0.10063, policy_loss: -27.67288, policy_entropy: 4.09969, alpha: 0.59133, time: 50.69977
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    20 ----
[CW] collect: return: 11.41234, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 0.08677, qf2_loss: 0.08698, policy_loss: -28.59214, policy_entropy: 4.09998, alpha: 0.57691, time: 50.52613
[CW] eval: return: 12.01392, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    21 ----
[CW] collect: return: 3.72502, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 0.09217, qf2_loss: 0.09246, policy_loss: -29.48981, policy_entropy: 4.09853, alpha: 0.56292, time: 50.24064
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    22 ----
[CW] collect: return: 8.37944, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 0.11720, qf2_loss: 0.11756, policy_loss: -30.35554, policy_entropy: 4.09745, alpha: 0.54935, time: 50.34693
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    23 ----
[CW] collect: return: 10.99265, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 0.07472, qf2_loss: 0.07498, policy_loss: -31.19650, policy_entropy: 4.09781, alpha: 0.53617, time: 50.34538
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    24 ----
[CW] collect: return: 8.77086, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 0.11134, qf2_loss: 0.11184, policy_loss: -32.01496, policy_entropy: 4.09705, alpha: 0.52336, time: 50.27261
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    25 ----
[CW] collect: return: 7.17358, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 0.10174, qf2_loss: 0.10227, policy_loss: -32.80695, policy_entropy: 4.09553, alpha: 0.51093, time: 50.31162
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    26 ----
[CW] collect: return: 14.92768, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 0.10405, qf2_loss: 0.10440, policy_loss: -33.58101, policy_entropy: 4.09616, alpha: 0.49884, time: 50.95669
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    27 ----
[CW] collect: return: 17.54517, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 0.09959, qf2_loss: 0.09991, policy_loss: -34.33469, policy_entropy: 4.09568, alpha: 0.48708, time: 50.75869
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    28 ----
[CW] collect: return: 13.64943, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 0.09840, qf2_loss: 0.09875, policy_loss: -35.05656, policy_entropy: 4.09399, alpha: 0.47564, time: 50.61241
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    29 ----
[CW] collect: return: 20.59347, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 0.11482, qf2_loss: 0.11543, policy_loss: -35.76553, policy_entropy: 4.09277, alpha: 0.46452, time: 50.51144
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    30 ----
[CW] collect: return: 17.16041, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 0.14405, qf2_loss: 0.14465, policy_loss: -36.45168, policy_entropy: 4.09106, alpha: 0.45369, time: 50.50271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    31 ----
[CW] collect: return: 5.68302, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 0.07251, qf2_loss: 0.07277, policy_loss: -37.09869, policy_entropy: 4.09073, alpha: 0.44315, time: 50.43936
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    32 ----
[CW] collect: return: 14.56932, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 0.10905, qf2_loss: 0.10965, policy_loss: -37.74254, policy_entropy: 4.08790, alpha: 0.43289, time: 50.35655
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    33 ----
[CW] collect: return: 13.46718, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 0.15867, qf2_loss: 0.15936, policy_loss: -38.35962, policy_entropy: 4.08636, alpha: 0.42290, time: 50.44229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    34 ----
[CW] collect: return: 22.28621, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 0.09080, qf2_loss: 0.09117, policy_loss: -38.95685, policy_entropy: 4.08661, alpha: 0.41316, time: 50.33032
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    35 ----
[CW] collect: return: 13.11992, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 0.11390, qf2_loss: 0.11430, policy_loss: -39.53445, policy_entropy: 4.08640, alpha: 0.40367, time: 50.25269
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    36 ----
[CW] collect: return: 26.79532, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 0.13618, qf2_loss: 0.13675, policy_loss: -40.10176, policy_entropy: 4.08642, alpha: 0.39442, time: 50.28106
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    37 ----
[CW] collect: return: 16.36227, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 0.11796, qf2_loss: 0.11831, policy_loss: -40.65122, policy_entropy: 4.08443, alpha: 0.38541, time: 50.13792
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    38 ----
[CW] collect: return: 16.54257, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 0.14090, qf2_loss: 0.14130, policy_loss: -41.15937, policy_entropy: 4.08523, alpha: 0.37662, time: 50.24402
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    39 ----
[CW] collect: return: 20.62342, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 0.11756, qf2_loss: 0.11735, policy_loss: -41.67112, policy_entropy: 4.08428, alpha: 0.36804, time: 50.25286
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    40 ----
[CW] collect: return: 7.23523, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 0.13909, qf2_loss: 0.13928, policy_loss: -42.16080, policy_entropy: 4.08328, alpha: 0.35968, time: 50.73606
[CW] eval: return: 15.18790, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    41 ----
[CW] collect: return: 22.76422, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 0.15436, qf2_loss: 0.15441, policy_loss: -42.61643, policy_entropy: 4.08280, alpha: 0.35152, time: 50.80498
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    42 ----
[CW] collect: return: 14.45404, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 0.10340, qf2_loss: 0.10352, policy_loss: -43.07766, policy_entropy: 4.08174, alpha: 0.34356, time: 50.50651
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    43 ----
[CW] collect: return: 7.36419, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 0.14336, qf2_loss: 0.14361, policy_loss: -43.50050, policy_entropy: 4.08266, alpha: 0.33580, time: 50.55932
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    44 ----
[CW] collect: return: 15.87532, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 0.13383, qf2_loss: 0.13451, policy_loss: -43.92295, policy_entropy: 4.07951, alpha: 0.32822, time: 50.52907
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    45 ----
[CW] collect: return: 27.16558, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 0.13912, qf2_loss: 0.13988, policy_loss: -44.32686, policy_entropy: 4.07773, alpha: 0.32082, time: 50.85558
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    46 ----
[CW] collect: return: 12.65961, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 0.14908, qf2_loss: 0.14933, policy_loss: -44.71999, policy_entropy: 4.07866, alpha: 0.31360, time: 50.60534
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    47 ----
[CW] collect: return: 16.86449, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 0.12374, qf2_loss: 0.12446, policy_loss: -45.09294, policy_entropy: 4.07650, alpha: 0.30655, time: 50.36281
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    48 ----
[CW] collect: return: 12.19455, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 0.15089, qf2_loss: 0.15182, policy_loss: -45.46006, policy_entropy: 4.07242, alpha: 0.29967, time: 50.18058
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    49 ----
[CW] collect: return: 24.19259, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 0.13752, qf2_loss: 0.13813, policy_loss: -45.80360, policy_entropy: 4.07409, alpha: 0.29295, time: 50.44518
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    50 ----
[CW] collect: return: 14.93644, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 0.14681, qf2_loss: 0.14789, policy_loss: -46.13866, policy_entropy: 4.06941, alpha: 0.28639, time: 50.51248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    51 ----
[CW] collect: return: 12.68099, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 0.14532, qf2_loss: 0.14583, policy_loss: -46.45516, policy_entropy: 4.06784, alpha: 0.27998, time: 50.73884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    52 ----
[CW] collect: return: 18.11111, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 0.24232, qf2_loss: 0.24432, policy_loss: -46.75184, policy_entropy: 4.06739, alpha: 0.27372, time: 50.63905
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    53 ----
[CW] collect: return: 21.51089, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 0.11459, qf2_loss: 0.11488, policy_loss: -47.03675, policy_entropy: 4.06222, alpha: 0.26761, time: 50.60872
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    54 ----
[CW] collect: return: 12.62268, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 0.12093, qf2_loss: 0.12133, policy_loss: -47.32919, policy_entropy: 4.05733, alpha: 0.26164, time: 50.41726
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    55 ----
[CW] collect: return: 20.15183, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 0.16732, qf2_loss: 0.16887, policy_loss: -47.57304, policy_entropy: 4.05656, alpha: 0.25580, time: 50.41100
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    56 ----
[CW] collect: return: 25.87503, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 0.12967, qf2_loss: 0.13038, policy_loss: -47.84846, policy_entropy: 4.04886, alpha: 0.25011, time: 50.60296
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    57 ----
[CW] collect: return: 19.80650, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 0.15591, qf2_loss: 0.15726, policy_loss: -48.09645, policy_entropy: 4.04524, alpha: 0.24454, time: 50.64033
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    58 ----
[CW] collect: return: 16.89395, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 0.14615, qf2_loss: 0.14744, policy_loss: -48.32941, policy_entropy: 4.03909, alpha: 0.23911, time: 50.53543
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    59 ----
[CW] collect: return: 18.67966, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 0.15999, qf2_loss: 0.16089, policy_loss: -48.54866, policy_entropy: 4.03182, alpha: 0.23380, time: 50.38928
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    60 ----
[CW] collect: return: 19.09043, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 0.17878, qf2_loss: 0.18031, policy_loss: -48.76297, policy_entropy: 4.02559, alpha: 0.22861, time: 50.34023
[CW] eval: return: 25.18519, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    61 ----
[CW] collect: return: 13.17698, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 0.20596, qf2_loss: 0.20756, policy_loss: -48.97770, policy_entropy: 4.01491, alpha: 0.22354, time: 50.32249
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    62 ----
[CW] collect: return: 26.40904, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 0.15561, qf2_loss: 0.15638, policy_loss: -49.15913, policy_entropy: 4.00949, alpha: 0.21859, time: 50.31941
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    63 ----
[CW] collect: return: 18.62606, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 0.18471, qf2_loss: 0.18551, policy_loss: -49.34694, policy_entropy: 4.00658, alpha: 0.21376, time: 50.38265
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    64 ----
[CW] collect: return: 33.42956, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 0.21009, qf2_loss: 0.21138, policy_loss: -49.53586, policy_entropy: 3.98707, alpha: 0.20903, time: 50.53058
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    65 ----
[CW] collect: return: 17.19842, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 0.18074, qf2_loss: 0.18264, policy_loss: -49.69278, policy_entropy: 3.99156, alpha: 0.20441, time: 50.60370
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    66 ----
[CW] collect: return: 28.75446, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 0.20116, qf2_loss: 0.20288, policy_loss: -49.85288, policy_entropy: 3.97811, alpha: 0.19990, time: 50.51662
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    67 ----
[CW] collect: return: 12.48120, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 0.17397, qf2_loss: 0.17503, policy_loss: -50.00532, policy_entropy: 3.96371, alpha: 0.19549, time: 50.57009
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    68 ----
[CW] collect: return: 31.80615, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 0.17979, qf2_loss: 0.18023, policy_loss: -50.16287, policy_entropy: 3.95262, alpha: 0.19118, time: 50.84195
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    69 ----
[CW] collect: return: 39.23771, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 0.23748, qf2_loss: 0.23841, policy_loss: -50.28802, policy_entropy: 3.94662, alpha: 0.18697, time: 50.48623
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    70 ----
[CW] collect: return: 34.07774, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 0.17310, qf2_loss: 0.17412, policy_loss: -50.41024, policy_entropy: 3.93341, alpha: 0.18285, time: 50.63517
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    71 ----
[CW] collect: return: 29.40429, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 0.18088, qf2_loss: 0.18209, policy_loss: -50.52319, policy_entropy: 3.90530, alpha: 0.17883, time: 50.88510
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    72 ----
[CW] collect: return: 38.86444, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 0.19340, qf2_loss: 0.19407, policy_loss: -50.64910, policy_entropy: 3.87794, alpha: 0.17491, time: 52.38559
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    73 ----
[CW] collect: return: 61.69913, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 0.20655, qf2_loss: 0.20845, policy_loss: -50.77680, policy_entropy: 3.82454, alpha: 0.17108, time: 54.35862
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    74 ----
[CW] collect: return: 42.93674, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 0.19322, qf2_loss: 0.19464, policy_loss: -50.87344, policy_entropy: 3.71113, alpha: 0.16737, time: 50.95628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    75 ----
[CW] collect: return: 72.55569, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 0.23873, qf2_loss: 0.24084, policy_loss: -51.02198, policy_entropy: 3.54838, alpha: 0.16377, time: 50.74847
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    76 ----
[CW] collect: return: 76.31942, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 0.21424, qf2_loss: 0.21550, policy_loss: -51.13105, policy_entropy: 3.28756, alpha: 0.16032, time: 50.82486
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    77 ----
[CW] collect: return: 75.45025, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 0.21954, qf2_loss: 0.22139, policy_loss: -51.32060, policy_entropy: 2.92007, alpha: 0.15704, time: 50.78768
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    78 ----
[CW] collect: return: 93.87482, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 0.23135, qf2_loss: 0.23321, policy_loss: -51.46888, policy_entropy: 2.62159, alpha: 0.15391, time: 50.85721
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    79 ----
[CW] collect: return: 92.39086, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 0.30392, qf2_loss: 0.30520, policy_loss: -51.72271, policy_entropy: 2.25088, alpha: 0.15093, time: 50.75577
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    80 ----
[CW] collect: return: 44.41398, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 0.53414, qf2_loss: 0.53724, policy_loss: -51.76704, policy_entropy: 2.39695, alpha: 0.14806, time: 50.82149
[CW] eval: return: 72.54046, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    81 ----
[CW] collect: return: 56.94306, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 0.38170, qf2_loss: 0.38173, policy_loss: -51.92236, policy_entropy: 2.18702, alpha: 0.14518, time: 50.61920
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    82 ----
[CW] collect: return: 113.78010, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 0.33987, qf2_loss: 0.34309, policy_loss: -52.14952, policy_entropy: 2.09450, alpha: 0.14242, time: 50.69097
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    83 ----
[CW] collect: return: 102.14152, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 0.26720, qf2_loss: 0.26781, policy_loss: -52.31530, policy_entropy: 2.01517, alpha: 0.13971, time: 50.58347
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    84 ----
[CW] collect: return: 112.35850, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 0.31682, qf2_loss: 0.31790, policy_loss: -52.53086, policy_entropy: 1.90671, alpha: 0.13703, time: 50.87269
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    85 ----
[CW] collect: return: 44.71481, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 0.33168, qf2_loss: 0.33268, policy_loss: -52.66368, policy_entropy: 1.95865, alpha: 0.13441, time: 50.85273
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    86 ----
[CW] collect: return: 101.79711, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 0.30121, qf2_loss: 0.30245, policy_loss: -52.87001, policy_entropy: 1.89016, alpha: 0.13180, time: 50.78948
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    87 ----
[CW] collect: return: 22.46167, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 0.33706, qf2_loss: 0.33670, policy_loss: -53.01881, policy_entropy: 1.78458, alpha: 0.12926, time: 50.75529
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    88 ----
[CW] collect: return: 106.68161, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 0.33901, qf2_loss: 0.34027, policy_loss: -53.24827, policy_entropy: 1.69208, alpha: 0.12676, time: 50.86897
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    89 ----
[CW] collect: return: 116.50729, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 0.42683, qf2_loss: 0.42518, policy_loss: -53.51043, policy_entropy: 1.39148, alpha: 0.12435, time: 50.69819
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    90 ----
[CW] collect: return: 42.90165, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 0.36259, qf2_loss: 0.36498, policy_loss: -53.68726, policy_entropy: 1.32546, alpha: 0.12202, time: 50.90433
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    91 ----
[CW] collect: return: 88.61302, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 0.34883, qf2_loss: 0.35359, policy_loss: -53.97267, policy_entropy: 0.95336, alpha: 0.11976, time: 50.74624
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    92 ----
[CW] collect: return: 75.37549, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 0.41830, qf2_loss: 0.42297, policy_loss: -54.21460, policy_entropy: 0.71711, alpha: 0.11762, time: 50.67708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    93 ----
[CW] collect: return: 79.86602, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 0.35175, qf2_loss: 0.35521, policy_loss: -54.44782, policy_entropy: 0.50152, alpha: 0.11556, time: 50.74946
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    94 ----
[CW] collect: return: 72.38467, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 0.49684, qf2_loss: 0.50198, policy_loss: -54.78770, policy_entropy: 0.32112, alpha: 0.11357, time: 50.56795
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    95 ----
[CW] collect: return: 94.32434, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 0.38090, qf2_loss: 0.38976, policy_loss: -55.02601, policy_entropy: 0.22637, alpha: 0.11162, time: 50.70044
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    96 ----
[CW] collect: return: 44.00696, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 0.41899, qf2_loss: 0.42318, policy_loss: -55.36550, policy_entropy: 0.08368, alpha: 0.10971, time: 50.80271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    97 ----
[CW] collect: return: 83.03361, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 0.39310, qf2_loss: 0.39562, policy_loss: -55.63847, policy_entropy: 0.13851, alpha: 0.10782, time: 50.78863
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    98 ----
[CW] collect: return: 35.37182, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 0.38875, qf2_loss: 0.39486, policy_loss: -55.91216, policy_entropy: 0.03121, alpha: 0.10594, time: 50.75618
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:    99 ----
[CW] collect: return: 50.42471, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 0.40859, qf2_loss: 0.41248, policy_loss: -56.17073, policy_entropy: 0.06895, alpha: 0.10407, time: 50.59296
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   100 ----
[CW] collect: return: 33.44172, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 0.40278, qf2_loss: 0.40316, policy_loss: -56.38491, policy_entropy: 0.10083, alpha: 0.10221, time: 50.67730
[CW] eval: return: 70.20114, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   101 ----
[CW] collect: return: 49.62933, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 0.37443, qf2_loss: 0.37735, policy_loss: -56.62481, policy_entropy: 0.10709, alpha: 0.10034, time: 50.51746
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   102 ----
[CW] collect: return: 60.90215, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 0.43542, qf2_loss: 0.44079, policy_loss: -56.78887, policy_entropy: 0.11077, alpha: 0.09849, time: 50.55034
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   103 ----
[CW] collect: return: 33.38192, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 0.35457, qf2_loss: 0.35515, policy_loss: -56.89563, policy_entropy: 0.25550, alpha: 0.09663, time: 50.71285
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   104 ----
[CW] collect: return: 50.68425, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 0.32669, qf2_loss: 0.33018, policy_loss: -57.06830, policy_entropy: 0.27157, alpha: 0.09476, time: 51.01682
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   105 ----
[CW] collect: return: 63.24082, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 0.34322, qf2_loss: 0.34492, policy_loss: -57.20873, policy_entropy: 0.30736, alpha: 0.09290, time: 50.67755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   106 ----
[CW] collect: return: 33.19691, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 0.30030, qf2_loss: 0.30207, policy_loss: -57.18919, policy_entropy: 0.41092, alpha: 0.09105, time: 50.17636
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   107 ----
[CW] collect: return: 139.13434, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 0.32246, qf2_loss: 0.32346, policy_loss: -57.34779, policy_entropy: 0.38954, alpha: 0.08921, time: 50.80800
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   108 ----
[CW] collect: return: 120.02361, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 0.32011, qf2_loss: 0.31956, policy_loss: -57.28446, policy_entropy: 0.51524, alpha: 0.08738, time: 50.81060
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   109 ----
[CW] collect: return: 156.90301, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 0.38468, qf2_loss: 0.38837, policy_loss: -57.46604, policy_entropy: 0.56673, alpha: 0.08555, time: 50.79868
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   110 ----
[CW] collect: return: 49.98142, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 0.27846, qf2_loss: 0.28003, policy_loss: -57.43693, policy_entropy: 0.65219, alpha: 0.08374, time: 50.84169
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   111 ----
[CW] collect: return: 61.86211, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 0.31441, qf2_loss: 0.31736, policy_loss: -57.39703, policy_entropy: 0.65188, alpha: 0.08195, time: 50.99201
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   112 ----
[CW] collect: return: 117.72865, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 0.29185, qf2_loss: 0.29232, policy_loss: -57.43177, policy_entropy: 0.69519, alpha: 0.08019, time: 50.87748
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   113 ----
[CW] collect: return: 115.00742, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 0.29889, qf2_loss: 0.30408, policy_loss: -57.47602, policy_entropy: 0.68225, alpha: 0.07845, time: 50.94057
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   114 ----
[CW] collect: return: 87.40432, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 0.26941, qf2_loss: 0.27143, policy_loss: -57.40852, policy_entropy: 0.75064, alpha: 0.07675, time: 50.72411
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   115 ----
[CW] collect: return: 138.95299, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 0.32085, qf2_loss: 0.32337, policy_loss: -57.47669, policy_entropy: 0.75295, alpha: 0.07508, time: 50.85240
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   116 ----
[CW] collect: return: 36.13051, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 0.29763, qf2_loss: 0.30274, policy_loss: -57.37738, policy_entropy: 0.75498, alpha: 0.07342, time: 50.81991
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   117 ----
[CW] collect: return: 70.58541, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 0.28434, qf2_loss: 0.28708, policy_loss: -57.42700, policy_entropy: 0.52550, alpha: 0.07184, time: 50.96647
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   118 ----
[CW] collect: return: 147.92488, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 0.30610, qf2_loss: 0.31060, policy_loss: -57.51068, policy_entropy: 0.33907, alpha: 0.07033, time: 50.78528
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   119 ----
[CW] collect: return: 68.74769, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 0.29494, qf2_loss: 0.29944, policy_loss: -57.60458, policy_entropy: 0.15178, alpha: 0.06889, time: 50.69653
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   120 ----
[CW] collect: return: 147.90660, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 0.29756, qf2_loss: 0.30237, policy_loss: -57.61399, policy_entropy: -0.02535, alpha: 0.06751, time: 50.86225
[CW] eval: return: 57.14669, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   121 ----
[CW] collect: return: 53.06946, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 0.34138, qf2_loss: 0.34744, policy_loss: -57.70407, policy_entropy: -0.18614, alpha: 0.06619, time: 50.59350
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   122 ----
[CW] collect: return: 19.28020, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 0.41481, qf2_loss: 0.42430, policy_loss: -57.59963, policy_entropy: -0.19764, alpha: 0.06491, time: 50.40939
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   123 ----
[CW] collect: return: 49.46750, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 0.35855, qf2_loss: 0.36311, policy_loss: -57.77701, policy_entropy: -0.47866, alpha: 0.06365, time: 50.53637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   124 ----
[CW] collect: return: 34.85470, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 0.33857, qf2_loss: 0.34422, policy_loss: -57.71596, policy_entropy: -0.54452, alpha: 0.06247, time: 50.41003
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   125 ----
[CW] collect: return: 43.65044, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 0.32091, qf2_loss: 0.32775, policy_loss: -57.81658, policy_entropy: -0.57253, alpha: 0.06129, time: 50.53051
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   126 ----
[CW] collect: return: 49.13237, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 0.44442, qf2_loss: 0.44741, policy_loss: -57.83141, policy_entropy: -0.57240, alpha: 0.06012, time: 50.76862
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   127 ----
[CW] collect: return: 111.55099, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 0.33593, qf2_loss: 0.34096, policy_loss: -57.89568, policy_entropy: -0.61026, alpha: 0.05898, time: 50.52792
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   128 ----
[CW] collect: return: 108.75302, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 0.35803, qf2_loss: 0.36308, policy_loss: -58.06346, policy_entropy: -0.68316, alpha: 0.05785, time: 50.65210
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   129 ----
[CW] collect: return: 78.27705, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 0.38221, qf2_loss: 0.38830, policy_loss: -58.11048, policy_entropy: -0.75324, alpha: 0.05675, time: 50.56479
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   130 ----
[CW] collect: return: 35.77302, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 0.45183, qf2_loss: 0.45829, policy_loss: -58.21539, policy_entropy: -0.74043, alpha: 0.05566, time: 50.60490
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   131 ----
[CW] collect: return: 93.17245, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 0.40685, qf2_loss: 0.41114, policy_loss: -58.29057, policy_entropy: -0.85657, alpha: 0.05460, time: 50.44245
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   132 ----
[CW] collect: return: 16.85554, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 0.44566, qf2_loss: 0.45052, policy_loss: -58.33823, policy_entropy: -0.83246, alpha: 0.05356, time: 50.37478
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   133 ----
[CW] collect: return: 82.97257, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 0.39587, qf2_loss: 0.39949, policy_loss: -58.42876, policy_entropy: -0.83451, alpha: 0.05252, time: 50.39324
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   134 ----
[CW] collect: return: 45.24438, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 0.41477, qf2_loss: 0.42028, policy_loss: -58.61699, policy_entropy: -0.95700, alpha: 0.05151, time: 50.73742
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   135 ----
[CW] collect: return: 150.86767, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 0.53014, qf2_loss: 0.53764, policy_loss: -58.78832, policy_entropy: -1.07852, alpha: 0.05053, time: 50.72755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   136 ----
[CW] collect: return: 105.99234, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 0.44894, qf2_loss: 0.45235, policy_loss: -58.66871, policy_entropy: -0.98812, alpha: 0.04955, time: 50.56382
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   137 ----
[CW] collect: return: 21.12341, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 0.59195, qf2_loss: 0.59632, policy_loss: -58.67831, policy_entropy: -1.01203, alpha: 0.04860, time: 50.90363
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   138 ----
[CW] collect: return: 85.65195, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 0.42522, qf2_loss: 0.43156, policy_loss: -58.78516, policy_entropy: -0.97069, alpha: 0.04764, time: 50.66242
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   139 ----
[CW] collect: return: 47.37483, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 0.44984, qf2_loss: 0.45367, policy_loss: -59.07257, policy_entropy: -1.10299, alpha: 0.04670, time: 50.50711
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   140 ----
[CW] collect: return: 121.71741, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 0.41344, qf2_loss: 0.41746, policy_loss: -58.91177, policy_entropy: -1.11995, alpha: 0.04579, time: 50.60919
[CW] eval: return: 84.92691, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   141 ----
[CW] collect: return: 76.21704, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 0.45285, qf2_loss: 0.45596, policy_loss: -59.08465, policy_entropy: -1.16599, alpha: 0.04489, time: 51.48339
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   142 ----
[CW] collect: return: 156.98963, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 0.52241, qf2_loss: 0.52741, policy_loss: -59.26520, policy_entropy: -1.08356, alpha: 0.04400, time: 51.34657
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   143 ----
[CW] collect: return: 194.36265, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 0.48452, qf2_loss: 0.49241, policy_loss: -59.39713, policy_entropy: -1.17593, alpha: 0.04313, time: 51.47704
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   144 ----
[CW] collect: return: 52.40392, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 0.45861, qf2_loss: 0.45779, policy_loss: -59.21207, policy_entropy: -1.11329, alpha: 0.04227, time: 55.00150
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   145 ----
[CW] collect: return: 142.07148, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 0.63145, qf2_loss: 0.63300, policy_loss: -59.30325, policy_entropy: -1.19735, alpha: 0.04143, time: 51.57914
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   146 ----
[CW] collect: return: 133.64701, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 0.55005, qf2_loss: 0.55277, policy_loss: -59.42224, policy_entropy: -1.20812, alpha: 0.04059, time: 53.42265
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   147 ----
[CW] collect: return: 49.62867, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 0.63512, qf2_loss: 0.63968, policy_loss: -59.39347, policy_entropy: -1.12767, alpha: 0.03977, time: 55.98739
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   148 ----
[CW] collect: return: 232.67576, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 0.47799, qf2_loss: 0.48103, policy_loss: -59.67442, policy_entropy: -1.24968, alpha: 0.03896, time: 51.28695
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   149 ----
[CW] collect: return: 242.33046, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 0.51288, qf2_loss: 0.51491, policy_loss: -59.65246, policy_entropy: -1.27223, alpha: 0.03817, time: 51.29599
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   150 ----
[CW] collect: return: 178.54052, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 0.52621, qf2_loss: 0.53374, policy_loss: -59.87418, policy_entropy: -1.52545, alpha: 0.03742, time: 51.47924
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   151 ----
[CW] collect: return: 222.29031, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 0.55269, qf2_loss: 0.55465, policy_loss: -60.10031, policy_entropy: -1.47397, alpha: 0.03669, time: 51.31773
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   152 ----
[CW] collect: return: 214.01745, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 0.76716, qf2_loss: 0.76902, policy_loss: -60.38690, policy_entropy: -1.64325, alpha: 0.03598, time: 51.22309
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   153 ----
[CW] collect: return: 141.54359, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 0.52898, qf2_loss: 0.53327, policy_loss: -60.50142, policy_entropy: -1.76697, alpha: 0.03531, time: 51.13798
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   154 ----
[CW] collect: return: 270.58248, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 0.54515, qf2_loss: 0.54980, policy_loss: -60.41082, policy_entropy: -1.67674, alpha: 0.03463, time: 51.19912
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   155 ----
[CW] collect: return: 197.97608, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 0.58069, qf2_loss: 0.58447, policy_loss: -60.75024, policy_entropy: -1.92489, alpha: 0.03398, time: 50.84031
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   156 ----
[CW] collect: return: 215.42992, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 0.57803, qf2_loss: 0.58016, policy_loss: -60.67243, policy_entropy: -2.03683, alpha: 0.03335, time: 51.01178
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   157 ----
[CW] collect: return: 59.64847, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 0.58085, qf2_loss: 0.58592, policy_loss: -60.86833, policy_entropy: -2.04016, alpha: 0.03274, time: 50.80804
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   158 ----
[CW] collect: return: 87.39977, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 0.75590, qf2_loss: 0.75901, policy_loss: -60.74803, policy_entropy: -2.09148, alpha: 0.03214, time: 50.81022
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   159 ----
[CW] collect: return: 297.84285, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 0.69036, qf2_loss: 0.69138, policy_loss: -60.95445, policy_entropy: -2.20858, alpha: 0.03156, time: 50.77372
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   160 ----
[CW] collect: return: 251.37508, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 0.63938, qf2_loss: 0.64681, policy_loss: -61.27923, policy_entropy: -2.47642, alpha: 0.03101, time: 50.84823
[CW] eval: return: 194.61827, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   161 ----
[CW] collect: return: 107.02846, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 0.65430, qf2_loss: 0.66257, policy_loss: -61.38124, policy_entropy: -2.52296, alpha: 0.03047, time: 50.48277
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   162 ----
[CW] collect: return: 259.40237, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 0.66415, qf2_loss: 0.66642, policy_loss: -61.34646, policy_entropy: -2.67402, alpha: 0.02995, time: 50.54310
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   163 ----
[CW] collect: return: 249.63449, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 0.61187, qf2_loss: 0.61894, policy_loss: -61.69273, policy_entropy: -2.78175, alpha: 0.02946, time: 50.59553
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   164 ----
[CW] collect: return: 289.58002, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 0.74087, qf2_loss: 0.74917, policy_loss: -61.82080, policy_entropy: -2.91868, alpha: 0.02899, time: 51.03025
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   165 ----
[CW] collect: return: 210.91204, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 0.65239, qf2_loss: 0.65666, policy_loss: -61.93794, policy_entropy: -3.17121, alpha: 0.02854, time: 51.14157
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   166 ----
[CW] collect: return: 165.12326, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 0.68892, qf2_loss: 0.69266, policy_loss: -61.84810, policy_entropy: -3.14027, alpha: 0.02811, time: 50.78861
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   167 ----
[CW] collect: return: 276.01552, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 0.77887, qf2_loss: 0.78766, policy_loss: -62.36417, policy_entropy: -3.34975, alpha: 0.02768, time: 50.57007
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   168 ----
[CW] collect: return: 229.52492, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 0.69774, qf2_loss: 0.70108, policy_loss: -62.13188, policy_entropy: -3.44973, alpha: 0.02727, time: 50.65616
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   169 ----
[CW] collect: return: 218.31455, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 0.79619, qf2_loss: 0.80363, policy_loss: -62.63703, policy_entropy: -3.61671, alpha: 0.02689, time: 50.46322
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   170 ----
[CW] collect: return: 267.61941, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 0.69711, qf2_loss: 0.70336, policy_loss: -62.84557, policy_entropy: -3.65168, alpha: 0.02652, time: 50.38501
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   171 ----
[CW] collect: return: 205.40991, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 0.69807, qf2_loss: 0.70792, policy_loss: -62.88280, policy_entropy: -3.62559, alpha: 0.02614, time: 50.64935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   172 ----
[CW] collect: return: 273.36424, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 0.74838, qf2_loss: 0.75667, policy_loss: -63.27234, policy_entropy: -3.77293, alpha: 0.02577, time: 50.87810
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   173 ----
[CW] collect: return: 21.61265, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 0.72932, qf2_loss: 0.73438, policy_loss: -63.05127, policy_entropy: -3.70910, alpha: 0.02540, time: 51.25701
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   174 ----
[CW] collect: return: 306.66932, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 0.65905, qf2_loss: 0.66775, policy_loss: -63.45547, policy_entropy: -3.82717, alpha: 0.02504, time: 51.47368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   175 ----
[CW] collect: return: 251.95328, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 0.65029, qf2_loss: 0.65739, policy_loss: -63.24700, policy_entropy: -3.87316, alpha: 0.02468, time: 51.26907
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   176 ----
[CW] collect: return: 46.51571, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 0.81373, qf2_loss: 0.82250, policy_loss: -63.39496, policy_entropy: -4.17863, alpha: 0.02435, time: 51.16496
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   177 ----
[CW] collect: return: 273.38089, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 0.76674, qf2_loss: 0.77588, policy_loss: -63.49527, policy_entropy: -4.09169, alpha: 0.02403, time: 50.94479
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   178 ----
[CW] collect: return: 224.29878, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 0.93042, qf2_loss: 0.93569, policy_loss: -64.02058, policy_entropy: -4.30708, alpha: 0.02372, time: 51.07006
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   179 ----
[CW] collect: return: 125.05551, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 0.70247, qf2_loss: 0.70674, policy_loss: -63.26510, policy_entropy: -4.25724, alpha: 0.02343, time: 51.00458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   180 ----
[CW] collect: return: 110.98744, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 0.93012, qf2_loss: 0.93336, policy_loss: -64.13865, policy_entropy: -4.27572, alpha: 0.02312, time: 50.76794
[CW] eval: return: 164.99190, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   181 ----
[CW] collect: return: 23.64094, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 0.71720, qf2_loss: 0.72386, policy_loss: -64.42960, policy_entropy: -4.34695, alpha: 0.02281, time: 50.73662
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   182 ----
[CW] collect: return: 209.93254, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 0.84482, qf2_loss: 0.84300, policy_loss: -63.99855, policy_entropy: -4.07371, alpha: 0.02249, time: 50.60297
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   183 ----
[CW] collect: return: 269.90088, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 0.87677, qf2_loss: 0.88048, policy_loss: -64.13274, policy_entropy: -4.24300, alpha: 0.02214, time: 50.66764
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   184 ----
[CW] collect: return: 149.20346, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 0.80589, qf2_loss: 0.81844, policy_loss: -64.40592, policy_entropy: -4.46620, alpha: 0.02184, time: 50.72992
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   185 ----
[CW] collect: return: 299.41841, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 0.83280, qf2_loss: 0.83697, policy_loss: -64.74876, policy_entropy: -4.38859, alpha: 0.02155, time: 51.38141
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   186 ----
[CW] collect: return: 86.60253, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 1.10171, qf2_loss: 1.11724, policy_loss: -65.11200, policy_entropy: -4.56649, alpha: 0.02126, time: 51.17094
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   187 ----
[CW] collect: return: 217.13188, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 0.84415, qf2_loss: 0.85186, policy_loss: -64.87834, policy_entropy: -4.66286, alpha: 0.02100, time: 51.43356
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   188 ----
[CW] collect: return: 238.89304, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 0.78604, qf2_loss: 0.79396, policy_loss: -65.02114, policy_entropy: -4.99434, alpha: 0.02077, time: 51.30047
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   189 ----
[CW] collect: return: 92.62835, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 0.81184, qf2_loss: 0.82253, policy_loss: -65.27686, policy_entropy: -5.22903, alpha: 0.02058, time: 51.27330
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   190 ----
[CW] collect: return: 246.44462, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 0.91899, qf2_loss: 0.92014, policy_loss: -65.34102, policy_entropy: -5.14799, alpha: 0.02042, time: 51.23013
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   191 ----
[CW] collect: return: 61.51152, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 1.00890, qf2_loss: 1.00968, policy_loss: -65.34499, policy_entropy: -5.05414, alpha: 0.02024, time: 51.27954
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   192 ----
[CW] collect: return: 296.69005, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 0.82831, qf2_loss: 0.84638, policy_loss: -65.52808, policy_entropy: -5.03577, alpha: 0.02003, time: 51.34436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   193 ----
[CW] collect: return: 120.06961, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 0.82268, qf2_loss: 0.82915, policy_loss: -65.67408, policy_entropy: -5.05632, alpha: 0.01982, time: 51.47098
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   194 ----
[CW] collect: return: 303.84473, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 0.85266, qf2_loss: 0.86084, policy_loss: -65.54315, policy_entropy: -5.23857, alpha: 0.01962, time: 53.23455
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   195 ----
[CW] collect: return: 43.16033, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 0.87111, qf2_loss: 0.88272, policy_loss: -65.90577, policy_entropy: -5.26429, alpha: 0.01945, time: 51.10336
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   196 ----
[CW] collect: return: 266.76026, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 0.93212, qf2_loss: 0.94526, policy_loss: -66.46050, policy_entropy: -5.52094, alpha: 0.01930, time: 50.87859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   197 ----
[CW] collect: return: 261.16268, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 0.96192, qf2_loss: 0.96793, policy_loss: -66.13608, policy_entropy: -5.43906, alpha: 0.01918, time: 50.94472
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   198 ----
[CW] collect: return: 320.69156, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 0.92842, qf2_loss: 0.93804, policy_loss: -66.45455, policy_entropy: -5.49102, alpha: 0.01904, time: 50.46431
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   199 ----
[CW] collect: return: 189.69468, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 0.99442, qf2_loss: 1.00809, policy_loss: -66.21953, policy_entropy: -5.34610, alpha: 0.01889, time: 50.26922
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   200 ----
[CW] collect: return: 274.50230, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 1.11857, qf2_loss: 1.13460, policy_loss: -66.51513, policy_entropy: -5.43069, alpha: 0.01873, time: 50.30850
[CW] eval: return: 239.70129, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   201 ----
[CW] collect: return: 312.83777, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 0.94738, qf2_loss: 0.95857, policy_loss: -67.05632, policy_entropy: -5.50809, alpha: 0.01858, time: 50.18136
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   202 ----
[CW] collect: return: 315.85753, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 1.02978, qf2_loss: 1.02840, policy_loss: -67.12040, policy_entropy: -5.76361, alpha: 0.01846, time: 50.04337
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   203 ----
[CW] collect: return: 299.02745, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 0.99192, qf2_loss: 0.99906, policy_loss: -67.31615, policy_entropy: -6.16182, alpha: 0.01843, time: 50.16590
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   204 ----
[CW] collect: return: 305.58205, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 1.03085, qf2_loss: 1.05359, policy_loss: -67.29552, policy_entropy: -5.87210, alpha: 0.01849, time: 49.97031
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   205 ----
[CW] collect: return: 274.20845, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 1.06557, qf2_loss: 1.07897, policy_loss: -67.29435, policy_entropy: -6.04437, alpha: 0.01844, time: 50.25533
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   206 ----
[CW] collect: return: 225.55953, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 1.37189, qf2_loss: 1.38712, policy_loss: -67.70030, policy_entropy: -6.46182, alpha: 0.01851, time: 50.03223
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   207 ----
[CW] collect: return: 88.20830, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 1.01240, qf2_loss: 1.01584, policy_loss: -67.97624, policy_entropy: -6.45321, alpha: 0.01868, time: 50.12453
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   208 ----
[CW] collect: return: 317.84358, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 0.90633, qf2_loss: 0.91315, policy_loss: -67.68806, policy_entropy: -6.26791, alpha: 0.01882, time: 50.53838
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   209 ----
[CW] collect: return: 19.54640, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 0.92713, qf2_loss: 0.92626, policy_loss: -67.93165, policy_entropy: -6.31080, alpha: 0.01894, time: 50.38756
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   210 ----
[CW] collect: return: 273.12054, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 1.05257, qf2_loss: 1.06509, policy_loss: -68.51146, policy_entropy: -5.76674, alpha: 0.01900, time: 50.49571
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   211 ----
[CW] collect: return: 326.10536, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 1.17560, qf2_loss: 1.17513, policy_loss: -68.64175, policy_entropy: -5.70588, alpha: 0.01886, time: 50.28818
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   212 ----
[CW] collect: return: 254.92048, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 1.07546, qf2_loss: 1.09455, policy_loss: -68.83006, policy_entropy: -5.59073, alpha: 0.01872, time: 50.27812
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   213 ----
[CW] collect: return: 161.04682, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 1.25030, qf2_loss: 1.26551, policy_loss: -68.62378, policy_entropy: -5.60477, alpha: 0.01852, time: 50.30601
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   214 ----
[CW] collect: return: 291.04689, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 1.16757, qf2_loss: 1.18792, policy_loss: -68.87029, policy_entropy: -5.79379, alpha: 0.01839, time: 50.30415
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   215 ----
[CW] collect: return: 160.22729, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 1.11588, qf2_loss: 1.13160, policy_loss: -68.86545, policy_entropy: -5.86015, alpha: 0.01831, time: 50.36159
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   216 ----
[CW] collect: return: 318.18273, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 1.04388, qf2_loss: 1.05604, policy_loss: -68.94729, policy_entropy: -5.82045, alpha: 0.01823, time: 50.17986
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   217 ----
[CW] collect: return: 51.02666, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 1.02282, qf2_loss: 1.03781, policy_loss: -69.09608, policy_entropy: -5.76194, alpha: 0.01811, time: 51.16475
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   218 ----
[CW] collect: return: 309.85154, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 1.11165, qf2_loss: 1.12331, policy_loss: -69.52125, policy_entropy: -5.94334, alpha: 0.01804, time: 51.28147
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   219 ----
[CW] collect: return: 58.89674, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 1.10615, qf2_loss: 1.12174, policy_loss: -69.65960, policy_entropy: -5.77823, alpha: 0.01797, time: 51.14555
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   220 ----
[CW] collect: return: 308.52765, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 1.09235, qf2_loss: 1.10966, policy_loss: -69.90148, policy_entropy: -5.92014, alpha: 0.01788, time: 51.42130
[CW] eval: return: 303.15223, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   221 ----
[CW] collect: return: 226.78555, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 1.54747, qf2_loss: 1.56307, policy_loss: -69.31847, policy_entropy: -5.86271, alpha: 0.01784, time: 55.70120
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   222 ----
[CW] collect: return: 320.94995, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 1.15280, qf2_loss: 1.17055, policy_loss: -70.38644, policy_entropy: -5.98971, alpha: 0.01777, time: 51.36480
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   223 ----
[CW] collect: return: 360.38284, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 1.14186, qf2_loss: 1.16080, policy_loss: -70.68849, policy_entropy: -6.08761, alpha: 0.01780, time: 51.47529
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   224 ----
[CW] collect: return: 108.25048, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 1.38401, qf2_loss: 1.40548, policy_loss: -70.11571, policy_entropy: -6.08944, alpha: 0.01786, time: 51.17911
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   225 ----
[CW] collect: return: 378.78613, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 1.24935, qf2_loss: 1.26856, policy_loss: -70.58388, policy_entropy: -6.09282, alpha: 0.01789, time: 51.35884
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   226 ----
[CW] collect: return: 135.50118, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 1.19987, qf2_loss: 1.22647, policy_loss: -70.43288, policy_entropy: -6.12238, alpha: 0.01797, time: 51.50750
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   227 ----
[CW] collect: return: 56.89966, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 1.20088, qf2_loss: 1.21307, policy_loss: -70.95445, policy_entropy: -6.33619, alpha: 0.01810, time: 51.10187
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   228 ----
[CW] collect: return: 136.01918, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 1.40320, qf2_loss: 1.41957, policy_loss: -70.73712, policy_entropy: -6.36696, alpha: 0.01835, time: 50.69015
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   229 ----
[CW] collect: return: 154.48816, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 1.71765, qf2_loss: 1.73483, policy_loss: -71.22904, policy_entropy: -6.15317, alpha: 0.01862, time: 50.74628
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   230 ----
[CW] collect: return: 183.16081, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 2.50668, qf2_loss: 2.53596, policy_loss: -71.26976, policy_entropy: -5.66966, alpha: 0.01851, time: 51.10939
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   231 ----
[CW] collect: return: 73.74270, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 1.16593, qf2_loss: 1.18112, policy_loss: -70.02198, policy_entropy: -5.68461, alpha: 0.01829, time: 51.16941
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   232 ----
[CW] collect: return: 372.36023, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 1.09117, qf2_loss: 1.10713, policy_loss: -71.39556, policy_entropy: -5.88154, alpha: 0.01810, time: 51.12163
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   233 ----
[CW] collect: return: 114.65355, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 1.45020, qf2_loss: 1.46207, policy_loss: -71.50037, policy_entropy: -5.95902, alpha: 0.01808, time: 51.17252
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   234 ----
[CW] collect: return: 241.50032, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 1.24508, qf2_loss: 1.26099, policy_loss: -71.03079, policy_entropy: -5.95134, alpha: 0.01797, time: 51.06161
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   235 ----
[CW] collect: return: 94.03083, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 1.19895, qf2_loss: 1.21205, policy_loss: -71.21204, policy_entropy: -6.25007, alpha: 0.01808, time: 51.22073
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   236 ----
[CW] collect: return: 385.85053, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 1.21289, qf2_loss: 1.22050, policy_loss: -71.26095, policy_entropy: -6.22378, alpha: 0.01825, time: 50.71524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   237 ----
[CW] collect: return: 88.54499, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 1.51933, qf2_loss: 1.54738, policy_loss: -72.28879, policy_entropy: -6.34555, alpha: 0.01849, time: 51.06458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   238 ----
[CW] collect: return: 443.64829, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 2.24625, qf2_loss: 2.26643, policy_loss: -72.16390, policy_entropy: -6.12925, alpha: 0.01868, time: 51.14045
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   239 ----
[CW] collect: return: 415.10063, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 1.55923, qf2_loss: 1.57088, policy_loss: -72.20623, policy_entropy: -6.10005, alpha: 0.01877, time: 51.04088
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   240 ----
[CW] collect: return: 191.27004, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 1.47146, qf2_loss: 1.49461, policy_loss: -72.29354, policy_entropy: -6.10851, alpha: 0.01886, time: 51.04868
[CW] eval: return: 363.99996, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   241 ----
[CW] collect: return: 452.66416, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 1.39544, qf2_loss: 1.40382, policy_loss: -71.78827, policy_entropy: -6.06272, alpha: 0.01894, time: 51.10453
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   242 ----
[CW] collect: return: 455.50885, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 1.32927, qf2_loss: 1.34650, policy_loss: -72.66129, policy_entropy: -6.21734, alpha: 0.01907, time: 50.96080
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   243 ----
[CW] collect: return: 321.44644, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 1.77289, qf2_loss: 1.78489, policy_loss: -73.15894, policy_entropy: -6.20252, alpha: 0.01922, time: 50.99440
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   244 ----
[CW] collect: return: 266.37740, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 2.42880, qf2_loss: 2.47021, policy_loss: -73.49419, policy_entropy: -6.35938, alpha: 0.01949, time: 51.21086
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   245 ----
[CW] collect: return: 206.26783, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 1.75147, qf2_loss: 1.75547, policy_loss: -73.53666, policy_entropy: -6.21884, alpha: 0.01981, time: 51.07158
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   246 ----
[CW] collect: return: 120.32024, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 1.37932, qf2_loss: 1.40105, policy_loss: -73.33269, policy_entropy: -6.12540, alpha: 0.02005, time: 51.01735
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   247 ----
[CW] collect: return: 410.68954, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 1.46241, qf2_loss: 1.46533, policy_loss: -73.42936, policy_entropy: -5.90947, alpha: 0.02010, time: 51.00590
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   248 ----
[CW] collect: return: 163.83945, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 1.71969, qf2_loss: 1.72553, policy_loss: -72.63593, policy_entropy: -5.47882, alpha: 0.01976, time: 51.13310
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   249 ----
[CW] collect: return: 401.07550, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 2.17521, qf2_loss: 2.19747, policy_loss: -73.16861, policy_entropy: -5.61193, alpha: 0.01932, time: 51.13936
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   250 ----
[CW] collect: return: 124.86124, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 2.04824, qf2_loss: 2.06263, policy_loss: -74.09954, policy_entropy: -5.94527, alpha: 0.01909, time: 50.88950
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   251 ----
[CW] collect: return: 446.07833, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 2.09264, qf2_loss: 2.10996, policy_loss: -74.17813, policy_entropy: -6.14303, alpha: 0.01915, time: 51.20966
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   252 ----
[CW] collect: return: 399.95138, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 1.83250, qf2_loss: 1.84145, policy_loss: -74.69621, policy_entropy: -5.93025, alpha: 0.01915, time: 50.87622
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   253 ----
[CW] collect: return: 170.28690, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 1.51305, qf2_loss: 1.52571, policy_loss: -73.37593, policy_entropy: -5.88544, alpha: 0.01909, time: 52.04130
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   254 ----
[CW] collect: return: 373.12667, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 1.89924, qf2_loss: 1.91499, policy_loss: -74.34076, policy_entropy: -5.90160, alpha: 0.01897, time: 51.98170
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   255 ----
[CW] collect: return: 120.23262, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 2.19402, qf2_loss: 2.20210, policy_loss: -75.79897, policy_entropy: -6.63503, alpha: 0.01912, time: 51.89994
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   256 ----
[CW] collect: return: 415.59239, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 2.65786, qf2_loss: 2.67079, policy_loss: -74.34030, policy_entropy: -6.31657, alpha: 0.01956, time: 52.14953
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   257 ----
[CW] collect: return: 132.86239, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 1.81654, qf2_loss: 1.84064, policy_loss: -74.99001, policy_entropy: -6.48510, alpha: 0.01997, time: 51.90231
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   258 ----
[CW] collect: return: 154.05450, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 2.78050, qf2_loss: 2.79454, policy_loss: -74.79753, policy_entropy: -6.33034, alpha: 0.02041, time: 52.14159
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   259 ----
[CW] collect: return: 460.03900, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 2.14335, qf2_loss: 2.18851, policy_loss: -75.11744, policy_entropy: -6.11423, alpha: 0.02062, time: 51.86827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   260 ----
[CW] collect: return: 429.40076, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 2.10011, qf2_loss: 2.12058, policy_loss: -75.58777, policy_entropy: -5.78041, alpha: 0.02055, time: 51.90029
[CW] eval: return: 397.07497, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   261 ----
[CW] collect: return: 413.12710, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 1.92238, qf2_loss: 1.94646, policy_loss: -76.19731, policy_entropy: -5.88072, alpha: 0.02042, time: 51.95150
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   262 ----
[CW] collect: return: 440.09783, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 2.92583, qf2_loss: 2.94602, policy_loss: -76.33978, policy_entropy: -5.99117, alpha: 0.02034, time: 51.91135
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   263 ----
[CW] collect: return: 364.60372, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 2.27567, qf2_loss: 2.30125, policy_loss: -76.33253, policy_entropy: -6.13589, alpha: 0.02039, time: 51.71240
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   264 ----
[CW] collect: return: 452.59852, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 2.99628, qf2_loss: 3.03430, policy_loss: -76.33876, policy_entropy: -6.11427, alpha: 0.02054, time: 51.74600
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   265 ----
[CW] collect: return: 188.45385, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 3.67206, qf2_loss: 3.71500, policy_loss: -76.75871, policy_entropy: -5.98654, alpha: 0.02059, time: 52.27940
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   266 ----
[CW] collect: return: 189.26573, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 3.01166, qf2_loss: 3.02941, policy_loss: -76.85753, policy_entropy: -6.00433, alpha: 0.02056, time: 54.88182
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   267 ----
[CW] collect: return: 417.37564, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 2.50514, qf2_loss: 2.53094, policy_loss: -77.30022, policy_entropy: -6.18025, alpha: 0.02062, time: 53.22484
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   268 ----
[CW] collect: return: 109.34084, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 3.35812, qf2_loss: 3.37881, policy_loss: -76.85658, policy_entropy: -6.08295, alpha: 0.02076, time: 51.65652
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   269 ----
[CW] collect: return: 130.58557, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 2.68388, qf2_loss: 2.74695, policy_loss: -76.76860, policy_entropy: -5.91836, alpha: 0.02080, time: 51.77449
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   270 ----
[CW] collect: return: 258.78352, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 2.30186, qf2_loss: 2.33785, policy_loss: -77.25851, policy_entropy: -5.83164, alpha: 0.02070, time: 51.71886
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   271 ----
[CW] collect: return: 171.56909, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 2.20697, qf2_loss: 2.23551, policy_loss: -77.27717, policy_entropy: -6.04871, alpha: 0.02059, time: 51.68446
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   272 ----
[CW] collect: return: 215.69926, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 2.00044, qf2_loss: 2.04142, policy_loss: -76.64829, policy_entropy: -5.86078, alpha: 0.02059, time: 51.93244
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   273 ----
[CW] collect: return: 57.83617, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 1.90424, qf2_loss: 1.92159, policy_loss: -77.08252, policy_entropy: -6.16767, alpha: 0.02057, time: 51.80830
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   274 ----
[CW] collect: return: 208.07498, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 2.09286, qf2_loss: 2.11201, policy_loss: -78.12265, policy_entropy: -6.25807, alpha: 0.02075, time: 51.57650
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   275 ----
[CW] collect: return: 431.52334, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 3.16136, qf2_loss: 3.21990, policy_loss: -77.91529, policy_entropy: -6.23655, alpha: 0.02094, time: 51.68121
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   276 ----
[CW] collect: return: 399.35085, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 7.31188, qf2_loss: 7.38784, policy_loss: -77.78196, policy_entropy: -5.98563, alpha: 0.02109, time: 51.66807
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   277 ----
[CW] collect: return: 429.67276, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 4.03337, qf2_loss: 4.05319, policy_loss: -78.57905, policy_entropy: -6.66146, alpha: 0.02133, time: 51.59388
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   278 ----
[CW] collect: return: 130.56369, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 2.33902, qf2_loss: 2.38888, policy_loss: -79.14577, policy_entropy: -6.79703, alpha: 0.02204, time: 51.48959
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   279 ----
[CW] collect: return: 346.04979, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 2.01811, qf2_loss: 2.05595, policy_loss: -77.88692, policy_entropy: -6.24968, alpha: 0.02251, time: 51.80617
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   280 ----
[CW] collect: return: 511.14691, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 1.94111, qf2_loss: 1.96944, policy_loss: -79.11567, policy_entropy: -6.34254, alpha: 0.02277, time: 51.99219
[CW] eval: return: 221.73200, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   281 ----
[CW] collect: return: 134.54380, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 2.06572, qf2_loss: 2.10440, policy_loss: -79.20253, policy_entropy: -6.22774, alpha: 0.02309, time: 51.82854
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   282 ----
[CW] collect: return: 431.27649, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 2.14383, qf2_loss: 2.19470, policy_loss: -79.42847, policy_entropy: -6.13481, alpha: 0.02326, time: 51.63516
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   283 ----
[CW] collect: return: 316.72520, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 2.19022, qf2_loss: 2.24367, policy_loss: -79.09232, policy_entropy: -5.95408, alpha: 0.02332, time: 51.49761
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   284 ----
[CW] collect: return: 422.03312, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 3.43002, qf2_loss: 3.48346, policy_loss: -78.81197, policy_entropy: -6.05023, alpha: 0.02334, time: 51.68881
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   285 ----
[CW] collect: return: 475.45921, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 3.03788, qf2_loss: 3.08685, policy_loss: -80.55720, policy_entropy: -6.34128, alpha: 0.02353, time: 51.47842
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   286 ----
[CW] collect: return: 101.21111, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 2.48485, qf2_loss: 2.52235, policy_loss: -79.99292, policy_entropy: -6.24177, alpha: 0.02380, time: 51.58590
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   287 ----
[CW] collect: return: 144.96895, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 2.60579, qf2_loss: 2.67216, policy_loss: -80.47848, policy_entropy: -6.13720, alpha: 0.02402, time: 51.53804
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   288 ----
[CW] collect: return: 37.78069, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 2.77386, qf2_loss: 2.82353, policy_loss: -80.41862, policy_entropy: -6.20000, alpha: 0.02415, time: 51.43187
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   289 ----
[CW] collect: return: 266.81552, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 3.60063, qf2_loss: 3.65104, policy_loss: -80.44761, policy_entropy: -6.31721, alpha: 0.02442, time: 51.57182
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   290 ----
[CW] collect: return: 476.27035, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 2.61094, qf2_loss: 2.65939, policy_loss: -81.16223, policy_entropy: -6.28284, alpha: 0.02479, time: 51.52770
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   291 ----
[CW] collect: return: 507.45480, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 2.56500, qf2_loss: 2.62440, policy_loss: -80.57181, policy_entropy: -6.12522, alpha: 0.02507, time: 51.27207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   292 ----
[CW] collect: return: 481.51664, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 2.63147, qf2_loss: 2.69180, policy_loss: -80.96998, policy_entropy: -6.13533, alpha: 0.02522, time: 51.54056
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   293 ----
[CW] collect: return: 388.81608, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 2.65270, qf2_loss: 2.70829, policy_loss: -81.45043, policy_entropy: -6.17884, alpha: 0.02534, time: 51.45754
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   294 ----
[CW] collect: return: 441.75360, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 2.93611, qf2_loss: 2.99192, policy_loss: -81.87573, policy_entropy: -6.30742, alpha: 0.02566, time: 51.52066
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   295 ----
[CW] collect: return: 309.20029, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 3.97282, qf2_loss: 3.98916, policy_loss: -82.03976, policy_entropy: -6.09645, alpha: 0.02582, time: 51.56138
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   296 ----
[CW] collect: return: 477.91484, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 4.53026, qf2_loss: 4.61248, policy_loss: -82.75265, policy_entropy: -6.37611, alpha: 0.02614, time: 51.28227
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   297 ----
[CW] collect: return: 468.33639, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 3.04898, qf2_loss: 3.12748, policy_loss: -81.77857, policy_entropy: -6.44482, alpha: 0.02674, time: 51.37695
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   298 ----
[CW] collect: return: 371.27236, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 3.15211, qf2_loss: 3.20546, policy_loss: -82.56966, policy_entropy: -6.28239, alpha: 0.02710, time: 51.47854
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   299 ----
[CW] collect: return: 480.14557, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 3.40270, qf2_loss: 3.47731, policy_loss: -83.19151, policy_entropy: -6.31078, alpha: 0.02758, time: 51.26345
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   300 ----
[CW] collect: return: 154.45357, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 3.50992, qf2_loss: 3.58320, policy_loss: -83.10273, policy_entropy: -6.12210, alpha: 0.02781, time: 51.35562
[CW] eval: return: 208.40419, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   301 ----
[CW] collect: return: 104.60782, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 3.29416, qf2_loss: 3.35257, policy_loss: -82.84680, policy_entropy: -6.16360, alpha: 0.02800, time: 51.23163
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   302 ----
[CW] collect: return: 254.06717, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 3.31154, qf2_loss: 3.39975, policy_loss: -82.62482, policy_entropy: -6.21397, alpha: 0.02824, time: 51.13110
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   303 ----
[CW] collect: return: 248.65595, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 3.17881, qf2_loss: 3.21227, policy_loss: -82.65180, policy_entropy: -6.43360, alpha: 0.02868, time: 51.39090
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   304 ----
[CW] collect: return: 120.29334, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 3.25606, qf2_loss: 3.29596, policy_loss: -83.82535, policy_entropy: -6.44524, alpha: 0.02933, time: 51.12357
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   305 ----
[CW] collect: return: 408.88682, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 5.19690, qf2_loss: 5.23511, policy_loss: -83.42611, policy_entropy: -6.16417, alpha: 0.02984, time: 51.08238
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   306 ----
[CW] collect: return: 460.72442, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 7.04798, qf2_loss: 7.15480, policy_loss: -83.34294, policy_entropy: -6.04295, alpha: 0.03006, time: 51.21804
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   307 ----
[CW] collect: return: 471.03772, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 3.24289, qf2_loss: 3.27238, policy_loss: -84.96870, policy_entropy: -5.97036, alpha: 0.02993, time: 51.15845
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   308 ----
[CW] collect: return: 54.91861, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 3.28009, qf2_loss: 3.32969, policy_loss: -84.74056, policy_entropy: -6.22198, alpha: 0.03005, time: 51.34996
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   309 ----
[CW] collect: return: 59.48194, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 3.12111, qf2_loss: 3.15045, policy_loss: -84.03010, policy_entropy: -6.24292, alpha: 0.03041, time: 51.13624
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   310 ----
[CW] collect: return: 66.28757, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 3.37751, qf2_loss: 3.44674, policy_loss: -84.35848, policy_entropy: -5.92376, alpha: 0.03063, time: 51.23433
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   311 ----
[CW] collect: return: 483.84636, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 4.52320, qf2_loss: 4.57993, policy_loss: -84.35439, policy_entropy: -5.86794, alpha: 0.03045, time: 51.25021
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   312 ----
[CW] collect: return: 445.89832, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 4.84863, qf2_loss: 4.87441, policy_loss: -84.24178, policy_entropy: -6.23527, alpha: 0.03047, time: 50.93459
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   313 ----
[CW] collect: return: 433.31991, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 4.12945, qf2_loss: 4.15047, policy_loss: -85.38182, policy_entropy: -6.25047, alpha: 0.03083, time: 51.08637
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   314 ----
[CW] collect: return: 486.12072, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 3.65552, qf2_loss: 3.66781, policy_loss: -84.81808, policy_entropy: -5.83109, alpha: 0.03094, time: 50.90577
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   315 ----
[CW] collect: return: 516.47218, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 3.96249, qf2_loss: 4.01965, policy_loss: -85.47673, policy_entropy: -6.16787, alpha: 0.03081, time: 51.07126
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   316 ----
[CW] collect: return: 326.12157, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 4.01964, qf2_loss: 4.05574, policy_loss: -85.44165, policy_entropy: -6.48507, alpha: 0.03123, time: 51.05930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   317 ----
[CW] collect: return: 478.08430, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 4.29815, qf2_loss: 4.40040, policy_loss: -85.95852, policy_entropy: -6.31209, alpha: 0.03199, time: 51.05531
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   318 ----
[CW] collect: return: 459.30078, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 4.92436, qf2_loss: 4.98919, policy_loss: -86.01317, policy_entropy: -6.45474, alpha: 0.03237, time: 51.16458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   319 ----
[CW] collect: return: 449.29039, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 8.87121, qf2_loss: 8.89537, policy_loss: -85.88629, policy_entropy: -6.17305, alpha: 0.03304, time: 50.73315
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   320 ----
[CW] collect: return: 455.43276, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 5.29689, qf2_loss: 5.34779, policy_loss: -86.02627, policy_entropy: -6.00711, alpha: 0.03313, time: 51.33867
[CW] eval: return: 341.13483, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   321 ----
[CW] collect: return: 459.26866, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 6.78834, qf2_loss: 6.84681, policy_loss: -87.14954, policy_entropy: -6.14344, alpha: 0.03325, time: 51.35058
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   322 ----
[CW] collect: return: 491.60059, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 6.29752, qf2_loss: 6.33041, policy_loss: -87.62703, policy_entropy: -6.07495, alpha: 0.03347, time: 51.19039
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   323 ----
[CW] collect: return: 414.00031, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 4.60213, qf2_loss: 4.63478, policy_loss: -87.81863, policy_entropy: -6.11812, alpha: 0.03356, time: 51.19615
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   324 ----
[CW] collect: return: 368.22938, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 4.01434, qf2_loss: 4.04843, policy_loss: -88.72660, policy_entropy: -6.55085, alpha: 0.03406, time: 51.20781
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   325 ----
[CW] collect: return: 459.37586, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 3.70078, qf2_loss: 3.75661, policy_loss: -88.70481, policy_entropy: -6.54843, alpha: 0.03495, time: 51.18140
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   326 ----
[CW] collect: return: 48.81744, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 3.65595, qf2_loss: 3.69230, policy_loss: -88.49895, policy_entropy: -6.37069, alpha: 0.03580, time: 51.10045
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   327 ----
[CW] collect: return: 486.74970, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 4.67403, qf2_loss: 4.72730, policy_loss: -89.07328, policy_entropy: -6.09642, alpha: 0.03635, time: 51.13222
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   328 ----
[CW] collect: return: 127.09452, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 4.26719, qf2_loss: 4.31310, policy_loss: -90.28286, policy_entropy: -6.12685, alpha: 0.03640, time: 51.06790
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   329 ----
[CW] collect: return: 61.94167, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 5.75696, qf2_loss: 5.84310, policy_loss: -88.02855, policy_entropy: -6.23678, alpha: 0.03682, time: 51.12195
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   330 ----
[CW] collect: return: 471.29294, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 5.12874, qf2_loss: 5.23783, policy_loss: -89.88802, policy_entropy: -6.14308, alpha: 0.03715, time: 51.06357
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   331 ----
[CW] collect: return: 281.38669, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 7.99235, qf2_loss: 8.11488, policy_loss: -89.65999, policy_entropy: -6.13255, alpha: 0.03746, time: 51.08201
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   332 ----
[CW] collect: return: 292.94702, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 8.40862, qf2_loss: 8.51359, policy_loss: -88.92873, policy_entropy: -5.84918, alpha: 0.03746, time: 51.13619
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   333 ----
[CW] collect: return: 465.33482, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 5.81595, qf2_loss: 5.88248, policy_loss: -90.08291, policy_entropy: -6.16691, alpha: 0.03745, time: 50.97951
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   334 ----
[CW] collect: return: 67.51151, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 6.32905, qf2_loss: 6.34666, policy_loss: -89.74142, policy_entropy: -6.04308, alpha: 0.03765, time: 51.22110
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   335 ----
[CW] collect: return: 413.71554, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 5.23065, qf2_loss: 5.23601, policy_loss: -89.99582, policy_entropy: -6.12436, alpha: 0.03769, time: 50.94032
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   336 ----
[CW] collect: return: 168.13269, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 4.96761, qf2_loss: 5.03340, policy_loss: -88.76759, policy_entropy: -6.12954, alpha: 0.03816, time: 50.98945
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   337 ----
[CW] collect: return: 545.34052, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 5.13090, qf2_loss: 5.10306, policy_loss: -90.83790, policy_entropy: -5.97518, alpha: 0.03810, time: 50.97135
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   338 ----
[CW] collect: return: 510.56559, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 7.03381, qf2_loss: 7.17535, policy_loss: -90.26355, policy_entropy: -6.37505, alpha: 0.03835, time: 51.03264
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   339 ----
[CW] collect: return: 409.92941, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 6.27243, qf2_loss: 6.41474, policy_loss: -91.06687, policy_entropy: -6.29848, alpha: 0.03897, time: 51.17577
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   340 ----
[CW] collect: return: 366.30141, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 6.05794, qf2_loss: 6.06164, policy_loss: -92.07299, policy_entropy: -6.26980, alpha: 0.03970, time: 50.97564
[CW] eval: return: 449.12178, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   341 ----
[CW] collect: return: 495.29543, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 6.34651, qf2_loss: 6.48081, policy_loss: -91.17825, policy_entropy: -6.26770, alpha: 0.04038, time: 51.09680
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   342 ----
[CW] collect: return: 488.81974, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 5.77285, qf2_loss: 5.82911, policy_loss: -92.93583, policy_entropy: -6.42675, alpha: 0.04110, time: 51.40585
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   343 ----
[CW] collect: return: 472.09384, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 5.27335, qf2_loss: 5.28848, policy_loss: -93.10196, policy_entropy: -6.22994, alpha: 0.04182, time: 51.20930
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   344 ----
[CW] collect: return: 495.52914, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 6.40229, qf2_loss: 6.42058, policy_loss: -92.41222, policy_entropy: -6.18127, alpha: 0.04244, time: 51.24342
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   345 ----
[CW] collect: return: 135.91259, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 6.40431, qf2_loss: 6.44746, policy_loss: -92.74913, policy_entropy: -6.18863, alpha: 0.04266, time: 51.14380
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   346 ----
[CW] collect: return: 217.87017, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 14.36487, qf2_loss: 14.35511, policy_loss: -92.32778, policy_entropy: -6.05262, alpha: 0.04313, time: 50.97290
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   347 ----
[CW] collect: return: 512.33833, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 6.65382, qf2_loss: 6.71306, policy_loss: -93.51352, policy_entropy: -5.96041, alpha: 0.04313, time: 51.03083
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   348 ----
[CW] collect: return: 492.01450, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 6.18027, qf2_loss: 6.22786, policy_loss: -93.71683, policy_entropy: -6.06416, alpha: 0.04304, time: 51.22841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   349 ----
[CW] collect: return: 97.76018, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 4.70588, qf2_loss: 4.78979, policy_loss: -92.58781, policy_entropy: -6.28745, alpha: 0.04358, time: 51.00856
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   350 ----
[CW] collect: return: 491.84595, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 5.93619, qf2_loss: 6.00952, policy_loss: -94.13862, policy_entropy: -6.14437, alpha: 0.04413, time: 51.07571
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   351 ----
[CW] collect: return: 149.81903, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 4.80691, qf2_loss: 4.86581, policy_loss: -93.82610, policy_entropy: -5.77727, alpha: 0.04415, time: 51.27505
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   352 ----
[CW] collect: return: 498.43544, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 4.23205, qf2_loss: 4.29017, policy_loss: -94.32475, policy_entropy: -5.67594, alpha: 0.04343, time: 51.27591
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   353 ----
[CW] collect: return: 506.61913, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 4.84633, qf2_loss: 4.87558, policy_loss: -95.15185, policy_entropy: -5.50411, alpha: 0.04251, time: 57.87524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   354 ----
[CW] collect: return: 469.29522, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 5.28547, qf2_loss: 5.36555, policy_loss: -95.63150, policy_entropy: -6.24449, alpha: 0.04185, time: 51.49349
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   355 ----
[CW] collect: return: 444.23430, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 4.84469, qf2_loss: 4.95912, policy_loss: -96.89940, policy_entropy: -6.49017, alpha: 0.04277, time: 51.35618
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   356 ----
[CW] collect: return: 504.67743, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 5.12849, qf2_loss: 5.11752, policy_loss: -96.53661, policy_entropy: -6.35090, alpha: 0.04378, time: 51.81223
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   357 ----
[CW] collect: return: 210.05227, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 4.58763, qf2_loss: 4.65924, policy_loss: -97.75151, policy_entropy: -6.18631, alpha: 0.04444, time: 51.35984
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   358 ----
[CW] collect: return: 479.20505, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 4.75422, qf2_loss: 4.79274, policy_loss: -96.89491, policy_entropy: -6.23730, alpha: 0.04505, time: 51.01994
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   359 ----
[CW] collect: return: 64.93389, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 5.21686, qf2_loss: 5.42748, policy_loss: -97.11408, policy_entropy: -5.75245, alpha: 0.04500, time: 51.06134
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   360 ----
[CW] collect: return: 480.71936, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 6.99823, qf2_loss: 7.17595, policy_loss: -97.40988, policy_entropy: -5.72284, alpha: 0.04436, time: 51.05493
[CW] eval: return: 453.60967, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   361 ----
[CW] collect: return: 441.70149, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 8.69817, qf2_loss: 8.94479, policy_loss: -98.25066, policy_entropy: -5.73204, alpha: 0.04396, time: 51.06885
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   362 ----
[CW] collect: return: 37.54074, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 6.17592, qf2_loss: 6.30609, policy_loss: -96.40847, policy_entropy: -5.85534, alpha: 0.04315, time: 51.31013
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   363 ----
[CW] collect: return: 311.86591, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 8.13143, qf2_loss: 7.87667, policy_loss: -96.83113, policy_entropy: -5.93176, alpha: 0.04317, time: 51.30701
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   364 ----
[CW] collect: return: 290.20027, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 5.66342, qf2_loss: 5.74234, policy_loss: -98.69271, policy_entropy: -5.63033, alpha: 0.04265, time: 50.89731
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   365 ----
[CW] collect: return: 354.17188, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 4.75244, qf2_loss: 4.90762, policy_loss: -99.37888, policy_entropy: -5.83639, alpha: 0.04204, time: 50.96189
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   366 ----
[CW] collect: return: 493.67050, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 4.78967, qf2_loss: 4.89825, policy_loss: -98.85208, policy_entropy: -5.64881, alpha: 0.04140, time: 51.02121
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   367 ----
[CW] collect: return: 204.46814, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 4.59668, qf2_loss: 4.65444, policy_loss: -99.03928, policy_entropy: -5.65702, alpha: 0.04088, time: 50.76127
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   368 ----
[CW] collect: return: 507.88360, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 4.92787, qf2_loss: 5.06754, policy_loss: -99.89943, policy_entropy: -5.72959, alpha: 0.04022, time: 51.05680
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   369 ----
[CW] collect: return: 481.72187, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 6.65580, qf2_loss: 6.81120, policy_loss: -101.02848, policy_entropy: -6.17849, alpha: 0.04003, time: 51.33194
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   370 ----
[CW] collect: return: 456.24483, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 5.63357, qf2_loss: 5.65532, policy_loss: -100.65532, policy_entropy: -5.79490, alpha: 0.04012, time: 51.16760
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   371 ----
[CW] collect: return: 458.80277, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 5.68943, qf2_loss: 5.87287, policy_loss: -100.04018, policy_entropy: -5.68182, alpha: 0.03960, time: 51.33809
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   372 ----
[CW] collect: return: 371.93008, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 5.30646, qf2_loss: 5.43711, policy_loss: -101.05122, policy_entropy: -6.29109, alpha: 0.03948, time: 51.09859
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   373 ----
[CW] collect: return: 3.55358, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 5.41390, qf2_loss: 5.44718, policy_loss: -102.79197, policy_entropy: -5.92784, alpha: 0.03983, time: 51.16350
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   374 ----
[CW] collect: return: 472.30986, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 7.76471, qf2_loss: 7.88390, policy_loss: -101.90108, policy_entropy: -5.57063, alpha: 0.03942, time: 51.29182
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   375 ----
[CW] collect: return: 298.42346, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 20.61471, qf2_loss: 20.96751, policy_loss: -102.72774, policy_entropy: -5.56518, alpha: 0.03872, time: 51.22248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   376 ----
[CW] collect: return: 486.80715, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 11.01216, qf2_loss: 11.01588, policy_loss: -102.64892, policy_entropy: -5.91940, alpha: 0.03805, time: 51.34129
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   377 ----
[CW] collect: return: 497.93289, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 6.30934, qf2_loss: 6.37136, policy_loss: -102.48349, policy_entropy: -6.73154, alpha: 0.03859, time: 51.35229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   378 ----
[CW] collect: return: 548.12529, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 5.78911, qf2_loss: 5.92766, policy_loss: -101.71157, policy_entropy: -6.56196, alpha: 0.03976, time: 51.18786
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   379 ----
[CW] collect: return: 210.32498, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 5.17671, qf2_loss: 5.22823, policy_loss: -103.83673, policy_entropy: -6.52094, alpha: 0.04073, time: 51.34775
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   380 ----
[CW] collect: return: 173.75328, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 5.19406, qf2_loss: 5.32237, policy_loss: -103.74625, policy_entropy: -6.35887, alpha: 0.04152, time: 51.31833
[CW] eval: return: 428.20325, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   381 ----
[CW] collect: return: 518.44626, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 4.72434, qf2_loss: 4.79186, policy_loss: -104.08351, policy_entropy: -6.18237, alpha: 0.04195, time: 50.93305
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   382 ----
[CW] collect: return: 539.07908, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 5.11907, qf2_loss: 5.19126, policy_loss: -103.92015, policy_entropy: -6.08881, alpha: 0.04232, time: 51.09362
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   383 ----
[CW] collect: return: 375.04512, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 5.12805, qf2_loss: 5.24231, policy_loss: -103.94890, policy_entropy: -5.76648, alpha: 0.04232, time: 51.32434
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   384 ----
[CW] collect: return: 514.09086, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 4.85255, qf2_loss: 4.96592, policy_loss: -103.76332, policy_entropy: -5.30893, alpha: 0.04133, time: 51.09497
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   385 ----
[CW] collect: return: 553.68419, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 10.97564, qf2_loss: 10.67356, policy_loss: -104.73866, policy_entropy: -5.54469, alpha: 0.04024, time: 51.51229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   386 ----
[CW] collect: return: 524.32612, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 16.89732, qf2_loss: 17.14398, policy_loss: -103.91461, policy_entropy: -5.81914, alpha: 0.03965, time: 51.46649
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   387 ----
[CW] collect: return: 156.28885, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 11.09971, qf2_loss: 11.38750, policy_loss: -105.21769, policy_entropy: -6.21852, alpha: 0.03960, time: 51.27713
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   388 ----
[CW] collect: return: 65.71497, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 6.62511, qf2_loss: 6.82743, policy_loss: -104.43905, policy_entropy: -6.10765, alpha: 0.04005, time: 51.34438
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   389 ----
[CW] collect: return: 496.29233, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 6.21674, qf2_loss: 6.31990, policy_loss: -105.34168, policy_entropy: -5.76139, alpha: 0.03985, time: 51.21068
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   390 ----
[CW] collect: return: 520.93826, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 5.29758, qf2_loss: 5.41590, policy_loss: -107.28882, policy_entropy: -5.98680, alpha: 0.03961, time: 51.49768
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   391 ----
[CW] collect: return: 515.41643, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 4.83304, qf2_loss: 4.95091, policy_loss: -105.67578, policy_entropy: -5.74382, alpha: 0.03940, time: 51.23922
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   392 ----
[CW] collect: return: 54.25969, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 4.70887, qf2_loss: 4.80148, policy_loss: -106.43652, policy_entropy: -5.83557, alpha: 0.03906, time: 51.20427
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   393 ----
[CW] collect: return: 524.03057, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 5.38417, qf2_loss: 5.58145, policy_loss: -107.61070, policy_entropy: -6.06829, alpha: 0.03882, time: 51.47021
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   394 ----
[CW] collect: return: 27.61152, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 7.32181, qf2_loss: 7.42714, policy_loss: -106.32002, policy_entropy: -5.84719, alpha: 0.03884, time: 51.14683
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   395 ----
[CW] collect: return: 54.79837, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 6.38210, qf2_loss: 6.53358, policy_loss: -108.45392, policy_entropy: -5.97073, alpha: 0.03865, time: 51.18979
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   396 ----
[CW] collect: return: 28.20328, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 5.25267, qf2_loss: 5.26989, policy_loss: -107.34841, policy_entropy: -6.07429, alpha: 0.03877, time: 51.17265
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   397 ----
[CW] collect: return: 54.52015, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 7.68752, qf2_loss: 7.83969, policy_loss: -108.12046, policy_entropy: -6.29632, alpha: 0.03903, time: 51.30677
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   398 ----
[CW] collect: return: 29.31063, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 6.28187, qf2_loss: 6.41254, policy_loss: -107.50697, policy_entropy: -5.92194, alpha: 0.03922, time: 51.42305
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   399 ----
[CW] collect: return: 52.99313, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 6.43185, qf2_loss: 6.59225, policy_loss: -108.52879, policy_entropy: -6.16203, alpha: 0.03915, time: 51.57194
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   400 ----
[CW] collect: return: 26.19271, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 7.68797, qf2_loss: 7.68413, policy_loss: -108.85880, policy_entropy: -6.24355, alpha: 0.03969, time: 51.57528
[CW] eval: return: 23.43341, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   401 ----
[CW] collect: return: 18.78930, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 6.72442, qf2_loss: 6.83745, policy_loss: -108.53422, policy_entropy: -6.50703, alpha: 0.04034, time: 51.41822
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   402 ----
[CW] collect: return: 17.87526, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 5.35798, qf2_loss: 5.51961, policy_loss: -109.42649, policy_entropy: -6.37472, alpha: 0.04117, time: 51.41271
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   403 ----
[CW] collect: return: 20.19742, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 7.63097, qf2_loss: 7.63310, policy_loss: -109.87309, policy_entropy: -6.36934, alpha: 0.04184, time: 51.38016
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   404 ----
[CW] collect: return: 31.87493, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 8.91451, qf2_loss: 9.07291, policy_loss: -109.61442, policy_entropy: -6.68733, alpha: 0.04277, time: 51.31461
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   405 ----
[CW] collect: return: 36.33560, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 7.16008, qf2_loss: 7.35079, policy_loss: -108.79627, policy_entropy: -6.46184, alpha: 0.04410, time: 51.29394
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   406 ----
[CW] collect: return: 25.02454, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 9.71236, qf2_loss: 9.86634, policy_loss: -108.50169, policy_entropy: -6.47082, alpha: 0.04504, time: 51.42676
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   407 ----
[CW] collect: return: 30.80894, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 9.36931, qf2_loss: 9.37335, policy_loss: -109.48205, policy_entropy: -5.93968, alpha: 0.04582, time: 51.68126
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   408 ----
[CW] collect: return: 40.02433, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 7.89370, qf2_loss: 8.05769, policy_loss: -111.58323, policy_entropy: -6.00312, alpha: 0.04547, time: 51.62881
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   409 ----
[CW] collect: return: 11.90084, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 9.70950, qf2_loss: 9.93861, policy_loss: -110.60839, policy_entropy: -6.10967, alpha: 0.04566, time: 51.80593
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   410 ----
[CW] collect: return: 29.10469, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 8.54635, qf2_loss: 8.71509, policy_loss: -110.39697, policy_entropy: -6.09489, alpha: 0.04597, time: 52.02124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   411 ----
[CW] collect: return: 28.40473, steps: 1000.00000, total_steps: 417000.00000
[CW] train: qf1_loss: 9.82154, qf2_loss: 10.08872, policy_loss: -110.20019, policy_entropy: -5.91408, alpha: 0.04593, time: 52.81941
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   412 ----
[CW] collect: return: 49.46288, steps: 1000.00000, total_steps: 418000.00000
[CW] train: qf1_loss: 7.83373, qf2_loss: 7.92791, policy_loss: -110.34689, policy_entropy: -5.90685, alpha: 0.04572, time: 51.56137
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   413 ----
[CW] collect: return: 41.22289, steps: 1000.00000, total_steps: 419000.00000
[CW] train: qf1_loss: 7.32248, qf2_loss: 7.37215, policy_loss: -109.69138, policy_entropy: -5.92509, alpha: 0.04545, time: 51.46174
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   414 ----
[CW] collect: return: 44.93632, steps: 1000.00000, total_steps: 420000.00000
[CW] train: qf1_loss: 7.61625, qf2_loss: 7.55986, policy_loss: -112.15205, policy_entropy: -5.91407, alpha: 0.04544, time: 51.23586
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   415 ----
[CW] collect: return: 58.52919, steps: 1000.00000, total_steps: 421000.00000
[CW] train: qf1_loss: 9.07154, qf2_loss: 9.01652, policy_loss: -110.49829, policy_entropy: -5.82914, alpha: 0.04508, time: 51.46136
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   416 ----
[CW] collect: return: 35.99130, steps: 1000.00000, total_steps: 422000.00000
[CW] train: qf1_loss: 12.45133, qf2_loss: 12.72078, policy_loss: -111.71386, policy_entropy: -6.23854, alpha: 0.04507, time: 51.31597
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   417 ----
[CW] collect: return: 42.50621, steps: 1000.00000, total_steps: 423000.00000
[CW] train: qf1_loss: 8.29965, qf2_loss: 8.31048, policy_loss: -112.20258, policy_entropy: -5.99868, alpha: 0.04539, time: 51.17429
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   418 ----
[CW] collect: return: 30.79722, steps: 1000.00000, total_steps: 424000.00000
[CW] train: qf1_loss: 7.70722, qf2_loss: 7.69320, policy_loss: -112.01929, policy_entropy: -5.99740, alpha: 0.04540, time: 51.39176
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   419 ----
[CW] collect: return: 23.46762, steps: 1000.00000, total_steps: 425000.00000
[CW] train: qf1_loss: 16.40905, qf2_loss: 16.40905, policy_loss: -112.36361, policy_entropy: -5.81868, alpha: 0.04516, time: 51.67819
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   420 ----
[CW] collect: return: 34.79936, steps: 1000.00000, total_steps: 426000.00000
[CW] train: qf1_loss: 11.17512, qf2_loss: 11.23781, policy_loss: -112.02926, policy_entropy: -5.79134, alpha: 0.04479, time: 51.56749
[CW] eval: return: 32.04718, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   421 ----
[CW] collect: return: 36.83361, steps: 1000.00000, total_steps: 427000.00000
[CW] train: qf1_loss: 10.03654, qf2_loss: 10.08200, policy_loss: -113.33417, policy_entropy: -5.99290, alpha: 0.04449, time: 51.72568
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   422 ----
[CW] collect: return: 33.43908, steps: 1000.00000, total_steps: 428000.00000
[CW] train: qf1_loss: 8.76353, qf2_loss: 8.80959, policy_loss: -112.11089, policy_entropy: -5.95319, alpha: 0.04450, time: 51.49046
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   423 ----
[CW] collect: return: 21.31228, steps: 1000.00000, total_steps: 429000.00000
[CW] train: qf1_loss: 9.30386, qf2_loss: 9.53159, policy_loss: -113.46145, policy_entropy: -6.16701, alpha: 0.04457, time: 51.36185
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   424 ----
[CW] collect: return: 24.12198, steps: 1000.00000, total_steps: 430000.00000
[CW] train: qf1_loss: 10.94735, qf2_loss: 10.72642, policy_loss: -115.09930, policy_entropy: -6.44416, alpha: 0.04508, time: 51.45656
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   425 ----
[CW] collect: return: 29.66443, steps: 1000.00000, total_steps: 431000.00000
[CW] train: qf1_loss: 10.64696, qf2_loss: 10.66240, policy_loss: -115.20077, policy_entropy: -6.28754, alpha: 0.04584, time: 54.53236
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   426 ----
[CW] collect: return: 27.17662, steps: 1000.00000, total_steps: 432000.00000
[CW] train: qf1_loss: 10.21380, qf2_loss: 10.31956, policy_loss: -114.84080, policy_entropy: -6.13346, alpha: 0.04632, time: 51.97329
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   427 ----
[CW] collect: return: 34.84353, steps: 1000.00000, total_steps: 433000.00000
[CW] train: qf1_loss: 9.97251, qf2_loss: 10.08103, policy_loss: -116.45857, policy_entropy: -6.06737, alpha: 0.04660, time: 51.63358
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   428 ----
[CW] collect: return: 48.69717, steps: 1000.00000, total_steps: 434000.00000
[CW] train: qf1_loss: 9.69485, qf2_loss: 9.60522, policy_loss: -116.58529, policy_entropy: -6.13655, alpha: 0.04668, time: 51.66286
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   429 ----
[CW] collect: return: 79.96147, steps: 1000.00000, total_steps: 435000.00000
[CW] train: qf1_loss: 9.21470, qf2_loss: 9.25593, policy_loss: -116.78825, policy_entropy: -6.06504, alpha: 0.04694, time: 51.73964
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   430 ----
[CW] collect: return: 28.61346, steps: 1000.00000, total_steps: 436000.00000
[CW] train: qf1_loss: 11.80606, qf2_loss: 11.78952, policy_loss: -117.20503, policy_entropy: -6.07118, alpha: 0.04706, time: 51.57200
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   431 ----
[CW] collect: return: 24.93942, steps: 1000.00000, total_steps: 437000.00000
[CW] train: qf1_loss: 9.27743, qf2_loss: 9.33651, policy_loss: -117.87314, policy_entropy: -5.94048, alpha: 0.04719, time: 51.71886
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   432 ----
[CW] collect: return: 29.25416, steps: 1000.00000, total_steps: 438000.00000
[CW] train: qf1_loss: 9.12402, qf2_loss: 9.22571, policy_loss: -119.08605, policy_entropy: -6.03825, alpha: 0.04703, time: 51.76418
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   433 ----
[CW] collect: return: 31.49772, steps: 1000.00000, total_steps: 439000.00000
[CW] train: qf1_loss: 11.09750, qf2_loss: 11.27758, policy_loss: -117.88019, policy_entropy: -5.81117, alpha: 0.04697, time: 51.57486
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   434 ----
[CW] collect: return: 34.70849, steps: 1000.00000, total_steps: 440000.00000
[CW] train: qf1_loss: 9.60105, qf2_loss: 9.59257, policy_loss: -120.27075, policy_entropy: -5.88197, alpha: 0.04673, time: 51.68443
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   435 ----
[CW] collect: return: 30.08124, steps: 1000.00000, total_steps: 441000.00000
[CW] train: qf1_loss: 9.34500, qf2_loss: 9.43859, policy_loss: -119.53005, policy_entropy: -5.83391, alpha: 0.04636, time: 51.60092
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   436 ----
[CW] collect: return: 35.41566, steps: 1000.00000, total_steps: 442000.00000
[CW] train: qf1_loss: 10.21469, qf2_loss: 10.23667, policy_loss: -120.19735, policy_entropy: -5.55900, alpha: 0.04582, time: 51.52303
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   437 ----
[CW] collect: return: 47.88296, steps: 1000.00000, total_steps: 443000.00000
[CW] train: qf1_loss: 9.27501, qf2_loss: 9.38290, policy_loss: -121.06601, policy_entropy: -5.65523, alpha: 0.04516, time: 51.68671
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   438 ----
[CW] collect: return: 29.62246, steps: 1000.00000, total_steps: 444000.00000
[CW] train: qf1_loss: 12.33817, qf2_loss: 12.43051, policy_loss: -120.89110, policy_entropy: -5.70319, alpha: 0.04461, time: 51.58479
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   439 ----
[CW] collect: return: 32.31039, steps: 1000.00000, total_steps: 445000.00000
[CW] train: qf1_loss: 11.12927, qf2_loss: 11.13844, policy_loss: -122.13022, policy_entropy: -5.79288, alpha: 0.04419, time: 51.64571
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   440 ----
[CW] collect: return: 29.59039, steps: 1000.00000, total_steps: 446000.00000
[CW] train: qf1_loss: 11.96859, qf2_loss: 12.05477, policy_loss: -121.34888, policy_entropy: -5.61473, alpha: 0.04372, time: 51.58083
[CW] eval: return: 29.62842, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   441 ----
[CW] collect: return: 29.11279, steps: 1000.00000, total_steps: 447000.00000
[CW] train: qf1_loss: 13.05429, qf2_loss: 13.13855, policy_loss: -121.40700, policy_entropy: -5.72464, alpha: 0.04308, time: 51.55237
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   442 ----
[CW] collect: return: 27.57312, steps: 1000.00000, total_steps: 448000.00000
[CW] train: qf1_loss: 9.50255, qf2_loss: 9.56466, policy_loss: -122.75346, policy_entropy: -6.58861, alpha: 0.04333, time: 51.54829
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   443 ----
[CW] collect: return: 23.87331, steps: 1000.00000, total_steps: 449000.00000
[CW] train: qf1_loss: 11.15829, qf2_loss: 11.30114, policy_loss: -122.43459, policy_entropy: -6.50837, alpha: 0.04425, time: 51.52513
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   444 ----
[CW] collect: return: 27.84537, steps: 1000.00000, total_steps: 450000.00000
[CW] train: qf1_loss: 9.69197, qf2_loss: 9.70773, policy_loss: -124.71293, policy_entropy: -6.56685, alpha: 0.04515, time: 51.19073
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   445 ----
[CW] collect: return: 36.72127, steps: 1000.00000, total_steps: 451000.00000
[CW] train: qf1_loss: 9.45126, qf2_loss: 9.55921, policy_loss: -124.59001, policy_entropy: -6.34231, alpha: 0.04602, time: 51.10047
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   446 ----
[CW] collect: return: 52.36762, steps: 1000.00000, total_steps: 452000.00000
[CW] train: qf1_loss: 9.96611, qf2_loss: 9.99355, policy_loss: -124.02300, policy_entropy: -6.02444, alpha: 0.04630, time: 51.36779
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   447 ----
[CW] collect: return: 37.15344, steps: 1000.00000, total_steps: 453000.00000
[CW] train: qf1_loss: 10.31983, qf2_loss: 10.40518, policy_loss: -125.52788, policy_entropy: -5.82017, alpha: 0.04628, time: 51.46598
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   448 ----
[CW] collect: return: 35.03375, steps: 1000.00000, total_steps: 454000.00000
[CW] train: qf1_loss: 11.47059, qf2_loss: 11.58475, policy_loss: -125.33636, policy_entropy: -5.68457, alpha: 0.04568, time: 51.52272
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   449 ----
[CW] collect: return: 40.96896, steps: 1000.00000, total_steps: 455000.00000
[CW] train: qf1_loss: 11.82147, qf2_loss: 11.80573, policy_loss: -125.45222, policy_entropy: -6.00493, alpha: 0.04551, time: 51.38372
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   450 ----
[CW] collect: return: 44.23210, steps: 1000.00000, total_steps: 456000.00000
[CW] train: qf1_loss: 9.48277, qf2_loss: 9.59043, policy_loss: -126.39833, policy_entropy: -5.73250, alpha: 0.04538, time: 51.57315
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   451 ----
[CW] collect: return: 38.27820, steps: 1000.00000, total_steps: 457000.00000
[CW] train: qf1_loss: 11.65401, qf2_loss: 11.95688, policy_loss: -125.39786, policy_entropy: -5.57590, alpha: 0.04468, time: 51.41128
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   452 ----
[CW] collect: return: 54.05615, steps: 1000.00000, total_steps: 458000.00000
[CW] train: qf1_loss: 10.51454, qf2_loss: 10.44700, policy_loss: -126.66050, policy_entropy: -5.90644, alpha: 0.04408, time: 51.60368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   453 ----
[CW] collect: return: 43.70052, steps: 1000.00000, total_steps: 459000.00000
[CW] train: qf1_loss: 12.14505, qf2_loss: 12.23770, policy_loss: -126.79975, policy_entropy: -5.97433, alpha: 0.04404, time: 51.76856
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   454 ----
[CW] collect: return: 148.79081, steps: 1000.00000, total_steps: 460000.00000
[CW] train: qf1_loss: 12.19104, qf2_loss: 12.33697, policy_loss: -127.23416, policy_entropy: -5.77061, alpha: 0.04385, time: 51.55960
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   455 ----
[CW] collect: return: 124.79914, steps: 1000.00000, total_steps: 461000.00000
[CW] train: qf1_loss: 9.94480, qf2_loss: 10.15102, policy_loss: -126.82314, policy_entropy: -5.81447, alpha: 0.04352, time: 51.73530
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   456 ----
[CW] collect: return: 84.25314, steps: 1000.00000, total_steps: 462000.00000
[CW] train: qf1_loss: 10.89862, qf2_loss: 11.12132, policy_loss: -126.49168, policy_entropy: -5.84838, alpha: 0.04319, time: 51.48295
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   457 ----
[CW] collect: return: 110.90969, steps: 1000.00000, total_steps: 463000.00000
[CW] train: qf1_loss: 9.05795, qf2_loss: 9.14994, policy_loss: -129.63328, policy_entropy: -5.93269, alpha: 0.04299, time: 51.39366
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   458 ----
[CW] collect: return: 164.77156, steps: 1000.00000, total_steps: 464000.00000
[CW] train: qf1_loss: 12.07253, qf2_loss: 12.03639, policy_loss: -128.39837, policy_entropy: -5.80439, alpha: 0.04276, time: 51.65902
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   459 ----
[CW] collect: return: 41.12925, steps: 1000.00000, total_steps: 465000.00000
[CW] train: qf1_loss: 9.89736, qf2_loss: 9.91161, policy_loss: -128.43964, policy_entropy: -5.83186, alpha: 0.04246, time: 51.97458
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   460 ----
[CW] collect: return: 40.46333, steps: 1000.00000, total_steps: 466000.00000
[CW] train: qf1_loss: 9.03716, qf2_loss: 9.04651, policy_loss: -129.70541, policy_entropy: -5.88796, alpha: 0.04219, time: 52.08775
[CW] eval: return: 247.44874, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   461 ----
[CW] collect: return: 193.78489, steps: 1000.00000, total_steps: 467000.00000
[CW] train: qf1_loss: 9.46859, qf2_loss: 9.48186, policy_loss: -129.17285, policy_entropy: -6.29026, alpha: 0.04234, time: 52.05643
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   462 ----
[CW] collect: return: 264.24831, steps: 1000.00000, total_steps: 468000.00000
[CW] train: qf1_loss: 9.64171, qf2_loss: 9.65497, policy_loss: -127.38725, policy_entropy: -6.09004, alpha: 0.04275, time: 51.86523
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   463 ----
[CW] collect: return: 281.65857, steps: 1000.00000, total_steps: 469000.00000
[CW] train: qf1_loss: 9.92522, qf2_loss: 10.02750, policy_loss: -129.28412, policy_entropy: -5.85022, alpha: 0.04275, time: 51.98209
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   464 ----
[CW] collect: return: 280.98933, steps: 1000.00000, total_steps: 470000.00000
[CW] train: qf1_loss: 10.51180, qf2_loss: 10.63861, policy_loss: -130.68851, policy_entropy: -5.71258, alpha: 0.04234, time: 51.93301
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   465 ----
[CW] collect: return: 56.15846, steps: 1000.00000, total_steps: 471000.00000
[CW] train: qf1_loss: 9.84674, qf2_loss: 9.87835, policy_loss: -129.21361, policy_entropy: -5.32026, alpha: 0.04149, time: 51.58839
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   466 ----
[CW] collect: return: 232.99025, steps: 1000.00000, total_steps: 472000.00000
[CW] train: qf1_loss: 12.54830, qf2_loss: 12.56786, policy_loss: -130.59567, policy_entropy: -5.39115, alpha: 0.04046, time: 52.28578
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   467 ----
[CW] collect: return: 199.77126, steps: 1000.00000, total_steps: 473000.00000
[CW] train: qf1_loss: 15.39995, qf2_loss: 15.46767, policy_loss: -130.28944, policy_entropy: -5.59770, alpha: 0.03975, time: 52.35783
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   468 ----
[CW] collect: return: 279.89877, steps: 1000.00000, total_steps: 474000.00000
[CW] train: qf1_loss: 10.07585, qf2_loss: 10.01221, policy_loss: -129.47618, policy_entropy: -5.43675, alpha: 0.03894, time: 52.28638
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   469 ----
[CW] collect: return: 214.47391, steps: 1000.00000, total_steps: 475000.00000
[CW] train: qf1_loss: 9.01078, qf2_loss: 9.06630, policy_loss: -133.40860, policy_entropy: -5.96182, alpha: 0.03845, time: 52.14128
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   470 ----
[CW] collect: return: 291.76470, steps: 1000.00000, total_steps: 476000.00000
[CW] train: qf1_loss: 9.51878, qf2_loss: 9.62384, policy_loss: -131.55553, policy_entropy: -6.02311, alpha: 0.03853, time: 51.98544
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   471 ----
[CW] collect: return: 288.86124, steps: 1000.00000, total_steps: 477000.00000
[CW] train: qf1_loss: 10.38973, qf2_loss: 10.49881, policy_loss: -131.14490, policy_entropy: -5.91544, alpha: 0.03844, time: 51.90558
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   472 ----
[CW] collect: return: 265.53279, steps: 1000.00000, total_steps: 478000.00000
[CW] train: qf1_loss: 8.87096, qf2_loss: 8.92912, policy_loss: -132.46363, policy_entropy: -6.04252, alpha: 0.03842, time: 51.93139
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   473 ----
[CW] collect: return: 269.28176, steps: 1000.00000, total_steps: 479000.00000
[CW] train: qf1_loss: 15.59003, qf2_loss: 15.43322, policy_loss: -132.54602, policy_entropy: -6.04047, alpha: 0.03853, time: 52.28961
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   474 ----
[CW] collect: return: 169.46733, steps: 1000.00000, total_steps: 480000.00000
[CW] train: qf1_loss: 18.07949, qf2_loss: 18.27835, policy_loss: -131.93324, policy_entropy: -5.82499, alpha: 0.03836, time: 52.11826
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   475 ----
[CW] collect: return: 267.63832, steps: 1000.00000, total_steps: 481000.00000
[CW] train: qf1_loss: 10.94803, qf2_loss: 10.96415, policy_loss: -132.21497, policy_entropy: -5.92428, alpha: 0.03814, time: 52.25719
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   476 ----
[CW] collect: return: 289.42196, steps: 1000.00000, total_steps: 482000.00000
[CW] train: qf1_loss: 8.77065, qf2_loss: 8.71300, policy_loss: -132.39764, policy_entropy: -6.26207, alpha: 0.03826, time: 52.17197
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   477 ----
[CW] collect: return: 317.06457, steps: 1000.00000, total_steps: 483000.00000
[CW] train: qf1_loss: 9.94162, qf2_loss: 9.95239, policy_loss: -132.82235, policy_entropy: -5.99029, alpha: 0.03857, time: 52.11124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   478 ----
[CW] collect: return: 312.07208, steps: 1000.00000, total_steps: 484000.00000
[CW] train: qf1_loss: 9.55225, qf2_loss: 9.41130, policy_loss: -133.63221, policy_entropy: -6.05874, alpha: 0.03850, time: 51.62025
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   479 ----
[CW] collect: return: 318.73893, steps: 1000.00000, total_steps: 485000.00000
[CW] train: qf1_loss: 10.25180, qf2_loss: 10.23143, policy_loss: -132.39640, policy_entropy: -6.09459, alpha: 0.03875, time: 51.32591
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   480 ----
[CW] collect: return: 292.13473, steps: 1000.00000, total_steps: 486000.00000
[CW] train: qf1_loss: 9.21003, qf2_loss: 9.33668, policy_loss: -131.76775, policy_entropy: -5.81902, alpha: 0.03874, time: 51.69311
[CW] eval: return: 300.51072, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   481 ----
[CW] collect: return: 298.15454, steps: 1000.00000, total_steps: 487000.00000
[CW] train: qf1_loss: 9.25790, qf2_loss: 9.30287, policy_loss: -134.87783, policy_entropy: -6.13738, alpha: 0.03855, time: 52.07925
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   482 ----
[CW] collect: return: 277.03459, steps: 1000.00000, total_steps: 488000.00000
[CW] train: qf1_loss: 9.71219, qf2_loss: 9.77689, policy_loss: -135.51813, policy_entropy: -6.06637, alpha: 0.03875, time: 54.15203
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   483 ----
[CW] collect: return: 262.46561, steps: 1000.00000, total_steps: 489000.00000
[CW] train: qf1_loss: 11.14348, qf2_loss: 11.11275, policy_loss: -134.50166, policy_entropy: -6.06162, alpha: 0.03886, time: 52.29351
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   484 ----
[CW] collect: return: 301.18989, steps: 1000.00000, total_steps: 490000.00000
[CW] train: qf1_loss: 9.76164, qf2_loss: 9.58954, policy_loss: -133.96086, policy_entropy: -6.10902, alpha: 0.03905, time: 52.37454
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   485 ----
[CW] collect: return: 307.27463, steps: 1000.00000, total_steps: 491000.00000
[CW] train: qf1_loss: 8.10131, qf2_loss: 8.15723, policy_loss: -135.40960, policy_entropy: -6.10301, alpha: 0.03928, time: 52.42708
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   486 ----
[CW] collect: return: 289.42656, steps: 1000.00000, total_steps: 492000.00000
[CW] train: qf1_loss: 7.79841, qf2_loss: 7.70977, policy_loss: -133.95095, policy_entropy: -5.91177, alpha: 0.03931, time: 52.14942
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   487 ----
[CW] collect: return: 316.88532, steps: 1000.00000, total_steps: 493000.00000
[CW] train: qf1_loss: 8.13159, qf2_loss: 8.19926, policy_loss: -134.77091, policy_entropy: -6.01252, alpha: 0.03929, time: 52.00702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   488 ----
[CW] collect: return: 285.47794, steps: 1000.00000, total_steps: 494000.00000
[CW] train: qf1_loss: 12.14549, qf2_loss: 12.14070, policy_loss: -135.79004, policy_entropy: -5.86360, alpha: 0.03910, time: 52.28183
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   489 ----
[CW] collect: return: 277.84948, steps: 1000.00000, total_steps: 495000.00000
[CW] train: qf1_loss: 15.47180, qf2_loss: 15.63546, policy_loss: -135.52963, policy_entropy: -5.85593, alpha: 0.03877, time: 52.29273
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   490 ----
[CW] collect: return: 343.36057, steps: 1000.00000, total_steps: 496000.00000
[CW] train: qf1_loss: 12.18266, qf2_loss: 12.38128, policy_loss: -136.43674, policy_entropy: -6.02410, alpha: 0.03873, time: 52.33041
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   491 ----
[CW] collect: return: 124.70859, steps: 1000.00000, total_steps: 497000.00000
[CW] train: qf1_loss: 9.98330, qf2_loss: 10.03019, policy_loss: -136.34023, policy_entropy: -5.97094, alpha: 0.03878, time: 52.09585
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   492 ----
[CW] collect: return: 301.17631, steps: 1000.00000, total_steps: 498000.00000
[CW] train: qf1_loss: 9.50580, qf2_loss: 9.49945, policy_loss: -133.72562, policy_entropy: -5.70768, alpha: 0.03839, time: 52.21350
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   493 ----
[CW] collect: return: 251.21551, steps: 1000.00000, total_steps: 499000.00000
[CW] train: qf1_loss: 7.60793, qf2_loss: 7.75548, policy_loss: -134.75839, policy_entropy: -5.78668, alpha: 0.03791, time: 52.04786
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   494 ----
[CW] collect: return: 285.67962, steps: 1000.00000, total_steps: 500000.00000
[CW] train: qf1_loss: 8.38028, qf2_loss: 8.41404, policy_loss: -135.15750, policy_entropy: -5.83496, alpha: 0.03757, time: 52.15269
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   495 ----
[CW] collect: return: 347.29788, steps: 1000.00000, total_steps: 501000.00000
[CW] train: qf1_loss: 7.86046, qf2_loss: 7.75932, policy_loss: -135.70544, policy_entropy: -5.93871, alpha: 0.03736, time: 51.93377
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   496 ----
[CW] collect: return: 321.31005, steps: 1000.00000, total_steps: 502000.00000
[CW] train: qf1_loss: 7.88400, qf2_loss: 7.92516, policy_loss: -137.09638, policy_entropy: -6.14511, alpha: 0.03740, time: 51.90304
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   497 ----
[CW] collect: return: 348.79089, steps: 1000.00000, total_steps: 503000.00000
[CW] train: qf1_loss: 8.93721, qf2_loss: 8.88935, policy_loss: -135.70986, policy_entropy: -6.18125, alpha: 0.03771, time: 52.43088
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   498 ----
[CW] collect: return: 358.65547, steps: 1000.00000, total_steps: 504000.00000
[CW] train: qf1_loss: 18.47854, qf2_loss: 18.91770, policy_loss: -137.07545, policy_entropy: -5.84298, alpha: 0.03781, time: 52.02192
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   499 ----
[CW] collect: return: 337.06359, steps: 1000.00000, total_steps: 505000.00000
[CW] train: qf1_loss: 17.29260, qf2_loss: 17.63586, policy_loss: -137.22267, policy_entropy: -5.99044, alpha: 0.03766, time: 52.00618
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   500 ----
[CW] collect: return: 316.45195, steps: 1000.00000, total_steps: 506000.00000
[CW] train: qf1_loss: 38.24900, qf2_loss: 38.75550, policy_loss: -136.91203, policy_entropy: -5.67097, alpha: 0.03720, time: 51.81942
[CW] eval: return: 317.20863, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   501 ----
[CW] collect: return: 329.59795, steps: 1000.00000, total_steps: 507000.00000
[CW] train: qf1_loss: 13.37330, qf2_loss: 13.55283, policy_loss: -137.30717, policy_entropy: -5.68192, alpha: 0.03676, time: 52.15308
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   502 ----
[CW] collect: return: 123.79715, steps: 1000.00000, total_steps: 508000.00000
[CW] train: qf1_loss: 11.63877, qf2_loss: 11.74133, policy_loss: -136.57771, policy_entropy: -5.85447, alpha: 0.03632, time: 51.95731
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   503 ----
[CW] collect: return: 326.06714, steps: 1000.00000, total_steps: 509000.00000
[CW] train: qf1_loss: 6.96518, qf2_loss: 7.01286, policy_loss: -135.20242, policy_entropy: -5.65085, alpha: 0.03590, time: 51.88569
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   504 ----
[CW] collect: return: 347.54208, steps: 1000.00000, total_steps: 510000.00000
[CW] train: qf1_loss: 7.25516, qf2_loss: 7.42805, policy_loss: -135.59700, policy_entropy: -5.34915, alpha: 0.03516, time: 52.39619
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   505 ----
[CW] collect: return: 303.64096, steps: 1000.00000, total_steps: 511000.00000
[CW] train: qf1_loss: 6.95291, qf2_loss: 7.06317, policy_loss: -137.03463, policy_entropy: -5.80506, alpha: 0.03434, time: 52.29088
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   506 ----
[CW] collect: return: 292.18616, steps: 1000.00000, total_steps: 512000.00000
[CW] train: qf1_loss: 11.35202, qf2_loss: 11.38244, policy_loss: -137.48376, policy_entropy: -6.00252, alpha: 0.03413, time: 52.15601
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   507 ----
[CW] collect: return: 328.39431, steps: 1000.00000, total_steps: 513000.00000
[CW] train: qf1_loss: 7.44253, qf2_loss: 7.44668, policy_loss: -136.06909, policy_entropy: -5.98654, alpha: 0.03424, time: 52.14821
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   508 ----
[CW] collect: return: 294.11441, steps: 1000.00000, total_steps: 514000.00000
[CW] train: qf1_loss: 6.99930, qf2_loss: 7.14484, policy_loss: -138.06557, policy_entropy: -6.09227, alpha: 0.03430, time: 52.06641
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   509 ----
[CW] collect: return: 43.68033, steps: 1000.00000, total_steps: 515000.00000
[CW] train: qf1_loss: 6.56936, qf2_loss: 6.69923, policy_loss: -136.41401, policy_entropy: -6.05178, alpha: 0.03436, time: 51.98844
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   510 ----
[CW] collect: return: 116.14693, steps: 1000.00000, total_steps: 516000.00000
[CW] train: qf1_loss: 7.80078, qf2_loss: 7.85033, policy_loss: -136.43907, policy_entropy: -6.02035, alpha: 0.03440, time: 52.17853
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   511 ----
[CW] collect: return: 267.29713, steps: 1000.00000, total_steps: 517000.00000
[CW] train: qf1_loss: 6.85206, qf2_loss: 6.98387, policy_loss: -135.77451, policy_entropy: -6.07246, alpha: 0.03446, time: 52.20415
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   512 ----
[CW] collect: return: 311.75274, steps: 1000.00000, total_steps: 518000.00000
[CW] train: qf1_loss: 6.81089, qf2_loss: 6.86194, policy_loss: -134.19269, policy_entropy: -5.73390, alpha: 0.03443, time: 52.20507
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   513 ----
[CW] collect: return: 227.62710, steps: 1000.00000, total_steps: 519000.00000
[CW] train: qf1_loss: 7.63100, qf2_loss: 7.74249, policy_loss: -136.13119, policy_entropy: -5.68315, alpha: 0.03401, time: 52.45189
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   514 ----
[CW] collect: return: 354.45769, steps: 1000.00000, total_steps: 520000.00000
[CW] train: qf1_loss: 6.41866, qf2_loss: 6.58052, policy_loss: -137.59997, policy_entropy: -6.03359, alpha: 0.03372, time: 52.25929
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   515 ----
[CW] collect: return: 319.64451, steps: 1000.00000, total_steps: 521000.00000
[CW] train: qf1_loss: 7.79244, qf2_loss: 7.89182, policy_loss: -137.15075, policy_entropy: -5.90394, alpha: 0.03366, time: 52.17091
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   516 ----
[CW] collect: return: 315.60798, steps: 1000.00000, total_steps: 522000.00000
[CW] train: qf1_loss: 7.55806, qf2_loss: 7.64131, policy_loss: -137.05596, policy_entropy: -5.79258, alpha: 0.03349, time: 52.00988
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   517 ----
[CW] collect: return: 196.82186, steps: 1000.00000, total_steps: 523000.00000
[CW] train: qf1_loss: 7.06134, qf2_loss: 7.16753, policy_loss: -135.65270, policy_entropy: -5.78250, alpha: 0.03317, time: 51.94475
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   518 ----
[CW] collect: return: 338.64310, steps: 1000.00000, total_steps: 524000.00000
[CW] train: qf1_loss: 7.97034, qf2_loss: 8.12273, policy_loss: -137.26332, policy_entropy: -5.92968, alpha: 0.03294, time: 52.03010
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   519 ----
[CW] collect: return: 323.13280, steps: 1000.00000, total_steps: 525000.00000
[CW] train: qf1_loss: 21.55135, qf2_loss: 22.46683, policy_loss: -138.36701, policy_entropy: -5.72889, alpha: 0.03274, time: 52.41203
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   520 ----
[CW] collect: return: 311.60747, steps: 1000.00000, total_steps: 526000.00000
[CW] train: qf1_loss: 23.57442, qf2_loss: 24.19237, policy_loss: -136.69177, policy_entropy: -5.38928, alpha: 0.03211, time: 52.18654
[CW] eval: return: 320.17581, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   521 ----
[CW] collect: return: 355.05924, steps: 1000.00000, total_steps: 527000.00000
[CW] train: qf1_loss: 11.71586, qf2_loss: 11.72800, policy_loss: -135.40305, policy_entropy: -5.62569, alpha: 0.03147, time: 51.92262
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   522 ----
[CW] collect: return: 122.99999, steps: 1000.00000, total_steps: 528000.00000
[CW] train: qf1_loss: 7.38812, qf2_loss: 7.41284, policy_loss: -136.96866, policy_entropy: -5.78334, alpha: 0.03110, time: 51.67267
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   523 ----
[CW] collect: return: 342.68349, steps: 1000.00000, total_steps: 529000.00000
[CW] train: qf1_loss: 11.12861, qf2_loss: 11.22970, policy_loss: -135.82178, policy_entropy: -5.71801, alpha: 0.03086, time: 52.37332
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   524 ----
[CW] collect: return: 333.12337, steps: 1000.00000, total_steps: 530000.00000
[CW] train: qf1_loss: 8.12895, qf2_loss: 8.21771, policy_loss: -137.43718, policy_entropy: -5.75095, alpha: 0.03044, time: 52.06218
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   525 ----
[CW] collect: return: 331.32031, steps: 1000.00000, total_steps: 531000.00000
[CW] train: qf1_loss: 11.20994, qf2_loss: 11.44871, policy_loss: -136.84719, policy_entropy: -5.99369, alpha: 0.03024, time: 52.03296
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   526 ----
[CW] collect: return: 214.40240, steps: 1000.00000, total_steps: 532000.00000
[CW] train: qf1_loss: 14.95647, qf2_loss: 15.04554, policy_loss: -137.17864, policy_entropy: -6.04204, alpha: 0.03032, time: 52.00802
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   527 ----
[CW] collect: return: 317.04928, steps: 1000.00000, total_steps: 533000.00000
[CW] train: qf1_loss: 12.42621, qf2_loss: 12.80390, policy_loss: -137.27370, policy_entropy: -5.87840, alpha: 0.03021, time: 52.03595
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   528 ----
[CW] collect: return: 365.26737, steps: 1000.00000, total_steps: 534000.00000
[CW] train: qf1_loss: 13.68844, qf2_loss: 13.76548, policy_loss: -138.11354, policy_entropy: -6.06992, alpha: 0.03019, time: 52.20574
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   529 ----
[CW] collect: return: 298.12444, steps: 1000.00000, total_steps: 535000.00000
[CW] train: qf1_loss: 10.05934, qf2_loss: 10.16719, policy_loss: -136.79802, policy_entropy: -6.16632, alpha: 0.03038, time: 51.94484
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   530 ----
[CW] collect: return: 371.75540, steps: 1000.00000, total_steps: 536000.00000
[CW] train: qf1_loss: 7.50894, qf2_loss: 7.64667, policy_loss: -135.60748, policy_entropy: -5.83603, alpha: 0.03040, time: 52.38323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   531 ----
[CW] collect: return: 300.58353, steps: 1000.00000, total_steps: 537000.00000
[CW] train: qf1_loss: 7.11432, qf2_loss: 7.33888, policy_loss: -135.63878, policy_entropy: -5.81651, alpha: 0.03026, time: 52.42048
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   532 ----
[CW] collect: return: 397.36859, steps: 1000.00000, total_steps: 538000.00000
[CW] train: qf1_loss: 7.25287, qf2_loss: 7.37337, policy_loss: -134.46094, policy_entropy: -5.72159, alpha: 0.02987, time: 52.43868
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   533 ----
[CW] collect: return: 357.32651, steps: 1000.00000, total_steps: 539000.00000
[CW] train: qf1_loss: 7.50277, qf2_loss: 7.69414, policy_loss: -137.26595, policy_entropy: -6.02919, alpha: 0.02971, time: 52.11007
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   534 ----
[CW] collect: return: 379.25042, steps: 1000.00000, total_steps: 540000.00000
[CW] train: qf1_loss: 11.67295, qf2_loss: 11.96617, policy_loss: -135.71932, policy_entropy: -6.01088, alpha: 0.02976, time: 51.91921
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   535 ----
[CW] collect: return: 340.85567, steps: 1000.00000, total_steps: 541000.00000
[CW] train: qf1_loss: 14.96821, qf2_loss: 15.13330, policy_loss: -137.47553, policy_entropy: -5.84646, alpha: 0.02967, time: 52.13613
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   536 ----
[CW] collect: return: 416.33250, steps: 1000.00000, total_steps: 542000.00000
[CW] train: qf1_loss: 15.20264, qf2_loss: 15.74956, policy_loss: -135.53718, policy_entropy: -5.70358, alpha: 0.02930, time: 51.99710
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   537 ----
[CW] collect: return: 434.52530, steps: 1000.00000, total_steps: 543000.00000
[CW] train: qf1_loss: 13.29846, qf2_loss: 13.46104, policy_loss: -135.07448, policy_entropy: -5.66272, alpha: 0.02899, time: 52.14510
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   538 ----
[CW] collect: return: 357.92575, steps: 1000.00000, total_steps: 544000.00000
[CW] train: qf1_loss: 8.62472, qf2_loss: 8.83816, policy_loss: -136.46206, policy_entropy: -5.89107, alpha: 0.02879, time: 52.17485
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   539 ----
[CW] collect: return: 400.23846, steps: 1000.00000, total_steps: 545000.00000
[CW] train: qf1_loss: 10.55665, qf2_loss: 10.57329, policy_loss: -135.24601, policy_entropy: -5.78725, alpha: 0.02854, time: 52.20903
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   540 ----
[CW] collect: return: 366.78951, steps: 1000.00000, total_steps: 546000.00000
[CW] train: qf1_loss: 19.39490, qf2_loss: 19.84748, policy_loss: -137.44919, policy_entropy: -5.84461, alpha: 0.02839, time: 52.22017
[CW] eval: return: 310.01218, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   541 ----
[CW] collect: return: 432.10395, steps: 1000.00000, total_steps: 547000.00000
[CW] train: qf1_loss: 11.85700, qf2_loss: 11.97709, policy_loss: -137.83914, policy_entropy: -6.20924, alpha: 0.02846, time: 52.04605
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   542 ----
[CW] collect: return: 358.21662, steps: 1000.00000, total_steps: 548000.00000
[CW] train: qf1_loss: 10.11240, qf2_loss: 10.65676, policy_loss: -136.56213, policy_entropy: -5.93519, alpha: 0.02847, time: 52.35268
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   543 ----
[CW] collect: return: 298.22668, steps: 1000.00000, total_steps: 549000.00000
[CW] train: qf1_loss: 8.84583, qf2_loss: 9.19644, policy_loss: -136.89563, policy_entropy: -5.98364, alpha: 0.02844, time: 52.27530
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   544 ----
[CW] collect: return: 299.35336, steps: 1000.00000, total_steps: 550000.00000
[CW] train: qf1_loss: 10.12005, qf2_loss: 10.21422, policy_loss: -136.57385, policy_entropy: -6.02732, alpha: 0.02841, time: 52.02599
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   545 ----
[CW] collect: return: 72.83747, steps: 1000.00000, total_steps: 551000.00000
[CW] train: qf1_loss: 15.94346, qf2_loss: 16.15387, policy_loss: -136.17039, policy_entropy: -5.76279, alpha: 0.02837, time: 52.05207
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   546 ----
[CW] collect: return: 395.17899, steps: 1000.00000, total_steps: 552000.00000
[CW] train: qf1_loss: 14.16133, qf2_loss: 14.43062, policy_loss: -133.25106, policy_entropy: -5.63098, alpha: 0.02793, time: 52.20051
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   547 ----
[CW] collect: return: 372.08940, steps: 1000.00000, total_steps: 553000.00000
[CW] train: qf1_loss: 38.74908, qf2_loss: 40.52962, policy_loss: -135.74667, policy_entropy: -6.12903, alpha: 0.02779, time: 52.17368
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   548 ----
[CW] collect: return: 255.35328, steps: 1000.00000, total_steps: 554000.00000
[CW] train: qf1_loss: 20.30814, qf2_loss: 20.88324, policy_loss: -136.81242, policy_entropy: -6.06550, alpha: 0.02788, time: 51.96524
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   549 ----
[CW] collect: return: 381.31832, steps: 1000.00000, total_steps: 555000.00000
[CW] train: qf1_loss: 35.41621, qf2_loss: 36.19179, policy_loss: -136.79003, policy_entropy: -6.22360, alpha: 0.02805, time: 52.60831
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   550 ----
[CW] collect: return: 396.05785, steps: 1000.00000, total_steps: 556000.00000
[CW] train: qf1_loss: 59.24770, qf2_loss: 60.21285, policy_loss: -138.04962, policy_entropy: -6.48927, alpha: 0.02841, time: 52.00353
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   551 ----
[CW] collect: return: 313.52327, steps: 1000.00000, total_steps: 557000.00000
[CW] train: qf1_loss: 16.17201, qf2_loss: 16.47663, policy_loss: -135.32991, policy_entropy: -6.83785, alpha: 0.02918, time: 51.93841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   552 ----
[CW] collect: return: 420.36206, steps: 1000.00000, total_steps: 558000.00000
[CW] train: qf1_loss: 16.26183, qf2_loss: 16.93226, policy_loss: -136.34712, policy_entropy: -6.59663, alpha: 0.03004, time: 52.20953
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   553 ----
[CW] collect: return: 270.10057, steps: 1000.00000, total_steps: 559000.00000
[CW] train: qf1_loss: 11.77229, qf2_loss: 12.08742, policy_loss: -137.23188, policy_entropy: -6.58599, alpha: 0.03069, time: 53.31847
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   554 ----
[CW] collect: return: 350.79763, steps: 1000.00000, total_steps: 560000.00000
[CW] train: qf1_loss: 10.48837, qf2_loss: 10.73142, policy_loss: -136.10974, policy_entropy: -6.51715, alpha: 0.03141, time: 52.19376
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   555 ----
[CW] collect: return: 426.77485, steps: 1000.00000, total_steps: 561000.00000
[CW] train: qf1_loss: 8.55107, qf2_loss: 8.64592, policy_loss: -135.57559, policy_entropy: -6.50486, alpha: 0.03197, time: 51.82952
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   556 ----
[CW] collect: return: 389.81126, steps: 1000.00000, total_steps: 562000.00000
[CW] train: qf1_loss: 14.50710, qf2_loss: 14.83412, policy_loss: -137.18995, policy_entropy: -6.47697, alpha: 0.03267, time: 51.80198
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   557 ----
[CW] collect: return: 355.98766, steps: 1000.00000, total_steps: 563000.00000
[CW] train: qf1_loss: 10.43480, qf2_loss: 10.45162, policy_loss: -134.84286, policy_entropy: -6.18175, alpha: 0.03311, time: 51.96743
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   558 ----
[CW] collect: return: 297.85034, steps: 1000.00000, total_steps: 564000.00000
[CW] train: qf1_loss: 9.22153, qf2_loss: 9.60146, policy_loss: -136.37027, policy_entropy: -6.34146, alpha: 0.03346, time: 52.20799
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   559 ----
[CW] collect: return: 415.85546, steps: 1000.00000, total_steps: 565000.00000
[CW] train: qf1_loss: 16.38395, qf2_loss: 16.97978, policy_loss: -134.72744, policy_entropy: -6.35839, alpha: 0.03391, time: 52.29419
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   560 ----
[CW] collect: return: 172.80822, steps: 1000.00000, total_steps: 566000.00000
[CW] train: qf1_loss: 9.95126, qf2_loss: 10.28494, policy_loss: -135.09151, policy_entropy: -6.21123, alpha: 0.03447, time: 53.03599
[CW] eval: return: 389.74354, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   561 ----
[CW] collect: return: 426.63041, steps: 1000.00000, total_steps: 567000.00000
[CW] train: qf1_loss: 7.64512, qf2_loss: 7.74179, policy_loss: -134.50034, policy_entropy: -5.95632, alpha: 0.03454, time: 52.16809
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   562 ----
[CW] collect: return: 147.55547, steps: 1000.00000, total_steps: 568000.00000
[CW] train: qf1_loss: 14.98570, qf2_loss: 15.33163, policy_loss: -135.66649, policy_entropy: -5.69615, alpha: 0.03429, time: 52.02361
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   563 ----
[CW] collect: return: 357.02875, steps: 1000.00000, total_steps: 569000.00000
[CW] train: qf1_loss: 7.83605, qf2_loss: 7.94525, policy_loss: -138.35229, policy_entropy: -5.96339, alpha: 0.03400, time: 52.04704
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   564 ----
[CW] collect: return: 439.52888, steps: 1000.00000, total_steps: 570000.00000
[CW] train: qf1_loss: 12.30248, qf2_loss: 12.44129, policy_loss: -136.34434, policy_entropy: -5.84557, alpha: 0.03386, time: 51.77471
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   565 ----
[CW] collect: return: 316.65887, steps: 1000.00000, total_steps: 571000.00000
[CW] train: qf1_loss: 12.70426, qf2_loss: 12.98979, policy_loss: -136.84282, policy_entropy: -5.72686, alpha: 0.03353, time: 52.21423
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   566 ----
[CW] collect: return: 429.04010, steps: 1000.00000, total_steps: 572000.00000
[CW] train: qf1_loss: 10.19311, qf2_loss: 10.41504, policy_loss: -136.24102, policy_entropy: -5.72408, alpha: 0.03326, time: 52.26547
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   567 ----
[CW] collect: return: 343.81997, steps: 1000.00000, total_steps: 573000.00000
[CW] train: qf1_loss: 9.34034, qf2_loss: 9.49590, policy_loss: -133.78516, policy_entropy: -5.64635, alpha: 0.03281, time: 52.04229
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   568 ----
[CW] collect: return: 480.82850, steps: 1000.00000, total_steps: 574000.00000
[CW] train: qf1_loss: 8.69153, qf2_loss: 8.87273, policy_loss: -135.67753, policy_entropy: -5.82806, alpha: 0.03243, time: 52.06899
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   569 ----
[CW] collect: return: 368.22989, steps: 1000.00000, total_steps: 575000.00000
[CW] train: qf1_loss: 9.51141, qf2_loss: 9.54256, policy_loss: -136.97919, policy_entropy: -5.88154, alpha: 0.03227, time: 51.79827
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   570 ----
[CW] collect: return: 121.43093, steps: 1000.00000, total_steps: 576000.00000
[CW] train: qf1_loss: 10.92064, qf2_loss: 10.93387, policy_loss: -137.77643, policy_entropy: -5.90256, alpha: 0.03210, time: 52.19977
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   571 ----
[CW] collect: return: 555.67847, steps: 1000.00000, total_steps: 577000.00000
[CW] train: qf1_loss: 12.86417, qf2_loss: 12.99460, policy_loss: -137.35498, policy_entropy: -5.94766, alpha: 0.03202, time: 52.17834
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   572 ----
[CW] collect: return: 474.45018, steps: 1000.00000, total_steps: 578000.00000
[CW] train: qf1_loss: 8.79363, qf2_loss: 8.89505, policy_loss: -135.53501, policy_entropy: -5.90262, alpha: 0.03200, time: 52.14483
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   573 ----
[CW] collect: return: 486.22251, steps: 1000.00000, total_steps: 579000.00000
[CW] train: qf1_loss: 7.74453, qf2_loss: 7.83368, policy_loss: -135.35168, policy_entropy: -5.88624, alpha: 0.03180, time: 52.09702
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   574 ----
[CW] collect: return: 482.97153, steps: 1000.00000, total_steps: 580000.00000
[CW] train: qf1_loss: 9.12618, qf2_loss: 9.46431, policy_loss: -134.93565, policy_entropy: -5.89513, alpha: 0.03167, time: 52.24171
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   575 ----
[CW] collect: return: 497.56806, steps: 1000.00000, total_steps: 581000.00000
[CW] train: qf1_loss: 7.94885, qf2_loss: 8.06452, policy_loss: -135.98469, policy_entropy: -5.89421, alpha: 0.03157, time: 52.25879
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   576 ----
[CW] collect: return: 522.60642, steps: 1000.00000, total_steps: 582000.00000
[CW] train: qf1_loss: 12.98086, qf2_loss: 13.23783, policy_loss: -138.13854, policy_entropy: -6.20039, alpha: 0.03160, time: 51.91669
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   577 ----
[CW] collect: return: 415.63168, steps: 1000.00000, total_steps: 583000.00000
[CW] train: qf1_loss: 11.94860, qf2_loss: 12.06164, policy_loss: -137.00087, policy_entropy: -6.02639, alpha: 0.03177, time: 52.09967
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   578 ----
[CW] collect: return: 427.74989, steps: 1000.00000, total_steps: 584000.00000
[CW] train: qf1_loss: 74.43488, qf2_loss: 75.15162, policy_loss: -135.15380, policy_entropy: -5.65949, alpha: 0.03153, time: 52.08357
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   579 ----
[CW] collect: return: 319.43014, steps: 1000.00000, total_steps: 585000.00000
[CW] train: qf1_loss: 136.14802, qf2_loss: 137.30350, policy_loss: -134.72970, policy_entropy: -6.38903, alpha: 0.03150, time: 52.15768
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   580 ----
[CW] collect: return: 272.89681, steps: 1000.00000, total_steps: 586000.00000
[CW] train: qf1_loss: 19.63301, qf2_loss: 20.06963, policy_loss: -136.21817, policy_entropy: -6.13603, alpha: 0.03186, time: 52.16309
[CW] eval: return: 406.06933, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   581 ----
[CW] collect: return: 396.18847, steps: 1000.00000, total_steps: 587000.00000
[CW] train: qf1_loss: 11.88314, qf2_loss: 12.10789, policy_loss: -135.56073, policy_entropy: -6.17589, alpha: 0.03206, time: 52.16436
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   582 ----
[CW] collect: return: 461.83803, steps: 1000.00000, total_steps: 588000.00000
[CW] train: qf1_loss: 12.48571, qf2_loss: 12.73820, policy_loss: -136.58903, policy_entropy: -6.31339, alpha: 0.03236, time: 51.96231
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   583 ----
[CW] collect: return: 467.88052, steps: 1000.00000, total_steps: 589000.00000
[CW] train: qf1_loss: 11.57098, qf2_loss: 11.71653, policy_loss: -135.19385, policy_entropy: -6.50088, alpha: 0.03289, time: 52.42451
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   584 ----
[CW] collect: return: 503.83204, steps: 1000.00000, total_steps: 590000.00000
[CW] train: qf1_loss: 9.92808, qf2_loss: 10.30485, policy_loss: -136.14147, policy_entropy: -6.28554, alpha: 0.03349, time: 52.29171
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   585 ----
[CW] collect: return: 209.99752, steps: 1000.00000, total_steps: 591000.00000
[CW] train: qf1_loss: 8.22035, qf2_loss: 8.38302, policy_loss: -135.10048, policy_entropy: -6.32537, alpha: 0.03391, time: 52.19048
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   586 ----
[CW] collect: return: 487.89059, steps: 1000.00000, total_steps: 592000.00000
[CW] train: qf1_loss: 56.65524, qf2_loss: 56.98802, policy_loss: -135.45240, policy_entropy: -6.23843, alpha: 0.03432, time: 52.08990
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   587 ----
[CW] collect: return: 151.41135, steps: 1000.00000, total_steps: 593000.00000
[CW] train: qf1_loss: 15.42802, qf2_loss: 15.48461, policy_loss: -135.20997, policy_entropy: -6.61309, alpha: 0.03483, time: 52.19197
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   588 ----
[CW] collect: return: 536.21498, steps: 1000.00000, total_steps: 594000.00000
[CW] train: qf1_loss: 12.17288, qf2_loss: 12.33657, policy_loss: -135.88108, policy_entropy: -6.27171, alpha: 0.03571, time: 52.21295
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   589 ----
[CW] collect: return: 441.47665, steps: 1000.00000, total_steps: 595000.00000
[CW] train: qf1_loss: 8.86143, qf2_loss: 8.90772, policy_loss: -135.46085, policy_entropy: -6.28261, alpha: 0.03600, time: 52.05841
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   590 ----
[CW] collect: return: 437.80251, steps: 1000.00000, total_steps: 596000.00000
[CW] train: qf1_loss: 8.19187, qf2_loss: 8.34169, policy_loss: -134.93171, policy_entropy: -6.10748, alpha: 0.03632, time: 52.24396
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   591 ----
[CW] collect: return: 497.07572, steps: 1000.00000, total_steps: 597000.00000
[CW] train: qf1_loss: 16.14410, qf2_loss: 16.33947, policy_loss: -136.48378, policy_entropy: -5.84742, alpha: 0.03644, time: 52.38935
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   592 ----
[CW] collect: return: 150.49554, steps: 1000.00000, total_steps: 598000.00000
[CW] train: qf1_loss: 17.16923, qf2_loss: 17.27131, policy_loss: -135.88846, policy_entropy: -6.10898, alpha: 0.03623, time: 52.30903
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   593 ----
[CW] collect: return: 133.08577, steps: 1000.00000, total_steps: 599000.00000
[CW] train: qf1_loss: 10.72075, qf2_loss: 10.82452, policy_loss: -135.58721, policy_entropy: -6.14342, alpha: 0.03647, time: 52.20601
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   594 ----
[CW] collect: return: 444.59329, steps: 1000.00000, total_steps: 600000.00000
[CW] train: qf1_loss: 9.18802, qf2_loss: 9.25474, policy_loss: -134.98875, policy_entropy: -6.32661, alpha: 0.03675, time: 52.29875
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   595 ----
[CW] collect: return: 480.00124, steps: 1000.00000, total_steps: 601000.00000
[CW] train: qf1_loss: 7.80008, qf2_loss: 7.92067, policy_loss: -134.72482, policy_entropy: -6.27120, alpha: 0.03727, time: 52.84773
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   596 ----
[CW] collect: return: 510.50985, steps: 1000.00000, total_steps: 602000.00000
[CW] train: qf1_loss: 10.41885, qf2_loss: 10.53736, policy_loss: -136.71092, policy_entropy: -6.38428, alpha: 0.03780, time: 52.32611
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   597 ----
[CW] collect: return: 391.01788, steps: 1000.00000, total_steps: 603000.00000
[CW] train: qf1_loss: 8.49822, qf2_loss: 8.39555, policy_loss: -134.97720, policy_entropy: -6.31132, alpha: 0.03834, time: 52.33354
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   598 ----
[CW] collect: return: 542.36905, steps: 1000.00000, total_steps: 604000.00000
[CW] train: qf1_loss: 9.06621, qf2_loss: 9.19253, policy_loss: -135.89007, policy_entropy: -6.07571, alpha: 0.03870, time: 52.14056
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   599 ----
[CW] collect: return: 92.53715, steps: 1000.00000, total_steps: 605000.00000
[CW] train: qf1_loss: 7.21527, qf2_loss: 7.27254, policy_loss: -137.29634, policy_entropy: -6.02509, alpha: 0.03874, time: 52.25248
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   600 ----
[CW] collect: return: 527.18004, steps: 1000.00000, total_steps: 606000.00000
[CW] train: qf1_loss: 7.60945, qf2_loss: 7.62591, policy_loss: -136.57288, policy_entropy: -6.05916, alpha: 0.03877, time: 52.22265
[CW] eval: return: 285.10001, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   601 ----
[CW] collect: return: 147.41767, steps: 1000.00000, total_steps: 607000.00000
[CW] train: qf1_loss: 8.03605, qf2_loss: 8.08799, policy_loss: -135.75642, policy_entropy: -5.94425, alpha: 0.03876, time: 52.17747
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   602 ----
[CW] collect: return: 582.54176, steps: 1000.00000, total_steps: 608000.00000
[CW] train: qf1_loss: 17.44334, qf2_loss: 18.10824, policy_loss: -135.32483, policy_entropy: -5.98915, alpha: 0.03882, time: 51.92429
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   603 ----
[CW] collect: return: 476.99684, steps: 1000.00000, total_steps: 609000.00000
[CW] train: qf1_loss: 15.42700, qf2_loss: 15.55226, policy_loss: -134.59508, policy_entropy: -5.84909, alpha: 0.03869, time: 51.69783
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   604 ----
[CW] collect: return: 388.20220, steps: 1000.00000, total_steps: 610000.00000
[CW] train: qf1_loss: 13.49351, qf2_loss: 13.55648, policy_loss: -134.38467, policy_entropy: -6.02526, alpha: 0.03854, time: 51.85309
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   605 ----
[CW] collect: return: 468.06858, steps: 1000.00000, total_steps: 611000.00000
[CW] train: qf1_loss: 23.27255, qf2_loss: 23.43748, policy_loss: -133.41791, policy_entropy: -5.99891, alpha: 0.03865, time: 51.98530
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   606 ----
[CW] collect: return: 491.28500, steps: 1000.00000, total_steps: 612000.00000
[CW] train: qf1_loss: 36.78751, qf2_loss: 37.18928, policy_loss: -137.04138, policy_entropy: -6.02390, alpha: 0.03862, time: 51.98956
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   607 ----
[CW] collect: return: 455.98399, steps: 1000.00000, total_steps: 613000.00000
[CW] train: qf1_loss: 24.66393, qf2_loss: 24.94996, policy_loss: -136.36535, policy_entropy: -6.20535, alpha: 0.03876, time: 52.07580
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   608 ----
[CW] collect: return: 482.04310, steps: 1000.00000, total_steps: 614000.00000
[CW] train: qf1_loss: 225.84131, qf2_loss: 224.81439, policy_loss: -138.38542, policy_entropy: -6.18731, alpha: 0.03915, time: 52.05602
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   609 ----
[CW] collect: return: 247.22084, steps: 1000.00000, total_steps: 615000.00000
[CW] train: qf1_loss: 149.75560, qf2_loss: 153.65721, policy_loss: -136.32016, policy_entropy: -5.79060, alpha: 0.03891, time: 51.92119
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   610 ----
[CW] collect: return: 333.50777, steps: 1000.00000, total_steps: 616000.00000
[CW] train: qf1_loss: 44.13563, qf2_loss: 45.42519, policy_loss: -134.81992, policy_entropy: -6.67045, alpha: 0.03925, time: 51.87965
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   611 ----
[CW] collect: return: 25.03013, steps: 1000.00000, total_steps: 617000.00000
[CW] train: qf1_loss: 19.77697, qf2_loss: 19.90836, policy_loss: -136.59446, policy_entropy: -6.51513, alpha: 0.04015, time: 51.96304
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   612 ----
[CW] collect: return: 389.55988, steps: 1000.00000, total_steps: 618000.00000
[CW] train: qf1_loss: 21.89242, qf2_loss: 21.93167, policy_loss: -135.85804, policy_entropy: -6.53450, alpha: 0.04104, time: 52.04124
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   613 ----
[CW] collect: return: 448.52724, steps: 1000.00000, total_steps: 619000.00000
[CW] train: qf1_loss: 13.77349, qf2_loss: 14.06650, policy_loss: -137.47364, policy_entropy: -6.64160, alpha: 0.04196, time: 52.03852
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   614 ----
[CW] collect: return: 426.66881, steps: 1000.00000, total_steps: 620000.00000
[CW] train: qf1_loss: 13.41572, qf2_loss: 13.43033, policy_loss: -137.54735, policy_entropy: -6.61046, alpha: 0.04301, time: 52.12117
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   615 ----
[CW] collect: return: 476.03416, steps: 1000.00000, total_steps: 621000.00000
[CW] train: qf1_loss: 10.47428, qf2_loss: 10.49628, policy_loss: -137.19324, policy_entropy: -6.25749, alpha: 0.04375, time: 52.02388
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   616 ----
[CW] collect: return: 461.62220, steps: 1000.00000, total_steps: 622000.00000
[CW] train: qf1_loss: 15.15824, qf2_loss: 15.42100, policy_loss: -136.89293, policy_entropy: -6.38785, alpha: 0.04425, time: 51.77983
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   617 ----
[CW] collect: return: 488.97041, steps: 1000.00000, total_steps: 623000.00000
[CW] train: qf1_loss: 12.04346, qf2_loss: 12.18809, policy_loss: -139.87490, policy_entropy: -6.37759, alpha: 0.04490, time: 51.72381
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   618 ----
[CW] collect: return: 498.07449, steps: 1000.00000, total_steps: 624000.00000
[CW] train: qf1_loss: 14.54995, qf2_loss: 14.50031, policy_loss: -138.06481, policy_entropy: -6.24773, alpha: 0.04546, time: 51.57676
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   619 ----
[CW] collect: return: 440.66482, steps: 1000.00000, total_steps: 625000.00000
[CW] train: qf1_loss: 14.04374, qf2_loss: 14.18244, policy_loss: -139.98768, policy_entropy: -6.45780, alpha: 0.04614, time: 52.15736
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   620 ----
[CW] collect: return: 431.44964, steps: 1000.00000, total_steps: 626000.00000
[CW] train: qf1_loss: 13.21488, qf2_loss: 13.59802, policy_loss: -137.91809, policy_entropy: -6.24228, alpha: 0.04678, time: 55.14805
[CW] eval: return: 396.60449, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   621 ----
[CW] collect: return: 453.38775, steps: 1000.00000, total_steps: 627000.00000
[CW] train: qf1_loss: 12.10562, qf2_loss: 12.29719, policy_loss: -139.26843, policy_entropy: -6.20369, alpha: 0.04722, time: 52.36151
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   622 ----
[CW] collect: return: 97.46677, steps: 1000.00000, total_steps: 628000.00000
[CW] train: qf1_loss: 12.90851, qf2_loss: 12.88404, policy_loss: -138.74415, policy_entropy: -5.98589, alpha: 0.04750, time: 51.93323
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   623 ----
[CW] collect: return: 396.29589, steps: 1000.00000, total_steps: 629000.00000
[CW] train: qf1_loss: 17.18445, qf2_loss: 17.39635, policy_loss: -138.74237, policy_entropy: -5.81852, alpha: 0.04730, time: 52.07545
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   624 ----
[CW] collect: return: 498.60696, steps: 1000.00000, total_steps: 630000.00000
[CW] train: qf1_loss: 17.44769, qf2_loss: 17.16294, policy_loss: -139.39161, policy_entropy: -6.03207, alpha: 0.04700, time: 52.23170
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   625 ----
[CW] collect: return: 442.74580, steps: 1000.00000, total_steps: 631000.00000
[CW] train: qf1_loss: 13.34685, qf2_loss: 12.93544, policy_loss: -141.01718, policy_entropy: -6.26769, alpha: 0.04731, time: 52.00205
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   626 ----
[CW] collect: return: 441.32230, steps: 1000.00000, total_steps: 632000.00000
[CW] train: qf1_loss: 21.59836, qf2_loss: 21.04998, policy_loss: -139.52371, policy_entropy: -5.82318, alpha: 0.04742, time: 52.12149
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   627 ----
[CW] collect: return: 195.26333, steps: 1000.00000, total_steps: 633000.00000
[CW] train: qf1_loss: 35.65098, qf2_loss: 35.65577, policy_loss: -137.42542, policy_entropy: -5.71930, alpha: 0.04694, time: 51.95282
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   628 ----
[CW] collect: return: 496.85373, steps: 1000.00000, total_steps: 634000.00000
[CW] train: qf1_loss: 66.71337, qf2_loss: 66.94819, policy_loss: -137.47460, policy_entropy: -5.92873, alpha: 0.04674, time: 52.61423
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   629 ----
[CW] collect: return: 363.39510, steps: 1000.00000, total_steps: 635000.00000
[CW] train: qf1_loss: 55.77182, qf2_loss: 55.34391, policy_loss: -137.84309, policy_entropy: -5.94308, alpha: 0.04651, time: 51.86413
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   630 ----
[CW] collect: return: 467.00125, steps: 1000.00000, total_steps: 636000.00000
[CW] train: qf1_loss: 61.55280, qf2_loss: 61.52170, policy_loss: -139.43194, policy_entropy: -5.77680, alpha: 0.04617, time: 51.88835
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   631 ----
[CW] collect: return: 290.84695, steps: 1000.00000, total_steps: 637000.00000
[CW] train: qf1_loss: 220.57201, qf2_loss: 225.75821, policy_loss: -141.26363, policy_entropy: -6.67537, alpha: 0.04636, time: 53.04086
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   632 ----
[CW] collect: return: 254.63848, steps: 1000.00000, total_steps: 638000.00000
[CW] train: qf1_loss: 228.33226, qf2_loss: 229.06662, policy_loss: -139.07719, policy_entropy: -7.44806, alpha: 0.04804, time: 51.88949
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   633 ----
[CW] collect: return: 252.97927, steps: 1000.00000, total_steps: 639000.00000
[CW] train: qf1_loss: 95.85016, qf2_loss: 96.46417, policy_loss: -140.54948, policy_entropy: -7.32390, alpha: 0.05015, time: 51.97974
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   634 ----
[CW] collect: return: 201.97694, steps: 1000.00000, total_steps: 640000.00000
[CW] train: qf1_loss: 61.55795, qf2_loss: 61.49488, policy_loss: -141.33692, policy_entropy: -7.09006, alpha: 0.05213, time: 51.84256
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   635 ----
[CW] collect: return: 141.05418, steps: 1000.00000, total_steps: 641000.00000
[CW] train: qf1_loss: 37.84699, qf2_loss: 37.63938, policy_loss: -142.47656, policy_entropy: -6.43141, alpha: 0.05327, time: 51.69527
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   636 ----
[CW] collect: return: 319.71249, steps: 1000.00000, total_steps: 642000.00000
[CW] train: qf1_loss: 57.07010, qf2_loss: 57.10019, policy_loss: -141.91199, policy_entropy: -6.02420, alpha: 0.05364, time: 51.65972
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   637 ----
[CW] collect: return: 347.09001, steps: 1000.00000, total_steps: 643000.00000
[CW] train: qf1_loss: 29.40560, qf2_loss: 29.91571, policy_loss: -144.64278, policy_entropy: -6.19777, alpha: 0.05377, time: 52.13755
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   638 ----
[CW] collect: return: 124.45228, steps: 1000.00000, total_steps: 644000.00000
[CW] train: qf1_loss: 53.29906, qf2_loss: 52.49071, policy_loss: -143.94189, policy_entropy: -5.93262, alpha: 0.05401, time: 51.88028
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   639 ----
[CW] collect: return: 248.40193, steps: 1000.00000, total_steps: 645000.00000
[CW] train: qf1_loss: 90.14568, qf2_loss: 91.37945, policy_loss: -139.45424, policy_entropy: -5.69510, alpha: 0.05367, time: 51.73962
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   640 ----
[CW] collect: return: 201.94039, steps: 1000.00000, total_steps: 646000.00000
[CW] train: qf1_loss: 42.90636, qf2_loss: 42.91572, policy_loss: -142.42449, policy_entropy: -5.85488, alpha: 0.05337, time: 51.85687
[CW] eval: return: 208.92073, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   641 ----
[CW] collect: return: 161.47208, steps: 1000.00000, total_steps: 647000.00000
[CW] train: qf1_loss: 35.12083, qf2_loss: 35.01024, policy_loss: -143.05924, policy_entropy: -5.93121, alpha: 0.05301, time: 51.76858
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   642 ----
[CW] collect: return: 226.94255, steps: 1000.00000, total_steps: 648000.00000
[CW] train: qf1_loss: 50.65140, qf2_loss: 49.71186, policy_loss: -144.77498, policy_entropy: -5.99723, alpha: 0.05306, time: 51.95779
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   643 ----
[CW] collect: return: 187.72688, steps: 1000.00000, total_steps: 649000.00000
[CW] train: qf1_loss: 42.29128, qf2_loss: 42.22443, policy_loss: -140.60717, policy_entropy: -5.81412, alpha: 0.05290, time: 52.04324
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   644 ----
[CW] collect: return: 296.08510, steps: 1000.00000, total_steps: 650000.00000
[CW] train: qf1_loss: 98.63324, qf2_loss: 101.62717, policy_loss: -145.83834, policy_entropy: -6.60117, alpha: 0.05299, time: 52.24832
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   645 ----
[CW] collect: return: 179.24741, steps: 1000.00000, total_steps: 651000.00000
[CW] train: qf1_loss: 37.77152, qf2_loss: 37.87897, policy_loss: -144.26985, policy_entropy: -5.88013, alpha: 0.05369, time: 51.60068
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   646 ----
[CW] collect: return: 182.21939, steps: 1000.00000, total_steps: 652000.00000
[CW] train: qf1_loss: 150.03264, qf2_loss: 152.99188, policy_loss: -143.91836, policy_entropy: -5.82845, alpha: 0.05344, time: 51.78957
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   647 ----
[CW] collect: return: 155.98730, steps: 1000.00000, total_steps: 653000.00000
[CW] train: qf1_loss: 269.66709, qf2_loss: 274.71025, policy_loss: -141.32376, policy_entropy: -6.58134, alpha: 0.05319, time: 51.88200
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   648 ----
[CW] collect: return: 70.34617, steps: 1000.00000, total_steps: 654000.00000
[CW] train: qf1_loss: 244.01315, qf2_loss: 246.45198, policy_loss: -143.15470, policy_entropy: -6.84820, alpha: 0.05473, time: 51.93527
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   649 ----
[CW] collect: return: 3.45998, steps: 1000.00000, total_steps: 655000.00000
[CW] train: qf1_loss: 221.49114, qf2_loss: 223.91401, policy_loss: -144.53734, policy_entropy: -6.81666, alpha: 0.05613, time: 51.86095
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   650 ----
[CW] collect: return: 225.18290, steps: 1000.00000, total_steps: 656000.00000
[CW] train: qf1_loss: 42.63277, qf2_loss: 42.12792, policy_loss: -144.46786, policy_entropy: -6.73727, alpha: 0.05738, time: 51.78029
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   651 ----
[CW] collect: return: 300.46016, steps: 1000.00000, total_steps: 657000.00000
[CW] train: qf1_loss: 34.88111, qf2_loss: 34.79122, policy_loss: -145.99786, policy_entropy: -6.42313, alpha: 0.05863, time: 52.00758
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   652 ----
[CW] collect: return: 289.52563, steps: 1000.00000, total_steps: 658000.00000
[CW] train: qf1_loss: 47.62489, qf2_loss: 47.28854, policy_loss: -145.62744, policy_entropy: -6.17209, alpha: 0.05919, time: 51.60520
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   653 ----
[CW] collect: return: 92.76463, steps: 1000.00000, total_steps: 659000.00000
[CW] train: qf1_loss: 34.40072, qf2_loss: 35.00576, policy_loss: -145.61395, policy_entropy: -6.10434, alpha: 0.05939, time: 51.92560
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   654 ----
[CW] collect: return: 121.92216, steps: 1000.00000, total_steps: 660000.00000
[CW] train: qf1_loss: 47.44926, qf2_loss: 46.96156, policy_loss: -146.18726, policy_entropy: -5.99928, alpha: 0.05957, time: 51.83541
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   655 ----
[CW] collect: return: 253.68021, steps: 1000.00000, total_steps: 661000.00000
[CW] train: qf1_loss: 42.72894, qf2_loss: 42.08272, policy_loss: -145.94842, policy_entropy: -6.12830, alpha: 0.05974, time: 51.83353
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   656 ----
[CW] collect: return: 179.50671, steps: 1000.00000, total_steps: 662000.00000
[CW] train: qf1_loss: 36.72024, qf2_loss: 37.16927, policy_loss: -146.67415, policy_entropy: -6.17261, alpha: 0.05997, time: 51.90331
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   657 ----
[CW] collect: return: 212.68859, steps: 1000.00000, total_steps: 663000.00000
[CW] train: qf1_loss: 32.94989, qf2_loss: 33.45968, policy_loss: -147.04231, policy_entropy: -6.40664, alpha: 0.06064, time: 52.08261
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   658 ----
[CW] collect: return: 84.31208, steps: 1000.00000, total_steps: 664000.00000
[CW] train: qf1_loss: 40.16208, qf2_loss: 39.83045, policy_loss: -143.64281, policy_entropy: -5.83356, alpha: 0.06091, time: 51.89164
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   659 ----
[CW] collect: return: 175.66162, steps: 1000.00000, total_steps: 665000.00000
[CW] train: qf1_loss: 46.24462, qf2_loss: 46.70841, policy_loss: -147.08599, policy_entropy: -5.87650, alpha: 0.06058, time: 51.57777
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   660 ----
[CW] collect: return: 115.99562, steps: 1000.00000, total_steps: 666000.00000
[CW] train: qf1_loss: 45.26463, qf2_loss: 45.06941, policy_loss: -148.60964, policy_entropy: -6.08574, alpha: 0.06069, time: 52.07663
[CW] eval: return: 203.87037, steps: 1000.00000
[CW] ---------------------------
Collecting train stats
[CW] ---- Iteration:   661 ----
[CW] collect: return: 265.98448, steps: 1000.00000, total_steps: 667000.00000
[CW] train: qf1_loss: 138.43933, qf2_loss: 138.85119, policy_loss: -146.67713, policy_entropy: -6.16067, alpha: 0.06065, time: 52.17395
[CW] ---------------------------

============================= JOB FEEDBACK =============================

NodeName=uc2n510
Job ID: 21913411
Array Job ID: 21913411_0
Cluster: uc2
User/Group: uprnr/stud
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 4
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 1-16:01:20 core-walltime
Job Wall-clock time: 10:00:20
Memory Utilized: 4.77 GB
Memory Efficiency: 8.13% of 58.59 GB
