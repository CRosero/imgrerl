{"collect/return": 265.98447682429105, "collect/steps": 1000.0, "collect/total_steps": 667000.0, "train/qf1_loss": 138.43932529449464, "train/qf2_loss": 138.8511851119995, "train/policy_loss": -146.67713455200195, "train/policy_entropy": -6.1606679534912105, "train/alpha": 0.06065153334289789, "train/time": 52.17394781112671, "eval/return": 203.87037237813473, "eval/steps": 1000.0, "_timestamp": 1678737257.6424766, "_runtime": 35902.07797455788, "_step": 661}