{"collect/return": 322.40185442545044, "collect/steps": 1000.0, "collect/total_steps": 688000.0, "train/qf1_loss": 43.344476537704466, "train/qf2_loss": 43.67788006782532, "train/policy_loss": -135.2047576904297, "train/policy_entropy": -5.989723100662231, "train/alpha": 0.04164361130446195, "train/time": 50.17527723312378, "eval/return": 251.49682419589354, "eval/steps": 1000.0, "_timestamp": 1678754344.0019708, "_runtime": 35841.86359786987, "_step": 682}