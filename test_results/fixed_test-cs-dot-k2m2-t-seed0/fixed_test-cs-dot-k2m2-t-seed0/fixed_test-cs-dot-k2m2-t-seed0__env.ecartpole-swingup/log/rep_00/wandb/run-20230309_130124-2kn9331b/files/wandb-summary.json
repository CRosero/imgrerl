{"collect/return": 643.0134378671646, "collect/steps": 1000.0, "collect/total_steps": 635000.0, "train/qf1_loss": 92.3061713027954, "train/qf2_loss": 93.61641788482666, "train/policy_loss": -505.56670471191404, "train/policy_entropy": -0.9886445200443268, "train/alpha": 0.635476079583168, "train/time": 44.0593843460083, "eval/return": 834.8960501893914, "eval/steps": 1000.0, "_timestamp": 1678391975.5046766, "_runtime": 28690.90744161606, "_step": 629}