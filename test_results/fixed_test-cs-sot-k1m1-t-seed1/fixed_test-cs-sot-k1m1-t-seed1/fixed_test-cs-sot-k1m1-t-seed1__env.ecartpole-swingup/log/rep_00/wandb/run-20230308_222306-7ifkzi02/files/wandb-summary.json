{"collect/return": 833.159538603446, "collect/steps": 1000.0, "collect/total_steps": 546000.0, "train/qf1_loss": 21.822297735214235, "train/qf2_loss": 21.94745810508728, "train/policy_loss": -407.62032409667967, "train/policy_entropy": -1.015084401369095, "train/alpha": 0.5153339958190918, "train/time": 51.16290640830994, "eval/return": 827.2542608145638, "eval/steps": 1000.0, "_timestamp": 1678339236.8359377, "_runtime": 28650.650958776474, "_step": 540}