{"collect/return": 837.5567521386547, "collect/steps": 1000.0, "collect/total_steps": 416000.0, "train/qf1_loss": 75.38062133789063, "train/qf2_loss": 74.59581211090088, "train/policy_loss": -397.8300720214844, "train/policy_entropy": -0.9886560541391373, "train/alpha": 0.5684670728445053, "train/time": 67.90452027320862, "eval/return": 688.806429600234, "eval/steps": 1000.0, "_timestamp": 1678336145.0106463, "_runtime": 28657.687167406082, "_step": 410}