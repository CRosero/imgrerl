[CW] ------------------------------------------
[CW] Starting Repetition 000
[CW] ------------------------------------------
[CW] ---- Iteration:     0 ----
[CW] collect: return: 126.53164, steps: 1000.00000, total_steps: 6000.00000
[CW] train: qf1_loss: 3.12007, qf2_loss: 3.11211, policy_loss: -2.72015, policy_entropy: 0.68262, alpha: 0.98504, time: 72.27175
[CW] eval: return: 152.92993, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:     1 ----
[CW] collect: return: 125.95231, steps: 1000.00000, total_steps: 7000.00000
[CW] train: qf1_loss: 0.19109, qf2_loss: 0.18982, policy_loss: -3.21476, policy_entropy: 0.68062, alpha: 0.95627, time: 66.80092
[CW] ---------------------------
[CW] ---- Iteration:     2 ----
[CW] collect: return: 49.55583, steps: 1000.00000, total_steps: 8000.00000
[CW] train: qf1_loss: 0.17716, qf2_loss: 0.17545, policy_loss: -3.74587, policy_entropy: 0.67633, alpha: 0.92877, time: 66.80893
[CW] ---------------------------
[CW] ---- Iteration:     3 ----
[CW] collect: return: 98.30127, steps: 1000.00000, total_steps: 9000.00000
[CW] train: qf1_loss: 0.17251, qf2_loss: 0.16810, policy_loss: -4.34117, policy_entropy: 0.67188, alpha: 0.90245, time: 67.01521
[CW] ---------------------------
[CW] ---- Iteration:     4 ----
[CW] collect: return: 120.67199, steps: 1000.00000, total_steps: 10000.00000
[CW] train: qf1_loss: 0.19179, qf2_loss: 0.18743, policy_loss: -4.98587, policy_entropy: 0.66757, alpha: 0.87723, time: 66.97279
[CW] ---------------------------
[CW] ---- Iteration:     5 ----
[CW] collect: return: 142.50352, steps: 1000.00000, total_steps: 11000.00000
[CW] train: qf1_loss: 0.23833, qf2_loss: 0.23463, policy_loss: -5.66327, policy_entropy: 0.66378, alpha: 0.85304, time: 67.15241
[CW] ---------------------------
[CW] ---- Iteration:     6 ----
[CW] collect: return: 164.97179, steps: 1000.00000, total_steps: 12000.00000
[CW] train: qf1_loss: 0.35017, qf2_loss: 0.35080, policy_loss: -6.43266, policy_entropy: 0.66063, alpha: 0.82981, time: 68.14556
[CW] ---------------------------
[CW] ---- Iteration:     7 ----
[CW] collect: return: 109.39520, steps: 1000.00000, total_steps: 13000.00000
[CW] train: qf1_loss: 0.39702, qf2_loss: 0.40058, policy_loss: -7.03649, policy_entropy: 0.65927, alpha: 0.80747, time: 68.35232
[CW] ---------------------------
[CW] ---- Iteration:     8 ----
[CW] collect: return: 84.41379, steps: 1000.00000, total_steps: 14000.00000
[CW] train: qf1_loss: 0.40272, qf2_loss: 0.40245, policy_loss: -7.54147, policy_entropy: 0.65566, alpha: 0.78595, time: 68.34169
[CW] ---------------------------
[CW] ---- Iteration:     9 ----
[CW] collect: return: 16.89993, steps: 1000.00000, total_steps: 15000.00000
[CW] train: qf1_loss: 0.52279, qf2_loss: 0.52165, policy_loss: -8.02548, policy_entropy: 0.65188, alpha: 0.76525, time: 68.25065
[CW] ---------------------------
[CW] ---- Iteration:    10 ----
[CW] collect: return: 172.08875, steps: 1000.00000, total_steps: 16000.00000
[CW] train: qf1_loss: 0.82178, qf2_loss: 0.81789, policy_loss: -8.81061, policy_entropy: 0.64511, alpha: 0.74531, time: 67.98075
[CW] ---------------------------
[CW] ---- Iteration:    11 ----
[CW] collect: return: 122.65725, steps: 1000.00000, total_steps: 17000.00000
[CW] train: qf1_loss: 0.58358, qf2_loss: 0.57849, policy_loss: -9.40828, policy_entropy: 0.63542, alpha: 0.72614, time: 67.92546
[CW] ---------------------------
[CW] ---- Iteration:    12 ----
[CW] collect: return: 163.30645, steps: 1000.00000, total_steps: 18000.00000
[CW] train: qf1_loss: 0.70021, qf2_loss: 0.69810, policy_loss: -10.17687, policy_entropy: 0.62378, alpha: 0.70770, time: 68.00122
[CW] ---------------------------
[CW] ---- Iteration:    13 ----
[CW] collect: return: 112.95164, steps: 1000.00000, total_steps: 19000.00000
[CW] train: qf1_loss: 0.99221, qf2_loss: 0.98891, policy_loss: -10.80832, policy_entropy: 0.61188, alpha: 0.68998, time: 68.12704
[CW] ---------------------------
[CW] ---- Iteration:    14 ----
[CW] collect: return: 149.29002, steps: 1000.00000, total_steps: 20000.00000
[CW] train: qf1_loss: 0.90936, qf2_loss: 0.90787, policy_loss: -11.49730, policy_entropy: 0.59556, alpha: 0.67291, time: 68.20102
[CW] ---------------------------
[CW] ---- Iteration:    15 ----
[CW] collect: return: 174.55013, steps: 1000.00000, total_steps: 21000.00000
[CW] train: qf1_loss: 1.00286, qf2_loss: 1.00135, policy_loss: -12.18167, policy_entropy: 0.57400, alpha: 0.65654, time: 68.31639
[CW] ---------------------------
[CW] ---- Iteration:    16 ----
[CW] collect: return: 283.07457, steps: 1000.00000, total_steps: 22000.00000
[CW] train: qf1_loss: 1.87949, qf2_loss: 1.89046, policy_loss: -13.22166, policy_entropy: 0.55852, alpha: 0.64079, time: 68.11827
[CW] ---------------------------
[CW] ---- Iteration:    17 ----
[CW] collect: return: 214.44129, steps: 1000.00000, total_steps: 23000.00000
[CW] train: qf1_loss: 1.48789, qf2_loss: 1.49358, policy_loss: -14.18567, policy_entropy: 0.52275, alpha: 0.62567, time: 68.12183
[CW] ---------------------------
[CW] ---- Iteration:    18 ----
[CW] collect: return: 129.14394, steps: 1000.00000, total_steps: 24000.00000
[CW] train: qf1_loss: 1.29265, qf2_loss: 1.28655, policy_loss: -14.78242, policy_entropy: 0.49689, alpha: 0.61121, time: 68.19067
[CW] ---------------------------
[CW] ---- Iteration:    19 ----
[CW] collect: return: 254.49989, steps: 1000.00000, total_steps: 25000.00000
[CW] train: qf1_loss: 1.80574, qf2_loss: 1.80352, policy_loss: -15.88786, policy_entropy: 0.46899, alpha: 0.59734, time: 67.86691
[CW] ---------------------------
[CW] ---- Iteration:    20 ----
[CW] collect: return: 190.47385, steps: 1000.00000, total_steps: 26000.00000
[CW] train: qf1_loss: 1.69872, qf2_loss: 1.69383, policy_loss: -16.93028, policy_entropy: 0.43782, alpha: 0.58399, time: 67.49473
[CW] eval: return: 213.18775, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    21 ----
[CW] collect: return: 222.63090, steps: 1000.00000, total_steps: 27000.00000
[CW] train: qf1_loss: 1.66181, qf2_loss: 1.64850, policy_loss: -17.75455, policy_entropy: 0.40570, alpha: 0.57121, time: 67.78982
[CW] ---------------------------
[CW] ---- Iteration:    22 ----
[CW] collect: return: 170.23879, steps: 1000.00000, total_steps: 28000.00000
[CW] train: qf1_loss: 1.85407, qf2_loss: 1.85185, policy_loss: -18.45529, policy_entropy: 0.36756, alpha: 0.55893, time: 67.71193
[CW] ---------------------------
[CW] ---- Iteration:    23 ----
[CW] collect: return: 198.34375, steps: 1000.00000, total_steps: 29000.00000
[CW] train: qf1_loss: 1.89850, qf2_loss: 1.88935, policy_loss: -19.09859, policy_entropy: 0.34012, alpha: 0.54715, time: 67.83783
[CW] ---------------------------
[CW] ---- Iteration:    24 ----
[CW] collect: return: 228.34504, steps: 1000.00000, total_steps: 30000.00000
[CW] train: qf1_loss: 2.06865, qf2_loss: 2.06037, policy_loss: -20.31658, policy_entropy: 0.30467, alpha: 0.53577, time: 67.89004
[CW] ---------------------------
[CW] ---- Iteration:    25 ----
[CW] collect: return: 232.83403, steps: 1000.00000, total_steps: 31000.00000
[CW] train: qf1_loss: 2.35970, qf2_loss: 2.34771, policy_loss: -21.27206, policy_entropy: 0.26483, alpha: 0.52488, time: 67.72760
[CW] ---------------------------
[CW] ---- Iteration:    26 ----
[CW] collect: return: 260.56877, steps: 1000.00000, total_steps: 32000.00000
[CW] train: qf1_loss: 2.05243, qf2_loss: 2.03246, policy_loss: -22.48072, policy_entropy: 0.21241, alpha: 0.51450, time: 67.55362
[CW] ---------------------------
[CW] ---- Iteration:    27 ----
[CW] collect: return: 270.23125, steps: 1000.00000, total_steps: 33000.00000
[CW] train: qf1_loss: 2.23018, qf2_loss: 2.21617, policy_loss: -23.42189, policy_entropy: 0.17449, alpha: 0.50456, time: 67.69993
[CW] ---------------------------
[CW] ---- Iteration:    28 ----
[CW] collect: return: 298.83128, steps: 1000.00000, total_steps: 34000.00000
[CW] train: qf1_loss: 2.53287, qf2_loss: 2.50884, policy_loss: -24.61881, policy_entropy: 0.13520, alpha: 0.49501, time: 67.57286
[CW] ---------------------------
[CW] ---- Iteration:    29 ----
[CW] collect: return: 191.26883, steps: 1000.00000, total_steps: 35000.00000
[CW] train: qf1_loss: 2.26381, qf2_loss: 2.25961, policy_loss: -25.42776, policy_entropy: 0.09084, alpha: 0.48585, time: 67.68025
[CW] ---------------------------
[CW] ---- Iteration:    30 ----
[CW] collect: return: 241.23611, steps: 1000.00000, total_steps: 36000.00000
[CW] train: qf1_loss: 2.59281, qf2_loss: 2.56850, policy_loss: -26.81916, policy_entropy: 0.04682, alpha: 0.47712, time: 67.47855
[CW] ---------------------------
[CW] ---- Iteration:    31 ----
[CW] collect: return: 243.07455, steps: 1000.00000, total_steps: 37000.00000
[CW] train: qf1_loss: 3.35844, qf2_loss: 3.34992, policy_loss: -27.95061, policy_entropy: 0.01552, alpha: 0.46873, time: 67.48519
[CW] ---------------------------
[CW] ---- Iteration:    32 ----
[CW] collect: return: 199.02883, steps: 1000.00000, total_steps: 38000.00000
[CW] train: qf1_loss: 2.60400, qf2_loss: 2.59196, policy_loss: -28.65583, policy_entropy: -0.02881, alpha: 0.46062, time: 67.57808
[CW] ---------------------------
[CW] ---- Iteration:    33 ----
[CW] collect: return: 272.35947, steps: 1000.00000, total_steps: 39000.00000
[CW] train: qf1_loss: 2.91570, qf2_loss: 2.90775, policy_loss: -30.01117, policy_entropy: -0.06388, alpha: 0.45279, time: 67.85957
[CW] ---------------------------
[CW] ---- Iteration:    34 ----
[CW] collect: return: 213.76156, steps: 1000.00000, total_steps: 40000.00000
[CW] train: qf1_loss: 2.79969, qf2_loss: 2.78887, policy_loss: -31.13643, policy_entropy: -0.05807, alpha: 0.44515, time: 68.06480
[CW] ---------------------------
[CW] ---- Iteration:    35 ----
[CW] collect: return: 203.65753, steps: 1000.00000, total_steps: 41000.00000
[CW] train: qf1_loss: 3.18352, qf2_loss: 3.17392, policy_loss: -32.25242, policy_entropy: -0.08793, alpha: 0.43762, time: 67.87669
[CW] ---------------------------
[CW] ---- Iteration:    36 ----
[CW] collect: return: 198.29801, steps: 1000.00000, total_steps: 42000.00000
[CW] train: qf1_loss: 3.12342, qf2_loss: 3.10376, policy_loss: -32.96994, policy_entropy: -0.10570, alpha: 0.43021, time: 67.85757
[CW] ---------------------------
[CW] ---- Iteration:    37 ----
[CW] collect: return: 229.97433, steps: 1000.00000, total_steps: 43000.00000
[CW] train: qf1_loss: 3.58596, qf2_loss: 3.57911, policy_loss: -34.27279, policy_entropy: -0.09726, alpha: 0.42289, time: 68.38971
[CW] ---------------------------
[CW] ---- Iteration:    38 ----
[CW] collect: return: 229.35063, steps: 1000.00000, total_steps: 44000.00000
[CW] train: qf1_loss: 3.76991, qf2_loss: 3.73241, policy_loss: -35.16489, policy_entropy: -0.14269, alpha: 0.41567, time: 68.51395
[CW] ---------------------------
[CW] ---- Iteration:    39 ----
[CW] collect: return: 210.37028, steps: 1000.00000, total_steps: 45000.00000
[CW] train: qf1_loss: 3.44731, qf2_loss: 3.43174, policy_loss: -36.03228, policy_entropy: -0.13805, alpha: 0.40868, time: 68.23230
[CW] ---------------------------
[CW] ---- Iteration:    40 ----
[CW] collect: return: 213.56290, steps: 1000.00000, total_steps: 46000.00000
[CW] train: qf1_loss: 3.67649, qf2_loss: 3.66556, policy_loss: -36.99039, policy_entropy: -0.14299, alpha: 0.40164, time: 68.28673
[CW] eval: return: 232.50085, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    41 ----
[CW] collect: return: 228.19657, steps: 1000.00000, total_steps: 47000.00000
[CW] train: qf1_loss: 3.75207, qf2_loss: 3.72098, policy_loss: -37.75788, policy_entropy: -0.16350, alpha: 0.39472, time: 68.09480
[CW] ---------------------------
[CW] ---- Iteration:    42 ----
[CW] collect: return: 219.47607, steps: 1000.00000, total_steps: 48000.00000
[CW] train: qf1_loss: 3.74862, qf2_loss: 3.72170, policy_loss: -38.90182, policy_entropy: -0.15660, alpha: 0.38789, time: 67.90026
[CW] ---------------------------
[CW] ---- Iteration:    43 ----
[CW] collect: return: 237.69306, steps: 1000.00000, total_steps: 49000.00000
[CW] train: qf1_loss: 4.03877, qf2_loss: 4.01681, policy_loss: -39.72082, policy_entropy: -0.17651, alpha: 0.38108, time: 68.02333
[CW] ---------------------------
[CW] ---- Iteration:    44 ----
[CW] collect: return: 214.65483, steps: 1000.00000, total_steps: 50000.00000
[CW] train: qf1_loss: 4.38386, qf2_loss: 4.36549, policy_loss: -40.57899, policy_entropy: -0.16973, alpha: 0.37438, time: 68.23779
[CW] ---------------------------
[CW] ---- Iteration:    45 ----
[CW] collect: return: 207.53021, steps: 1000.00000, total_steps: 51000.00000
[CW] train: qf1_loss: 4.18037, qf2_loss: 4.16236, policy_loss: -41.65563, policy_entropy: -0.18912, alpha: 0.36774, time: 68.18433
[CW] ---------------------------
[CW] ---- Iteration:    46 ----
[CW] collect: return: 200.18638, steps: 1000.00000, total_steps: 52000.00000
[CW] train: qf1_loss: 4.21134, qf2_loss: 4.20523, policy_loss: -42.24078, policy_entropy: -0.19547, alpha: 0.36121, time: 68.13208
[CW] ---------------------------
[CW] ---- Iteration:    47 ----
[CW] collect: return: 256.00418, steps: 1000.00000, total_steps: 53000.00000
[CW] train: qf1_loss: 4.52318, qf2_loss: 4.50670, policy_loss: -43.72172, policy_entropy: -0.18882, alpha: 0.35475, time: 68.11490
[CW] ---------------------------
[CW] ---- Iteration:    48 ----
[CW] collect: return: 204.97125, steps: 1000.00000, total_steps: 54000.00000
[CW] train: qf1_loss: 5.27283, qf2_loss: 5.23993, policy_loss: -44.46242, policy_entropy: -0.21207, alpha: 0.34832, time: 68.12068
[CW] ---------------------------
[CW] ---- Iteration:    49 ----
[CW] collect: return: 270.47733, steps: 1000.00000, total_steps: 55000.00000
[CW] train: qf1_loss: 4.26391, qf2_loss: 4.25186, policy_loss: -45.28521, policy_entropy: -0.21049, alpha: 0.34211, time: 68.10634
[CW] ---------------------------
[CW] ---- Iteration:    50 ----
[CW] collect: return: 175.69133, steps: 1000.00000, total_steps: 56000.00000
[CW] train: qf1_loss: 4.38072, qf2_loss: 4.36667, policy_loss: -45.89921, policy_entropy: -0.23040, alpha: 0.33587, time: 68.13942
[CW] ---------------------------
[CW] ---- Iteration:    51 ----
[CW] collect: return: 287.52848, steps: 1000.00000, total_steps: 57000.00000
[CW] train: qf1_loss: 4.89869, qf2_loss: 4.89543, policy_loss: -46.90055, policy_entropy: -0.22893, alpha: 0.32982, time: 68.03795
[CW] ---------------------------
[CW] ---- Iteration:    52 ----
[CW] collect: return: 271.95734, steps: 1000.00000, total_steps: 58000.00000
[CW] train: qf1_loss: 4.05144, qf2_loss: 4.01999, policy_loss: -47.64086, policy_entropy: -0.24955, alpha: 0.32381, time: 68.04748
[CW] ---------------------------
[CW] ---- Iteration:    53 ----
[CW] collect: return: 276.65837, steps: 1000.00000, total_steps: 59000.00000
[CW] train: qf1_loss: 4.97314, qf2_loss: 4.92656, policy_loss: -48.84351, policy_entropy: -0.26353, alpha: 0.31802, time: 68.25981
[CW] ---------------------------
[CW] ---- Iteration:    54 ----
[CW] collect: return: 306.06373, steps: 1000.00000, total_steps: 60000.00000
[CW] train: qf1_loss: 22.07271, qf2_loss: 22.21294, policy_loss: -49.78924, policy_entropy: -0.29384, alpha: 0.31236, time: 68.40021
[CW] ---------------------------
[CW] ---- Iteration:    55 ----
[CW] collect: return: 238.08400, steps: 1000.00000, total_steps: 61000.00000
[CW] train: qf1_loss: 8.01181, qf2_loss: 8.17567, policy_loss: -51.05318, policy_entropy: -0.32415, alpha: 0.30700, time: 68.44603
[CW] ---------------------------
[CW] ---- Iteration:    56 ----
[CW] collect: return: 298.74199, steps: 1000.00000, total_steps: 62000.00000
[CW] train: qf1_loss: 4.84120, qf2_loss: 4.78501, policy_loss: -51.93076, policy_entropy: -0.35743, alpha: 0.30183, time: 68.97192
[CW] ---------------------------
[CW] ---- Iteration:    57 ----
[CW] collect: return: 234.41603, steps: 1000.00000, total_steps: 63000.00000
[CW] train: qf1_loss: 4.62681, qf2_loss: 4.61004, policy_loss: -52.92724, policy_entropy: -0.38335, alpha: 0.29697, time: 68.48907
[CW] ---------------------------
[CW] ---- Iteration:    58 ----
[CW] collect: return: 304.78366, steps: 1000.00000, total_steps: 64000.00000
[CW] train: qf1_loss: 4.74384, qf2_loss: 4.72235, policy_loss: -53.80773, policy_entropy: -0.37927, alpha: 0.29215, time: 68.44622
[CW] ---------------------------
[CW] ---- Iteration:    59 ----
[CW] collect: return: 283.57915, steps: 1000.00000, total_steps: 65000.00000
[CW] train: qf1_loss: 4.57503, qf2_loss: 4.54537, policy_loss: -55.12524, policy_entropy: -0.39580, alpha: 0.28735, time: 68.90598
[CW] ---------------------------
[CW] ---- Iteration:    60 ----
[CW] collect: return: 282.62819, steps: 1000.00000, total_steps: 66000.00000
[CW] train: qf1_loss: 4.68983, qf2_loss: 4.68074, policy_loss: -55.99367, policy_entropy: -0.38757, alpha: 0.28262, time: 68.33339
[CW] eval: return: 292.39265, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    61 ----
[CW] collect: return: 205.04085, steps: 1000.00000, total_steps: 67000.00000
[CW] train: qf1_loss: 4.37718, qf2_loss: 4.33937, policy_loss: -56.33224, policy_entropy: -0.40880, alpha: 0.27790, time: 68.26389
[CW] ---------------------------
[CW] ---- Iteration:    62 ----
[CW] collect: return: 332.85520, steps: 1000.00000, total_steps: 68000.00000
[CW] train: qf1_loss: 5.04454, qf2_loss: 4.97790, policy_loss: -57.44437, policy_entropy: -0.39702, alpha: 0.27325, time: 68.24999
[CW] ---------------------------
[CW] ---- Iteration:    63 ----
[CW] collect: return: 290.90489, steps: 1000.00000, total_steps: 69000.00000
[CW] train: qf1_loss: 4.59539, qf2_loss: 4.56505, policy_loss: -58.50489, policy_entropy: -0.42219, alpha: 0.26860, time: 68.43647
[CW] ---------------------------
[CW] ---- Iteration:    64 ----
[CW] collect: return: 364.96128, steps: 1000.00000, total_steps: 70000.00000
[CW] train: qf1_loss: 5.04269, qf2_loss: 5.02075, policy_loss: -59.43468, policy_entropy: -0.41814, alpha: 0.26406, time: 68.62679
[CW] ---------------------------
[CW] ---- Iteration:    65 ----
[CW] collect: return: 232.18197, steps: 1000.00000, total_steps: 71000.00000
[CW] train: qf1_loss: 4.90701, qf2_loss: 4.91370, policy_loss: -60.32394, policy_entropy: -0.41498, alpha: 0.25949, time: 68.70095
[CW] ---------------------------
[CW] ---- Iteration:    66 ----
[CW] collect: return: 259.81323, steps: 1000.00000, total_steps: 72000.00000
[CW] train: qf1_loss: 4.95901, qf2_loss: 4.94615, policy_loss: -61.19492, policy_entropy: -0.45579, alpha: 0.25509, time: 68.50654
[CW] ---------------------------
[CW] ---- Iteration:    67 ----
[CW] collect: return: 282.46040, steps: 1000.00000, total_steps: 73000.00000
[CW] train: qf1_loss: 5.40017, qf2_loss: 5.36520, policy_loss: -62.49204, policy_entropy: -0.45134, alpha: 0.25083, time: 68.52394
[CW] ---------------------------
[CW] ---- Iteration:    68 ----
[CW] collect: return: 344.85582, steps: 1000.00000, total_steps: 74000.00000
[CW] train: qf1_loss: 5.27074, qf2_loss: 5.25646, policy_loss: -63.15008, policy_entropy: -0.44697, alpha: 0.24654, time: 68.44662
[CW] ---------------------------
[CW] ---- Iteration:    69 ----
[CW] collect: return: 316.33682, steps: 1000.00000, total_steps: 75000.00000
[CW] train: qf1_loss: 5.02966, qf2_loss: 5.00794, policy_loss: -63.77206, policy_entropy: -0.46487, alpha: 0.24233, time: 68.24078
[CW] ---------------------------
[CW] ---- Iteration:    70 ----
[CW] collect: return: 304.49235, steps: 1000.00000, total_steps: 76000.00000
[CW] train: qf1_loss: 5.46826, qf2_loss: 5.45407, policy_loss: -65.17009, policy_entropy: -0.46375, alpha: 0.23814, time: 68.29416
[CW] ---------------------------
[CW] ---- Iteration:    71 ----
[CW] collect: return: 234.39394, steps: 1000.00000, total_steps: 77000.00000
[CW] train: qf1_loss: 6.10096, qf2_loss: 6.03500, policy_loss: -66.10595, policy_entropy: -0.50092, alpha: 0.23414, time: 68.18995
[CW] ---------------------------
[CW] ---- Iteration:    72 ----
[CW] collect: return: 264.62726, steps: 1000.00000, total_steps: 78000.00000
[CW] train: qf1_loss: 5.73970, qf2_loss: 5.71619, policy_loss: -66.66219, policy_entropy: -0.50789, alpha: 0.23027, time: 68.24729
[CW] ---------------------------
[CW] ---- Iteration:    73 ----
[CW] collect: return: 275.88606, steps: 1000.00000, total_steps: 79000.00000
[CW] train: qf1_loss: 5.27029, qf2_loss: 5.21670, policy_loss: -67.61176, policy_entropy: -0.50334, alpha: 0.22643, time: 68.42663
[CW] ---------------------------
[CW] ---- Iteration:    74 ----
[CW] collect: return: 286.53827, steps: 1000.00000, total_steps: 80000.00000
[CW] train: qf1_loss: 5.78235, qf2_loss: 5.73295, policy_loss: -68.38732, policy_entropy: -0.52524, alpha: 0.22266, time: 68.24774
[CW] ---------------------------
[CW] ---- Iteration:    75 ----
[CW] collect: return: 354.63734, steps: 1000.00000, total_steps: 81000.00000
[CW] train: qf1_loss: 10.29500, qf2_loss: 10.34866, policy_loss: -69.61470, policy_entropy: -0.52463, alpha: 0.21893, time: 67.99237
[CW] ---------------------------
[CW] ---- Iteration:    76 ----
[CW] collect: return: 302.70920, steps: 1000.00000, total_steps: 82000.00000
[CW] train: qf1_loss: 8.65177, qf2_loss: 8.56577, policy_loss: -70.82063, policy_entropy: -0.53344, alpha: 0.21524, time: 68.07880
[CW] ---------------------------
[CW] ---- Iteration:    77 ----
[CW] collect: return: 272.40142, steps: 1000.00000, total_steps: 83000.00000
[CW] train: qf1_loss: 6.19115, qf2_loss: 6.08836, policy_loss: -71.61856, policy_entropy: -0.55140, alpha: 0.21171, time: 68.18519
[CW] ---------------------------
[CW] ---- Iteration:    78 ----
[CW] collect: return: 253.30519, steps: 1000.00000, total_steps: 84000.00000
[CW] train: qf1_loss: 7.34657, qf2_loss: 7.23875, policy_loss: -72.37194, policy_entropy: -0.56469, alpha: 0.20828, time: 68.37748
[CW] ---------------------------
[CW] ---- Iteration:    79 ----
[CW] collect: return: 475.19737, steps: 1000.00000, total_steps: 85000.00000
[CW] train: qf1_loss: 6.26884, qf2_loss: 6.21626, policy_loss: -73.45760, policy_entropy: -0.60069, alpha: 0.20501, time: 68.14018
[CW] ---------------------------
[CW] ---- Iteration:    80 ----
[CW] collect: return: 314.06492, steps: 1000.00000, total_steps: 86000.00000
[CW] train: qf1_loss: 6.37344, qf2_loss: 6.28808, policy_loss: -74.52423, policy_entropy: -0.59867, alpha: 0.20189, time: 67.85581
[CW] eval: return: 278.74396, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:    81 ----
[CW] collect: return: 301.66348, steps: 1000.00000, total_steps: 87000.00000
[CW] train: qf1_loss: 6.15399, qf2_loss: 6.11681, policy_loss: -74.99240, policy_entropy: -0.63096, alpha: 0.19880, time: 68.33233
[CW] ---------------------------
[CW] ---- Iteration:    82 ----
[CW] collect: return: 298.78772, steps: 1000.00000, total_steps: 88000.00000
[CW] train: qf1_loss: 6.55848, qf2_loss: 6.50574, policy_loss: -76.27883, policy_entropy: -0.65518, alpha: 0.19593, time: 68.40374
[CW] ---------------------------
[CW] ---- Iteration:    83 ----
[CW] collect: return: 277.77207, steps: 1000.00000, total_steps: 89000.00000
[CW] train: qf1_loss: 6.61356, qf2_loss: 6.54111, policy_loss: -77.48409, policy_entropy: -0.66634, alpha: 0.19318, time: 68.47992
[CW] ---------------------------
[CW] ---- Iteration:    84 ----
[CW] collect: return: 308.42195, steps: 1000.00000, total_steps: 90000.00000
[CW] train: qf1_loss: 6.95906, qf2_loss: 6.91892, policy_loss: -78.46709, policy_entropy: -0.69191, alpha: 0.19059, time: 68.20125
[CW] ---------------------------
[CW] ---- Iteration:    85 ----
[CW] collect: return: 249.25538, steps: 1000.00000, total_steps: 91000.00000
[CW] train: qf1_loss: 7.91286, qf2_loss: 7.89959, policy_loss: -79.21386, policy_entropy: -0.70870, alpha: 0.18810, time: 67.96779
[CW] ---------------------------
[CW] ---- Iteration:    86 ----
[CW] collect: return: 271.10482, steps: 1000.00000, total_steps: 92000.00000
[CW] train: qf1_loss: 8.53480, qf2_loss: 8.47994, policy_loss: -80.11364, policy_entropy: -0.70424, alpha: 0.18570, time: 67.71889
[CW] ---------------------------
[CW] ---- Iteration:    87 ----
[CW] collect: return: 257.24903, steps: 1000.00000, total_steps: 93000.00000
[CW] train: qf1_loss: 7.98844, qf2_loss: 7.89336, policy_loss: -81.46559, policy_entropy: -0.73528, alpha: 0.18329, time: 67.77306
[CW] ---------------------------
[CW] ---- Iteration:    88 ----
[CW] collect: return: 282.88148, steps: 1000.00000, total_steps: 94000.00000
[CW] train: qf1_loss: 6.73622, qf2_loss: 6.74646, policy_loss: -81.90025, policy_entropy: -0.75827, alpha: 0.18108, time: 67.79156
[CW] ---------------------------
[CW] ---- Iteration:    89 ----
[CW] collect: return: 258.59114, steps: 1000.00000, total_steps: 95000.00000
[CW] train: qf1_loss: 8.05197, qf2_loss: 7.97561, policy_loss: -83.39576, policy_entropy: -0.76453, alpha: 0.17896, time: 68.16454
[CW] ---------------------------
[CW] ---- Iteration:    90 ----
[CW] collect: return: 303.44200, steps: 1000.00000, total_steps: 96000.00000
[CW] train: qf1_loss: 9.94411, qf2_loss: 9.91353, policy_loss: -84.22107, policy_entropy: -0.77352, alpha: 0.17696, time: 68.30657
[CW] ---------------------------
[CW] ---- Iteration:    91 ----
[CW] collect: return: 276.30334, steps: 1000.00000, total_steps: 97000.00000
[CW] train: qf1_loss: 7.41667, qf2_loss: 7.43758, policy_loss: -85.11809, policy_entropy: -0.80444, alpha: 0.17498, time: 68.47200
[CW] ---------------------------
[CW] ---- Iteration:    92 ----
[CW] collect: return: 256.07668, steps: 1000.00000, total_steps: 98000.00000
[CW] train: qf1_loss: 8.72307, qf2_loss: 8.65180, policy_loss: -85.94457, policy_entropy: -0.78982, alpha: 0.17310, time: 68.42486
[CW] ---------------------------
[CW] ---- Iteration:    93 ----
[CW] collect: return: 211.74342, steps: 1000.00000, total_steps: 99000.00000
[CW] train: qf1_loss: 8.05866, qf2_loss: 7.97981, policy_loss: -86.71969, policy_entropy: -0.82098, alpha: 0.17118, time: 68.35700
[CW] ---------------------------
[CW] ---- Iteration:    94 ----
[CW] collect: return: 343.14824, steps: 1000.00000, total_steps: 100000.00000
[CW] train: qf1_loss: 7.77904, qf2_loss: 7.73384, policy_loss: -88.10413, policy_entropy: -0.84172, alpha: 0.16955, time: 68.21820
[CW] ---------------------------
[CW] ---- Iteration:    95 ----
[CW] collect: return: 431.27384, steps: 1000.00000, total_steps: 101000.00000
[CW] train: qf1_loss: 8.62526, qf2_loss: 8.54351, policy_loss: -89.07530, policy_entropy: -0.85023, alpha: 0.16801, time: 68.39403
[CW] ---------------------------
[CW] ---- Iteration:    96 ----
[CW] collect: return: 265.65416, steps: 1000.00000, total_steps: 102000.00000
[CW] train: qf1_loss: 8.72616, qf2_loss: 8.68083, policy_loss: -89.97697, policy_entropy: -0.87136, alpha: 0.16659, time: 68.39443
[CW] ---------------------------
[CW] ---- Iteration:    97 ----
[CW] collect: return: 286.31806, steps: 1000.00000, total_steps: 103000.00000
[CW] train: qf1_loss: 8.12427, qf2_loss: 8.06332, policy_loss: -90.60970, policy_entropy: -0.88662, alpha: 0.16529, time: 68.13299
[CW] ---------------------------
[CW] ---- Iteration:    98 ----
[CW] collect: return: 275.38994, steps: 1000.00000, total_steps: 104000.00000
[CW] train: qf1_loss: 8.58457, qf2_loss: 8.55384, policy_loss: -91.60014, policy_entropy: -0.91814, alpha: 0.16421, time: 67.74326
[CW] ---------------------------
[CW] ---- Iteration:    99 ----
[CW] collect: return: 292.60694, steps: 1000.00000, total_steps: 105000.00000
[CW] train: qf1_loss: 7.58065, qf2_loss: 7.50425, policy_loss: -92.90696, policy_entropy: -0.91848, alpha: 0.16323, time: 67.74037
[CW] ---------------------------
[CW] ---- Iteration:   100 ----
[CW] collect: return: 355.64702, steps: 1000.00000, total_steps: 106000.00000
[CW] train: qf1_loss: 7.85943, qf2_loss: 7.78043, policy_loss: -94.29457, policy_entropy: -0.93051, alpha: 0.16240, time: 68.02253
[CW] eval: return: 304.65064, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   101 ----
[CW] collect: return: 407.21173, steps: 1000.00000, total_steps: 107000.00000
[CW] train: qf1_loss: 8.91474, qf2_loss: 8.87807, policy_loss: -94.34251, policy_entropy: -0.93079, alpha: 0.16154, time: 67.88445
[CW] ---------------------------
[CW] ---- Iteration:   102 ----
[CW] collect: return: 274.65090, steps: 1000.00000, total_steps: 108000.00000
[CW] train: qf1_loss: 8.89758, qf2_loss: 8.77961, policy_loss: -95.14511, policy_entropy: -0.95009, alpha: 0.16078, time: 67.86546
[CW] ---------------------------
[CW] ---- Iteration:   103 ----
[CW] collect: return: 283.71535, steps: 1000.00000, total_steps: 109000.00000
[CW] train: qf1_loss: 8.23101, qf2_loss: 8.16522, policy_loss: -96.81104, policy_entropy: -0.97304, alpha: 0.16030, time: 67.91942
[CW] ---------------------------
[CW] ---- Iteration:   104 ----
[CW] collect: return: 375.04308, steps: 1000.00000, total_steps: 110000.00000
[CW] train: qf1_loss: 8.54757, qf2_loss: 8.48364, policy_loss: -97.38813, policy_entropy: -1.00020, alpha: 0.16009, time: 67.80896
[CW] ---------------------------
[CW] ---- Iteration:   105 ----
[CW] collect: return: 319.34006, steps: 1000.00000, total_steps: 111000.00000
[CW] train: qf1_loss: 9.64785, qf2_loss: 9.58520, policy_loss: -98.67909, policy_entropy: -0.98230, alpha: 0.15989, time: 67.78590
[CW] ---------------------------
[CW] ---- Iteration:   106 ----
[CW] collect: return: 284.44940, steps: 1000.00000, total_steps: 112000.00000
[CW] train: qf1_loss: 9.90513, qf2_loss: 9.85569, policy_loss: -99.76669, policy_entropy: -1.01565, alpha: 0.15989, time: 67.90660
[CW] ---------------------------
[CW] ---- Iteration:   107 ----
[CW] collect: return: 339.91538, steps: 1000.00000, total_steps: 113000.00000
[CW] train: qf1_loss: 9.48820, qf2_loss: 9.39848, policy_loss: -100.77496, policy_entropy: -1.01535, alpha: 0.16011, time: 68.03444
[CW] ---------------------------
[CW] ---- Iteration:   108 ----
[CW] collect: return: 416.34325, steps: 1000.00000, total_steps: 114000.00000
[CW] train: qf1_loss: 9.02774, qf2_loss: 8.91852, policy_loss: -101.27564, policy_entropy: -1.02081, alpha: 0.16041, time: 68.39785
[CW] ---------------------------
[CW] ---- Iteration:   109 ----
[CW] collect: return: 251.64503, steps: 1000.00000, total_steps: 115000.00000
[CW] train: qf1_loss: 13.84805, qf2_loss: 13.86186, policy_loss: -102.82646, policy_entropy: -1.02087, alpha: 0.16089, time: 68.40704
[CW] ---------------------------
[CW] ---- Iteration:   110 ----
[CW] collect: return: 275.97554, steps: 1000.00000, total_steps: 116000.00000
[CW] train: qf1_loss: 12.32851, qf2_loss: 12.19118, policy_loss: -104.13612, policy_entropy: -1.02434, alpha: 0.16123, time: 71.98081
[CW] ---------------------------
[CW] ---- Iteration:   111 ----
[CW] collect: return: 370.98793, steps: 1000.00000, total_steps: 117000.00000
[CW] train: qf1_loss: 9.04915, qf2_loss: 8.94953, policy_loss: -104.11811, policy_entropy: -1.04351, alpha: 0.16185, time: 69.65641
[CW] ---------------------------
[CW] ---- Iteration:   112 ----
[CW] collect: return: 327.25708, steps: 1000.00000, total_steps: 118000.00000
[CW] train: qf1_loss: 8.70577, qf2_loss: 8.59691, policy_loss: -105.62693, policy_entropy: -1.04459, alpha: 0.16279, time: 68.25973
[CW] ---------------------------
[CW] ---- Iteration:   113 ----
[CW] collect: return: 359.75207, steps: 1000.00000, total_steps: 119000.00000
[CW] train: qf1_loss: 9.08484, qf2_loss: 9.00603, policy_loss: -106.81505, policy_entropy: -1.06320, alpha: 0.16388, time: 67.83283
[CW] ---------------------------
[CW] ---- Iteration:   114 ----
[CW] collect: return: 365.27644, steps: 1000.00000, total_steps: 120000.00000
[CW] train: qf1_loss: 9.43888, qf2_loss: 9.32846, policy_loss: -107.76103, policy_entropy: -1.08174, alpha: 0.16569, time: 67.81761
[CW] ---------------------------
[CW] ---- Iteration:   115 ----
[CW] collect: return: 334.84848, steps: 1000.00000, total_steps: 121000.00000
[CW] train: qf1_loss: 9.23151, qf2_loss: 9.14671, policy_loss: -108.49452, policy_entropy: -1.09280, alpha: 0.16769, time: 68.83905
[CW] ---------------------------
[CW] ---- Iteration:   116 ----
[CW] collect: return: 362.89953, steps: 1000.00000, total_steps: 122000.00000
[CW] train: qf1_loss: 9.74377, qf2_loss: 9.65031, policy_loss: -109.95022, policy_entropy: -1.09066, alpha: 0.16994, time: 68.29458
[CW] ---------------------------
[CW] ---- Iteration:   117 ----
[CW] collect: return: 303.10470, steps: 1000.00000, total_steps: 123000.00000
[CW] train: qf1_loss: 10.89524, qf2_loss: 10.81159, policy_loss: -110.57047, policy_entropy: -1.08871, alpha: 0.17260, time: 68.33538
[CW] ---------------------------
[CW] ---- Iteration:   118 ----
[CW] collect: return: 256.78900, steps: 1000.00000, total_steps: 124000.00000
[CW] train: qf1_loss: 9.31187, qf2_loss: 9.26786, policy_loss: -111.48209, policy_entropy: -1.08071, alpha: 0.17503, time: 68.24556
[CW] ---------------------------
[CW] ---- Iteration:   119 ----
[CW] collect: return: 285.09692, steps: 1000.00000, total_steps: 125000.00000
[CW] train: qf1_loss: 9.86925, qf2_loss: 9.79124, policy_loss: -112.73897, policy_entropy: -1.06766, alpha: 0.17761, time: 68.34429
[CW] ---------------------------
[CW] ---- Iteration:   120 ----
[CW] collect: return: 254.78994, steps: 1000.00000, total_steps: 126000.00000
[CW] train: qf1_loss: 9.43506, qf2_loss: 9.31704, policy_loss: -113.29518, policy_entropy: -1.07075, alpha: 0.17979, time: 68.48894
[CW] eval: return: 357.67379, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   121 ----
[CW] collect: return: 441.17119, steps: 1000.00000, total_steps: 127000.00000
[CW] train: qf1_loss: 9.25337, qf2_loss: 9.19917, policy_loss: -114.27558, policy_entropy: -1.08014, alpha: 0.18228, time: 67.94551
[CW] ---------------------------
[CW] ---- Iteration:   122 ----
[CW] collect: return: 374.50700, steps: 1000.00000, total_steps: 128000.00000
[CW] train: qf1_loss: 10.58370, qf2_loss: 10.36957, policy_loss: -115.28543, policy_entropy: -1.05592, alpha: 0.18508, time: 67.81863
[CW] ---------------------------
[CW] ---- Iteration:   123 ----
[CW] collect: return: 410.99619, steps: 1000.00000, total_steps: 129000.00000
[CW] train: qf1_loss: 10.72074, qf2_loss: 10.65322, policy_loss: -116.63698, policy_entropy: -1.04948, alpha: 0.18702, time: 67.83448
[CW] ---------------------------
[CW] ---- Iteration:   124 ----
[CW] collect: return: 418.95289, steps: 1000.00000, total_steps: 130000.00000
[CW] train: qf1_loss: 9.93636, qf2_loss: 9.87030, policy_loss: -117.11161, policy_entropy: -1.03693, alpha: 0.18888, time: 67.94558
[CW] ---------------------------
[CW] ---- Iteration:   125 ----
[CW] collect: return: 291.85245, steps: 1000.00000, total_steps: 131000.00000
[CW] train: qf1_loss: 9.76363, qf2_loss: 9.72464, policy_loss: -118.55124, policy_entropy: -1.04367, alpha: 0.19092, time: 68.12738
[CW] ---------------------------
[CW] ---- Iteration:   126 ----
[CW] collect: return: 447.42172, steps: 1000.00000, total_steps: 132000.00000
[CW] train: qf1_loss: 9.85116, qf2_loss: 9.76979, policy_loss: -119.40370, policy_entropy: -1.05061, alpha: 0.19287, time: 67.87454
[CW] ---------------------------
[CW] ---- Iteration:   127 ----
[CW] collect: return: 339.20878, steps: 1000.00000, total_steps: 133000.00000
[CW] train: qf1_loss: 12.11065, qf2_loss: 12.06142, policy_loss: -120.22524, policy_entropy: -1.04703, alpha: 0.19539, time: 67.88397
[CW] ---------------------------
[CW] ---- Iteration:   128 ----
[CW] collect: return: 277.05945, steps: 1000.00000, total_steps: 134000.00000
[CW] train: qf1_loss: 11.87702, qf2_loss: 11.71542, policy_loss: -121.34944, policy_entropy: -1.03869, alpha: 0.19769, time: 68.00034
[CW] ---------------------------
[CW] ---- Iteration:   129 ----
[CW] collect: return: 419.64100, steps: 1000.00000, total_steps: 135000.00000
[CW] train: qf1_loss: 10.84660, qf2_loss: 10.81745, policy_loss: -122.32543, policy_entropy: -1.04822, alpha: 0.20009, time: 68.07514
[CW] ---------------------------
[CW] ---- Iteration:   130 ----
[CW] collect: return: 444.05487, steps: 1000.00000, total_steps: 136000.00000
[CW] train: qf1_loss: 11.11555, qf2_loss: 11.04567, policy_loss: -123.06169, policy_entropy: -1.04235, alpha: 0.20283, time: 68.00859
[CW] ---------------------------
[CW] ---- Iteration:   131 ----
[CW] collect: return: 444.77632, steps: 1000.00000, total_steps: 137000.00000
[CW] train: qf1_loss: 12.92910, qf2_loss: 12.84728, policy_loss: -124.14573, policy_entropy: -1.04088, alpha: 0.20546, time: 67.94938
[CW] ---------------------------
[CW] ---- Iteration:   132 ----
[CW] collect: return: 397.98030, steps: 1000.00000, total_steps: 138000.00000
[CW] train: qf1_loss: 10.97180, qf2_loss: 10.85028, policy_loss: -124.77265, policy_entropy: -1.02535, alpha: 0.20762, time: 68.09646
[CW] ---------------------------
[CW] ---- Iteration:   133 ----
[CW] collect: return: 447.94672, steps: 1000.00000, total_steps: 139000.00000
[CW] train: qf1_loss: 11.07362, qf2_loss: 11.00417, policy_loss: -126.94665, policy_entropy: -1.02547, alpha: 0.20935, time: 68.54934
[CW] ---------------------------
[CW] ---- Iteration:   134 ----
[CW] collect: return: 369.65957, steps: 1000.00000, total_steps: 140000.00000
[CW] train: qf1_loss: 10.95981, qf2_loss: 10.91969, policy_loss: -127.54525, policy_entropy: -1.02706, alpha: 0.21123, time: 69.07533
[CW] ---------------------------
[CW] ---- Iteration:   135 ----
[CW] collect: return: 433.28166, steps: 1000.00000, total_steps: 141000.00000
[CW] train: qf1_loss: 11.67815, qf2_loss: 11.58676, policy_loss: -127.83039, policy_entropy: -1.03657, alpha: 0.21316, time: 69.22842
[CW] ---------------------------
[CW] ---- Iteration:   136 ----
[CW] collect: return: 478.07224, steps: 1000.00000, total_steps: 142000.00000
[CW] train: qf1_loss: 12.12394, qf2_loss: 12.07619, policy_loss: -129.21144, policy_entropy: -1.01707, alpha: 0.21537, time: 68.82178
[CW] ---------------------------
[CW] ---- Iteration:   137 ----
[CW] collect: return: 483.81211, steps: 1000.00000, total_steps: 143000.00000
[CW] train: qf1_loss: 11.49808, qf2_loss: 11.47675, policy_loss: -130.47229, policy_entropy: -0.99360, alpha: 0.21569, time: 68.55234
[CW] ---------------------------
[CW] ---- Iteration:   138 ----
[CW] collect: return: 452.79319, steps: 1000.00000, total_steps: 144000.00000
[CW] train: qf1_loss: 11.56467, qf2_loss: 11.47839, policy_loss: -131.42194, policy_entropy: -0.99200, alpha: 0.21513, time: 68.82313
[CW] ---------------------------
[CW] ---- Iteration:   139 ----
[CW] collect: return: 505.23738, steps: 1000.00000, total_steps: 145000.00000
[CW] train: qf1_loss: 11.28088, qf2_loss: 11.30047, policy_loss: -132.91140, policy_entropy: -1.01255, alpha: 0.21544, time: 68.86999
[CW] ---------------------------
[CW] ---- Iteration:   140 ----
[CW] collect: return: 377.56550, steps: 1000.00000, total_steps: 146000.00000
[CW] train: qf1_loss: 11.95407, qf2_loss: 11.87794, policy_loss: -133.79773, policy_entropy: -1.00395, alpha: 0.21645, time: 69.03295
[CW] eval: return: 383.03442, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   141 ----
[CW] collect: return: 397.13082, steps: 1000.00000, total_steps: 147000.00000
[CW] train: qf1_loss: 12.63669, qf2_loss: 12.65263, policy_loss: -135.03058, policy_entropy: -1.01925, alpha: 0.21724, time: 69.01630
[CW] ---------------------------
[CW] ---- Iteration:   142 ----
[CW] collect: return: 456.99667, steps: 1000.00000, total_steps: 148000.00000
[CW] train: qf1_loss: 12.38951, qf2_loss: 12.36693, policy_loss: -135.67021, policy_entropy: -1.01706, alpha: 0.21858, time: 69.31788
[CW] ---------------------------
[CW] ---- Iteration:   143 ----
[CW] collect: return: 329.60648, steps: 1000.00000, total_steps: 149000.00000
[CW] train: qf1_loss: 11.65067, qf2_loss: 11.67055, policy_loss: -136.80514, policy_entropy: -1.00943, alpha: 0.22000, time: 69.02067
[CW] ---------------------------
[CW] ---- Iteration:   144 ----
[CW] collect: return: 372.45673, steps: 1000.00000, total_steps: 150000.00000
[CW] train: qf1_loss: 12.80178, qf2_loss: 12.73702, policy_loss: -137.60538, policy_entropy: -1.00204, alpha: 0.22103, time: 68.57933
[CW] ---------------------------
[CW] ---- Iteration:   145 ----
[CW] collect: return: 455.22429, steps: 1000.00000, total_steps: 151000.00000
[CW] train: qf1_loss: 13.47594, qf2_loss: 13.45362, policy_loss: -139.22934, policy_entropy: -0.99891, alpha: 0.22089, time: 68.62185
[CW] ---------------------------
[CW] ---- Iteration:   146 ----
[CW] collect: return: 454.85560, steps: 1000.00000, total_steps: 152000.00000
[CW] train: qf1_loss: 13.94413, qf2_loss: 13.89521, policy_loss: -139.78455, policy_entropy: -1.01113, alpha: 0.22125, time: 68.77287
[CW] ---------------------------
[CW] ---- Iteration:   147 ----
[CW] collect: return: 422.12003, steps: 1000.00000, total_steps: 153000.00000
[CW] train: qf1_loss: 11.96832, qf2_loss: 11.89378, policy_loss: -140.66673, policy_entropy: -1.00747, alpha: 0.22232, time: 68.74715
[CW] ---------------------------
[CW] ---- Iteration:   148 ----
[CW] collect: return: 407.67886, steps: 1000.00000, total_steps: 154000.00000
[CW] train: qf1_loss: 12.11580, qf2_loss: 12.10371, policy_loss: -142.32126, policy_entropy: -1.01115, alpha: 0.22293, time: 68.65091
[CW] ---------------------------
[CW] ---- Iteration:   149 ----
[CW] collect: return: 398.77096, steps: 1000.00000, total_steps: 155000.00000
[CW] train: qf1_loss: 11.78051, qf2_loss: 11.72880, policy_loss: -143.10121, policy_entropy: -1.01486, alpha: 0.22444, time: 68.50235
[CW] ---------------------------
[CW] ---- Iteration:   150 ----
[CW] collect: return: 457.71500, steps: 1000.00000, total_steps: 156000.00000
[CW] train: qf1_loss: 13.61539, qf2_loss: 13.56422, policy_loss: -144.71526, policy_entropy: -1.00097, alpha: 0.22537, time: 68.53185
[CW] ---------------------------
[CW] ---- Iteration:   151 ----
[CW] collect: return: 357.76501, steps: 1000.00000, total_steps: 157000.00000
[CW] train: qf1_loss: 12.60048, qf2_loss: 12.47619, policy_loss: -144.95113, policy_entropy: -1.01019, alpha: 0.22551, time: 68.44283
[CW] ---------------------------
[CW] ---- Iteration:   152 ----
[CW] collect: return: 347.19438, steps: 1000.00000, total_steps: 158000.00000
[CW] train: qf1_loss: 13.01740, qf2_loss: 12.92442, policy_loss: -146.77881, policy_entropy: -0.99995, alpha: 0.22666, time: 68.51094
[CW] ---------------------------
[CW] ---- Iteration:   153 ----
[CW] collect: return: 446.34779, steps: 1000.00000, total_steps: 159000.00000
[CW] train: qf1_loss: 12.77136, qf2_loss: 12.77416, policy_loss: -147.32188, policy_entropy: -1.01483, alpha: 0.22728, time: 68.52979
[CW] ---------------------------
[CW] ---- Iteration:   154 ----
[CW] collect: return: 467.00008, steps: 1000.00000, total_steps: 160000.00000
[CW] train: qf1_loss: 15.00807, qf2_loss: 14.95395, policy_loss: -149.08851, policy_entropy: -1.00312, alpha: 0.22839, time: 68.68495
[CW] ---------------------------
[CW] ---- Iteration:   155 ----
[CW] collect: return: 352.87763, steps: 1000.00000, total_steps: 161000.00000
[CW] train: qf1_loss: 13.77264, qf2_loss: 13.82944, policy_loss: -149.43820, policy_entropy: -1.01187, alpha: 0.22870, time: 68.72669
[CW] ---------------------------
[CW] ---- Iteration:   156 ----
[CW] collect: return: 443.76543, steps: 1000.00000, total_steps: 162000.00000
[CW] train: qf1_loss: 13.88452, qf2_loss: 13.80716, policy_loss: -150.92978, policy_entropy: -0.99177, alpha: 0.22943, time: 68.72562
[CW] ---------------------------
[CW] ---- Iteration:   157 ----
[CW] collect: return: 467.30974, steps: 1000.00000, total_steps: 163000.00000
[CW] train: qf1_loss: 12.41044, qf2_loss: 12.37408, policy_loss: -151.18414, policy_entropy: -1.00908, alpha: 0.22986, time: 68.60516
[CW] ---------------------------
[CW] ---- Iteration:   158 ----
[CW] collect: return: 413.82358, steps: 1000.00000, total_steps: 164000.00000
[CW] train: qf1_loss: 11.97551, qf2_loss: 11.90492, policy_loss: -152.41254, policy_entropy: -1.01364, alpha: 0.23041, time: 68.60036
[CW] ---------------------------
[CW] ---- Iteration:   159 ----
[CW] collect: return: 481.38777, steps: 1000.00000, total_steps: 165000.00000
[CW] train: qf1_loss: 14.08763, qf2_loss: 14.06799, policy_loss: -153.62741, policy_entropy: -1.00310, alpha: 0.23137, time: 68.61915
[CW] ---------------------------
[CW] ---- Iteration:   160 ----
[CW] collect: return: 501.81124, steps: 1000.00000, total_steps: 166000.00000
[CW] train: qf1_loss: 13.41202, qf2_loss: 13.35237, policy_loss: -154.55879, policy_entropy: -1.01341, alpha: 0.23274, time: 68.93552
[CW] eval: return: 432.78368, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   161 ----
[CW] collect: return: 433.11246, steps: 1000.00000, total_steps: 167000.00000
[CW] train: qf1_loss: 12.92504, qf2_loss: 12.82640, policy_loss: -156.05938, policy_entropy: -1.01865, alpha: 0.23439, time: 69.34297
[CW] ---------------------------
[CW] ---- Iteration:   162 ----
[CW] collect: return: 436.30918, steps: 1000.00000, total_steps: 168000.00000
[CW] train: qf1_loss: 13.74311, qf2_loss: 13.68640, policy_loss: -156.81335, policy_entropy: -1.01430, alpha: 0.23685, time: 69.27536
[CW] ---------------------------
[CW] ---- Iteration:   163 ----
[CW] collect: return: 500.77813, steps: 1000.00000, total_steps: 169000.00000
[CW] train: qf1_loss: 14.54319, qf2_loss: 14.42818, policy_loss: -158.05078, policy_entropy: -1.01016, alpha: 0.23821, time: 69.21472
[CW] ---------------------------
[CW] ---- Iteration:   164 ----
[CW] collect: return: 442.53138, steps: 1000.00000, total_steps: 170000.00000
[CW] train: qf1_loss: 14.46945, qf2_loss: 14.43223, policy_loss: -158.48197, policy_entropy: -1.01425, alpha: 0.23919, time: 69.25860
[CW] ---------------------------
[CW] ---- Iteration:   165 ----
[CW] collect: return: 390.52983, steps: 1000.00000, total_steps: 171000.00000
[CW] train: qf1_loss: 13.95361, qf2_loss: 13.90306, policy_loss: -160.23920, policy_entropy: -0.99803, alpha: 0.24043, time: 69.33065
[CW] ---------------------------
[CW] ---- Iteration:   166 ----
[CW] collect: return: 328.74567, steps: 1000.00000, total_steps: 172000.00000
[CW] train: qf1_loss: 12.41531, qf2_loss: 12.37562, policy_loss: -160.88392, policy_entropy: -1.01467, alpha: 0.24084, time: 77.66509
[CW] ---------------------------
[CW] ---- Iteration:   167 ----
[CW] collect: return: 486.44021, steps: 1000.00000, total_steps: 173000.00000
[CW] train: qf1_loss: 12.37084, qf2_loss: 12.31301, policy_loss: -161.78720, policy_entropy: -1.03237, alpha: 0.24389, time: 70.25573
[CW] ---------------------------
[CW] ---- Iteration:   168 ----
[CW] collect: return: 351.13540, steps: 1000.00000, total_steps: 174000.00000
[CW] train: qf1_loss: 13.90405, qf2_loss: 13.97398, policy_loss: -162.38368, policy_entropy: -1.00474, alpha: 0.24633, time: 68.51246
[CW] ---------------------------
[CW] ---- Iteration:   169 ----
[CW] collect: return: 509.76507, steps: 1000.00000, total_steps: 175000.00000
[CW] train: qf1_loss: 13.61373, qf2_loss: 13.57062, policy_loss: -163.94799, policy_entropy: -1.00516, alpha: 0.24682, time: 68.45696
[CW] ---------------------------
[CW] ---- Iteration:   170 ----
[CW] collect: return: 438.69649, steps: 1000.00000, total_steps: 176000.00000
[CW] train: qf1_loss: 14.09833, qf2_loss: 13.85951, policy_loss: -164.74243, policy_entropy: -1.01049, alpha: 0.24835, time: 68.38581
[CW] ---------------------------
[CW] ---- Iteration:   171 ----
[CW] collect: return: 508.61307, steps: 1000.00000, total_steps: 177000.00000
[CW] train: qf1_loss: 16.25733, qf2_loss: 16.22022, policy_loss: -165.59777, policy_entropy: -1.00918, alpha: 0.24914, time: 68.40990
[CW] ---------------------------
[CW] ---- Iteration:   172 ----
[CW] collect: return: 437.01154, steps: 1000.00000, total_steps: 178000.00000
[CW] train: qf1_loss: 13.77959, qf2_loss: 13.68520, policy_loss: -167.01460, policy_entropy: -0.99968, alpha: 0.25014, time: 68.46257
[CW] ---------------------------
[CW] ---- Iteration:   173 ----
[CW] collect: return: 531.05170, steps: 1000.00000, total_steps: 179000.00000
[CW] train: qf1_loss: 13.96539, qf2_loss: 13.95884, policy_loss: -167.93583, policy_entropy: -1.00971, alpha: 0.25075, time: 68.52518
[CW] ---------------------------
[CW] ---- Iteration:   174 ----
[CW] collect: return: 517.67501, steps: 1000.00000, total_steps: 180000.00000
[CW] train: qf1_loss: 14.95829, qf2_loss: 14.74135, policy_loss: -168.62786, policy_entropy: -1.00034, alpha: 0.25111, time: 68.56813
[CW] ---------------------------
[CW] ---- Iteration:   175 ----
[CW] collect: return: 492.12109, steps: 1000.00000, total_steps: 181000.00000
[CW] train: qf1_loss: 14.61188, qf2_loss: 14.49993, policy_loss: -169.82674, policy_entropy: -1.00034, alpha: 0.25149, time: 68.50192
[CW] ---------------------------
[CW] ---- Iteration:   176 ----
[CW] collect: return: 428.49300, steps: 1000.00000, total_steps: 182000.00000
[CW] train: qf1_loss: 14.77811, qf2_loss: 14.68889, policy_loss: -170.70315, policy_entropy: -1.00881, alpha: 0.25209, time: 68.54395
[CW] ---------------------------
[CW] ---- Iteration:   177 ----
[CW] collect: return: 440.77724, steps: 1000.00000, total_steps: 183000.00000
[CW] train: qf1_loss: 14.28005, qf2_loss: 14.20394, policy_loss: -172.09851, policy_entropy: -0.99705, alpha: 0.25215, time: 68.67431
[CW] ---------------------------
[CW] ---- Iteration:   178 ----
[CW] collect: return: 580.64575, steps: 1000.00000, total_steps: 184000.00000
[CW] train: qf1_loss: 17.22474, qf2_loss: 17.13480, policy_loss: -172.72078, policy_entropy: -1.00792, alpha: 0.25217, time: 68.71496
[CW] ---------------------------
[CW] ---- Iteration:   179 ----
[CW] collect: return: 483.83539, steps: 1000.00000, total_steps: 185000.00000
[CW] train: qf1_loss: 15.64952, qf2_loss: 15.43165, policy_loss: -173.52939, policy_entropy: -1.00598, alpha: 0.25324, time: 68.52254
[CW] ---------------------------
[CW] ---- Iteration:   180 ----
[CW] collect: return: 488.87509, steps: 1000.00000, total_steps: 186000.00000
[CW] train: qf1_loss: 13.72045, qf2_loss: 13.72331, policy_loss: -174.44572, policy_entropy: -1.00630, alpha: 0.25391, time: 68.59354
[CW] eval: return: 462.05828, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   181 ----
[CW] collect: return: 436.80259, steps: 1000.00000, total_steps: 187000.00000
[CW] train: qf1_loss: 15.71427, qf2_loss: 15.63294, policy_loss: -176.06416, policy_entropy: -1.01311, alpha: 0.25557, time: 68.54091
[CW] ---------------------------
[CW] ---- Iteration:   182 ----
[CW] collect: return: 498.18217, steps: 1000.00000, total_steps: 188000.00000
[CW] train: qf1_loss: 16.02559, qf2_loss: 15.93120, policy_loss: -177.04298, policy_entropy: -0.99546, alpha: 0.25597, time: 68.65272
[CW] ---------------------------
[CW] ---- Iteration:   183 ----
[CW] collect: return: 424.29160, steps: 1000.00000, total_steps: 189000.00000
[CW] train: qf1_loss: 16.27573, qf2_loss: 16.21928, policy_loss: -177.64087, policy_entropy: -1.00304, alpha: 0.25574, time: 68.46831
[CW] ---------------------------
[CW] ---- Iteration:   184 ----
[CW] collect: return: 466.43045, steps: 1000.00000, total_steps: 190000.00000
[CW] train: qf1_loss: 17.16526, qf2_loss: 17.04951, policy_loss: -178.87810, policy_entropy: -1.01090, alpha: 0.25692, time: 68.39310
[CW] ---------------------------
[CW] ---- Iteration:   185 ----
[CW] collect: return: 446.06525, steps: 1000.00000, total_steps: 191000.00000
[CW] train: qf1_loss: 14.24311, qf2_loss: 14.24076, policy_loss: -179.50607, policy_entropy: -1.00262, alpha: 0.25781, time: 68.43245
[CW] ---------------------------
[CW] ---- Iteration:   186 ----
[CW] collect: return: 505.70589, steps: 1000.00000, total_steps: 192000.00000
[CW] train: qf1_loss: 16.69633, qf2_loss: 16.54525, policy_loss: -180.95064, policy_entropy: -0.99546, alpha: 0.25784, time: 68.74584
[CW] ---------------------------
[CW] ---- Iteration:   187 ----
[CW] collect: return: 446.18736, steps: 1000.00000, total_steps: 193000.00000
[CW] train: qf1_loss: 18.37390, qf2_loss: 18.19469, policy_loss: -181.90419, policy_entropy: -1.00760, alpha: 0.25868, time: 69.28270
[CW] ---------------------------
[CW] ---- Iteration:   188 ----
[CW] collect: return: 452.40156, steps: 1000.00000, total_steps: 194000.00000
[CW] train: qf1_loss: 17.43044, qf2_loss: 17.34818, policy_loss: -182.08852, policy_entropy: -1.00037, alpha: 0.25840, time: 69.28856
[CW] ---------------------------
[CW] ---- Iteration:   189 ----
[CW] collect: return: 439.32660, steps: 1000.00000, total_steps: 195000.00000
[CW] train: qf1_loss: 16.62437, qf2_loss: 16.47834, policy_loss: -183.08959, policy_entropy: -1.01048, alpha: 0.25926, time: 69.14978
[CW] ---------------------------
[CW] ---- Iteration:   190 ----
[CW] collect: return: 438.50961, steps: 1000.00000, total_steps: 196000.00000
[CW] train: qf1_loss: 16.84050, qf2_loss: 16.81231, policy_loss: -184.77806, policy_entropy: -1.00490, alpha: 0.26029, time: 68.93326
[CW] ---------------------------
[CW] ---- Iteration:   191 ----
[CW] collect: return: 446.32869, steps: 1000.00000, total_steps: 197000.00000
[CW] train: qf1_loss: 17.41560, qf2_loss: 17.15362, policy_loss: -185.66531, policy_entropy: -1.01645, alpha: 0.26142, time: 69.06531
[CW] ---------------------------
[CW] ---- Iteration:   192 ----
[CW] collect: return: 512.31899, steps: 1000.00000, total_steps: 198000.00000
[CW] train: qf1_loss: 17.63884, qf2_loss: 17.51962, policy_loss: -186.83905, policy_entropy: -0.99580, alpha: 0.26235, time: 69.05345
[CW] ---------------------------
[CW] ---- Iteration:   193 ----
[CW] collect: return: 532.54764, steps: 1000.00000, total_steps: 199000.00000
[CW] train: qf1_loss: 16.30486, qf2_loss: 16.25991, policy_loss: -187.60222, policy_entropy: -1.00318, alpha: 0.26256, time: 68.56308
[CW] ---------------------------
[CW] ---- Iteration:   194 ----
[CW] collect: return: 465.33318, steps: 1000.00000, total_steps: 200000.00000
[CW] train: qf1_loss: 18.10129, qf2_loss: 18.05180, policy_loss: -187.98435, policy_entropy: -1.01015, alpha: 0.26345, time: 68.34898
[CW] ---------------------------
[CW] ---- Iteration:   195 ----
[CW] collect: return: 493.91809, steps: 1000.00000, total_steps: 201000.00000
[CW] train: qf1_loss: 17.90349, qf2_loss: 17.71529, policy_loss: -189.28557, policy_entropy: -0.98581, alpha: 0.26372, time: 68.06271
[CW] ---------------------------
[CW] ---- Iteration:   196 ----
[CW] collect: return: 436.03498, steps: 1000.00000, total_steps: 202000.00000
[CW] train: qf1_loss: 17.73393, qf2_loss: 17.59958, policy_loss: -189.28113, policy_entropy: -1.01105, alpha: 0.26290, time: 68.39203
[CW] ---------------------------
[CW] ---- Iteration:   197 ----
[CW] collect: return: 436.73396, steps: 1000.00000, total_steps: 203000.00000
[CW] train: qf1_loss: 19.32911, qf2_loss: 19.25371, policy_loss: -190.83432, policy_entropy: -1.01030, alpha: 0.26472, time: 68.38212
[CW] ---------------------------
[CW] ---- Iteration:   198 ----
[CW] collect: return: 510.13640, steps: 1000.00000, total_steps: 204000.00000
[CW] train: qf1_loss: 18.74561, qf2_loss: 18.64860, policy_loss: -191.95497, policy_entropy: -0.99678, alpha: 0.26551, time: 68.18800
[CW] ---------------------------
[CW] ---- Iteration:   199 ----
[CW] collect: return: 668.12357, steps: 1000.00000, total_steps: 205000.00000
[CW] train: qf1_loss: 17.85326, qf2_loss: 17.75369, policy_loss: -193.53883, policy_entropy: -0.99885, alpha: 0.26461, time: 68.35247
[CW] ---------------------------
[CW] ---- Iteration:   200 ----
[CW] collect: return: 614.25244, steps: 1000.00000, total_steps: 206000.00000
[CW] train: qf1_loss: 17.74438, qf2_loss: 17.63735, policy_loss: -194.18560, policy_entropy: -0.99302, alpha: 0.26395, time: 68.25032
[CW] eval: return: 526.68380, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   201 ----
[CW] collect: return: 518.78798, steps: 1000.00000, total_steps: 207000.00000
[CW] train: qf1_loss: 17.73003, qf2_loss: 17.72511, policy_loss: -195.44374, policy_entropy: -1.01395, alpha: 0.26458, time: 68.14072
[CW] ---------------------------
[CW] ---- Iteration:   202 ----
[CW] collect: return: 586.88796, steps: 1000.00000, total_steps: 208000.00000
[CW] train: qf1_loss: 19.89898, qf2_loss: 19.59076, policy_loss: -195.96737, policy_entropy: -1.01093, alpha: 0.26615, time: 68.53887
[CW] ---------------------------
[CW] ---- Iteration:   203 ----
[CW] collect: return: 290.71056, steps: 1000.00000, total_steps: 209000.00000
[CW] train: qf1_loss: 18.49098, qf2_loss: 18.44783, policy_loss: -196.68673, policy_entropy: -1.01997, alpha: 0.26779, time: 68.67965
[CW] ---------------------------
[CW] ---- Iteration:   204 ----
[CW] collect: return: 506.49819, steps: 1000.00000, total_steps: 210000.00000
[CW] train: qf1_loss: 19.69277, qf2_loss: 19.55438, policy_loss: -198.43625, policy_entropy: -1.00408, alpha: 0.27022, time: 68.46150
[CW] ---------------------------
[CW] ---- Iteration:   205 ----
[CW] collect: return: 535.11751, steps: 1000.00000, total_steps: 211000.00000
[CW] train: qf1_loss: 21.24972, qf2_loss: 20.90641, policy_loss: -198.66940, policy_entropy: -1.01082, alpha: 0.27175, time: 68.71526
[CW] ---------------------------
[CW] ---- Iteration:   206 ----
[CW] collect: return: 489.89763, steps: 1000.00000, total_steps: 212000.00000
[CW] train: qf1_loss: 19.46117, qf2_loss: 19.53695, policy_loss: -199.48666, policy_entropy: -0.99927, alpha: 0.27189, time: 68.23239
[CW] ---------------------------
[CW] ---- Iteration:   207 ----
[CW] collect: return: 418.71774, steps: 1000.00000, total_steps: 213000.00000
[CW] train: qf1_loss: 19.08701, qf2_loss: 18.87746, policy_loss: -201.08796, policy_entropy: -1.00957, alpha: 0.27260, time: 68.38815
[CW] ---------------------------
[CW] ---- Iteration:   208 ----
[CW] collect: return: 591.69243, steps: 1000.00000, total_steps: 214000.00000
[CW] train: qf1_loss: 23.46976, qf2_loss: 23.40364, policy_loss: -201.24867, policy_entropy: -1.02505, alpha: 0.27569, time: 68.50010
[CW] ---------------------------
[CW] ---- Iteration:   209 ----
[CW] collect: return: 531.32339, steps: 1000.00000, total_steps: 215000.00000
[CW] train: qf1_loss: 20.39306, qf2_loss: 20.35482, policy_loss: -202.76594, policy_entropy: -1.00272, alpha: 0.27767, time: 68.35532
[CW] ---------------------------
[CW] ---- Iteration:   210 ----
[CW] collect: return: 532.68947, steps: 1000.00000, total_steps: 216000.00000
[CW] train: qf1_loss: 21.35695, qf2_loss: 21.31551, policy_loss: -203.32906, policy_entropy: -0.99672, alpha: 0.27837, time: 68.18858
[CW] ---------------------------
[CW] ---- Iteration:   211 ----
[CW] collect: return: 570.69436, steps: 1000.00000, total_steps: 217000.00000
[CW] train: qf1_loss: 20.25898, qf2_loss: 19.97673, policy_loss: -204.25301, policy_entropy: -1.00677, alpha: 0.27814, time: 68.17904
[CW] ---------------------------
[CW] ---- Iteration:   212 ----
[CW] collect: return: 506.91237, steps: 1000.00000, total_steps: 218000.00000
[CW] train: qf1_loss: 22.29391, qf2_loss: 22.18898, policy_loss: -205.20405, policy_entropy: -1.00287, alpha: 0.27893, time: 67.93549
[CW] ---------------------------
[CW] ---- Iteration:   213 ----
[CW] collect: return: 592.36547, steps: 1000.00000, total_steps: 219000.00000
[CW] train: qf1_loss: 19.97138, qf2_loss: 19.74353, policy_loss: -206.08784, policy_entropy: -1.02243, alpha: 0.28019, time: 67.93271
[CW] ---------------------------
[CW] ---- Iteration:   214 ----
[CW] collect: return: 517.66604, steps: 1000.00000, total_steps: 220000.00000
[CW] train: qf1_loss: 21.19279, qf2_loss: 21.03422, policy_loss: -206.73995, policy_entropy: -1.01769, alpha: 0.28398, time: 67.90774
[CW] ---------------------------
[CW] ---- Iteration:   215 ----
[CW] collect: return: 611.09929, steps: 1000.00000, total_steps: 221000.00000
[CW] train: qf1_loss: 20.15543, qf2_loss: 19.99054, policy_loss: -207.52848, policy_entropy: -1.01716, alpha: 0.28739, time: 67.88511
[CW] ---------------------------
[CW] ---- Iteration:   216 ----
[CW] collect: return: 532.91471, steps: 1000.00000, total_steps: 222000.00000
[CW] train: qf1_loss: 20.33683, qf2_loss: 20.11996, policy_loss: -208.60045, policy_entropy: -1.01086, alpha: 0.28910, time: 67.87979
[CW] ---------------------------
[CW] ---- Iteration:   217 ----
[CW] collect: return: 526.04361, steps: 1000.00000, total_steps: 223000.00000
[CW] train: qf1_loss: 22.17233, qf2_loss: 22.00800, policy_loss: -209.59115, policy_entropy: -1.00238, alpha: 0.29089, time: 68.39462
[CW] ---------------------------
[CW] ---- Iteration:   218 ----
[CW] collect: return: 553.31007, steps: 1000.00000, total_steps: 224000.00000
[CW] train: qf1_loss: 22.86611, qf2_loss: 22.61429, policy_loss: -210.56245, policy_entropy: -1.01062, alpha: 0.29110, time: 68.45474
[CW] ---------------------------
[CW] ---- Iteration:   219 ----
[CW] collect: return: 523.50928, steps: 1000.00000, total_steps: 225000.00000
[CW] train: qf1_loss: 23.02284, qf2_loss: 22.96821, policy_loss: -211.72081, policy_entropy: -1.00035, alpha: 0.29219, time: 68.11900
[CW] ---------------------------
[CW] ---- Iteration:   220 ----
[CW] collect: return: 516.72276, steps: 1000.00000, total_steps: 226000.00000
[CW] train: qf1_loss: 20.66999, qf2_loss: 20.57748, policy_loss: -213.40977, policy_entropy: -0.99270, alpha: 0.29180, time: 68.06267
[CW] eval: return: 558.03101, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   221 ----
[CW] collect: return: 537.08718, steps: 1000.00000, total_steps: 227000.00000
[CW] train: qf1_loss: 21.78732, qf2_loss: 21.80608, policy_loss: -214.27711, policy_entropy: -1.01353, alpha: 0.29269, time: 70.12027
[CW] ---------------------------
[CW] ---- Iteration:   222 ----
[CW] collect: return: 564.48973, steps: 1000.00000, total_steps: 228000.00000
[CW] train: qf1_loss: 22.79465, qf2_loss: 22.55062, policy_loss: -215.16330, policy_entropy: -1.00687, alpha: 0.29441, time: 69.02152
[CW] ---------------------------
[CW] ---- Iteration:   223 ----
[CW] collect: return: 519.26979, steps: 1000.00000, total_steps: 229000.00000
[CW] train: qf1_loss: 24.77506, qf2_loss: 24.73067, policy_loss: -215.78479, policy_entropy: -0.99912, alpha: 0.29441, time: 68.03700
[CW] ---------------------------
[CW] ---- Iteration:   224 ----
[CW] collect: return: 553.82772, steps: 1000.00000, total_steps: 230000.00000
[CW] train: qf1_loss: 22.34712, qf2_loss: 22.35427, policy_loss: -215.89664, policy_entropy: -1.00868, alpha: 0.29528, time: 68.18526
[CW] ---------------------------
[CW] ---- Iteration:   225 ----
[CW] collect: return: 605.38909, steps: 1000.00000, total_steps: 231000.00000
[CW] train: qf1_loss: 23.67828, qf2_loss: 23.61066, policy_loss: -217.28915, policy_entropy: -1.02211, alpha: 0.29778, time: 68.00853
[CW] ---------------------------
[CW] ---- Iteration:   226 ----
[CW] collect: return: 517.34147, steps: 1000.00000, total_steps: 232000.00000
[CW] train: qf1_loss: 23.99004, qf2_loss: 23.96928, policy_loss: -218.03456, policy_entropy: -1.01306, alpha: 0.30051, time: 68.14523
[CW] ---------------------------
[CW] ---- Iteration:   227 ----
[CW] collect: return: 582.48653, steps: 1000.00000, total_steps: 233000.00000
[CW] train: qf1_loss: 21.68276, qf2_loss: 21.61505, policy_loss: -219.19986, policy_entropy: -1.01178, alpha: 0.30325, time: 67.95820
[CW] ---------------------------
[CW] ---- Iteration:   228 ----
[CW] collect: return: 604.48622, steps: 1000.00000, total_steps: 234000.00000
[CW] train: qf1_loss: 23.52348, qf2_loss: 23.36884, policy_loss: -220.56299, policy_entropy: -1.00626, alpha: 0.30461, time: 68.07160
[CW] ---------------------------
[CW] ---- Iteration:   229 ----
[CW] collect: return: 601.37308, steps: 1000.00000, total_steps: 235000.00000
[CW] train: qf1_loss: 24.27166, qf2_loss: 24.11324, policy_loss: -220.45910, policy_entropy: -0.99951, alpha: 0.30553, time: 67.83417
[CW] ---------------------------
[CW] ---- Iteration:   230 ----
[CW] collect: return: 599.26324, steps: 1000.00000, total_steps: 236000.00000
[CW] train: qf1_loss: 20.43831, qf2_loss: 20.40933, policy_loss: -222.61692, policy_entropy: -1.02197, alpha: 0.30681, time: 67.95746
[CW] ---------------------------
[CW] ---- Iteration:   231 ----
[CW] collect: return: 562.78015, steps: 1000.00000, total_steps: 237000.00000
[CW] train: qf1_loss: 23.58263, qf2_loss: 23.48565, policy_loss: -224.19124, policy_entropy: -0.99746, alpha: 0.30843, time: 67.86840
[CW] ---------------------------
[CW] ---- Iteration:   232 ----
[CW] collect: return: 590.71780, steps: 1000.00000, total_steps: 238000.00000
[CW] train: qf1_loss: 21.87509, qf2_loss: 21.33727, policy_loss: -222.71676, policy_entropy: -1.01827, alpha: 0.30946, time: 67.95328
[CW] ---------------------------
[CW] ---- Iteration:   233 ----
[CW] collect: return: 489.82670, steps: 1000.00000, total_steps: 239000.00000
[CW] train: qf1_loss: 21.65432, qf2_loss: 21.50154, policy_loss: -225.04334, policy_entropy: -1.00956, alpha: 0.31292, time: 68.00806
[CW] ---------------------------
[CW] ---- Iteration:   234 ----
[CW] collect: return: 585.56638, steps: 1000.00000, total_steps: 240000.00000
[CW] train: qf1_loss: 23.52883, qf2_loss: 23.25573, policy_loss: -225.01778, policy_entropy: -1.01527, alpha: 0.31479, time: 67.89265
[CW] ---------------------------
[CW] ---- Iteration:   235 ----
[CW] collect: return: 591.81905, steps: 1000.00000, total_steps: 241000.00000
[CW] train: qf1_loss: 25.95119, qf2_loss: 25.69840, policy_loss: -226.63212, policy_entropy: -0.99595, alpha: 0.31635, time: 67.96835
[CW] ---------------------------
[CW] ---- Iteration:   236 ----
[CW] collect: return: 583.99855, steps: 1000.00000, total_steps: 242000.00000
[CW] train: qf1_loss: 37.90833, qf2_loss: 38.03621, policy_loss: -228.02854, policy_entropy: -0.98901, alpha: 0.31464, time: 68.33299
[CW] ---------------------------
[CW] ---- Iteration:   237 ----
[CW] collect: return: 661.97931, steps: 1000.00000, total_steps: 243000.00000
[CW] train: qf1_loss: 23.97823, qf2_loss: 23.78459, policy_loss: -228.43701, policy_entropy: -1.00911, alpha: 0.31422, time: 68.10073
[CW] ---------------------------
[CW] ---- Iteration:   238 ----
[CW] collect: return: 528.07684, steps: 1000.00000, total_steps: 244000.00000
[CW] train: qf1_loss: 21.17656, qf2_loss: 21.16799, policy_loss: -228.43139, policy_entropy: -1.01520, alpha: 0.31628, time: 68.11577
[CW] ---------------------------
[CW] ---- Iteration:   239 ----
[CW] collect: return: 585.17611, steps: 1000.00000, total_steps: 245000.00000
[CW] train: qf1_loss: 21.51665, qf2_loss: 21.21710, policy_loss: -230.86881, policy_entropy: -1.00725, alpha: 0.31826, time: 68.12878
[CW] ---------------------------
[CW] ---- Iteration:   240 ----
[CW] collect: return: 501.21776, steps: 1000.00000, total_steps: 246000.00000
[CW] train: qf1_loss: 22.89425, qf2_loss: 22.60006, policy_loss: -231.01830, policy_entropy: -1.00069, alpha: 0.31868, time: 67.89916
[CW] eval: return: 616.51315, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   241 ----
[CW] collect: return: 611.69697, steps: 1000.00000, total_steps: 247000.00000
[CW] train: qf1_loss: 21.97815, qf2_loss: 21.90498, policy_loss: -232.14883, policy_entropy: -1.00817, alpha: 0.31956, time: 67.94612
[CW] ---------------------------
[CW] ---- Iteration:   242 ----
[CW] collect: return: 570.34869, steps: 1000.00000, total_steps: 248000.00000
[CW] train: qf1_loss: 21.84715, qf2_loss: 21.78964, policy_loss: -232.76295, policy_entropy: -1.00990, alpha: 0.32057, time: 68.05361
[CW] ---------------------------
[CW] ---- Iteration:   243 ----
[CW] collect: return: 603.79931, steps: 1000.00000, total_steps: 249000.00000
[CW] train: qf1_loss: 22.31040, qf2_loss: 22.17947, policy_loss: -234.05675, policy_entropy: -1.01030, alpha: 0.32192, time: 68.15868
[CW] ---------------------------
[CW] ---- Iteration:   244 ----
[CW] collect: return: 682.61404, steps: 1000.00000, total_steps: 250000.00000
[CW] train: qf1_loss: 23.77596, qf2_loss: 23.59065, policy_loss: -235.01541, policy_entropy: -1.01065, alpha: 0.32477, time: 68.11498
[CW] ---------------------------
[CW] ---- Iteration:   245 ----
[CW] collect: return: 608.73344, steps: 1000.00000, total_steps: 251000.00000
[CW] train: qf1_loss: 45.54182, qf2_loss: 44.76093, policy_loss: -235.61631, policy_entropy: -0.98606, alpha: 0.32477, time: 68.28734
[CW] ---------------------------
[CW] ---- Iteration:   246 ----
[CW] collect: return: 541.63586, steps: 1000.00000, total_steps: 252000.00000
[CW] train: qf1_loss: 32.75356, qf2_loss: 32.64678, policy_loss: -237.40617, policy_entropy: -1.00724, alpha: 0.32360, time: 68.36755
[CW] ---------------------------
[CW] ---- Iteration:   247 ----
[CW] collect: return: 529.92977, steps: 1000.00000, total_steps: 253000.00000
[CW] train: qf1_loss: 23.54529, qf2_loss: 23.44728, policy_loss: -237.70693, policy_entropy: -1.00681, alpha: 0.32438, time: 68.07908
[CW] ---------------------------
[CW] ---- Iteration:   248 ----
[CW] collect: return: 553.71251, steps: 1000.00000, total_steps: 254000.00000
[CW] train: qf1_loss: 21.10993, qf2_loss: 20.78356, policy_loss: -238.72214, policy_entropy: -1.01321, alpha: 0.32719, time: 68.33490
[CW] ---------------------------
[CW] ---- Iteration:   249 ----
[CW] collect: return: 653.96973, steps: 1000.00000, total_steps: 255000.00000
[CW] train: qf1_loss: 22.44417, qf2_loss: 22.27282, policy_loss: -240.06192, policy_entropy: -1.01636, alpha: 0.32881, time: 67.95378
[CW] ---------------------------
[CW] ---- Iteration:   250 ----
[CW] collect: return: 478.93216, steps: 1000.00000, total_steps: 256000.00000
[CW] train: qf1_loss: 24.57018, qf2_loss: 24.33234, policy_loss: -240.14746, policy_entropy: -1.01571, alpha: 0.33195, time: 68.32362
[CW] ---------------------------
[CW] ---- Iteration:   251 ----
[CW] collect: return: 600.07425, steps: 1000.00000, total_steps: 257000.00000
[CW] train: qf1_loss: 23.70084, qf2_loss: 23.69507, policy_loss: -242.65924, policy_entropy: -0.99840, alpha: 0.33399, time: 68.27131
[CW] ---------------------------
[CW] ---- Iteration:   252 ----
[CW] collect: return: 609.10693, steps: 1000.00000, total_steps: 258000.00000
[CW] train: qf1_loss: 22.48662, qf2_loss: 22.39946, policy_loss: -243.24468, policy_entropy: -1.01800, alpha: 0.33483, time: 68.05183
[CW] ---------------------------
[CW] ---- Iteration:   253 ----
[CW] collect: return: 655.14809, steps: 1000.00000, total_steps: 259000.00000
[CW] train: qf1_loss: 27.24451, qf2_loss: 27.21951, policy_loss: -243.40133, policy_entropy: -0.99889, alpha: 0.33650, time: 67.85815
[CW] ---------------------------
[CW] ---- Iteration:   254 ----
[CW] collect: return: 565.04072, steps: 1000.00000, total_steps: 260000.00000
[CW] train: qf1_loss: 24.67007, qf2_loss: 24.48920, policy_loss: -244.56788, policy_entropy: -1.00734, alpha: 0.33708, time: 67.96045
[CW] ---------------------------
[CW] ---- Iteration:   255 ----
[CW] collect: return: 654.56826, steps: 1000.00000, total_steps: 261000.00000
[CW] train: qf1_loss: 23.40477, qf2_loss: 23.27443, policy_loss: -244.89356, policy_entropy: -1.01946, alpha: 0.33973, time: 67.93723
[CW] ---------------------------
[CW] ---- Iteration:   256 ----
[CW] collect: return: 675.00789, steps: 1000.00000, total_steps: 262000.00000
[CW] train: qf1_loss: 27.00091, qf2_loss: 26.87920, policy_loss: -246.51383, policy_entropy: -1.01156, alpha: 0.34354, time: 68.11174
[CW] ---------------------------
[CW] ---- Iteration:   257 ----
[CW] collect: return: 514.63998, steps: 1000.00000, total_steps: 263000.00000
[CW] train: qf1_loss: 37.57517, qf2_loss: 36.96273, policy_loss: -247.05543, policy_entropy: -0.99023, alpha: 0.34357, time: 68.14739
[CW] ---------------------------
[CW] ---- Iteration:   258 ----
[CW] collect: return: 555.32244, steps: 1000.00000, total_steps: 264000.00000
[CW] train: qf1_loss: 26.31616, qf2_loss: 26.11645, policy_loss: -247.80435, policy_entropy: -1.01419, alpha: 0.34267, time: 68.23044
[CW] ---------------------------
[CW] ---- Iteration:   259 ----
[CW] collect: return: 605.68889, steps: 1000.00000, total_steps: 265000.00000
[CW] train: qf1_loss: 23.86916, qf2_loss: 23.79016, policy_loss: -249.57911, policy_entropy: -1.02039, alpha: 0.34716, time: 68.14194
[CW] ---------------------------
[CW] ---- Iteration:   260 ----
[CW] collect: return: 665.11658, steps: 1000.00000, total_steps: 266000.00000
[CW] train: qf1_loss: 24.13057, qf2_loss: 24.07453, policy_loss: -250.01750, policy_entropy: -1.01356, alpha: 0.35033, time: 68.22013
[CW] eval: return: 662.71661, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   261 ----
[CW] collect: return: 821.59235, steps: 1000.00000, total_steps: 267000.00000
[CW] train: qf1_loss: 28.74360, qf2_loss: 28.40152, policy_loss: -250.64206, policy_entropy: -1.00401, alpha: 0.35227, time: 68.09965
[CW] ---------------------------
[CW] ---- Iteration:   262 ----
[CW] collect: return: 573.23476, steps: 1000.00000, total_steps: 268000.00000
[CW] train: qf1_loss: 29.18875, qf2_loss: 29.39617, policy_loss: -251.60787, policy_entropy: -1.00686, alpha: 0.35367, time: 67.99820
[CW] ---------------------------
[CW] ---- Iteration:   263 ----
[CW] collect: return: 614.03409, steps: 1000.00000, total_steps: 269000.00000
[CW] train: qf1_loss: 25.43659, qf2_loss: 25.20382, policy_loss: -252.74851, policy_entropy: -1.00786, alpha: 0.35452, time: 67.94728
[CW] ---------------------------
[CW] ---- Iteration:   264 ----
[CW] collect: return: 604.61777, steps: 1000.00000, total_steps: 270000.00000
[CW] train: qf1_loss: 25.04610, qf2_loss: 24.62682, policy_loss: -253.63724, policy_entropy: -1.01061, alpha: 0.35704, time: 67.93226
[CW] ---------------------------
[CW] ---- Iteration:   265 ----
[CW] collect: return: 818.57352, steps: 1000.00000, total_steps: 271000.00000
[CW] train: qf1_loss: 24.79290, qf2_loss: 24.64556, policy_loss: -254.76918, policy_entropy: -1.00531, alpha: 0.35925, time: 68.17904
[CW] ---------------------------
[CW] ---- Iteration:   266 ----
[CW] collect: return: 497.31718, steps: 1000.00000, total_steps: 272000.00000
[CW] train: qf1_loss: 25.75089, qf2_loss: 25.83961, policy_loss: -255.71268, policy_entropy: -1.01227, alpha: 0.36021, time: 68.32171
[CW] ---------------------------
[CW] ---- Iteration:   267 ----
[CW] collect: return: 573.94512, steps: 1000.00000, total_steps: 273000.00000
[CW] train: qf1_loss: 29.68826, qf2_loss: 29.29764, policy_loss: -256.52274, policy_entropy: -1.01505, alpha: 0.36333, time: 68.33342
[CW] ---------------------------
[CW] ---- Iteration:   268 ----
[CW] collect: return: 587.13591, steps: 1000.00000, total_steps: 274000.00000
[CW] train: qf1_loss: 26.51819, qf2_loss: 26.30067, policy_loss: -257.68199, policy_entropy: -1.00806, alpha: 0.36618, time: 69.38289
[CW] ---------------------------
[CW] ---- Iteration:   269 ----
[CW] collect: return: 472.61717, steps: 1000.00000, total_steps: 275000.00000
[CW] train: qf1_loss: 26.70050, qf2_loss: 26.47299, policy_loss: -258.11112, policy_entropy: -1.00399, alpha: 0.36763, time: 68.27805
[CW] ---------------------------
[CW] ---- Iteration:   270 ----
[CW] collect: return: 575.29076, steps: 1000.00000, total_steps: 276000.00000
[CW] train: qf1_loss: 27.87650, qf2_loss: 27.61052, policy_loss: -259.24729, policy_entropy: -0.99620, alpha: 0.36768, time: 68.11764
[CW] ---------------------------
[CW] ---- Iteration:   271 ----
[CW] collect: return: 520.25881, steps: 1000.00000, total_steps: 277000.00000
[CW] train: qf1_loss: 29.81153, qf2_loss: 29.46450, policy_loss: -260.13891, policy_entropy: -0.99880, alpha: 0.36779, time: 68.42254
[CW] ---------------------------
[CW] ---- Iteration:   272 ----
[CW] collect: return: 833.58010, steps: 1000.00000, total_steps: 278000.00000
[CW] train: qf1_loss: 32.58687, qf2_loss: 32.41477, policy_loss: -260.97210, policy_entropy: -1.00290, alpha: 0.36703, time: 68.18029
[CW] ---------------------------
[CW] ---- Iteration:   273 ----
[CW] collect: return: 503.49236, steps: 1000.00000, total_steps: 279000.00000
[CW] train: qf1_loss: 28.93188, qf2_loss: 28.82822, policy_loss: -262.21785, policy_entropy: -1.00394, alpha: 0.36791, time: 70.95816
[CW] ---------------------------
[CW] ---- Iteration:   274 ----
[CW] collect: return: 586.87022, steps: 1000.00000, total_steps: 280000.00000
[CW] train: qf1_loss: 28.68908, qf2_loss: 28.57421, policy_loss: -262.42310, policy_entropy: -1.01347, alpha: 0.36935, time: 68.23584
[CW] ---------------------------
[CW] ---- Iteration:   275 ----
[CW] collect: return: 527.25676, steps: 1000.00000, total_steps: 281000.00000
[CW] train: qf1_loss: 55.54389, qf2_loss: 55.56010, policy_loss: -263.20480, policy_entropy: -0.98839, alpha: 0.37051, time: 68.22800
[CW] ---------------------------
[CW] ---- Iteration:   276 ----
[CW] collect: return: 658.72090, steps: 1000.00000, total_steps: 282000.00000
[CW] train: qf1_loss: 27.25052, qf2_loss: 27.03310, policy_loss: -265.20916, policy_entropy: -1.00891, alpha: 0.36902, time: 68.34171
[CW] ---------------------------
[CW] ---- Iteration:   277 ----
[CW] collect: return: 515.84954, steps: 1000.00000, total_steps: 283000.00000
[CW] train: qf1_loss: 25.51724, qf2_loss: 25.24015, policy_loss: -265.93333, policy_entropy: -1.01527, alpha: 0.37230, time: 68.06745
[CW] ---------------------------
[CW] ---- Iteration:   278 ----
[CW] collect: return: 598.05539, steps: 1000.00000, total_steps: 284000.00000
[CW] train: qf1_loss: 27.82848, qf2_loss: 27.82606, policy_loss: -267.00053, policy_entropy: -1.02032, alpha: 0.37596, time: 71.94535
[CW] ---------------------------
[CW] ---- Iteration:   279 ----
[CW] collect: return: 600.40778, steps: 1000.00000, total_steps: 285000.00000
[CW] train: qf1_loss: 26.90962, qf2_loss: 27.08899, policy_loss: -268.81716, policy_entropy: -1.02247, alpha: 0.38045, time: 68.16145
[CW] ---------------------------
[CW] ---- Iteration:   280 ----
[CW] collect: return: 614.90907, steps: 1000.00000, total_steps: 286000.00000
[CW] train: qf1_loss: 29.00700, qf2_loss: 28.82108, policy_loss: -268.63294, policy_entropy: -1.01789, alpha: 0.38537, time: 68.17929
[CW] eval: return: 602.54616, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   281 ----
[CW] collect: return: 556.07305, steps: 1000.00000, total_steps: 287000.00000
[CW] train: qf1_loss: 27.76065, qf2_loss: 27.53818, policy_loss: -269.13504, policy_entropy: -1.01581, alpha: 0.38957, time: 68.29486
[CW] ---------------------------
[CW] ---- Iteration:   282 ----
[CW] collect: return: 575.74942, steps: 1000.00000, total_steps: 288000.00000
[CW] train: qf1_loss: 27.91142, qf2_loss: 27.67493, policy_loss: -269.08501, policy_entropy: -1.00226, alpha: 0.39169, time: 68.21434
[CW] ---------------------------
[CW] ---- Iteration:   283 ----
[CW] collect: return: 591.51917, steps: 1000.00000, total_steps: 289000.00000
[CW] train: qf1_loss: 31.01785, qf2_loss: 30.90014, policy_loss: -270.89364, policy_entropy: -0.99994, alpha: 0.39181, time: 68.25827
[CW] ---------------------------
[CW] ---- Iteration:   284 ----
[CW] collect: return: 606.06729, steps: 1000.00000, total_steps: 290000.00000
[CW] train: qf1_loss: 42.35284, qf2_loss: 42.01889, policy_loss: -271.84363, policy_entropy: -0.99233, alpha: 0.39181, time: 68.07609
[CW] ---------------------------
[CW] ---- Iteration:   285 ----
[CW] collect: return: 830.63243, steps: 1000.00000, total_steps: 291000.00000
[CW] train: qf1_loss: 106.00788, qf2_loss: 105.27089, policy_loss: -270.52420, policy_entropy: -0.96577, alpha: 0.38691, time: 67.93637
[CW] ---------------------------
[CW] ---- Iteration:   286 ----
[CW] collect: return: 591.78744, steps: 1000.00000, total_steps: 292000.00000
[CW] train: qf1_loss: 34.35135, qf2_loss: 34.67656, policy_loss: -273.78469, policy_entropy: -1.01307, alpha: 0.38363, time: 67.89015
[CW] ---------------------------
[CW] ---- Iteration:   287 ----
[CW] collect: return: 593.38108, steps: 1000.00000, total_steps: 293000.00000
[CW] train: qf1_loss: 28.81424, qf2_loss: 28.42075, policy_loss: -274.65949, policy_entropy: -1.01604, alpha: 0.38678, time: 67.89991
[CW] ---------------------------
[CW] ---- Iteration:   288 ----
[CW] collect: return: 842.41049, steps: 1000.00000, total_steps: 294000.00000
[CW] train: qf1_loss: 27.82169, qf2_loss: 27.67501, policy_loss: -275.33819, policy_entropy: -1.00145, alpha: 0.38887, time: 67.91133
[CW] ---------------------------
[CW] ---- Iteration:   289 ----
[CW] collect: return: 757.09671, steps: 1000.00000, total_steps: 295000.00000
[CW] train: qf1_loss: 28.57904, qf2_loss: 28.50653, policy_loss: -276.74565, policy_entropy: -1.02473, alpha: 0.39170, time: 68.09809
[CW] ---------------------------
[CW] ---- Iteration:   290 ----
[CW] collect: return: 836.26184, steps: 1000.00000, total_steps: 296000.00000
[CW] train: qf1_loss: 29.19214, qf2_loss: 28.80685, policy_loss: -275.99740, policy_entropy: -1.01094, alpha: 0.39668, time: 68.01702
[CW] ---------------------------
[CW] ---- Iteration:   291 ----
[CW] collect: return: 581.49516, steps: 1000.00000, total_steps: 297000.00000
[CW] train: qf1_loss: 28.96760, qf2_loss: 28.75434, policy_loss: -277.47482, policy_entropy: -1.01209, alpha: 0.39835, time: 67.89749
[CW] ---------------------------
[CW] ---- Iteration:   292 ----
[CW] collect: return: 454.61529, steps: 1000.00000, total_steps: 298000.00000
[CW] train: qf1_loss: 89.84368, qf2_loss: 90.67069, policy_loss: -278.59733, policy_entropy: -0.98611, alpha: 0.40069, time: 67.86730
[CW] ---------------------------
[CW] ---- Iteration:   293 ----
[CW] collect: return: 485.88218, steps: 1000.00000, total_steps: 299000.00000
[CW] train: qf1_loss: 59.11320, qf2_loss: 58.11255, policy_loss: -280.21960, policy_entropy: -0.98268, alpha: 0.39426, time: 67.92088
[CW] ---------------------------
[CW] ---- Iteration:   294 ----
[CW] collect: return: 835.37276, steps: 1000.00000, total_steps: 300000.00000
[CW] train: qf1_loss: 32.14653, qf2_loss: 31.70994, policy_loss: -279.86927, policy_entropy: -1.01315, alpha: 0.39372, time: 67.92371
[CW] ---------------------------
[CW] ---- Iteration:   295 ----
[CW] collect: return: 839.48948, steps: 1000.00000, total_steps: 301000.00000
[CW] train: qf1_loss: 30.29180, qf2_loss: 29.97279, policy_loss: -282.66119, policy_entropy: -1.02587, alpha: 0.39923, time: 67.89712
[CW] ---------------------------
[CW] ---- Iteration:   296 ----
[CW] collect: return: 684.01992, steps: 1000.00000, total_steps: 302000.00000
[CW] train: qf1_loss: 30.14586, qf2_loss: 29.94366, policy_loss: -283.00615, policy_entropy: -1.01603, alpha: 0.40388, time: 67.82137
[CW] ---------------------------
[CW] ---- Iteration:   297 ----
[CW] collect: return: 840.02837, steps: 1000.00000, total_steps: 303000.00000
[CW] train: qf1_loss: 30.59688, qf2_loss: 30.33653, policy_loss: -284.33594, policy_entropy: -1.02451, alpha: 0.40819, time: 67.84712
[CW] ---------------------------
[CW] ---- Iteration:   298 ----
[CW] collect: return: 845.93659, steps: 1000.00000, total_steps: 304000.00000
[CW] train: qf1_loss: 30.12478, qf2_loss: 30.00725, policy_loss: -284.23784, policy_entropy: -1.00204, alpha: 0.41225, time: 67.88471
[CW] ---------------------------
[CW] ---- Iteration:   299 ----
[CW] collect: return: 841.18920, steps: 1000.00000, total_steps: 305000.00000
[CW] train: qf1_loss: 30.68613, qf2_loss: 30.43835, policy_loss: -287.21322, policy_entropy: -1.01352, alpha: 0.41293, time: 67.84947
[CW] ---------------------------
[CW] ---- Iteration:   300 ----
[CW] collect: return: 464.88821, steps: 1000.00000, total_steps: 306000.00000
[CW] train: qf1_loss: 34.16972, qf2_loss: 34.07031, policy_loss: -286.47535, policy_entropy: -1.01125, alpha: 0.41605, time: 68.20462
[CW] eval: return: 593.55825, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   301 ----
[CW] collect: return: 696.03139, steps: 1000.00000, total_steps: 307000.00000
[CW] train: qf1_loss: 33.14983, qf2_loss: 32.93085, policy_loss: -289.48348, policy_entropy: -1.00423, alpha: 0.41766, time: 67.87964
[CW] ---------------------------
[CW] ---- Iteration:   302 ----
[CW] collect: return: 845.73940, steps: 1000.00000, total_steps: 308000.00000
[CW] train: qf1_loss: 32.62252, qf2_loss: 32.24504, policy_loss: -288.31891, policy_entropy: -1.00241, alpha: 0.41845, time: 67.94986
[CW] ---------------------------
[CW] ---- Iteration:   303 ----
[CW] collect: return: 515.94047, steps: 1000.00000, total_steps: 309000.00000
[CW] train: qf1_loss: 32.07209, qf2_loss: 31.98247, policy_loss: -289.87614, policy_entropy: -1.01485, alpha: 0.42077, time: 67.94109
[CW] ---------------------------
[CW] ---- Iteration:   304 ----
[CW] collect: return: 678.89156, steps: 1000.00000, total_steps: 310000.00000
[CW] train: qf1_loss: 36.07549, qf2_loss: 36.15159, policy_loss: -289.86354, policy_entropy: -1.01703, alpha: 0.42424, time: 67.91893
[CW] ---------------------------
[CW] ---- Iteration:   305 ----
[CW] collect: return: 565.84106, steps: 1000.00000, total_steps: 311000.00000
[CW] train: qf1_loss: 35.16694, qf2_loss: 34.92211, policy_loss: -292.23321, policy_entropy: -1.00408, alpha: 0.42761, time: 67.80807
[CW] ---------------------------
[CW] ---- Iteration:   306 ----
[CW] collect: return: 730.29666, steps: 1000.00000, total_steps: 312000.00000
[CW] train: qf1_loss: 50.02346, qf2_loss: 49.88488, policy_loss: -293.64517, policy_entropy: -0.99485, alpha: 0.42761, time: 67.90816
[CW] ---------------------------
[CW] ---- Iteration:   307 ----
[CW] collect: return: 595.12913, steps: 1000.00000, total_steps: 313000.00000
[CW] train: qf1_loss: 37.95676, qf2_loss: 38.07747, policy_loss: -293.82115, policy_entropy: -1.00555, alpha: 0.42685, time: 68.01016
[CW] ---------------------------
[CW] ---- Iteration:   308 ----
[CW] collect: return: 840.03717, steps: 1000.00000, total_steps: 314000.00000
[CW] train: qf1_loss: 33.59541, qf2_loss: 33.23308, policy_loss: -294.80946, policy_entropy: -1.01194, alpha: 0.42875, time: 68.21485
[CW] ---------------------------
[CW] ---- Iteration:   309 ----
[CW] collect: return: 808.34027, steps: 1000.00000, total_steps: 315000.00000
[CW] train: qf1_loss: 38.25632, qf2_loss: 38.34349, policy_loss: -296.05361, policy_entropy: -1.01964, alpha: 0.43409, time: 68.00479
[CW] ---------------------------
[CW] ---- Iteration:   310 ----
[CW] collect: return: 606.36286, steps: 1000.00000, total_steps: 316000.00000
[CW] train: qf1_loss: 38.31815, qf2_loss: 37.92131, policy_loss: -296.58755, policy_entropy: -1.00476, alpha: 0.43598, time: 67.78240
[CW] ---------------------------
[CW] ---- Iteration:   311 ----
[CW] collect: return: 839.88838, steps: 1000.00000, total_steps: 317000.00000
[CW] train: qf1_loss: 34.89157, qf2_loss: 34.73041, policy_loss: -298.12257, policy_entropy: -1.00184, alpha: 0.43734, time: 67.83975
[CW] ---------------------------
[CW] ---- Iteration:   312 ----
[CW] collect: return: 845.67672, steps: 1000.00000, total_steps: 318000.00000
[CW] train: qf1_loss: 36.63662, qf2_loss: 36.48876, policy_loss: -299.97951, policy_entropy: -1.00828, alpha: 0.43826, time: 67.82813
[CW] ---------------------------
[CW] ---- Iteration:   313 ----
[CW] collect: return: 681.89714, steps: 1000.00000, total_steps: 319000.00000
[CW] train: qf1_loss: 42.35116, qf2_loss: 42.34975, policy_loss: -299.27704, policy_entropy: -1.00451, alpha: 0.43956, time: 67.88596
[CW] ---------------------------
[CW] ---- Iteration:   314 ----
[CW] collect: return: 845.67951, steps: 1000.00000, total_steps: 320000.00000
[CW] train: qf1_loss: 34.82195, qf2_loss: 34.42750, policy_loss: -302.15130, policy_entropy: -1.01237, alpha: 0.44310, time: 68.08684
[CW] ---------------------------
[CW] ---- Iteration:   315 ----
[CW] collect: return: 552.39453, steps: 1000.00000, total_steps: 321000.00000
[CW] train: qf1_loss: 36.28262, qf2_loss: 35.85831, policy_loss: -303.45161, policy_entropy: -1.01192, alpha: 0.44507, time: 68.12118
[CW] ---------------------------
[CW] ---- Iteration:   316 ----
[CW] collect: return: 667.37860, steps: 1000.00000, total_steps: 322000.00000
[CW] train: qf1_loss: 143.21358, qf2_loss: 144.21497, policy_loss: -300.44920, policy_entropy: -0.93914, alpha: 0.44309, time: 68.01362
[CW] ---------------------------
[CW] ---- Iteration:   317 ----
[CW] collect: return: 524.14841, steps: 1000.00000, total_steps: 323000.00000
[CW] train: qf1_loss: 72.22890, qf2_loss: 71.43418, policy_loss: -304.45935, policy_entropy: -0.97891, alpha: 0.42965, time: 67.68567
[CW] ---------------------------
[CW] ---- Iteration:   318 ----
[CW] collect: return: 670.82334, steps: 1000.00000, total_steps: 324000.00000
[CW] train: qf1_loss: 39.88165, qf2_loss: 39.73283, policy_loss: -306.12486, policy_entropy: -1.01364, alpha: 0.42831, time: 67.70985
[CW] ---------------------------
[CW] ---- Iteration:   319 ----
[CW] collect: return: 836.10381, steps: 1000.00000, total_steps: 325000.00000
[CW] train: qf1_loss: 36.16214, qf2_loss: 35.88487, policy_loss: -306.04576, policy_entropy: -1.02187, alpha: 0.43232, time: 67.74543
[CW] ---------------------------
[CW] ---- Iteration:   320 ----
[CW] collect: return: 845.53746, steps: 1000.00000, total_steps: 326000.00000
[CW] train: qf1_loss: 36.62314, qf2_loss: 36.35564, policy_loss: -308.35864, policy_entropy: -1.02330, alpha: 0.43794, time: 67.68967
[CW] eval: return: 811.04442, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   321 ----
[CW] collect: return: 844.37916, steps: 1000.00000, total_steps: 327000.00000
[CW] train: qf1_loss: 40.44424, qf2_loss: 39.73893, policy_loss: -307.29530, policy_entropy: -1.02832, alpha: 0.44285, time: 67.72822
[CW] ---------------------------
[CW] ---- Iteration:   322 ----
[CW] collect: return: 511.92527, steps: 1000.00000, total_steps: 328000.00000
[CW] train: qf1_loss: 37.78849, qf2_loss: 37.14825, policy_loss: -309.20190, policy_entropy: -1.01216, alpha: 0.44934, time: 67.61273
[CW] ---------------------------
[CW] ---- Iteration:   323 ----
[CW] collect: return: 844.32945, steps: 1000.00000, total_steps: 329000.00000
[CW] train: qf1_loss: 36.99275, qf2_loss: 36.73401, policy_loss: -310.79444, policy_entropy: -1.00764, alpha: 0.45113, time: 67.74867
[CW] ---------------------------
[CW] ---- Iteration:   324 ----
[CW] collect: return: 845.37008, steps: 1000.00000, total_steps: 330000.00000
[CW] train: qf1_loss: 39.84242, qf2_loss: 39.58460, policy_loss: -310.76927, policy_entropy: -1.02170, alpha: 0.45548, time: 67.87994
[CW] ---------------------------
[CW] ---- Iteration:   325 ----
[CW] collect: return: 837.09826, steps: 1000.00000, total_steps: 331000.00000
[CW] train: qf1_loss: 41.45494, qf2_loss: 41.10079, policy_loss: -313.47554, policy_entropy: -1.01860, alpha: 0.45990, time: 67.77945
[CW] ---------------------------
[CW] ---- Iteration:   326 ----
[CW] collect: return: 842.37467, steps: 1000.00000, total_steps: 332000.00000
[CW] train: qf1_loss: 42.70478, qf2_loss: 42.49415, policy_loss: -314.51798, policy_entropy: -1.01415, alpha: 0.46404, time: 67.92572
[CW] ---------------------------
[CW] ---- Iteration:   327 ----
[CW] collect: return: 843.55394, steps: 1000.00000, total_steps: 333000.00000
[CW] train: qf1_loss: 40.53209, qf2_loss: 39.95978, policy_loss: -314.69788, policy_entropy: -1.00894, alpha: 0.46821, time: 67.76789
[CW] ---------------------------
[CW] ---- Iteration:   328 ----
[CW] collect: return: 678.56415, steps: 1000.00000, total_steps: 334000.00000
[CW] train: qf1_loss: 39.85121, qf2_loss: 39.90867, policy_loss: -314.91355, policy_entropy: -1.00279, alpha: 0.46949, time: 67.86689
[CW] ---------------------------
[CW] ---- Iteration:   329 ----
[CW] collect: return: 836.11425, steps: 1000.00000, total_steps: 335000.00000
[CW] train: qf1_loss: 44.78411, qf2_loss: 44.36364, policy_loss: -317.18838, policy_entropy: -1.01416, alpha: 0.47261, time: 68.25330
[CW] ---------------------------
[CW] ---- Iteration:   330 ----
[CW] collect: return: 835.38687, steps: 1000.00000, total_steps: 336000.00000
[CW] train: qf1_loss: 43.18001, qf2_loss: 43.31005, policy_loss: -318.46559, policy_entropy: -1.01672, alpha: 0.47475, time: 68.45520
[CW] ---------------------------
[CW] ---- Iteration:   331 ----
[CW] collect: return: 841.73959, steps: 1000.00000, total_steps: 337000.00000
[CW] train: qf1_loss: 41.72441, qf2_loss: 41.34688, policy_loss: -319.02130, policy_entropy: -1.00252, alpha: 0.47817, time: 70.13563
[CW] ---------------------------
[CW] ---- Iteration:   332 ----
[CW] collect: return: 841.66539, steps: 1000.00000, total_steps: 338000.00000
[CW] train: qf1_loss: 56.81435, qf2_loss: 56.22583, policy_loss: -319.42333, policy_entropy: -0.99535, alpha: 0.47823, time: 67.73188
[CW] ---------------------------
[CW] ---- Iteration:   333 ----
[CW] collect: return: 752.60363, steps: 1000.00000, total_steps: 339000.00000
[CW] train: qf1_loss: 51.83223, qf2_loss: 51.61130, policy_loss: -320.21491, policy_entropy: -0.98826, alpha: 0.47583, time: 67.52527
[CW] ---------------------------
[CW] ---- Iteration:   334 ----
[CW] collect: return: 669.30701, steps: 1000.00000, total_steps: 340000.00000
[CW] train: qf1_loss: 45.03328, qf2_loss: 44.99959, policy_loss: -323.34458, policy_entropy: -1.01138, alpha: 0.47574, time: 71.52174
[CW] ---------------------------
[CW] ---- Iteration:   335 ----
[CW] collect: return: 838.71419, steps: 1000.00000, total_steps: 341000.00000
[CW] train: qf1_loss: 39.85043, qf2_loss: 39.79281, policy_loss: -321.42790, policy_entropy: -1.01236, alpha: 0.47876, time: 68.10201
[CW] ---------------------------
[CW] ---- Iteration:   336 ----
[CW] collect: return: 515.88751, steps: 1000.00000, total_steps: 342000.00000
[CW] train: qf1_loss: 47.59170, qf2_loss: 47.37119, policy_loss: -323.92220, policy_entropy: -1.00357, alpha: 0.48127, time: 67.93550
[CW] ---------------------------
[CW] ---- Iteration:   337 ----
[CW] collect: return: 839.58419, steps: 1000.00000, total_steps: 343000.00000
[CW] train: qf1_loss: 47.71706, qf2_loss: 47.43041, policy_loss: -324.32003, policy_entropy: -1.00357, alpha: 0.48224, time: 67.82009
[CW] ---------------------------
[CW] ---- Iteration:   338 ----
[CW] collect: return: 460.45230, steps: 1000.00000, total_steps: 344000.00000
[CW] train: qf1_loss: 63.08970, qf2_loss: 62.44295, policy_loss: -323.68996, policy_entropy: -0.98820, alpha: 0.48118, time: 68.11032
[CW] ---------------------------
[CW] ---- Iteration:   339 ----
[CW] collect: return: 447.42084, steps: 1000.00000, total_steps: 345000.00000
[CW] train: qf1_loss: 54.22696, qf2_loss: 54.81465, policy_loss: -327.56799, policy_entropy: -0.98702, alpha: 0.47724, time: 68.21925
[CW] ---------------------------
[CW] ---- Iteration:   340 ----
[CW] collect: return: 834.81484, steps: 1000.00000, total_steps: 346000.00000
[CW] train: qf1_loss: 54.43698, qf2_loss: 53.54230, policy_loss: -329.20901, policy_entropy: -1.00461, alpha: 0.47620, time: 67.73961
[CW] eval: return: 670.16623, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   341 ----
[CW] collect: return: 671.84510, steps: 1000.00000, total_steps: 347000.00000
[CW] train: qf1_loss: 77.17604, qf2_loss: 77.25534, policy_loss: -330.29905, policy_entropy: -0.99770, alpha: 0.47662, time: 67.55488
[CW] ---------------------------
[CW] ---- Iteration:   342 ----
[CW] collect: return: 743.58420, steps: 1000.00000, total_steps: 348000.00000
[CW] train: qf1_loss: 52.61592, qf2_loss: 52.30406, policy_loss: -330.74281, policy_entropy: -1.00894, alpha: 0.47820, time: 67.50293
[CW] ---------------------------
[CW] ---- Iteration:   343 ----
[CW] collect: return: 827.71001, steps: 1000.00000, total_steps: 349000.00000
[CW] train: qf1_loss: 45.91116, qf2_loss: 45.38819, policy_loss: -331.56333, policy_entropy: -1.00910, alpha: 0.48046, time: 67.60780
[CW] ---------------------------
[CW] ---- Iteration:   344 ----
[CW] collect: return: 825.66639, steps: 1000.00000, total_steps: 350000.00000
[CW] train: qf1_loss: 57.75105, qf2_loss: 57.80204, policy_loss: -331.85423, policy_entropy: -0.99909, alpha: 0.48055, time: 67.66835
[CW] ---------------------------
[CW] ---- Iteration:   345 ----
[CW] collect: return: 760.53291, steps: 1000.00000, total_steps: 351000.00000
[CW] train: qf1_loss: 58.98557, qf2_loss: 57.71992, policy_loss: -332.96600, policy_entropy: -1.00327, alpha: 0.48229, time: 67.85537
[CW] ---------------------------
[CW] ---- Iteration:   346 ----
[CW] collect: return: 580.01000, steps: 1000.00000, total_steps: 352000.00000
[CW] train: qf1_loss: 52.44502, qf2_loss: 52.45760, policy_loss: -333.65111, policy_entropy: -1.00784, alpha: 0.48390, time: 68.01471
[CW] ---------------------------
[CW] ---- Iteration:   347 ----
[CW] collect: return: 639.77407, steps: 1000.00000, total_steps: 353000.00000
[CW] train: qf1_loss: 54.68923, qf2_loss: 54.56879, policy_loss: -333.82779, policy_entropy: -1.02334, alpha: 0.48742, time: 67.98782
[CW] ---------------------------
[CW] ---- Iteration:   348 ----
[CW] collect: return: 839.67997, steps: 1000.00000, total_steps: 354000.00000
[CW] train: qf1_loss: 53.48610, qf2_loss: 53.35977, policy_loss: -338.28395, policy_entropy: -1.01573, alpha: 0.49222, time: 67.64651
[CW] ---------------------------
[CW] ---- Iteration:   349 ----
[CW] collect: return: 585.82778, steps: 1000.00000, total_steps: 355000.00000
[CW] train: qf1_loss: 59.39046, qf2_loss: 58.46505, policy_loss: -337.47423, policy_entropy: -1.00585, alpha: 0.49628, time: 67.73335
[CW] ---------------------------
[CW] ---- Iteration:   350 ----
[CW] collect: return: 523.04799, steps: 1000.00000, total_steps: 356000.00000
[CW] train: qf1_loss: 68.68394, qf2_loss: 68.55631, policy_loss: -338.45676, policy_entropy: -0.99844, alpha: 0.49665, time: 67.66920
[CW] ---------------------------
[CW] ---- Iteration:   351 ----
[CW] collect: return: 834.72246, steps: 1000.00000, total_steps: 357000.00000
[CW] train: qf1_loss: 58.53115, qf2_loss: 58.01458, policy_loss: -336.21201, policy_entropy: -1.00717, alpha: 0.49623, time: 67.77892
[CW] ---------------------------
[CW] ---- Iteration:   352 ----
[CW] collect: return: 754.36985, steps: 1000.00000, total_steps: 358000.00000
[CW] train: qf1_loss: 54.35969, qf2_loss: 53.75691, policy_loss: -340.18388, policy_entropy: -1.00758, alpha: 0.49969, time: 67.73835
[CW] ---------------------------
[CW] ---- Iteration:   353 ----
[CW] collect: return: 842.35344, steps: 1000.00000, total_steps: 359000.00000
[CW] train: qf1_loss: 63.62666, qf2_loss: 63.86957, policy_loss: -341.17105, policy_entropy: -1.00601, alpha: 0.50209, time: 67.58738
[CW] ---------------------------
[CW] ---- Iteration:   354 ----
[CW] collect: return: 506.10414, steps: 1000.00000, total_steps: 360000.00000
[CW] train: qf1_loss: 60.69314, qf2_loss: 60.48184, policy_loss: -341.74629, policy_entropy: -1.01155, alpha: 0.50376, time: 67.41372
[CW] ---------------------------
[CW] ---- Iteration:   355 ----
[CW] collect: return: 842.54105, steps: 1000.00000, total_steps: 361000.00000
[CW] train: qf1_loss: 49.29920, qf2_loss: 49.08504, policy_loss: -341.95194, policy_entropy: -1.00866, alpha: 0.50703, time: 67.64803
[CW] ---------------------------
[CW] ---- Iteration:   356 ----
[CW] collect: return: 842.56993, steps: 1000.00000, total_steps: 362000.00000
[CW] train: qf1_loss: 52.69778, qf2_loss: 52.04589, policy_loss: -344.49113, policy_entropy: -1.01401, alpha: 0.51166, time: 67.61980
[CW] ---------------------------
[CW] ---- Iteration:   357 ----
[CW] collect: return: 839.04303, steps: 1000.00000, total_steps: 363000.00000
[CW] train: qf1_loss: 56.50234, qf2_loss: 55.73044, policy_loss: -345.85264, policy_entropy: -1.01498, alpha: 0.51493, time: 67.54042
[CW] ---------------------------
[CW] ---- Iteration:   358 ----
[CW] collect: return: 837.28165, steps: 1000.00000, total_steps: 364000.00000
[CW] train: qf1_loss: 54.86194, qf2_loss: 54.43026, policy_loss: -346.42980, policy_entropy: -1.01624, alpha: 0.51843, time: 67.46968
[CW] ---------------------------
[CW] ---- Iteration:   359 ----
[CW] collect: return: 842.97812, steps: 1000.00000, total_steps: 365000.00000
[CW] train: qf1_loss: 79.09792, qf2_loss: 79.13020, policy_loss: -345.24256, policy_entropy: -0.98644, alpha: 0.52098, time: 67.65179
[CW] ---------------------------
[CW] ---- Iteration:   360 ----
[CW] collect: return: 562.47557, steps: 1000.00000, total_steps: 366000.00000
[CW] train: qf1_loss: 116.04085, qf2_loss: 115.77967, policy_loss: -347.75379, policy_entropy: -0.98463, alpha: 0.51606, time: 67.93457
[CW] eval: return: 786.18681, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   361 ----
[CW] collect: return: 834.33044, steps: 1000.00000, total_steps: 367000.00000
[CW] train: qf1_loss: 57.01966, qf2_loss: 56.54533, policy_loss: -350.09739, policy_entropy: -1.02011, alpha: 0.51571, time: 67.78623
[CW] ---------------------------
[CW] ---- Iteration:   362 ----
[CW] collect: return: 842.28957, steps: 1000.00000, total_steps: 368000.00000
[CW] train: qf1_loss: 57.29912, qf2_loss: 57.01053, policy_loss: -351.12520, policy_entropy: -1.00000, alpha: 0.51930, time: 67.74444
[CW] ---------------------------
[CW] ---- Iteration:   363 ----
[CW] collect: return: 840.51064, steps: 1000.00000, total_steps: 369000.00000
[CW] train: qf1_loss: 57.99183, qf2_loss: 57.29086, policy_loss: -348.30064, policy_entropy: -1.00525, alpha: 0.52028, time: 67.70820
[CW] ---------------------------
[CW] ---- Iteration:   364 ----
[CW] collect: return: 840.99878, steps: 1000.00000, total_steps: 370000.00000
[CW] train: qf1_loss: 57.00154, qf2_loss: 56.25124, policy_loss: -351.10199, policy_entropy: -1.00532, alpha: 0.52180, time: 67.75836
[CW] ---------------------------
[CW] ---- Iteration:   365 ----
[CW] collect: return: 836.99755, steps: 1000.00000, total_steps: 371000.00000
[CW] train: qf1_loss: 60.51363, qf2_loss: 60.03816, policy_loss: -353.81157, policy_entropy: -0.99680, alpha: 0.52276, time: 67.73881
[CW] ---------------------------
[CW] ---- Iteration:   366 ----
[CW] collect: return: 671.99706, steps: 1000.00000, total_steps: 372000.00000
[CW] train: qf1_loss: 65.28254, qf2_loss: 65.04465, policy_loss: -353.59244, policy_entropy: -1.01243, alpha: 0.52353, time: 67.87406
[CW] ---------------------------
[CW] ---- Iteration:   367 ----
[CW] collect: return: 835.42900, steps: 1000.00000, total_steps: 373000.00000
[CW] train: qf1_loss: 70.65272, qf2_loss: 70.65659, policy_loss: -354.94819, policy_entropy: -1.00342, alpha: 0.52617, time: 67.77777
[CW] ---------------------------
[CW] ---- Iteration:   368 ----
[CW] collect: return: 833.80059, steps: 1000.00000, total_steps: 374000.00000
[CW] train: qf1_loss: 62.55634, qf2_loss: 61.97140, policy_loss: -355.41131, policy_entropy: -1.01047, alpha: 0.52810, time: 67.57986
[CW] ---------------------------
[CW] ---- Iteration:   369 ----
[CW] collect: return: 830.56401, steps: 1000.00000, total_steps: 375000.00000
[CW] train: qf1_loss: 65.28878, qf2_loss: 64.97454, policy_loss: -357.82601, policy_entropy: -1.01324, alpha: 0.53161, time: 67.56436
[CW] ---------------------------
[CW] ---- Iteration:   370 ----
[CW] collect: return: 832.37655, steps: 1000.00000, total_steps: 376000.00000
[CW] train: qf1_loss: 77.20662, qf2_loss: 76.31692, policy_loss: -358.60698, policy_entropy: -1.01051, alpha: 0.53460, time: 67.70366
[CW] ---------------------------
[CW] ---- Iteration:   371 ----
[CW] collect: return: 820.95638, steps: 1000.00000, total_steps: 377000.00000
[CW] train: qf1_loss: 67.08593, qf2_loss: 65.97972, policy_loss: -359.27785, policy_entropy: -1.00578, alpha: 0.53796, time: 67.86194
[CW] ---------------------------
[CW] ---- Iteration:   372 ----
[CW] collect: return: 842.38308, steps: 1000.00000, total_steps: 378000.00000
[CW] train: qf1_loss: 64.49427, qf2_loss: 64.21369, policy_loss: -359.98763, policy_entropy: -1.01300, alpha: 0.54170, time: 69.42759
[CW] ---------------------------
[CW] ---- Iteration:   373 ----
[CW] collect: return: 839.18790, steps: 1000.00000, total_steps: 379000.00000
[CW] train: qf1_loss: 65.05953, qf2_loss: 65.06721, policy_loss: -361.28256, policy_entropy: -0.99911, alpha: 0.54349, time: 67.84479
[CW] ---------------------------
[CW] ---- Iteration:   374 ----
[CW] collect: return: 840.79668, steps: 1000.00000, total_steps: 380000.00000
[CW] train: qf1_loss: 65.92532, qf2_loss: 65.68373, policy_loss: -361.58178, policy_entropy: -1.01142, alpha: 0.54404, time: 67.83616
[CW] ---------------------------
[CW] ---- Iteration:   375 ----
[CW] collect: return: 834.00545, steps: 1000.00000, total_steps: 381000.00000
[CW] train: qf1_loss: 73.81005, qf2_loss: 72.47480, policy_loss: -365.71847, policy_entropy: -1.01598, alpha: 0.54942, time: 67.89344
[CW] ---------------------------
[CW] ---- Iteration:   376 ----
[CW] collect: return: 836.97270, steps: 1000.00000, total_steps: 382000.00000
[CW] train: qf1_loss: 79.06506, qf2_loss: 79.59966, policy_loss: -363.31495, policy_entropy: -0.99381, alpha: 0.55123, time: 67.99404
[CW] ---------------------------
[CW] ---- Iteration:   377 ----
[CW] collect: return: 838.33443, steps: 1000.00000, total_steps: 383000.00000
[CW] train: qf1_loss: 75.26438, qf2_loss: 75.35694, policy_loss: -363.75096, policy_entropy: -0.99665, alpha: 0.54949, time: 68.16049
[CW] ---------------------------
[CW] ---- Iteration:   378 ----
[CW] collect: return: 848.18819, steps: 1000.00000, total_steps: 384000.00000
[CW] train: qf1_loss: 86.67414, qf2_loss: 86.73490, policy_loss: -365.37752, policy_entropy: -1.00929, alpha: 0.55038, time: 67.80963
[CW] ---------------------------
[CW] ---- Iteration:   379 ----
[CW] collect: return: 519.63486, steps: 1000.00000, total_steps: 385000.00000
[CW] train: qf1_loss: 71.12969, qf2_loss: 70.15050, policy_loss: -368.99420, policy_entropy: -1.00064, alpha: 0.55201, time: 67.67740
[CW] ---------------------------
[CW] ---- Iteration:   380 ----
[CW] collect: return: 842.19318, steps: 1000.00000, total_steps: 386000.00000
[CW] train: qf1_loss: 74.33098, qf2_loss: 74.64275, policy_loss: -366.81447, policy_entropy: -1.00191, alpha: 0.55315, time: 67.67793
[CW] eval: return: 831.26903, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   381 ----
[CW] collect: return: 840.70668, steps: 1000.00000, total_steps: 387000.00000
[CW] train: qf1_loss: 77.23209, qf2_loss: 76.33891, policy_loss: -369.69874, policy_entropy: -1.00361, alpha: 0.55314, time: 67.86615
[CW] ---------------------------
[CW] ---- Iteration:   382 ----
[CW] collect: return: 822.57019, steps: 1000.00000, total_steps: 388000.00000
[CW] train: qf1_loss: 77.07452, qf2_loss: 77.27203, policy_loss: -370.42159, policy_entropy: -1.01169, alpha: 0.55504, time: 67.91600
[CW] ---------------------------
[CW] ---- Iteration:   383 ----
[CW] collect: return: 820.36964, steps: 1000.00000, total_steps: 389000.00000
[CW] train: qf1_loss: 69.27374, qf2_loss: 68.70161, policy_loss: -371.78020, policy_entropy: -1.01576, alpha: 0.56064, time: 67.66842
[CW] ---------------------------
[CW] ---- Iteration:   384 ----
[CW] collect: return: 837.79104, steps: 1000.00000, total_steps: 390000.00000
[CW] train: qf1_loss: 82.44338, qf2_loss: 82.31863, policy_loss: -374.30960, policy_entropy: -1.01089, alpha: 0.56537, time: 67.64207
[CW] ---------------------------
[CW] ---- Iteration:   385 ----
[CW] collect: return: 402.65593, steps: 1000.00000, total_steps: 391000.00000
[CW] train: qf1_loss: 78.32889, qf2_loss: 78.49346, policy_loss: -374.19774, policy_entropy: -0.98538, alpha: 0.56455, time: 67.72401
[CW] ---------------------------
[CW] ---- Iteration:   386 ----
[CW] collect: return: 840.47189, steps: 1000.00000, total_steps: 392000.00000
[CW] train: qf1_loss: 67.17471, qf2_loss: 67.43222, policy_loss: -373.92770, policy_entropy: -1.00249, alpha: 0.56323, time: 68.00383
[CW] ---------------------------
[CW] ---- Iteration:   387 ----
[CW] collect: return: 835.40220, steps: 1000.00000, total_steps: 393000.00000
[CW] train: qf1_loss: 69.42621, qf2_loss: 69.17048, policy_loss: -376.49775, policy_entropy: -0.99622, alpha: 0.56299, time: 67.71588
[CW] ---------------------------
[CW] ---- Iteration:   388 ----
[CW] collect: return: 840.97083, steps: 1000.00000, total_steps: 394000.00000
[CW] train: qf1_loss: 75.45637, qf2_loss: 75.53225, policy_loss: -377.15449, policy_entropy: -1.00846, alpha: 0.56248, time: 69.78109
[CW] ---------------------------
[CW] ---- Iteration:   389 ----
[CW] collect: return: 578.31167, steps: 1000.00000, total_steps: 395000.00000
[CW] train: qf1_loss: 64.63593, qf2_loss: 64.68632, policy_loss: -377.34380, policy_entropy: -0.99910, alpha: 0.56520, time: 68.14365
[CW] ---------------------------
[CW] ---- Iteration:   390 ----
[CW] collect: return: 428.70469, steps: 1000.00000, total_steps: 396000.00000
[CW] train: qf1_loss: 73.98668, qf2_loss: 73.42971, policy_loss: -377.59254, policy_entropy: -1.01458, alpha: 0.56613, time: 67.78410
[CW] ---------------------------
[CW] ---- Iteration:   391 ----
[CW] collect: return: 736.14457, steps: 1000.00000, total_steps: 397000.00000
[CW] train: qf1_loss: 80.47108, qf2_loss: 80.22369, policy_loss: -378.66675, policy_entropy: -1.01753, alpha: 0.57025, time: 71.10953
[CW] ---------------------------
[CW] ---- Iteration:   392 ----
[CW] collect: return: 525.42197, steps: 1000.00000, total_steps: 398000.00000
[CW] train: qf1_loss: 100.75150, qf2_loss: 99.91520, policy_loss: -380.16668, policy_entropy: -0.99444, alpha: 0.57508, time: 68.10320
[CW] ---------------------------
[CW] ---- Iteration:   393 ----
[CW] collect: return: 760.55379, steps: 1000.00000, total_steps: 399000.00000
[CW] train: qf1_loss: 92.87353, qf2_loss: 93.19707, policy_loss: -382.36752, policy_entropy: -1.00502, alpha: 0.57392, time: 67.74688
[CW] ---------------------------
[CW] ---- Iteration:   394 ----
[CW] collect: return: 680.30075, steps: 1000.00000, total_steps: 400000.00000
[CW] train: qf1_loss: 72.88751, qf2_loss: 72.56244, policy_loss: -380.66274, policy_entropy: -0.98331, alpha: 0.57220, time: 67.70063
[CW] ---------------------------
[CW] ---- Iteration:   395 ----
[CW] collect: return: 837.09548, steps: 1000.00000, total_steps: 401000.00000
[CW] train: qf1_loss: 73.08195, qf2_loss: 72.43005, policy_loss: -384.97455, policy_entropy: -1.00637, alpha: 0.57069, time: 67.61394
[CW] ---------------------------
[CW] ---- Iteration:   396 ----
[CW] collect: return: 835.72234, steps: 1000.00000, total_steps: 402000.00000
[CW] train: qf1_loss: 68.88657, qf2_loss: 68.30938, policy_loss: -388.05355, policy_entropy: -0.99843, alpha: 0.57228, time: 67.61461
[CW] ---------------------------
[CW] ---- Iteration:   397 ----
[CW] collect: return: 839.89531, steps: 1000.00000, total_steps: 403000.00000
[CW] train: qf1_loss: 66.66258, qf2_loss: 66.73374, policy_loss: -384.67405, policy_entropy: -1.00390, alpha: 0.57212, time: 67.65995
[CW] ---------------------------
[CW] ---- Iteration:   398 ----
[CW] collect: return: 838.40187, steps: 1000.00000, total_steps: 404000.00000
[CW] train: qf1_loss: 74.96988, qf2_loss: 74.49519, policy_loss: -389.57309, policy_entropy: -0.98696, alpha: 0.57073, time: 67.72741
[CW] ---------------------------
[CW] ---- Iteration:   399 ----
[CW] collect: return: 831.88948, steps: 1000.00000, total_steps: 405000.00000
[CW] train: qf1_loss: 87.23982, qf2_loss: 86.62990, policy_loss: -388.61222, policy_entropy: -0.98578, alpha: 0.56629, time: 67.73513
[CW] ---------------------------
[CW] ---- Iteration:   400 ----
[CW] collect: return: 671.87360, steps: 1000.00000, total_steps: 406000.00000
[CW] train: qf1_loss: 129.45293, qf2_loss: 128.26222, policy_loss: -387.61824, policy_entropy: -1.01421, alpha: 0.56459, time: 67.87136
[CW] eval: return: 688.80643, steps: 1000.00000
[CW] ---------------------------
[CW] ---- Iteration:   401 ----
[CW] collect: return: 836.70379, steps: 1000.00000, total_steps: 407000.00000
[CW] train: qf1_loss: 119.21989, qf2_loss: 119.88253, policy_loss: -387.84456, policy_entropy: -0.99881, alpha: 0.56762, time: 68.00596
[CW] ---------------------------
[CW] ---- Iteration:   402 ----
[CW] collect: return: 831.82431, steps: 1000.00000, total_steps: 408000.00000
[CW] train: qf1_loss: 75.41754, qf2_loss: 75.44863, policy_loss: -388.91417, policy_entropy: -1.00182, alpha: 0.56777, time: 67.88677
[CW] ---------------------------
[CW] ---- Iteration:   403 ----
[CW] collect: return: 595.69545, steps: 1000.00000, total_steps: 409000.00000
[CW] train: qf1_loss: 79.09341, qf2_loss: 78.39248, policy_loss: -390.92416, policy_entropy: -1.01067, alpha: 0.56980, time: 67.86163
[CW] ---------------------------
[CW] ---- Iteration:   404 ----
[CW] collect: return: 839.57243, steps: 1000.00000, total_steps: 410000.00000
[CW] train: qf1_loss: 69.59382, qf2_loss: 69.74018, policy_loss: -392.36481, policy_entropy: -0.99394, alpha: 0.57003, time: 67.69785
[CW] ---------------------------
[CW] ---- Iteration:   405 ----
[CW] collect: return: 842.47665, steps: 1000.00000, total_steps: 411000.00000
[CW] train: qf1_loss: 76.61002, qf2_loss: 75.96791, policy_loss: -393.61405, policy_entropy: -0.99201, alpha: 0.56920, time: 67.67765
[CW] ---------------------------
[CW] ---- Iteration:   406 ----
[CW] collect: return: 577.43724, steps: 1000.00000, total_steps: 412000.00000
[CW] train: qf1_loss: 81.35743, qf2_loss: 82.29015, policy_loss: -396.72990, policy_entropy: -1.01641, alpha: 0.57037, time: 67.74029
[CW] ---------------------------
[CW] ---- Iteration:   407 ----
[CW] collect: return: 653.94950, steps: 1000.00000, total_steps: 413000.00000
[CW] train: qf1_loss: 93.24739, qf2_loss: 93.19412, policy_loss: -394.17472, policy_entropy: -0.99887, alpha: 0.57421, time: 67.86224
[CW] ---------------------------
[CW] ---- Iteration:   408 ----
[CW] collect: return: 834.56316, steps: 1000.00000, total_steps: 414000.00000
[CW] train: qf1_loss: 105.17420, qf2_loss: 105.27636, policy_loss: -394.01896, policy_entropy: -1.01164, alpha: 0.57354, time: 67.77398
[CW] ---------------------------
[CW] ---- Iteration:   409 ----
[CW] collect: return: 495.31893, steps: 1000.00000, total_steps: 415000.00000
[CW] train: qf1_loss: 93.03381, qf2_loss: 93.55316, policy_loss: -396.11782, policy_entropy: -0.98258, alpha: 0.57356, time: 67.99646
[CW] ---------------------------
[CW] ---- Iteration:   410 ----
[CW] collect: return: 837.55675, steps: 1000.00000, total_steps: 416000.00000
[CW] train: qf1_loss: 75.38062, qf2_loss: 74.59581, policy_loss: -397.83007, policy_entropy: -0.98866, alpha: 0.56847, time: 67.90452
[CW] ---------------------------
