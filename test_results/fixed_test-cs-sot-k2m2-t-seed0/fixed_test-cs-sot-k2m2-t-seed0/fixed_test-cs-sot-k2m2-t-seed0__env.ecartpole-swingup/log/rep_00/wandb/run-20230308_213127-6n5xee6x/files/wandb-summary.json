{"collect/return": 685.9707911771839, "collect/steps": 1000.0, "collect/total_steps": 415000.0, "train/qf1_loss": 68.5843898010254, "train/qf2_loss": 68.56546463012695, "train/policy_loss": -423.7779598999023, "train/policy_entropy": -1.0071937906742097, "train/alpha": 0.6217813432216645, "train/time": 67.79033589363098, "eval/return": 759.1841164996061, "eval/steps": 1000.0, "_timestamp": 1678336120.4122896, "_runtime": 28633.08358860016, "_step": 409}